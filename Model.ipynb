{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3a13e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "True\n",
      "GPU name:  NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as tf\n",
    "import glob\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "import random\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(88)\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print('GPU name: ',torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d63bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Unet\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Downsample, self).__init__()\n",
    "        self.conv_relu = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                                       nn.BatchNorm2d(out_channels),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                                       nn.BatchNorm2d(out_channels),\n",
    "                                       nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2)\n",
    "    \n",
    "    def forward(self, x, is_pool=True):\n",
    "\n",
    "        if is_pool: \n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.conv_relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.conv_relu = nn.Sequential(nn.Conv2d(2*channels, channels, kernel_size=3, padding=1),\n",
    "                                       nn.BatchNorm2d(channels),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       \n",
    "                                       nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "                                       nn.BatchNorm2d(channels),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.upconv = nn.Sequential(nn.ConvTranspose2d(channels, channels//2, kernel_size=3, stride=2,padding=1,output_padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_relu(x)\n",
    "        x = self.upconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Unet_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet_model, self).__init__()\n",
    "        self.down1 = Downsample(1,64) # \n",
    "        self.down2 = Downsample(64,128)\n",
    "        self.down3 = Downsample(128,256)\n",
    "        self.down4 = Downsample(256,512)\n",
    "        self.down5 = Downsample(512,1024)\n",
    "\n",
    "        self.up = nn.Sequential(nn.ConvTranspose2d(1024,512,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "                                #nn.Dropout(p=0.5),\n",
    "                                nn.BatchNorm2d(512),\n",
    "                                nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up1 = Upsample(512)\n",
    "        self.up2 = Upsample(256)\n",
    "        self.up3 = Upsample(128)\n",
    "\n",
    "        self.conv_2 = Downsample(128,64)\n",
    "\n",
    "        self.last = nn.Sequential(nn.Conv2d(64,26,kernel_size=1),\n",
    "                                  #nn.Dropout(p=0.5)\n",
    "                                  ) # \n",
    "    \n",
    "    def forward(self, input):\n",
    "        x1 = self.down1(input, is_pool=False)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        x5 = self.down5(x4)\n",
    "\n",
    "        x5 = self.up(x5)\n",
    "\n",
    "        x5 = torch.cat([x4,x5], dim=1) \n",
    "        x5 = self.up1(x5) \n",
    "\n",
    "        x5 = torch.cat([x3,x5], dim=1)\n",
    "        x5 = self.up2(x5) \n",
    "\n",
    "        x5 = torch.cat([x2,x5], dim=1)\n",
    "        x5 = self.up3(x5) \n",
    "\n",
    "        x5 = torch.cat([x1,x5], dim=1)\n",
    "\n",
    "        x5 = self.conv_2(x5, is_pool=False)\n",
    "\n",
    "        x5 = self.last(x5)\n",
    "\n",
    "        return x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae78ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AFFU\n",
    "# CBAM attention module\n",
    "class ChannelAttentionModule(nn.Module):\n",
    "    def __init__(self, channel, ratio=16):\n",
    "        super(ChannelAttentionModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.shared_MLP = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channel // ratio, channel, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avgout = self.shared_MLP(self.avg_pool(x))\n",
    "        maxout = self.shared_MLP(self.max_pool(x))\n",
    "        return self.sigmoid(avgout + maxout)\n",
    "\n",
    "class SpatialAttentionModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttentionModule, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avgout = torch.mean(x, dim=1, keepdim=True)\n",
    "        maxout, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avgout, maxout], dim=1)\n",
    "        out = self.sigmoid(self.conv2d(out))\n",
    "        return out\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttentionModule(channel)\n",
    "        self.spatial_attention = SpatialAttentionModule()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.channel_attention(x) * x\n",
    "        out = self.spatial_attention(out) * out\n",
    "        return out\n",
    "\n",
    "# attention Unet 5layer\n",
    "# 注意力模块\n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(Attention_block, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x * psi\n",
    "# 金字塔池化层ASPP\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, rate=[6, 12, 18]):\n",
    "        super(ASPP, self).__init__()\n",
    "\n",
    "        self.aspp_block1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                ch_in, ch_out, 3, stride=1, padding=rate[0], dilation=rate[0]\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "        )\n",
    "        self.aspp_block2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                ch_in, ch_out, 3, stride=1, padding=rate[1], dilation=rate[1]\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "        )\n",
    "        self.aspp_block3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                ch_in, ch_out, 3, stride=1, padding=rate[2], dilation=rate[2]\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "        )\n",
    "\n",
    "        self.output = nn.Conv2d(len(rate) * ch_out, ch_out, 1)\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.aspp_block1(x)\n",
    "        x2 = self.aspp_block2(x)\n",
    "        x3 = self.aspp_block3(x)\n",
    "        out = torch.cat([x1, x2, x3], dim=1)\n",
    "        return self.output(out)\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "# 残差模块\n",
    "class Residual_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Residual_block, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "        self.conv_skip = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.conv_block(x)+self.conv_skip(x)\n",
    "\n",
    "# 编码连续卷积层\n",
    "def contracting_block(in_channels, out_channels):\n",
    "    block = torch.nn.Sequential(\n",
    "        nn.Conv2d(kernel_size=(3, 3), in_channels=in_channels, out_channels=out_channels, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Conv2d(kernel_size=(3, 3), in_channels=out_channels, out_channels=out_channels, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(out_channels)\n",
    "    )\n",
    "    return block\n",
    "\n",
    "\n",
    "# 上采样过程中也反复使用了两层卷积的结构\n",
    "double_conv = contracting_block\n",
    "\n",
    "# 上采样反卷积模块\n",
    "class expansive_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(expansive_block, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(kernel_size=(3, 3), in_channels=in_channels, out_channels=out_channels, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "def final_block(in_channels, out_channels):\n",
    "    return nn.Conv2d(kernel_size=1, in_channels=in_channels, out_channels=out_channels, stride=1, padding=0)\n",
    "\n",
    "\n",
    "#——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "#——————————————————————————————————————————————5层 Attention_Unet——————————————————————————————————————————————\n",
    "class AttUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(AttUNet, self).__init__()\n",
    "        # Encode\n",
    "        self.conv_encode1 = Residual_block(in_channels=in_channel, out_channels=64)\n",
    "        self.conv_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv_encode2 = Residual_block(in_channels=64, out_channels=128)\n",
    "        self.conv_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv_encode3 = Residual_block(in_channels=128, out_channels=256)\n",
    "        self.conv_pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv_encode4 = Residual_block(in_channels=256, out_channels=512)\n",
    "        self.conv_pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv_encode5 = Residual_block(in_channels=512, out_channels=1024)\n",
    "\n",
    "        # Decode\n",
    "        self.conv_decode4 = expansive_block(1024, 512)\n",
    "        self.att4 = Attention_block(F_g=512, F_l=512, F_int=256)\n",
    "        self.double_conv4 = double_conv(1024, 512)\n",
    "\n",
    "        self.conv_decode3 = expansive_block(512, 256)\n",
    "        self.att3 = Attention_block(F_g=256, F_l=256, F_int=128)\n",
    "        self.double_conv3 = double_conv(512, 256)\n",
    "\n",
    "        self.conv_decode2 = expansive_block(256, 128)\n",
    "        self.att2 = Attention_block(F_g=128, F_l=128, F_int=64)\n",
    "        self.double_conv2 = double_conv(256, 128)\n",
    "\n",
    "        self.conv_decode1 = expansive_block(128, 64)\n",
    "        self.att1 = Attention_block(F_g=64, F_l=64, F_int=32)\n",
    "        self.double_conv1 = double_conv(128, 64)\n",
    "\n",
    "        self.final_layer = final_block(64+32+128, out_channel)\n",
    "        \n",
    "        #----------------------------------------\n",
    "        self.UpFromInput = nn.Sequential(nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "                                        nn.BatchNorm2d(32),\n",
    "                                        nn.ReLU(inplace=True))\n",
    "        self.CBAM_upside = CBAM(channel=32)\n",
    "        \n",
    "\n",
    "        self.CBAM_layer4 = CBAM(channel=512)\n",
    "        self.CBAM_layer3 = CBAM(channel=256)\n",
    "        self.CBAM_layer2 = CBAM(channel=128)\n",
    "        self.aspp_bridge = ASPP(ch_in=(512+256+128), ch_out=128)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        up_bridge = self.UpFromInput(x)\n",
    "        up_bridge = self.CBAM_upside(up_bridge)\n",
    "        \n",
    "        # Encode\n",
    "        encode_block1 = self.conv_encode1(x)\n",
    "        encode_pool1 = self.conv_pool1(encode_block1)\n",
    "        encode_block2 = self.conv_encode2(encode_pool1)\n",
    "        encode_pool2 = self.conv_pool2(encode_block2)\n",
    "        encode_block3 = self.conv_encode3(encode_pool2)\n",
    "        encode_pool3 = self.conv_pool3(encode_block3)\n",
    "        encode_block4 = self.conv_encode4(encode_pool3)\n",
    "        encode_pool4 = self.conv_pool4(encode_block4)\n",
    "        encode_block5 = self.conv_encode5(encode_pool4)\n",
    "\n",
    "        # Decode\n",
    "        decode_block4 = self.conv_decode4(encode_block5)\n",
    "        encode_block4 = self.att4(g=decode_block4, x=encode_block4)\n",
    "        decode_block4 = torch.cat((encode_block4, decode_block4), dim=1)\n",
    "        decode_block4 = self.double_conv4(decode_block4)\n",
    "\n",
    "        decode_block3 = self.conv_decode3(decode_block4)\n",
    "        encode_block3 = self.att3(g=decode_block3, x=encode_block3)\n",
    "        decode_block3 = torch.cat((encode_block3, decode_block3), dim=1)\n",
    "        decode_block3 = self.double_conv3(decode_block3)\n",
    "\n",
    "        decode_block2 = self.conv_decode2(decode_block3)\n",
    "        encode_block2 = self.att2(g=decode_block2, x=encode_block2)\n",
    "        decode_block2 = torch.cat((encode_block2, decode_block2), dim=1)\n",
    "        decode_block2 = self.double_conv2(decode_block2)\n",
    "\n",
    "        decode_block1 = self.conv_decode1(decode_block2)\n",
    "        encode_block1 = self.att1(g=decode_block1, x=encode_block1)\n",
    "        decode_block1 = torch.cat((encode_block1, decode_block1), dim=1)\n",
    "        decode_block1 = self.double_conv1(decode_block1)\n",
    "        \n",
    "        #------------------------------\n",
    "        CBAM4 = self.CBAM_layer4(encode_block4)\n",
    "        CBAM3 = self.CBAM_layer3(encode_block3)\n",
    "        CBAM2 = self.CBAM_layer2(encode_block2)\n",
    "        \n",
    "        CBAM4 = F.interpolate(CBAM4, scale_factor=8, mode='bilinear', align_corners=True) # CHANNEL 512\n",
    "        CBAM3 = F.interpolate(CBAM3, scale_factor=4, mode='bilinear', align_corners=True) # 256\n",
    "        CBAM2 = F.interpolate(CBAM2, scale_factor=2, mode='bilinear', align_corners=True) # 128\n",
    "        \n",
    "        PRE_OUT = torch.cat((CBAM2,CBAM3,CBAM4),dim=1) # 896 CHANNEL\n",
    "        OUT = self.aspp_bridge(PRE_OUT) # CHANNEL 128\n",
    "        \n",
    "        Upside_out = torch.cat((decode_block1,up_bridge),dim=1) # CHANNEL 96\n",
    "        \n",
    "        \n",
    "\n",
    "        final_layer = self.final_layer(torch.cat((OUT,Upside_out),dim=1))\n",
    "\n",
    "        return final_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b550748",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNet++\n",
    "class VGGBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, middle_channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(middle_channels)\n",
    "        self.conv2 = nn.Conv2d(middle_channels, out_channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Up(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NestedUNet(nn.Module):\n",
    "    def __init__(self, num_classes=26, input_channels=1, deep_supervision=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        nb_filter = [32, 64, 128, 256, 512]\n",
    "\n",
    "        self.deep_supervision = deep_supervision\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up = Up()\n",
    "\n",
    "        self.conv0_0 = VGGBlock(input_channels, nb_filter[0], nb_filter[0])\n",
    "        self.conv1_0 = VGGBlock(nb_filter[0], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_0 = VGGBlock(nb_filter[1], nb_filter[2], nb_filter[2])\n",
    "        self.conv3_0 = VGGBlock(nb_filter[2], nb_filter[3], nb_filter[3])\n",
    "        self.conv4_0 = VGGBlock(nb_filter[3], nb_filter[4], nb_filter[4])\n",
    "\n",
    "        self.conv0_1 = VGGBlock(nb_filter[0]+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_1 = VGGBlock(nb_filter[1]+nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_1 = VGGBlock(nb_filter[2]+nb_filter[3], nb_filter[2], nb_filter[2])\n",
    "        self.conv3_1 = VGGBlock(nb_filter[3]+nb_filter[4], nb_filter[3], nb_filter[3])\n",
    "\n",
    "        self.conv0_2 = VGGBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_2 = VGGBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_2 = VGGBlock(nb_filter[2]*2+nb_filter[3], nb_filter[2], nb_filter[2])\n",
    "\n",
    "        self.conv0_3 = VGGBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_3 = VGGBlock(nb_filter[1]*3+nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "\n",
    "        self.conv0_4 = VGGBlock(nb_filter[0]*4+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "\n",
    "        if self.deep_supervision:\n",
    "            self.final1 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "            self.final2 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "            self.final3 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "            self.final4 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "        else:\n",
    "            self.final = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x0_0 = self.conv0_0(input)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "        x0_1 = self.conv0_1(self.up(x1_0, x0_0))\n",
    "\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x1_1 = self.conv1_1(self.up(x2_0, x1_0))\n",
    "        x0_2 = self.conv0_2(self.up(x1_1, torch.cat([x0_0, x0_1], 1)))\n",
    "\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x2_1 = self.conv2_1(self.up(x3_0, x2_0))   \n",
    "        x1_2 = self.conv1_2(self.up(x2_1, torch.cat([x1_0, x1_1], 1)))\n",
    "        x0_3 = self.conv0_3(self.up(x1_2, torch.cat([x0_0, x0_1, x0_2], 1)))\n",
    "\n",
    "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
    "        x3_1 = self.conv3_1(self.up(x4_0, x3_0))\n",
    "        x2_2 = self.conv2_2(self.up(x3_1, torch.cat([x2_0, x2_1], 1)))\n",
    "        x1_3 = self.conv1_3(self.up(x2_2, torch.cat([x1_0, x1_1, x1_2], 1)))\n",
    "        x0_4 = self.conv0_4(self.up(x1_3, torch.cat([x0_0, x0_1, x0_2, x0_3], 1)))\n",
    "\n",
    "        if self.deep_supervision:\n",
    "            output1 = self.final1(x0_1)\n",
    "            output2 = self.final2(x0_2)\n",
    "            output3 = self.final3(x0_3)\n",
    "            output4 = self.final4(x0_4)\n",
    "            return [output1, output2, output3, output4]\n",
    "\n",
    "        else:\n",
    "            output = self.final(x0_4)\n",
    "            return output\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #tensorboard --logdir logs_model\n",
    "    net = NestedUNet()\n",
    "\n",
    "    input = torch.ones(( 16,1, 256, 128))\n",
    "    y = net(input)\n",
    "    print(y.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention UNet\n",
    "# Class built to make implementation of the double convolutions easier\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self,F_g,F_l,F_int):\n",
    "        super(Attention_block,self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "            )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1,stride=2,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,g,x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        if x1.shape != g1.shape:\n",
    "            x1 = tf.resize(x1, size=g1.shape[2:])\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "        if psi.shape != x.shape:\n",
    "            psi = tf.resize(psi, size=x.shape[2:])\n",
    "        return x*psi\n",
    "\n",
    "class ATT_UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=1, out_channels=26, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super(ATT_UNET, self).__init__()\n",
    "\n",
    "        # Defining the layers of the network\n",
    "        # Convolutions on downward half of UNet:\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.down_sample = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.deepest_conv = DoubleConv(in_channels=features[3],out_channels=features[3]*2)\n",
    "        self.up_convs = nn.ModuleList()\n",
    "        self.up_samples = nn.ModuleList()\n",
    "        self.att_block = nn.ModuleList()\n",
    "        self.final_conv = nn.Conv2d(in_channels=features[0], out_channels=out_channels, kernel_size=1)\n",
    "\n",
    "        # Downward part of UNet\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "        \n",
    "        # Attention blocks\n",
    "        for feature in reversed(features):\n",
    "            self.att_block.append(\n",
    "                Attention_block(F_g= feature*2,F_l=feature,F_int=feature)\n",
    "                )\n",
    "            self.up_convs.append(\n",
    "                DoubleConv(feature*2, feature)\n",
    "                )\n",
    "            self.up_samples.append(\n",
    "                nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2)\n",
    "                )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # initialise the skip connections variable - this will feed into the attention blocks\n",
    "        skip_connections = []\n",
    "\n",
    "        # downward path, identical to UNet\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.down_sample(x)\n",
    "\n",
    "        # Deepest convolution layer\n",
    "        x = self.deepest_conv(x)\n",
    "\n",
    "        # Readying skip connections tensor\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        \n",
    "        ## upward path with attention gates\n",
    "\n",
    "        for idx in range(len(self.up_convs)):\n",
    "            a = self.att_block[idx](g=x,x=skip_connections[idx])\n",
    "            x = self.up_samples[idx](x)\n",
    "            if x.shape != a.shape:                       # resizing to allow the concatenation, done at all stages\n",
    "                x = tf.resize(x, size=a.shape[2:])\n",
    "            x = torch.cat((a,x),dim=1)\n",
    "            x = self.up_convs[idx](x)\n",
    "\n",
    "        x_out = self.final_conv(x)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "## Simple test function desinged to test that the UNet is taking in and outputting tensors of the correct size\n",
    "def test():\n",
    "    # x is a random tensor representing an input to UNet [batch=1, channels=1, height=321, width=321]\n",
    "    x = torch.randn((1, 1, 128, 128))\n",
    "    model = ATT_UNET(in_channels=1, out_channels=26)\n",
    "    preds = model(x)\n",
    "    preds_2 = torch.unsqueeze(torch.argmax(preds,dim=1),dim=1)\n",
    "    print(x.shape)\n",
    "    print(preds.shape)\n",
    "    assert preds_2.shape == x.shape\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
