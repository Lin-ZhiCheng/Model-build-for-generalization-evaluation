{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3a13e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "True\n",
      "GPU name:  NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as tf\n",
    "import glob\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "import random\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(88)\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print('GPU name: ',torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d20d85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD02_R:  11\n",
      "TD02_R_anno:  11\n"
     ]
    }
   ],
   "source": [
    "# load the other cohort's img and MRI\n",
    "# right part for children\n",
    "# MRI \n",
    "TD02_R = glob.glob('./data/png/*.png');\n",
    "TD02_R = sorted(TD02_R);print('TD02_R: ',len(TD02_R))\n",
    "\n",
    "\n",
    "# label \n",
    "TD02_R_anno = glob.glob('./data/label/*.png');\n",
    "TD02_R_anno = sorted(TD02_R_anno);print('TD02_R_anno: ',len(TD02_R_anno))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ce8f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model load\n",
    "# load the trained wight\n",
    "\n",
    "#——————————————————————————————————————————————————————————————————————\n",
    "# Unet\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Downsample, self).__init__()\n",
    "        self.conv_relu = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                                       nn.BatchNorm2d(out_channels),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                                       nn.BatchNorm2d(out_channels),\n",
    "                                       nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2)\n",
    "    \n",
    "    def forward(self, x, is_pool=True):\n",
    "\n",
    "        if is_pool: # 先看一下是否需要下采样pool层\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.conv_relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 上采样模块\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.conv_relu = nn.Sequential(nn.Conv2d(2*channels, channels, kernel_size=3, padding=1),\n",
    "                                       nn.BatchNorm2d(channels),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       \n",
    "                                       nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "                                       nn.BatchNorm2d(channels),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.upconv = nn.Sequential(nn.ConvTranspose2d(channels, channels//2, kernel_size=3, stride=2,padding=1,output_padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_relu(x)\n",
    "        x = self.upconv(x)\n",
    "        return x\n",
    "\n",
    "# 初始化模型\n",
    "class Unet_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet_model, self).__init__()\n",
    "        self.down1 = Downsample(1,64) # 输入图片是3通道的\n",
    "        self.down2 = Downsample(64,128)\n",
    "        self.down3 = Downsample(128,256)\n",
    "        self.down4 = Downsample(256,512)\n",
    "        self.down5 = Downsample(512,1024)\n",
    "\n",
    "        self.up = nn.Sequential(nn.ConvTranspose2d(1024,512,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "                                #nn.Dropout(p=0.5),\n",
    "                                nn.BatchNorm2d(512),\n",
    "                                nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up1 = Upsample(512)\n",
    "        self.up2 = Upsample(256)\n",
    "        self.up3 = Upsample(128)\n",
    "\n",
    "        self.conv_2 = Downsample(128,64)\n",
    "\n",
    "        self.last = nn.Sequential(nn.Conv2d(64,26,kernel_size=1),\n",
    "                                  #nn.Dropout(p=0.5)\n",
    "                                  ) # 因为是分25类，所以输出是26  25+1 \n",
    "    \n",
    "    def forward(self, input):\n",
    "        x1 = self.down1(input, is_pool=False)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        x5 = self.down5(x4)\n",
    "\n",
    "        x5 = self.up(x5)\n",
    "\n",
    "        x5 = torch.cat([x4,x5], dim=1) #沿着channel维度进行合并\n",
    "        x5 = self.up1(x5) \n",
    "\n",
    "        x5 = torch.cat([x3,x5], dim=1)\n",
    "        x5 = self.up2(x5) \n",
    "\n",
    "        x5 = torch.cat([x2,x5], dim=1)\n",
    "        x5 = self.up3(x5) \n",
    "\n",
    "        x5 = torch.cat([x1,x5], dim=1)\n",
    "\n",
    "        x5 = self.conv_2(x5, is_pool=False)\n",
    "\n",
    "        x5 = self.last(x5)\n",
    "\n",
    "        return x5\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "621b849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the dataset\n",
    "# train dataset\n",
    "\n",
    "train_images = ( TD02_R\n",
    "             \n",
    "                \n",
    "               )\n",
    "train_images = sorted(train_images)\n",
    "                      \n",
    "train_images = train_images[0:len(train_images):2]\n",
    "\n",
    "\n",
    "# train_ground\n",
    "train_annos = (  TD02_R_anno\n",
    "          \n",
    "               )\n",
    "      \n",
    "train_annos = sorted(train_annos) \n",
    "\n",
    "train_annos = train_annos[0:len(train_annos):2]\n",
    "#\n",
    "\n",
    "test_images = TD02_R\n",
    "test_images = sorted(test_images) \n",
    "test_images = test_images[0:len(test_images):1]\n",
    "\n",
    "test_annos = TD02_R_anno\n",
    "test_annos = sorted(test_annos) \n",
    "test_annos = test_annos[0:len(test_annos):1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8bf58ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all number: [3 0 4 1 5 2]\n"
     ]
    }
   ],
   "source": [
    "# shuffle\n",
    "np.random.seed(188)\n",
    "index = np.random.permutation(len(train_images))\n",
    "print('all number:',index)\n",
    "\n",
    "train_images=np.array(train_images)[index]\n",
    "train_annos=np.array(train_annos)[index]\n",
    "\n",
    "\n",
    "\n",
    "# 分trian集和vali集\n",
    "vali_images=train_images\n",
    "vali_annos=train_annos\n",
    "\n",
    "train_images = train_images[0:int(0.9*len(index))]\n",
    "train_annos = train_annos[0:int(0.9*len(index))]\n",
    "\n",
    "vali_images = vali_images[int(0.9*len(index)):]\n",
    "vali_annos = vali_annos[int(0.9*len(index)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a906116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————————————————————————————————————————————————————————————————————————\n",
    "# define transform\n",
    "# randomly crop the image size to 128*128 to save the cost\n",
    "transform = transforms.Compose([\n",
    "                                \n",
    "                                transforms.ToTensor(), \n",
    "])\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, imgs_path, annos_path,cropp=False):\n",
    "        self.imgs_path = imgs_path\n",
    "        self.annos_path = annos_path\n",
    "        self.cropp = cropp\n",
    "    def crop(self,img,label,size):\n",
    "        t = transforms.RandomCrop.get_params(img=img,output_size=(size,size))\n",
    "        img = tf.crop(img, *t)\n",
    "        label = tf.crop(label, *t)\n",
    "        return img,label\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs_path[index]\n",
    "        anno = self.annos_path[index]\n",
    "\n",
    "        pil_img = Image.open(img)#.convert('RGB')\n",
    "        anno_img = Image.open(anno).convert('L')\n",
    "        \n",
    "        if self.cropp:\n",
    "            pil_img,anno_img = self.crop(pil_img,anno_img,128)\n",
    "        \n",
    "        img_tensor = transform(pil_img)\n",
    "        img_tensor = img_tensor.to(torch.float)\n",
    "        anno_tensor = transform(anno_img)\n",
    "        anno_tensor = anno_tensor*255. \n",
    "        anno_tensor = torch.squeeze(anno_tensor).type(torch.long) \n",
    "\n",
    "        return img_tensor, anno_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cfd63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合成数据\n",
    "train_dataset = Dataset(train_images,train_annos,cropp=True)\n",
    "\n",
    "vali_dataset = Dataset(vali_images,vali_annos,cropp=True)\n",
    "\n",
    "test_dataset = Dataset(test_images,test_annos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67392903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "the shape of img: torch.Size([2, 1, 128, 128])\n",
      "the shape of anno: torch.Size([2, 128, 128])\n",
      "(128, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2afb72c4730>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 198,
       "width": 375
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('ok')\n",
    "\n",
    "batchsize = 2\n",
    "# test1 is test data\n",
    "test_dataloader = data.DataLoader(test_dataset,\n",
    "                                   batch_size = 1,\n",
    "                                   shuffle=True,\n",
    "                                  num_workers=0) # 因为是预测，所以测试集的batchsize就设置成1\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset,\n",
    "                                   batch_size = batchsize,\n",
    "                                   shuffle=True,\n",
    "                                   num_workers=0)\n",
    "\n",
    "#validation data\n",
    "vali_dataloader = data.DataLoader(vali_dataset,\n",
    "                                   batch_size = batchsize,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=0)\n",
    "\n",
    "#————————————————————————————————————————————————————————————————————————————————————————\n",
    "img_batch, anno_batch = next(iter(train_dataloader))\n",
    "print('the shape of img:',img_batch.shape) # [b,3,256,256]\n",
    "print('the shape of anno:',anno_batch.shape) # [b,256,256]\n",
    "\n",
    "#————————————————————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "\n",
    "# img = img_batch[0].permute(1,2,0).numpy() \n",
    "img = torch.squeeze(img_batch[0].permute(1,2,0))\n",
    "img = img.numpy()\n",
    "anno = anno_batch[0].numpy()\n",
    "print(img.shape)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('MRI')\n",
    "plt.imshow(img)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('mask')\n",
    "plt.imshow(anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d63bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "#    Unet\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Downsample, self).__init__()\n",
    "        self.conv_relu = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                                       nn.BatchNorm2d(out_channels),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                                       nn.BatchNorm2d(out_channels),\n",
    "                                       nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2)\n",
    "    \n",
    "    def forward(self, x, is_pool=True):\n",
    "\n",
    "        if is_pool: \n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.conv_relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.conv_relu = nn.Sequential(nn.Conv2d(2*channels, channels, kernel_size=3, padding=1),\n",
    "                                       nn.BatchNorm2d(channels),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       \n",
    "                                       nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "                                       nn.BatchNorm2d(channels),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.upconv = nn.Sequential(nn.ConvTranspose2d(channels, channels//2, kernel_size=3, stride=2,padding=1,output_padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_relu(x)\n",
    "        x = self.upconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Unet_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet_model, self).__init__()\n",
    "        self.down1 = Downsample(1,64) # \n",
    "        self.down2 = Downsample(64,128)\n",
    "        self.down3 = Downsample(128,256)\n",
    "        self.down4 = Downsample(256,512)\n",
    "        self.down5 = Downsample(512,1024)\n",
    "\n",
    "        self.up = nn.Sequential(nn.ConvTranspose2d(1024,512,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "                                #nn.Dropout(p=0.5),\n",
    "                                nn.BatchNorm2d(512),\n",
    "                                nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up1 = Upsample(512)\n",
    "        self.up2 = Upsample(256)\n",
    "        self.up3 = Upsample(128)\n",
    "\n",
    "        self.conv_2 = Downsample(128,64)\n",
    "\n",
    "        self.last = nn.Sequential(nn.Conv2d(64,26,kernel_size=1),\n",
    "                                  #nn.Dropout(p=0.5)\n",
    "                                  ) # \n",
    "    \n",
    "    def forward(self, input):\n",
    "        x1 = self.down1(input, is_pool=False)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        x5 = self.down5(x4)\n",
    "\n",
    "        x5 = self.up(x5)\n",
    "\n",
    "        x5 = torch.cat([x4,x5], dim=1) \n",
    "        x5 = self.up1(x5) \n",
    "\n",
    "        x5 = torch.cat([x3,x5], dim=1)\n",
    "        x5 = self.up2(x5) \n",
    "\n",
    "        x5 = torch.cat([x2,x5], dim=1)\n",
    "        x5 = self.up3(x5) \n",
    "\n",
    "        x5 = torch.cat([x1,x5], dim=1)\n",
    "\n",
    "        x5 = self.conv_2(x5, is_pool=False)\n",
    "\n",
    "        x5 = self.last(x5)\n",
    "\n",
    "        return x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e2384b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when epoch = 1, run this code to set up a new file to record loss in each step\n",
    "# if epoch !=1, don't run this code\n",
    "\n",
    "train_loss_each_step = []\n",
    "np.save('train_loss_each_step.npy',train_loss_each_step)\n",
    "train_acc_each_step = []\n",
    "np.save('train_acc_each_step.npy',train_acc_each_step)\n",
    "vali_dice_each_epoch = []\n",
    "np.save('vali_dice_each_epoch.npy',vali_dice_each_epoch)\n",
    "\n",
    "vali_loss_each_epoch = []\n",
    "np.save('vali_loss_each_epoch.npy',vali_loss_each_epoch)\n",
    "vali_acc_each_epoch = []\n",
    "np.save('vali_acc_each_epoch.npy',vali_acc_each_epoch)\n",
    "\n",
    "test_dice_each_epoch = []\n",
    "np.save('test_dice_each_epoch.npy',test_dice_each_epoch)\n",
    "\n",
    "\n",
    "os.makedirs('./Save_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9eea10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  Unet_model()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight = torch.from_numpy(np.array([0.1, 1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0 ])).float()).to(device)\n",
    "# crossentropy is used for multi segmentaion task\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001*(0.9**0),weight_decay=0.001)\n",
    "from torch.optim import lr_scheduler\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9) # 学习速率进行衰减\n",
    "\n",
    "def fit(epoch,model,trian_dataloader,test_dataloder,train_loss_each_step,train_acc_each_step,vali_dice_each_epoch,vali_loss_each_epoch,vali_acc_each_epoch,test_dice_each_epoch):\n",
    "    \n",
    "    #——————————————————————————————————————————————————————————————————————\n",
    "    #————————————————train—————————————————————————————————————————————————\n",
    "    model.train()\n",
    "    step = 0; #define the step in each epoch\n",
    "    for x, y in train_dataloader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "\n",
    "        step = step+1\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y) \n",
    "        # y_pred [batch,class_num,depth,height,width]\n",
    "        # y      [batch,depth,height,width]\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # acc caculate\n",
    "        y_pred = torch.argmax(y_pred,dim=1)\n",
    "        correct = 0\n",
    "        correct += (y == y_pred).sum().item()\n",
    "        N = y.size(0)\n",
    "        acc = correct/(N*128*128)\n",
    "\n",
    "        \n",
    "        #print for every step\n",
    "        print('epoch:',epoch,'step:',step,'loss:',loss.detach().cpu().numpy(),'acc:',acc)\n",
    "        train_loss_each_step = np.append(train_loss_each_step, loss.detach().cpu().numpy())\n",
    "        train_acc_each_step = np.append(train_acc_each_step, acc) \n",
    "\n",
    "\n",
    "        ##################################################\n",
    "       # wandb.log({\"train_acc_each_step\": acc})\n",
    "       # wandb.log({\"train_loss_each_step\": loss.detach().cpu().numpy()})\n",
    "\n",
    "    exp_lr_scheduler.step()\n",
    "    \n",
    "    \n",
    "    #——————————————————————————————————————————————————————————————————————\n",
    "    #————————————————vali——————————————————————————————————————————————————\n",
    "    model.eval()\n",
    "    vali_running_loss = 0 #record the sum of loss\n",
    "    dice_total = 0\n",
    "    acc_vali_total = 0\n",
    "    acc_num = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dice_1 = [] # \n",
    "        dice_2 = [] # \n",
    "        dice_3 = [] # \n",
    "        dice_4 = [] # \n",
    "        dice_5 = [] # \n",
    "        dice_6 = [] # \n",
    "        dice_7 = [] # \n",
    "        dice_8 = [] # \n",
    "        dice_9 = []\n",
    "        dice_10 = []\n",
    "        dice_11 = []\n",
    "        dice_12 = [] # \n",
    "        dice_13 = [] # \n",
    "        dice_14 = [] # \n",
    "        dice_15 = [] # \n",
    "        dice_16 = [] # \n",
    "        dice_17 = [] # \n",
    "        dice_18 = [] # \n",
    "        dice_19 = []\n",
    "        dice_20 = []\n",
    "        dice_21 = []\n",
    "        dice_22 = []\n",
    "        dice_23 = []\n",
    "        dice_24 = []\n",
    "        dice_25 = []\n",
    "        for x, y in vali_dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            # acc caculate\n",
    "            y_pred_here = y_pred\n",
    "            y_pred_here = torch.argmax(y_pred_here,dim=1)\n",
    "            correct = 0\n",
    "            correct += (y ==y_pred_here).sum().item()\n",
    "            N = y.size(0)\n",
    "            acc = correct/(N*128*128)\n",
    "            acc_vali_total = acc_vali_total + acc\n",
    "            acc_num = acc_num + 1\n",
    "\n",
    "\n",
    "\n",
    "            # loss ca\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            vali_running_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            y_pred = y_pred.cpu().detach().numpy()\n",
    "            smooth = 1e-5\n",
    "            y = y.cpu().detach().numpy()\n",
    "\n",
    "            aa = 0.\n",
    "            lossFdice = 0.\n",
    "\n",
    "\n",
    "            # calculate each class's dice\n",
    "            # total 25 classes\n",
    "            if 1 in y:\n",
    "              intersection_1 = np.sum((y == 1) & (y_pred == 1))\n",
    "              union_set_1 = np.sum(y == 1) + np.sum(y_pred == 1)\n",
    "              lossFdice_1 = 2*(intersection_1 + smooth)/(union_set_1 + smooth)\n",
    "              dice_1.append(lossFdice_1)\n",
    "            if 2 in y:\n",
    "              intersection_2 = np.sum((y == 2) & (y_pred == 2))\n",
    "              union_set_2 = np.sum(y == 2) + np.sum(y_pred == 2)\n",
    "              lossFdice_2 = 2*(intersection_2 + smooth)/(union_set_2 + smooth)\n",
    "              dice_2.append(lossFdice_2)\n",
    "            if 3 in y:\n",
    "              intersection_3 = np.sum((y == 3) & (y_pred == 3))\n",
    "              union_set_3 = np.sum(y == 3) + np.sum(y_pred == 3)\n",
    "              lossFdice_3 = 2*(intersection_3 + smooth)/(union_set_3 + smooth)\n",
    "              dice_3.append(lossFdice_3)\n",
    "            if 4 in y:\n",
    "              intersection_4 = np.sum((y == 4) & (y_pred == 4))\n",
    "              union_set_4 = np.sum(y == 4) + np.sum(y_pred == 4)\n",
    "              lossFdice_4 = 2*(intersection_4 + smooth)/(union_set_4 + smooth)\n",
    "              dice_4.append(lossFdice_4)\n",
    "            if 5 in y:\n",
    "              intersection_5 = np.sum((y == 5) & (y_pred == 5))\n",
    "              union_set_5 = np.sum(y == 5) + np.sum(y_pred == 5)\n",
    "              lossFdice_5 = 2*(intersection_5 + smooth)/(union_set_5 + smooth)\n",
    "              dice_5.append(lossFdice_5)\n",
    "            if 6 in y: \n",
    "              intersection_6 = np.sum((y == 6) & (y_pred == 6))\n",
    "              union_set_6 = np.sum(y == 6) + np.sum(y_pred == 6)\n",
    "              lossFdice_6 = 2*(intersection_6 + smooth)/(union_set_6 + smooth)\n",
    "              dice_6.append(lossFdice_6)\n",
    "            if 7 in y:\n",
    "              intersection_7 = np.sum((y == 7) & (y_pred == 7))\n",
    "              union_set_7 = np.sum(y == 7) + np.sum(y_pred == 7)\n",
    "              lossFdice_7 = 2*(intersection_7 + smooth)/(union_set_7 + smooth)\n",
    "              dice_7.append(lossFdice_7)\n",
    "            if 8 in y:\n",
    "              intersection_8 = np.sum((y == 8) & (y_pred == 8))\n",
    "              union_set_8 = np.sum(y == 8) + np.sum(y_pred == 8)\n",
    "              lossFdice_8 = 2*(intersection_8 + smooth)/(union_set_8 + smooth)\n",
    "              dice_8.append(lossFdice_8)\n",
    "            if 9 in y:\n",
    "              intersection_9 = np.sum((y == 9) & (y_pred == 9))\n",
    "              union_set_9 = np.sum(y == 9) + np.sum(y_pred == 9)\n",
    "              lossFdice_9 = 2*(intersection_9 + smooth)/(union_set_9 + smooth)\n",
    "              dice_9.append(lossFdice_9)\n",
    "            if 10 in y:\n",
    "              intersection_10 = np.sum((y == 10) & (y_pred == 10))\n",
    "              union_set_10 = np.sum(y == 10) + np.sum(y_pred == 10)\n",
    "              lossFdice_10 = 2*(intersection_10 + smooth)/(union_set_10 + smooth)\n",
    "              dice_10.append(lossFdice_10)\n",
    "            if 11 in y:\n",
    "              intersection_11 = np.sum((y == 11) & (y_pred == 11))\n",
    "              union_set_11 = np.sum(y == 11) + np.sum(y_pred == 11)\n",
    "              lossFdice_11 = 2*(intersection_11 + smooth)/(union_set_11 + smooth)\n",
    "              dice_11.append(lossFdice_11)\n",
    "            if 12 in y:\n",
    "              intersection_12 = np.sum((y == 12) & (y_pred == 12))\n",
    "              union_set_12 = np.sum(y == 12) + np.sum(y_pred == 12)\n",
    "              lossFdice_12 = 2*(intersection_12 + smooth)/(union_set_12 + smooth)\n",
    "              dice_12.append(lossFdice_12)\n",
    "            if 13 in y:\n",
    "              intersection_13 = np.sum((y == 13) & (y_pred == 13))\n",
    "              union_set_13 = np.sum(y == 13) + np.sum(y_pred == 13)\n",
    "              lossFdice_13 = 2*(intersection_13 + smooth)/(union_set_13 + smooth)\n",
    "              dice_13.append(lossFdice_13)\n",
    "            if 14 in y:\n",
    "              intersection_14 = np.sum((y == 14) & (y_pred == 14))\n",
    "              union_set_14 = np.sum(y == 14) + np.sum(y_pred == 14)\n",
    "              lossFdice_14 = 2*(intersection_14 + smooth)/(union_set_14 + smooth)\n",
    "              dice_14.append(lossFdice_14)\n",
    "            if 15 in y:\n",
    "              intersection_15 = np.sum((y == 15) & (y_pred == 15))\n",
    "              union_set_15 = np.sum(y == 15) + np.sum(y_pred == 15)\n",
    "              lossFdice_15 = 2*(intersection_15 + smooth)/(union_set_15 + smooth)\n",
    "              dice_15.append(lossFdice_15)\n",
    "            if 16 in y:\n",
    "              intersection_16 = np.sum((y == 16) & (y_pred == 16))\n",
    "              union_set_16 = np.sum(y == 16) + np.sum(y_pred == 16)\n",
    "              lossFdice_16 = 2*(intersection_16 + smooth)/(union_set_16 + smooth)\n",
    "              dice_16.append(lossFdice_16)\n",
    "            if 17 in y:\n",
    "              intersection_17 = np.sum((y == 17) & (y_pred == 17))\n",
    "              union_set_17 = np.sum(y == 17) + np.sum(y_pred == 17)\n",
    "              lossFdice_17 = 2*(intersection_17 + smooth)/(union_set_17 + smooth)\n",
    "              dice_17.append(lossFdice_17)\n",
    "            if 18 in y:\n",
    "              intersection_18 = np.sum((y == 18) & (y_pred == 18))\n",
    "              union_set_18 = np.sum(y == 18) + np.sum(y_pred == 18)\n",
    "              lossFdice_18 = 2*(intersection_18 + smooth)/(union_set_18 + smooth)\n",
    "              dice_18.append(lossFdice_18)\n",
    "            if 19 in y:\n",
    "              intersection_19 = np.sum((y == 19) & (y_pred == 19))\n",
    "              union_set_19 = np.sum(y == 19) + np.sum(y_pred == 19)\n",
    "              lossFdice_19 = 2*(intersection_19 + smooth)/(union_set_19 + smooth)\n",
    "              dice_19.append(lossFdice_19)\n",
    "            if 20 in y:\n",
    "              intersection_20 = np.sum((y == 20) & (y_pred == 20))\n",
    "              union_set_20 = np.sum(y == 20) + np.sum(y_pred == 20)\n",
    "              lossFdice_20 = 2*(intersection_20 + smooth)/(union_set_20 + smooth)\n",
    "              dice_20.append(lossFdice_20)\n",
    "            if 21 in y:\n",
    "              intersection_21 = np.sum((y == 21) & (y_pred == 21))\n",
    "              union_set_21 = np.sum(y == 21) + np.sum(y_pred == 21)\n",
    "              lossFdice_21 = 2*(intersection_21 + smooth)/(union_set_21 + smooth)\n",
    "              dice_21.append(lossFdice_21)\n",
    "            if 22 in y:\n",
    "              intersection_22 = np.sum((y == 22) & (y_pred == 22))\n",
    "              union_set_22 = np.sum(y == 22) + np.sum(y_pred == 22)\n",
    "              lossFdice_22 = 2*(intersection_22 + smooth)/(union_set_22 + smooth)\n",
    "              dice_22.append(lossFdice_22)\n",
    "            if 23 in y:\n",
    "              intersection_23 = np.sum((y == 23) & (y_pred == 23))\n",
    "              union_set_23 = np.sum(y == 23) + np.sum(y_pred == 23)\n",
    "              lossFdice_23 = 2*(intersection_23 + smooth)/(union_set_23 + smooth)\n",
    "              dice_23.append(lossFdice_23)\n",
    "            if 24 in y:\n",
    "              intersection_24 = np.sum((y == 24) & (y_pred == 24))\n",
    "              union_set_24 = np.sum(y == 24) + np.sum(y_pred == 24)\n",
    "              lossFdice_24 = 2*(intersection_24 + smooth)/(union_set_24 + smooth)\n",
    "              dice_24.append(lossFdice_24)\n",
    "            if 25 in y:\n",
    "              intersection_25 = np.sum((y == 25) & (y_pred == 25))\n",
    "              union_set_25 = np.sum(y == 25) + np.sum(y_pred == 25)\n",
    "              lossFdice_25 = 2*(intersection_25 + smooth)/(union_set_25 + smooth)\n",
    "              dice_25.append(lossFdice_25)\n",
    "    \n",
    "\n",
    "        acc_vali_total = acc_vali_total / acc_num\n",
    "     \n",
    "        vali_acc_each_epoch = np.append(vali_acc_each_epoch, acc_vali_total)\n",
    "\n",
    "        dice_vali=(mean(dice_1)+mean(dice_2)+mean(dice_3)+mean(dice_4)+mean(dice_5)\n",
    "                 +mean(dice_6)+mean(dice_7)+mean(dice_8)+mean(dice_9)+mean(dice_10)\n",
    "                 +mean(dice_11)+mean(dice_12)+mean(dice_13)+mean(dice_14)+mean(dice_15)\n",
    "                 +mean(dice_16)+mean(dice_17)+mean(dice_18)+mean(dice_19)+mean(dice_20)\n",
    "                 +mean(dice_21)+mean(dice_22)+mean(dice_23)+mean(dice_24)+mean(dice_25))/25\n",
    "\n",
    "        # 这里的len(test_dataloader.dataset)是所有testdata的总长，不计算batchsize在内的\n",
    "        #dice_11 = (dice_total / len(test_dataloader.dataset))*16 # the total of testdata \n",
    "        vali_dice_each_epoch = np.append(vali_dice_each_epoch, dice_vali)\n",
    "        epoch_vali_loss = vali_running_loss / len(vali_dataloader.dataset)*16 # batchsize=16\n",
    "        vali_loss_each_epoch = np.append(vali_loss_each_epoch, epoch_vali_loss)\n",
    "        print('epoch:',epoch,'validation_loss:',round(epoch_vali_loss,3),'validation_dice:',dice_vali)\n",
    "\n",
    "        ##########################################################\n",
    "    #    wandb.log({\"vali_dice_each_epoch\":dice_vali})\n",
    "    #    wandb.log({\"vali_loss_each_epoch\":epoch_vali_loss})\n",
    "    \n",
    "\n",
    "\n",
    "#————————————————————————————————————————————————————————————————————————————————\n",
    "    #——————————————————————————————————————————————————————————————————————\n",
    "    #————————————————test——————————————————————————————————————————————————\n",
    "    model.eval()\n",
    "    test_running_loss = 0 #record the sum of loss\n",
    "    dice_total = 0\n",
    "    acc_test_total = 0\n",
    "    acc_num = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dice_1 = [] # \n",
    "        dice_2 = [] # \n",
    "        dice_3 = [] # \n",
    "        dice_4 = [] # \n",
    "        dice_5 = [] # \n",
    "        dice_6 = [] # \n",
    "        dice_7 = [] # \n",
    "        dice_8 = [] # \n",
    "        dice_9 = []\n",
    "        dice_10 = []\n",
    "        dice_11 = []\n",
    "        dice_12 = [] # \n",
    "        dice_13 = [] # \n",
    "        dice_14 = [] # \n",
    "        dice_15 = [] # \n",
    "        dice_16 = [] # \n",
    "        dice_17 = [] # \n",
    "        dice_18 = [] # \n",
    "        dice_19 = []\n",
    "        dice_20 = []\n",
    "        dice_21 = []\n",
    "        dice_22 = []\n",
    "        dice_23 = []\n",
    "        dice_24 = []\n",
    "        dice_25 = []\n",
    "        for x, y in test_dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            # acc caculate\n",
    "            y_pred_here = y_pred\n",
    "            y_pred_here = torch.argmax(y_pred_here,dim=1)\n",
    "            correct = 0\n",
    "            correct += (y ==y_pred_here).sum().item()\n",
    "            N = y.size(0)\n",
    "            acc = correct/(N*256*256)\n",
    "            acc_test_total = acc_test_total + acc\n",
    "            acc_num = acc_num + 1\n",
    "\n",
    "\n",
    "\n",
    "            # loss ca\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_running_loss += loss.item()\n",
    "\n",
    "\n",
    "            #\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            y_pred = y_pred.cpu().detach().numpy()\n",
    "            smooth = 1e-5\n",
    "            y = y.cpu().detach().numpy()\n",
    "\n",
    "            aa = 0.\n",
    "            lossFdice = 0.\n",
    "            # calculate each class's dice\n",
    "            # total 25 classes\n",
    "            if 1 in y:\n",
    "              intersection_1 = np.sum((y == 1) & (y_pred == 1))\n",
    "              union_set_1 = np.sum(y == 1) + np.sum(y_pred == 1)\n",
    "              lossFdice_1 = 2*(intersection_1 + smooth)/(union_set_1 + smooth)\n",
    "              dice_1.append(lossFdice_1)\n",
    "            if 2 in y:\n",
    "              intersection_2 = np.sum((y == 2) & (y_pred == 2))\n",
    "              union_set_2 = np.sum(y == 2) + np.sum(y_pred == 2)\n",
    "              lossFdice_2 = 2*(intersection_2 + smooth)/(union_set_2 + smooth)\n",
    "              dice_2.append(lossFdice_2)\n",
    "            if 3 in y:\n",
    "              intersection_3 = np.sum((y == 3) & (y_pred == 3))\n",
    "              union_set_3 = np.sum(y == 3) + np.sum(y_pred == 3)\n",
    "              lossFdice_3 = 2*(intersection_3 + smooth)/(union_set_3 + smooth)\n",
    "              dice_3.append(lossFdice_3)\n",
    "            if 4 in y:\n",
    "              intersection_4 = np.sum((y == 4) & (y_pred == 4))\n",
    "              union_set_4 = np.sum(y == 4) + np.sum(y_pred == 4)\n",
    "              lossFdice_4 = 2*(intersection_4 + smooth)/(union_set_4 + smooth)\n",
    "              dice_4.append(lossFdice_4)\n",
    "            if 5 in y:\n",
    "              intersection_5 = np.sum((y == 5) & (y_pred == 5))\n",
    "              union_set_5 = np.sum(y == 5) + np.sum(y_pred == 5)\n",
    "              lossFdice_5 = 2*(intersection_5 + smooth)/(union_set_5 + smooth)\n",
    "              dice_5.append(lossFdice_5)\n",
    "            if 6 in y: \n",
    "              intersection_6 = np.sum((y == 6) & (y_pred == 6))\n",
    "              union_set_6 = np.sum(y == 6) + np.sum(y_pred == 6)\n",
    "              lossFdice_6 = 2*(intersection_6 + smooth)/(union_set_6 + smooth)\n",
    "              dice_6.append(lossFdice_6)\n",
    "            if 7 in y:\n",
    "              intersection_7 = np.sum((y == 7) & (y_pred == 7))\n",
    "              union_set_7 = np.sum(y == 7) + np.sum(y_pred == 7)\n",
    "              lossFdice_7 = 2*(intersection_7 + smooth)/(union_set_7 + smooth)\n",
    "              dice_7.append(lossFdice_7)\n",
    "            if 8 in y:\n",
    "              intersection_8 = np.sum((y == 8) & (y_pred == 8))\n",
    "              union_set_8 = np.sum(y == 8) + np.sum(y_pred == 8)\n",
    "              lossFdice_8 = 2*(intersection_8 + smooth)/(union_set_8 + smooth)\n",
    "              dice_8.append(lossFdice_8)\n",
    "            if 9 in y:\n",
    "              intersection_9 = np.sum((y == 9) & (y_pred == 9))\n",
    "              union_set_9 = np.sum(y == 9) + np.sum(y_pred == 9)\n",
    "              lossFdice_9 = 2*(intersection_9 + smooth)/(union_set_9 + smooth)\n",
    "              dice_9.append(lossFdice_9)\n",
    "            if 10 in y:\n",
    "              intersection_10 = np.sum((y == 10) & (y_pred == 10))\n",
    "              union_set_10 = np.sum(y == 10) + np.sum(y_pred == 10)\n",
    "              lossFdice_10 = 2*(intersection_10 + smooth)/(union_set_10 + smooth)\n",
    "              dice_10.append(lossFdice_10)\n",
    "            if 11 in y:\n",
    "              intersection_11 = np.sum((y == 11) & (y_pred == 11))\n",
    "              union_set_11 = np.sum(y == 11) + np.sum(y_pred == 11)\n",
    "              lossFdice_11 = 2*(intersection_11 + smooth)/(union_set_11 + smooth)\n",
    "              dice_11.append(lossFdice_11)\n",
    "            if 12 in y:\n",
    "              intersection_12 = np.sum((y == 12) & (y_pred == 12))\n",
    "              union_set_12 = np.sum(y == 12) + np.sum(y_pred == 12)\n",
    "              lossFdice_12 = 2*(intersection_12 + smooth)/(union_set_12 + smooth)\n",
    "              dice_12.append(lossFdice_12)\n",
    "            if 13 in y:\n",
    "              intersection_13 = np.sum((y == 13) & (y_pred == 13))\n",
    "              union_set_13 = np.sum(y == 13) + np.sum(y_pred == 13)\n",
    "              lossFdice_13 = 2*(intersection_13 + smooth)/(union_set_13 + smooth)\n",
    "              dice_13.append(lossFdice_13)\n",
    "            if 14 in y:\n",
    "              intersection_14 = np.sum((y == 14) & (y_pred == 14))\n",
    "              union_set_14 = np.sum(y == 14) + np.sum(y_pred == 14)\n",
    "              lossFdice_14 = 2*(intersection_14 + smooth)/(union_set_14 + smooth)\n",
    "              dice_14.append(lossFdice_14)\n",
    "            if 15 in y:\n",
    "              intersection_15 = np.sum((y == 15) & (y_pred == 15))\n",
    "              union_set_15 = np.sum(y == 15) + np.sum(y_pred == 15)\n",
    "              lossFdice_15 = 2*(intersection_15 + smooth)/(union_set_15 + smooth)\n",
    "              dice_15.append(lossFdice_15)\n",
    "            if 16 in y:\n",
    "              intersection_16 = np.sum((y == 16) & (y_pred == 16))\n",
    "              union_set_16 = np.sum(y == 16) + np.sum(y_pred == 16)\n",
    "              lossFdice_16 = 2*(intersection_16 + smooth)/(union_set_16 + smooth)\n",
    "              dice_16.append(lossFdice_16)\n",
    "            if 17 in y:\n",
    "              intersection_17 = np.sum((y == 17) & (y_pred == 17))\n",
    "              union_set_17 = np.sum(y == 17) + np.sum(y_pred == 17)\n",
    "              lossFdice_17 = 2*(intersection_17 + smooth)/(union_set_17 + smooth)\n",
    "              dice_17.append(lossFdice_17)\n",
    "            if 18 in y:\n",
    "              intersection_18 = np.sum((y == 18) & (y_pred == 18))\n",
    "              union_set_18 = np.sum(y == 18) + np.sum(y_pred == 18)\n",
    "              lossFdice_18 = 2*(intersection_18 + smooth)/(union_set_18 + smooth)\n",
    "              dice_18.append(lossFdice_18)\n",
    "            if 19 in y:\n",
    "              intersection_19 = np.sum((y == 19) & (y_pred == 19))\n",
    "              union_set_19 = np.sum(y == 19) + np.sum(y_pred == 19)\n",
    "              lossFdice_19 = 2*(intersection_19 + smooth)/(union_set_19 + smooth)\n",
    "              dice_19.append(lossFdice_19)\n",
    "            if 20 in y:\n",
    "              intersection_20 = np.sum((y == 20) & (y_pred == 20))\n",
    "              union_set_20 = np.sum(y == 20) + np.sum(y_pred == 20)\n",
    "              lossFdice_20 = 2*(intersection_20 + smooth)/(union_set_20 + smooth)\n",
    "              dice_20.append(lossFdice_20)\n",
    "            if 21 in y:\n",
    "              intersection_21 = np.sum((y == 21) & (y_pred == 21))\n",
    "              union_set_21 = np.sum(y == 21) + np.sum(y_pred == 21)\n",
    "              lossFdice_21 = 2*(intersection_21 + smooth)/(union_set_21 + smooth)\n",
    "              dice_21.append(lossFdice_21)\n",
    "            if 22 in y:\n",
    "              intersection_22 = np.sum((y == 22) & (y_pred == 22))\n",
    "              union_set_22 = np.sum(y == 22) + np.sum(y_pred == 22)\n",
    "              lossFdice_22 = 2*(intersection_22 + smooth)/(union_set_22 + smooth)\n",
    "              dice_22.append(lossFdice_22)\n",
    "            if 23 in y:\n",
    "              intersection_23 = np.sum((y == 23) & (y_pred == 23))\n",
    "              union_set_23 = np.sum(y == 23) + np.sum(y_pred == 23)\n",
    "              lossFdice_23 = 2*(intersection_23 + smooth)/(union_set_23 + smooth)\n",
    "              dice_23.append(lossFdice_23)\n",
    "            if 24 in y:\n",
    "              intersection_24 = np.sum((y == 24) & (y_pred == 24))\n",
    "              union_set_24 = np.sum(y == 24) + np.sum(y_pred == 24)\n",
    "              lossFdice_24 = 2*(intersection_24 + smooth)/(union_set_24 + smooth)\n",
    "              dice_24.append(lossFdice_24)\n",
    "            if 25 in y:\n",
    "              intersection_25 = np.sum((y == 25) & (y_pred == 25))\n",
    "              union_set_25 = np.sum(y == 25) + np.sum(y_pred == 25)\n",
    "              lossFdice_25 = 2*(intersection_25 + smooth)/(union_set_25 + smooth)\n",
    "              dice_25.append(lossFdice_25)\n",
    "\n",
    "\n",
    "        dice_test=(mean(dice_1)+mean(dice_2)+mean(dice_3)+mean(dice_4)+mean(dice_5)\n",
    "                 +mean(dice_6)+mean(dice_7)+mean(dice_8)+mean(dice_9)+mean(dice_10)\n",
    "                 +mean(dice_11)+mean(dice_12)+mean(dice_13)+mean(dice_14)+mean(dice_15)\n",
    "                 +mean(dice_16)+mean(dice_17)+mean(dice_18)+mean(dice_19)+mean(dice_20)\n",
    "                 +mean(dice_21)+mean(dice_22)+mean(dice_23)+mean(dice_24)+mean(dice_25))/25.\n",
    "        test_dice_each_epoch = np.append(test_dice_each_epoch, dice_test)\n",
    "\n",
    "        print('epoch:',epoch,'test_dataset dice:',dice_test)\n",
    "\n",
    "        ##########################################################\n",
    "      # wandb.log({\"test_dice_each_epoch\":dice_test})\n",
    "\n",
    "    return train_loss_each_step,train_acc_each_step,vali_dice_each_epoch,vali_loss_each_epoch,vali_acc_each_epoch,test_dice_each_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2b65f7aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1 loss: 3.204321 acc: 0.13512420654296875\n",
      "epoch: 1 step: 2 loss: 3.0742378 acc: 0.5600624084472656\n",
      "epoch: 1 step: 3 loss: 2.8782907 acc: 0.6103744506835938\n",
      "epoch: 1 step: 4 loss: 2.8351471 acc: 0.5226020812988281\n",
      "epoch: 1 step: 5 loss: 2.7900229 acc: 0.5511093139648438\n",
      "epoch: 1 step: 6 loss: 2.6962516 acc: 0.5965766906738281\n",
      "epoch: 1 step: 7 loss: 2.7958512 acc: 0.5778770446777344\n",
      "epoch: 1 step: 8 loss: 2.683036 acc: 0.6037330627441406\n",
      "epoch: 1 step: 9 loss: 2.6360462 acc: 0.6425552368164062\n",
      "epoch: 1 step: 10 loss: 2.6470973 acc: 0.6454849243164062\n",
      "epoch: 1 step: 11 loss: 2.581485 acc: 0.6475868225097656\n",
      "epoch: 1 step: 12 loss: 2.5466545 acc: 0.6650657653808594\n",
      "epoch: 1 step: 13 loss: 2.5223403 acc: 0.7158737182617188\n",
      "epoch: 1 step: 14 loss: 2.4050574 acc: 0.7366218566894531\n",
      "epoch: 1 step: 15 loss: 2.4011364 acc: 0.7863731384277344\n",
      "epoch: 1 step: 16 loss: 2.5608635 acc: 0.6450881958007812\n",
      "epoch: 1 step: 17 loss: 2.3744137 acc: 0.7616615295410156\n",
      "epoch: 1 step: 18 loss: 2.5354197 acc: 0.6397056579589844\n",
      "epoch: 1 step: 19 loss: 2.3992448 acc: 0.6721611022949219\n",
      "epoch: 1 step: 20 loss: 2.38043 acc: 0.7101554870605469\n",
      "epoch: 1 step: 21 loss: 2.5364847 acc: 0.6823539733886719\n",
      "epoch: 1 step: 22 loss: 2.3792522 acc: 0.7500801086425781\n",
      "epoch: 1 step: 23 loss: 2.4299684 acc: 0.7121238708496094\n",
      "epoch: 1 step: 24 loss: 2.1765754 acc: 0.8038368225097656\n",
      "epoch: 1 step: 25 loss: 2.2250361 acc: 0.7515869140625\n",
      "epoch: 1 step: 26 loss: 2.202296 acc: 0.7446708679199219\n",
      "epoch: 1 step: 27 loss: 2.3203847 acc: 0.7213783264160156\n",
      "epoch: 1 step: 28 loss: 2.2345417 acc: 0.7120742797851562\n",
      "epoch: 1 step: 29 loss: 2.208248 acc: 0.7277755737304688\n",
      "epoch: 1 step: 30 loss: 2.2238004 acc: 0.7123832702636719\n",
      "epoch: 1 step: 31 loss: 1.9978765 acc: 0.7810478210449219\n",
      "epoch: 1 step: 32 loss: 2.0927458 acc: 0.7600860595703125\n",
      "epoch: 1 step: 33 loss: 2.3511035 acc: 0.7208175659179688\n",
      "epoch: 1 step: 34 loss: 2.2989194 acc: 0.6923561096191406\n",
      "epoch: 1 step: 35 loss: 2.248814 acc: 0.73394775390625\n",
      "epoch: 1 step: 36 loss: 2.2374911 acc: 0.6928443908691406\n",
      "epoch: 1 step: 37 loss: 1.9444127 acc: 0.8191108703613281\n",
      "epoch: 1 step: 38 loss: 1.9813484 acc: 0.7764091491699219\n",
      "epoch: 1 step: 39 loss: 2.0223413 acc: 0.8295135498046875\n",
      "epoch: 1 step: 40 loss: 2.220472 acc: 0.7359771728515625\n",
      "epoch: 1 step: 41 loss: 2.0971222 acc: 0.73675537109375\n",
      "epoch: 1 step: 42 loss: 1.9850109 acc: 0.7761306762695312\n",
      "epoch: 1 step: 43 loss: 1.9613502 acc: 0.7613372802734375\n",
      "epoch: 1 step: 44 loss: 2.027529 acc: 0.7448768615722656\n",
      "epoch: 1 step: 45 loss: 1.8913728 acc: 0.7848663330078125\n",
      "epoch: 1 step: 46 loss: 1.8787637 acc: 0.7957687377929688\n",
      "epoch: 1 step: 47 loss: 2.0816946 acc: 0.8110580444335938\n",
      "epoch: 1 step: 48 loss: 2.0540864 acc: 0.7699508666992188\n",
      "epoch: 1 step: 49 loss: 1.985971 acc: 0.7865715026855469\n",
      "epoch: 1 step: 50 loss: 1.8517611 acc: 0.7910232543945312\n",
      "epoch: 1 step: 51 loss: 1.9078214 acc: 0.7769508361816406\n",
      "epoch: 1 step: 52 loss: 1.8597093 acc: 0.7953033447265625\n",
      "epoch: 1 step: 53 loss: 1.845234 acc: 0.7974510192871094\n",
      "epoch: 1 step: 54 loss: 1.9981 acc: 0.808258056640625\n",
      "epoch: 1 step: 55 loss: 1.8703734 acc: 0.7946052551269531\n",
      "epoch: 1 step: 56 loss: 2.115991 acc: 0.7747840881347656\n",
      "epoch: 1 step: 57 loss: 1.7068778 acc: 0.8177757263183594\n",
      "epoch: 1 step: 58 loss: 2.0586793 acc: 0.7703857421875\n",
      "epoch: 1 step: 59 loss: 1.8386188 acc: 0.7910652160644531\n",
      "epoch: 1 step: 60 loss: 2.010074 acc: 0.7702369689941406\n",
      "epoch: 1 step: 61 loss: 2.0707886 acc: 0.7696075439453125\n",
      "epoch: 1 step: 62 loss: 1.7393172 acc: 0.8263893127441406\n",
      "epoch: 1 step: 63 loss: 1.8086638 acc: 0.8010406494140625\n",
      "epoch: 1 step: 64 loss: 1.8359454 acc: 0.8325157165527344\n",
      "epoch: 1 step: 65 loss: 1.8997971 acc: 0.7775917053222656\n",
      "epoch: 1 step: 66 loss: 1.7744788 acc: 0.8060722351074219\n",
      "epoch: 1 step: 67 loss: 1.606614 acc: 0.854827880859375\n",
      "epoch: 1 step: 68 loss: 1.6827569 acc: 0.8055076599121094\n",
      "epoch: 1 step: 69 loss: 1.7653385 acc: 0.8002090454101562\n",
      "epoch: 1 step: 70 loss: 1.575362 acc: 0.8392333984375\n",
      "epoch: 1 step: 71 loss: 1.5508488 acc: 0.8519172668457031\n",
      "epoch: 1 step: 72 loss: 1.778836 acc: 0.8148765563964844\n",
      "epoch: 1 step: 73 loss: 1.7404623 acc: 0.832672119140625\n",
      "epoch: 1 step: 74 loss: 1.7859427 acc: 0.7728843688964844\n",
      "epoch: 1 step: 75 loss: 1.8013059 acc: 0.7872695922851562\n",
      "epoch: 1 step: 76 loss: 1.6426998 acc: 0.81341552734375\n",
      "epoch: 1 step: 77 loss: 1.8418553 acc: 0.7977027893066406\n",
      "epoch: 1 step: 78 loss: 1.7306436 acc: 0.7922897338867188\n",
      "epoch: 1 step: 79 loss: 1.5515084 acc: 0.8018760681152344\n",
      "epoch: 1 step: 80 loss: 1.4677905 acc: 0.8263435363769531\n",
      "epoch: 1 step: 81 loss: 1.5555216 acc: 0.792236328125\n",
      "epoch: 1 step: 82 loss: 1.6211975 acc: 0.8182792663574219\n",
      "epoch: 1 step: 83 loss: 1.8747638 acc: 0.7737503051757812\n",
      "epoch: 1 step: 84 loss: 1.5612563 acc: 0.8284187316894531\n",
      "epoch: 1 step: 85 loss: 1.7405934 acc: 0.8068313598632812\n",
      "epoch: 1 step: 86 loss: 1.7079781 acc: 0.7973175048828125\n",
      "epoch: 1 step: 87 loss: 1.6961542 acc: 0.801177978515625\n",
      "epoch: 1 step: 88 loss: 1.7125658 acc: 0.8036880493164062\n",
      "epoch: 1 step: 89 loss: 1.4425138 acc: 0.8625831604003906\n",
      "epoch: 1 step: 90 loss: 1.8667605 acc: 0.7925910949707031\n",
      "epoch: 1 step: 91 loss: 1.6476961 acc: 0.8009262084960938\n",
      "epoch: 1 step: 92 loss: 1.7091199 acc: 0.8042716979980469\n",
      "epoch: 1 step: 93 loss: 1.6255672 acc: 0.8237380981445312\n",
      "epoch: 1 step: 94 loss: 1.5786337 acc: 0.8191261291503906\n",
      "epoch: 1 step: 95 loss: 1.361827 acc: 0.8447685241699219\n",
      "epoch: 1 step: 96 loss: 1.5937618 acc: 0.8067817687988281\n",
      "epoch: 1 step: 97 loss: 1.6445752 acc: 0.7957077026367188\n",
      "epoch: 1 step: 98 loss: 1.9047437 acc: 0.7550086975097656\n",
      "epoch: 1 step: 99 loss: 1.595963 acc: 0.8076705932617188\n",
      "epoch: 1 step: 100 loss: 1.5141298 acc: 0.8029098510742188\n",
      "epoch: 1 step: 101 loss: 1.5431536 acc: 0.82086181640625\n",
      "epoch: 1 step: 102 loss: 1.7287211 acc: 0.7909088134765625\n",
      "epoch: 1 step: 103 loss: 1.8931363 acc: 0.7695350646972656\n",
      "epoch: 1 step: 104 loss: 1.7387687 acc: 0.7883872985839844\n",
      "epoch: 1 step: 105 loss: 1.6740154 acc: 0.797210693359375\n",
      "epoch: 1 step: 106 loss: 1.8957499 acc: 0.7313270568847656\n",
      "epoch: 1 step: 107 loss: 1.5378803 acc: 0.8186454772949219\n",
      "epoch: 1 step: 108 loss: 1.4497981 acc: 0.8184127807617188\n",
      "epoch: 1 step: 109 loss: 1.4249357 acc: 0.8255882263183594\n",
      "epoch: 1 step: 110 loss: 1.4438143 acc: 0.8604698181152344\n",
      "epoch: 1 step: 111 loss: 1.522154 acc: 0.8266181945800781\n",
      "epoch: 1 step: 112 loss: 1.4656296 acc: 0.8032112121582031\n",
      "epoch: 1 step: 113 loss: 1.412073 acc: 0.8097114562988281\n",
      "epoch: 1 step: 114 loss: 1.5854893 acc: 0.8098526000976562\n",
      "epoch: 1 step: 115 loss: 1.5530069 acc: 0.7948570251464844\n",
      "epoch: 1 step: 116 loss: 1.561332 acc: 0.7888565063476562\n",
      "epoch: 1 step: 117 loss: 1.6307821 acc: 0.7898025512695312\n",
      "epoch: 1 step: 118 loss: 1.6687433 acc: 0.7710914611816406\n",
      "epoch: 1 step: 119 loss: 1.5772936 acc: 0.8188133239746094\n",
      "epoch: 1 step: 120 loss: 1.2515142 acc: 0.85400390625\n",
      "epoch: 1 step: 121 loss: 1.677736 acc: 0.8004188537597656\n",
      "epoch: 1 step: 122 loss: 1.4923303 acc: 0.832244873046875\n",
      "epoch: 1 step: 123 loss: 1.640203 acc: 0.7917861938476562\n",
      "epoch: 1 step: 124 loss: 1.591382 acc: 0.8205653599330357\n",
      "epoch: 1 validation_loss: 1.519 validation_dice: 0.11323207169254063\n",
      "epoch: 1 test_dataset dice: 0.15696461880801307\n",
      "time cost 0.5462248841921489 min\n",
      "dice_best: 0\n",
      "******************************** epoch  1  is finished. *********************************\n",
      "epoch: 2 step: 1 loss: 1.6415147 acc: 0.7923622131347656\n",
      "epoch: 2 step: 2 loss: 1.6637832 acc: 0.7900161743164062\n",
      "epoch: 2 step: 3 loss: 1.6194623 acc: 0.8174781799316406\n",
      "epoch: 2 step: 4 loss: 1.6135079 acc: 0.7757377624511719\n",
      "epoch: 2 step: 5 loss: 1.6115199 acc: 0.8098640441894531\n",
      "epoch: 2 step: 6 loss: 1.6307347 acc: 0.7948951721191406\n",
      "epoch: 2 step: 7 loss: 1.4649847 acc: 0.79217529296875\n",
      "epoch: 2 step: 8 loss: 1.5326625 acc: 0.8014602661132812\n",
      "epoch: 2 step: 9 loss: 1.5283648 acc: 0.7668228149414062\n",
      "epoch: 2 step: 10 loss: 1.5336375 acc: 0.793609619140625\n",
      "epoch: 2 step: 11 loss: 1.4454937 acc: 0.822479248046875\n",
      "epoch: 2 step: 12 loss: 1.6165264 acc: 0.7890510559082031\n",
      "epoch: 2 step: 13 loss: 1.6879102 acc: 0.7645492553710938\n",
      "epoch: 2 step: 14 loss: 1.4751095 acc: 0.8091964721679688\n",
      "epoch: 2 step: 15 loss: 1.6455176 acc: 0.780029296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 16 loss: 1.3047432 acc: 0.8415946960449219\n",
      "epoch: 2 step: 17 loss: 1.3703203 acc: 0.8279800415039062\n",
      "epoch: 2 step: 18 loss: 1.3849974 acc: 0.819671630859375\n",
      "epoch: 2 step: 19 loss: 1.6080629 acc: 0.7875747680664062\n",
      "epoch: 2 step: 20 loss: 1.5273544 acc: 0.8070259094238281\n",
      "epoch: 2 step: 21 loss: 1.3367827 acc: 0.8197555541992188\n",
      "epoch: 2 step: 22 loss: 1.4548486 acc: 0.8382301330566406\n",
      "epoch: 2 step: 23 loss: 1.2894096 acc: 0.8054313659667969\n",
      "epoch: 2 step: 24 loss: 1.5817364 acc: 0.7973251342773438\n",
      "epoch: 2 step: 25 loss: 1.4128174 acc: 0.8287734985351562\n",
      "epoch: 2 step: 26 loss: 1.4220376 acc: 0.8179550170898438\n",
      "epoch: 2 step: 27 loss: 1.4232581 acc: 0.8004341125488281\n",
      "epoch: 2 step: 28 loss: 1.2835393 acc: 0.8174362182617188\n",
      "epoch: 2 step: 29 loss: 1.3671302 acc: 0.7965888977050781\n",
      "epoch: 2 step: 30 loss: 1.4545891 acc: 0.8258438110351562\n",
      "epoch: 2 step: 31 loss: 1.6185181 acc: 0.7830810546875\n",
      "epoch: 2 step: 32 loss: 1.4876873 acc: 0.7919654846191406\n",
      "epoch: 2 step: 33 loss: 1.3360443 acc: 0.8213691711425781\n",
      "epoch: 2 step: 34 loss: 1.5386509 acc: 0.8169326782226562\n",
      "epoch: 2 step: 35 loss: 1.5824049 acc: 0.7992019653320312\n",
      "epoch: 2 step: 36 loss: 1.4190348 acc: 0.8415718078613281\n",
      "epoch: 2 step: 37 loss: 1.4524983 acc: 0.8267936706542969\n",
      "epoch: 2 step: 38 loss: 1.4674503 acc: 0.79541015625\n",
      "epoch: 2 step: 39 loss: 1.5958586 acc: 0.818084716796875\n",
      "epoch: 2 step: 40 loss: 1.4714583 acc: 0.8087997436523438\n",
      "epoch: 2 step: 41 loss: 1.6275483 acc: 0.799285888671875\n",
      "epoch: 2 step: 42 loss: 1.6686262 acc: 0.7810134887695312\n",
      "epoch: 2 step: 43 loss: 1.384855 acc: 0.8307609558105469\n",
      "epoch: 2 step: 44 loss: 1.3527431 acc: 0.8307991027832031\n",
      "epoch: 2 step: 45 loss: 1.351008 acc: 0.8224029541015625\n",
      "epoch: 2 step: 46 loss: 1.5073485 acc: 0.7908058166503906\n",
      "epoch: 2 step: 47 loss: 1.3073523 acc: 0.8263893127441406\n",
      "epoch: 2 step: 48 loss: 1.5133866 acc: 0.7828292846679688\n",
      "epoch: 2 step: 49 loss: 1.3893998 acc: 0.8198280334472656\n",
      "epoch: 2 step: 50 loss: 1.4311258 acc: 0.8417205810546875\n",
      "epoch: 2 step: 51 loss: 1.2194933 acc: 0.8758277893066406\n",
      "epoch: 2 step: 52 loss: 1.4081062 acc: 0.8473777770996094\n",
      "epoch: 2 step: 53 loss: 1.2132087 acc: 0.8611869812011719\n",
      "epoch: 2 step: 54 loss: 1.2042824 acc: 0.8402137756347656\n",
      "epoch: 2 step: 55 loss: 1.3980974 acc: 0.8180618286132812\n",
      "epoch: 2 step: 56 loss: 1.4430186 acc: 0.8237266540527344\n",
      "epoch: 2 step: 57 loss: 1.3217099 acc: 0.8421821594238281\n",
      "epoch: 2 step: 58 loss: 1.1612815 acc: 0.8464317321777344\n",
      "epoch: 2 step: 59 loss: 1.0641879 acc: 0.8801040649414062\n",
      "epoch: 2 step: 60 loss: 1.1256541 acc: 0.8468132019042969\n",
      "epoch: 2 step: 61 loss: 1.296045 acc: 0.8471488952636719\n",
      "epoch: 2 step: 62 loss: 1.1753864 acc: 0.8531570434570312\n",
      "epoch: 2 step: 63 loss: 1.2537451 acc: 0.8332557678222656\n",
      "epoch: 2 step: 64 loss: 1.3642924 acc: 0.8349533081054688\n",
      "epoch: 2 step: 65 loss: 1.1141715 acc: 0.879180908203125\n",
      "epoch: 2 step: 66 loss: 1.517311 acc: 0.8421478271484375\n",
      "epoch: 2 step: 67 loss: 1.2871768 acc: 0.8413848876953125\n",
      "epoch: 2 step: 68 loss: 1.278662 acc: 0.8420448303222656\n",
      "epoch: 2 step: 69 loss: 1.119584 acc: 0.8385696411132812\n",
      "epoch: 2 step: 70 loss: 1.207224 acc: 0.8309898376464844\n",
      "epoch: 2 step: 71 loss: 1.5677421 acc: 0.7748565673828125\n",
      "epoch: 2 step: 72 loss: 1.185925 acc: 0.8266983032226562\n",
      "epoch: 2 step: 73 loss: 1.1030016 acc: 0.8315963745117188\n",
      "epoch: 2 step: 74 loss: 1.1717142 acc: 0.8388404846191406\n",
      "epoch: 2 step: 75 loss: 1.274826 acc: 0.84503173828125\n",
      "epoch: 2 step: 76 loss: 1.1488038 acc: 0.8482284545898438\n",
      "epoch: 2 step: 77 loss: 1.3750283 acc: 0.8256378173828125\n",
      "epoch: 2 step: 78 loss: 1.54026 acc: 0.8170585632324219\n",
      "epoch: 2 step: 79 loss: 1.091179 acc: 0.8614921569824219\n",
      "epoch: 2 step: 80 loss: 1.8003424 acc: 0.7771835327148438\n",
      "epoch: 2 step: 81 loss: 1.7235649 acc: 0.8287773132324219\n",
      "epoch: 2 step: 82 loss: 1.3389677 acc: 0.8591232299804688\n",
      "epoch: 2 step: 83 loss: 1.516668 acc: 0.8143806457519531\n",
      "epoch: 2 step: 84 loss: 1.359281 acc: 0.8277778625488281\n",
      "epoch: 2 step: 85 loss: 1.3686143 acc: 0.8478507995605469\n",
      "epoch: 2 step: 86 loss: 1.6183813 acc: 0.7860755920410156\n",
      "epoch: 2 step: 87 loss: 1.3104466 acc: 0.8103485107421875\n",
      "epoch: 2 step: 88 loss: 1.3736365 acc: 0.8385200500488281\n",
      "epoch: 2 step: 89 loss: 1.2062556 acc: 0.8449172973632812\n",
      "epoch: 2 step: 90 loss: 1.3200556 acc: 0.8089866638183594\n",
      "epoch: 2 step: 91 loss: 1.2379746 acc: 0.8405876159667969\n",
      "epoch: 2 step: 92 loss: 1.508151 acc: 0.8200149536132812\n",
      "epoch: 2 step: 93 loss: 1.3001556 acc: 0.8348312377929688\n",
      "epoch: 2 step: 94 loss: 1.346811 acc: 0.8380966186523438\n",
      "epoch: 2 step: 95 loss: 1.3064842 acc: 0.8097114562988281\n",
      "epoch: 2 step: 96 loss: 1.3413826 acc: 0.82855224609375\n",
      "epoch: 2 step: 97 loss: 1.4247833 acc: 0.8093185424804688\n",
      "epoch: 2 step: 98 loss: 1.3782319 acc: 0.8423309326171875\n",
      "epoch: 2 step: 99 loss: 1.0126208 acc: 0.85797119140625\n",
      "epoch: 2 step: 100 loss: 1.2456689 acc: 0.8282623291015625\n",
      "epoch: 2 step: 101 loss: 1.4623642 acc: 0.82781982421875\n",
      "epoch: 2 step: 102 loss: 1.2445246 acc: 0.851470947265625\n",
      "epoch: 2 step: 103 loss: 1.2475394 acc: 0.8474922180175781\n",
      "epoch: 2 step: 104 loss: 1.211104 acc: 0.857421875\n",
      "epoch: 2 step: 105 loss: 1.2106233 acc: 0.8308792114257812\n",
      "epoch: 2 step: 106 loss: 1.3263154 acc: 0.828582763671875\n",
      "epoch: 2 step: 107 loss: 1.2541182 acc: 0.8274116516113281\n",
      "epoch: 2 step: 108 loss: 1.1916848 acc: 0.8335075378417969\n",
      "epoch: 2 step: 109 loss: 1.1814799 acc: 0.8588027954101562\n",
      "epoch: 2 step: 110 loss: 1.1922767 acc: 0.8464546203613281\n",
      "epoch: 2 step: 111 loss: 1.3019128 acc: 0.8201751708984375\n",
      "epoch: 2 step: 112 loss: 1.1023481 acc: 0.851287841796875\n",
      "epoch: 2 step: 113 loss: 1.2581819 acc: 0.8244705200195312\n",
      "epoch: 2 step: 114 loss: 1.3276489 acc: 0.8066062927246094\n",
      "epoch: 2 step: 115 loss: 1.208975 acc: 0.839874267578125\n",
      "epoch: 2 step: 116 loss: 1.2288004 acc: 0.8326835632324219\n",
      "epoch: 2 step: 117 loss: 1.3759137 acc: 0.8143386840820312\n",
      "epoch: 2 step: 118 loss: 1.2607962 acc: 0.8364410400390625\n",
      "epoch: 2 step: 119 loss: 1.154627 acc: 0.8397789001464844\n",
      "epoch: 2 step: 120 loss: 1.2751245 acc: 0.8474006652832031\n",
      "epoch: 2 step: 121 loss: 1.1438569 acc: 0.8509864807128906\n",
      "epoch: 2 step: 122 loss: 1.181109 acc: 0.8417243957519531\n",
      "epoch: 2 step: 123 loss: 1.0605911 acc: 0.8570938110351562\n",
      "epoch: 2 step: 124 loss: 1.0394502 acc: 0.8762468610491071\n",
      "epoch: 2 validation_loss: 1.455 validation_dice: 0.20280731270812946\n",
      "epoch: 2 test_dataset dice: 0.1763728667146958\n",
      "time cost 0.5361121376355489 min\n",
      "dice_best: 0\n",
      "******************************** epoch  2  is finished. *********************************\n",
      "epoch: 3 step: 1 loss: 1.1876411 acc: 0.8635711669921875\n",
      "epoch: 3 step: 2 loss: 1.3053286 acc: 0.8325843811035156\n",
      "epoch: 3 step: 3 loss: 1.0758294 acc: 0.8668441772460938\n",
      "epoch: 3 step: 4 loss: 1.0556306 acc: 0.8530807495117188\n",
      "epoch: 3 step: 5 loss: 1.0373251 acc: 0.861083984375\n",
      "epoch: 3 step: 6 loss: 1.080528 acc: 0.8386993408203125\n",
      "epoch: 3 step: 7 loss: 1.0313696 acc: 0.8347320556640625\n",
      "epoch: 3 step: 8 loss: 1.5088737 acc: 0.7851715087890625\n",
      "epoch: 3 step: 9 loss: 1.1679845 acc: 0.8448486328125\n",
      "epoch: 3 step: 10 loss: 1.1032284 acc: 0.8459129333496094\n",
      "epoch: 3 step: 11 loss: 1.0612342 acc: 0.8742523193359375\n",
      "epoch: 3 step: 12 loss: 1.1203707 acc: 0.848419189453125\n",
      "epoch: 3 step: 13 loss: 1.1385335 acc: 0.8486518859863281\n",
      "epoch: 3 step: 14 loss: 1.1215459 acc: 0.8195037841796875\n",
      "epoch: 3 step: 15 loss: 1.1936957 acc: 0.8258209228515625\n",
      "epoch: 3 step: 16 loss: 0.9422465 acc: 0.8629074096679688\n",
      "epoch: 3 step: 17 loss: 1.1576173 acc: 0.8333168029785156\n",
      "epoch: 3 step: 18 loss: 1.2010121 acc: 0.8396568298339844\n",
      "epoch: 3 step: 19 loss: 1.1608299 acc: 0.8568305969238281\n",
      "epoch: 3 step: 20 loss: 1.1220268 acc: 0.8539276123046875\n",
      "epoch: 3 step: 21 loss: 0.9950479 acc: 0.8468437194824219\n",
      "epoch: 3 step: 22 loss: 1.1128927 acc: 0.8507804870605469\n",
      "epoch: 3 step: 23 loss: 0.99143326 acc: 0.8529586791992188\n",
      "epoch: 3 step: 24 loss: 1.0546806 acc: 0.85101318359375\n",
      "epoch: 3 step: 25 loss: 0.9258087 acc: 0.8712005615234375\n",
      "epoch: 3 step: 26 loss: 0.9758994 acc: 0.8821182250976562\n",
      "epoch: 3 step: 27 loss: 1.4541738 acc: 0.8096923828125\n",
      "epoch: 3 step: 28 loss: 1.2107391 acc: 0.8228340148925781\n",
      "epoch: 3 step: 29 loss: 1.014175 acc: 0.8563919067382812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 30 loss: 1.1561177 acc: 0.8604049682617188\n",
      "epoch: 3 step: 31 loss: 1.0485585 acc: 0.8527565002441406\n",
      "epoch: 3 step: 32 loss: 1.0396826 acc: 0.8629837036132812\n",
      "epoch: 3 step: 33 loss: 1.1603003 acc: 0.845306396484375\n",
      "epoch: 3 step: 34 loss: 1.1876383 acc: 0.8406791687011719\n",
      "epoch: 3 step: 35 loss: 1.1369189 acc: 0.8485641479492188\n",
      "epoch: 3 step: 36 loss: 1.1201053 acc: 0.8572349548339844\n",
      "epoch: 3 step: 37 loss: 1.1822735 acc: 0.8128089904785156\n",
      "epoch: 3 step: 38 loss: 1.032954 acc: 0.8546524047851562\n",
      "epoch: 3 step: 39 loss: 1.1387914 acc: 0.8071174621582031\n",
      "epoch: 3 step: 40 loss: 1.0256064 acc: 0.8303871154785156\n",
      "epoch: 3 step: 41 loss: 1.093803 acc: 0.8242340087890625\n",
      "epoch: 3 step: 42 loss: 1.1669551 acc: 0.8373374938964844\n",
      "epoch: 3 step: 43 loss: 1.3343627 acc: 0.8046417236328125\n",
      "epoch: 3 step: 44 loss: 1.0166636 acc: 0.8557662963867188\n",
      "epoch: 3 step: 45 loss: 0.81369543 acc: 0.8842506408691406\n",
      "epoch: 3 step: 46 loss: 1.2593306 acc: 0.8346328735351562\n",
      "epoch: 3 step: 47 loss: 1.360122 acc: 0.8438034057617188\n",
      "epoch: 3 step: 48 loss: 0.91731924 acc: 0.8851966857910156\n",
      "epoch: 3 step: 49 loss: 1.1110287 acc: 0.83404541015625\n",
      "epoch: 3 step: 50 loss: 1.0565901 acc: 0.8552780151367188\n",
      "epoch: 3 step: 51 loss: 1.1666821 acc: 0.8372879028320312\n",
      "epoch: 3 step: 52 loss: 1.0408361 acc: 0.8620376586914062\n",
      "epoch: 3 step: 53 loss: 0.87546384 acc: 0.8814315795898438\n",
      "epoch: 3 step: 54 loss: 1.0157759 acc: 0.8585739135742188\n",
      "epoch: 3 step: 55 loss: 0.94382626 acc: 0.8446807861328125\n",
      "epoch: 3 step: 56 loss: 1.1640546 acc: 0.8339385986328125\n",
      "epoch: 3 step: 57 loss: 1.0629107 acc: 0.8267593383789062\n",
      "epoch: 3 step: 58 loss: 1.3736212 acc: 0.8002738952636719\n",
      "epoch: 3 step: 59 loss: 1.0445873 acc: 0.8366317749023438\n",
      "epoch: 3 step: 60 loss: 1.0409149 acc: 0.8481559753417969\n",
      "epoch: 3 step: 61 loss: 1.2050571 acc: 0.8377609252929688\n",
      "epoch: 3 step: 62 loss: 1.0597993 acc: 0.8414649963378906\n",
      "epoch: 3 step: 63 loss: 1.1450468 acc: 0.8329238891601562\n",
      "epoch: 3 step: 64 loss: 1.0949425 acc: 0.8352699279785156\n",
      "epoch: 3 step: 65 loss: 1.3183101 acc: 0.8188629150390625\n",
      "epoch: 3 step: 66 loss: 0.96851486 acc: 0.8776702880859375\n",
      "epoch: 3 step: 67 loss: 0.9014306 acc: 0.8555946350097656\n",
      "epoch: 3 step: 68 loss: 0.97563356 acc: 0.8708724975585938\n",
      "epoch: 3 step: 69 loss: 1.138236 acc: 0.8574600219726562\n",
      "epoch: 3 step: 70 loss: 1.0542167 acc: 0.865386962890625\n",
      "epoch: 3 step: 71 loss: 1.1959407 acc: 0.8211631774902344\n",
      "epoch: 3 step: 72 loss: 1.0544045 acc: 0.8623466491699219\n",
      "epoch: 3 step: 73 loss: 0.9777138 acc: 0.8717613220214844\n",
      "epoch: 3 step: 74 loss: 1.0011731 acc: 0.8529815673828125\n",
      "epoch: 3 step: 75 loss: 1.1396906 acc: 0.83447265625\n",
      "epoch: 3 step: 76 loss: 1.1829226 acc: 0.8176612854003906\n",
      "epoch: 3 step: 77 loss: 1.0700502 acc: 0.8556747436523438\n",
      "epoch: 3 step: 78 loss: 0.98592824 acc: 0.8556976318359375\n",
      "epoch: 3 step: 79 loss: 1.2097491 acc: 0.8129806518554688\n",
      "epoch: 3 step: 80 loss: 1.0526483 acc: 0.8438453674316406\n",
      "epoch: 3 step: 81 loss: 0.91142005 acc: 0.8760490417480469\n",
      "epoch: 3 step: 82 loss: 0.916365 acc: 0.8746795654296875\n",
      "epoch: 3 step: 83 loss: 1.1158197 acc: 0.8638954162597656\n",
      "epoch: 3 step: 84 loss: 1.0412779 acc: 0.86322021484375\n",
      "epoch: 3 step: 85 loss: 0.8250623 acc: 0.8876152038574219\n",
      "epoch: 3 step: 86 loss: 0.9011146 acc: 0.8634147644042969\n",
      "epoch: 3 step: 87 loss: 1.0617412 acc: 0.8412246704101562\n",
      "epoch: 3 step: 88 loss: 1.0951216 acc: 0.845611572265625\n",
      "epoch: 3 step: 89 loss: 0.98744977 acc: 0.8555564880371094\n",
      "epoch: 3 step: 90 loss: 1.053765 acc: 0.8710365295410156\n",
      "epoch: 3 step: 91 loss: 0.9031265 acc: 0.8651084899902344\n",
      "epoch: 3 step: 92 loss: 1.1246337 acc: 0.8561210632324219\n",
      "epoch: 3 step: 93 loss: 0.9565496 acc: 0.8592300415039062\n",
      "epoch: 3 step: 94 loss: 0.9378501 acc: 0.8577079772949219\n",
      "epoch: 3 step: 95 loss: 0.89190817 acc: 0.871337890625\n",
      "epoch: 3 step: 96 loss: 1.2842909 acc: 0.8325271606445312\n",
      "epoch: 3 step: 97 loss: 0.8902447 acc: 0.878814697265625\n",
      "epoch: 3 step: 98 loss: 0.976813 acc: 0.8650054931640625\n",
      "epoch: 3 step: 99 loss: 1.10509 acc: 0.8350944519042969\n",
      "epoch: 3 step: 100 loss: 0.8997413 acc: 0.8527297973632812\n",
      "epoch: 3 step: 101 loss: 0.84720933 acc: 0.8620758056640625\n",
      "epoch: 3 step: 102 loss: 0.9767704 acc: 0.8635978698730469\n",
      "epoch: 3 step: 103 loss: 0.8187726 acc: 0.8853073120117188\n",
      "epoch: 3 step: 104 loss: 0.98685163 acc: 0.86431884765625\n",
      "epoch: 3 step: 105 loss: 0.99118733 acc: 0.8606834411621094\n",
      "epoch: 3 step: 106 loss: 1.1631073 acc: 0.8353462219238281\n",
      "epoch: 3 step: 107 loss: 0.9504702 acc: 0.8741607666015625\n",
      "epoch: 3 step: 108 loss: 0.82561386 acc: 0.9004592895507812\n",
      "epoch: 3 step: 109 loss: 0.8723835 acc: 0.8731040954589844\n",
      "epoch: 3 step: 110 loss: 0.88138825 acc: 0.8642463684082031\n",
      "epoch: 3 step: 111 loss: 1.0509028 acc: 0.834991455078125\n",
      "epoch: 3 step: 112 loss: 1.1446136 acc: 0.8477554321289062\n",
      "epoch: 3 step: 113 loss: 1.1730986 acc: 0.8238105773925781\n",
      "epoch: 3 step: 114 loss: 1.1404985 acc: 0.8316497802734375\n",
      "epoch: 3 step: 115 loss: 0.90853584 acc: 0.8588981628417969\n",
      "epoch: 3 step: 116 loss: 1.1755859 acc: 0.8323783874511719\n",
      "epoch: 3 step: 117 loss: 0.8266691 acc: 0.8610038757324219\n",
      "epoch: 3 step: 118 loss: 0.9323825 acc: 0.8653297424316406\n",
      "epoch: 3 step: 119 loss: 0.9826521 acc: 0.8563728332519531\n",
      "epoch: 3 step: 120 loss: 0.88826996 acc: 0.8754844665527344\n",
      "epoch: 3 step: 121 loss: 0.9020071 acc: 0.8548660278320312\n",
      "epoch: 3 step: 122 loss: 0.9802896 acc: 0.8555679321289062\n",
      "epoch: 3 step: 123 loss: 1.5192367 acc: 0.7834854125976562\n",
      "epoch: 3 step: 124 loss: 0.8151703 acc: 0.8531145368303571\n",
      "epoch: 3 validation_loss: 1.076 validation_dice: 0.2801617199621918\n",
      "epoch: 3 test_dataset dice: 0.2390899065686589\n",
      "time cost 0.5383722066879273 min\n",
      "dice_best: 0\n",
      "******************************** epoch  3  is finished. *********************************\n",
      "epoch: 4 step: 1 loss: 0.94729894 acc: 0.8547554016113281\n",
      "epoch: 4 step: 2 loss: 1.0261853 acc: 0.8673477172851562\n",
      "epoch: 4 step: 3 loss: 1.0446658 acc: 0.8427925109863281\n",
      "epoch: 4 step: 4 loss: 1.0981789 acc: 0.8581886291503906\n",
      "epoch: 4 step: 5 loss: 0.8940801 acc: 0.8538894653320312\n",
      "epoch: 4 step: 6 loss: 0.9201756 acc: 0.8635597229003906\n",
      "epoch: 4 step: 7 loss: 0.80937725 acc: 0.8690910339355469\n",
      "epoch: 4 step: 8 loss: 0.9372366 acc: 0.8703727722167969\n",
      "epoch: 4 step: 9 loss: 0.87304115 acc: 0.8718910217285156\n",
      "epoch: 4 step: 10 loss: 0.92503566 acc: 0.8706092834472656\n",
      "epoch: 4 step: 11 loss: 1.0954684 acc: 0.8542098999023438\n",
      "epoch: 4 step: 12 loss: 0.94971144 acc: 0.8687858581542969\n",
      "epoch: 4 step: 13 loss: 0.88640237 acc: 0.8630104064941406\n",
      "epoch: 4 step: 14 loss: 0.9860137 acc: 0.8503227233886719\n",
      "epoch: 4 step: 15 loss: 1.038547 acc: 0.8543777465820312\n",
      "epoch: 4 step: 16 loss: 0.8713823 acc: 0.8744239807128906\n",
      "epoch: 4 step: 17 loss: 1.0314858 acc: 0.8356094360351562\n",
      "epoch: 4 step: 18 loss: 0.8818164 acc: 0.8222885131835938\n",
      "epoch: 4 step: 19 loss: 1.004773 acc: 0.8368415832519531\n",
      "epoch: 4 step: 20 loss: 0.7999835 acc: 0.8588523864746094\n",
      "epoch: 4 step: 21 loss: 0.7728473 acc: 0.8599700927734375\n",
      "epoch: 4 step: 22 loss: 0.7540499 acc: 0.8907089233398438\n",
      "epoch: 4 step: 23 loss: 1.1163349 acc: 0.8491668701171875\n",
      "epoch: 4 step: 24 loss: 0.88399374 acc: 0.8796005249023438\n",
      "epoch: 4 step: 25 loss: 0.92809427 acc: 0.8845176696777344\n",
      "epoch: 4 step: 26 loss: 1.0323387 acc: 0.8476066589355469\n",
      "epoch: 4 step: 27 loss: 0.8199565 acc: 0.8808822631835938\n",
      "epoch: 4 step: 28 loss: 0.86452776 acc: 0.8791389465332031\n",
      "epoch: 4 step: 29 loss: 0.92822224 acc: 0.8412284851074219\n",
      "epoch: 4 step: 30 loss: 0.9038175 acc: 0.8527069091796875\n",
      "epoch: 4 step: 31 loss: 0.87963647 acc: 0.8566093444824219\n",
      "epoch: 4 step: 32 loss: 0.9999497 acc: 0.8374862670898438\n",
      "epoch: 4 step: 33 loss: 1.0014029 acc: 0.8430290222167969\n",
      "epoch: 4 step: 34 loss: 0.74573505 acc: 0.8790664672851562\n",
      "epoch: 4 step: 35 loss: 0.84027797 acc: 0.854461669921875\n",
      "epoch: 4 step: 36 loss: 1.0060872 acc: 0.8438796997070312\n",
      "epoch: 4 step: 37 loss: 0.6904236 acc: 0.8830299377441406\n",
      "epoch: 4 step: 38 loss: 0.80562145 acc: 0.876312255859375\n",
      "epoch: 4 step: 39 loss: 0.95865244 acc: 0.8566322326660156\n",
      "epoch: 4 step: 40 loss: 0.7957992 acc: 0.8800239562988281\n",
      "epoch: 4 step: 41 loss: 1.1068203 acc: 0.8554039001464844\n",
      "epoch: 4 step: 42 loss: 0.78106797 acc: 0.8976707458496094\n",
      "epoch: 4 step: 43 loss: 0.9115314 acc: 0.8831825256347656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 44 loss: 0.86101156 acc: 0.8959007263183594\n",
      "epoch: 4 step: 45 loss: 0.87797284 acc: 0.8780441284179688\n",
      "epoch: 4 step: 46 loss: 0.98382705 acc: 0.8286781311035156\n",
      "epoch: 4 step: 47 loss: 0.927577 acc: 0.856536865234375\n",
      "epoch: 4 step: 48 loss: 0.7606202 acc: 0.852325439453125\n",
      "epoch: 4 step: 49 loss: 0.86132795 acc: 0.8448715209960938\n",
      "epoch: 4 step: 50 loss: 0.779834 acc: 0.8828659057617188\n",
      "epoch: 4 step: 51 loss: 0.7894187 acc: 0.87860107421875\n",
      "epoch: 4 step: 52 loss: 1.0103924 acc: 0.8535995483398438\n",
      "epoch: 4 step: 53 loss: 0.75614905 acc: 0.8640060424804688\n",
      "epoch: 4 step: 54 loss: 0.9011123 acc: 0.880462646484375\n",
      "epoch: 4 step: 55 loss: 0.7293961 acc: 0.8903388977050781\n",
      "epoch: 4 step: 56 loss: 0.7875085 acc: 0.8869743347167969\n",
      "epoch: 4 step: 57 loss: 0.9010218 acc: 0.8597564697265625\n",
      "epoch: 4 step: 58 loss: 0.8326334 acc: 0.8781776428222656\n",
      "epoch: 4 step: 59 loss: 0.8393449 acc: 0.8815345764160156\n",
      "epoch: 4 step: 60 loss: 0.93005353 acc: 0.8713264465332031\n",
      "epoch: 4 step: 61 loss: 0.8286695 acc: 0.8838233947753906\n",
      "epoch: 4 step: 62 loss: 0.85780877 acc: 0.8509483337402344\n",
      "epoch: 4 step: 63 loss: 0.84375894 acc: 0.8575096130371094\n",
      "epoch: 4 step: 64 loss: 1.0626025 acc: 0.8472442626953125\n",
      "epoch: 4 step: 65 loss: 0.755888 acc: 0.8726921081542969\n",
      "epoch: 4 step: 66 loss: 0.77521694 acc: 0.8678703308105469\n",
      "epoch: 4 step: 67 loss: 0.93840045 acc: 0.8503532409667969\n",
      "epoch: 4 step: 68 loss: 0.7762451 acc: 0.886962890625\n",
      "epoch: 4 step: 69 loss: 0.7723174 acc: 0.8872261047363281\n",
      "epoch: 4 step: 70 loss: 0.90138745 acc: 0.8750228881835938\n",
      "epoch: 4 step: 71 loss: 0.66708577 acc: 0.9001083374023438\n",
      "epoch: 4 step: 72 loss: 0.7510945 acc: 0.8795509338378906\n",
      "epoch: 4 step: 73 loss: 0.9871178 acc: 0.8321418762207031\n",
      "epoch: 4 step: 74 loss: 0.7707958 acc: 0.8847007751464844\n",
      "epoch: 4 step: 75 loss: 0.78649 acc: 0.8724098205566406\n",
      "epoch: 4 step: 76 loss: 0.84937483 acc: 0.8684196472167969\n",
      "epoch: 4 step: 77 loss: 1.2424252 acc: 0.7934303283691406\n",
      "epoch: 4 step: 78 loss: 0.8744647 acc: 0.8571701049804688\n",
      "epoch: 4 step: 79 loss: 0.7748924 acc: 0.8758163452148438\n",
      "epoch: 4 step: 80 loss: 0.92678046 acc: 0.8863029479980469\n",
      "epoch: 4 step: 81 loss: 0.7910895 acc: 0.8683052062988281\n",
      "epoch: 4 step: 82 loss: 0.8261211 acc: 0.8540916442871094\n",
      "epoch: 4 step: 83 loss: 0.845318 acc: 0.8541450500488281\n",
      "epoch: 4 step: 84 loss: 0.9188351 acc: 0.8505058288574219\n",
      "epoch: 4 step: 85 loss: 0.89387935 acc: 0.8708992004394531\n",
      "epoch: 4 step: 86 loss: 0.74602675 acc: 0.8868827819824219\n",
      "epoch: 4 step: 87 loss: 0.7601697 acc: 0.873931884765625\n",
      "epoch: 4 step: 88 loss: 0.67434436 acc: 0.8843421936035156\n",
      "epoch: 4 step: 89 loss: 0.80483454 acc: 0.8485374450683594\n",
      "epoch: 4 step: 90 loss: 0.91992426 acc: 0.8584823608398438\n",
      "epoch: 4 step: 91 loss: 0.810492 acc: 0.8677940368652344\n",
      "epoch: 4 step: 92 loss: 0.899617 acc: 0.8636817932128906\n",
      "epoch: 4 step: 93 loss: 0.8069762 acc: 0.8691902160644531\n",
      "epoch: 4 step: 94 loss: 0.8194201 acc: 0.8736686706542969\n",
      "epoch: 4 step: 95 loss: 1.1138518 acc: 0.8110923767089844\n",
      "epoch: 4 step: 96 loss: 0.74943644 acc: 0.8841590881347656\n",
      "epoch: 4 step: 97 loss: 0.85455626 acc: 0.8850746154785156\n",
      "epoch: 4 step: 98 loss: 0.8338673 acc: 0.8727340698242188\n",
      "epoch: 4 step: 99 loss: 0.9766064 acc: 0.8587226867675781\n",
      "epoch: 4 step: 100 loss: 0.9022522 acc: 0.8628349304199219\n",
      "epoch: 4 step: 101 loss: 0.79799855 acc: 0.8761329650878906\n",
      "epoch: 4 step: 102 loss: 0.73712677 acc: 0.8790473937988281\n",
      "epoch: 4 step: 103 loss: 0.6283138 acc: 0.8814697265625\n",
      "epoch: 4 step: 104 loss: 0.76322025 acc: 0.8691940307617188\n",
      "epoch: 4 step: 105 loss: 0.79439396 acc: 0.867523193359375\n",
      "epoch: 4 step: 106 loss: 0.81329465 acc: 0.8580322265625\n",
      "epoch: 4 step: 107 loss: 0.6489541 acc: 0.887054443359375\n",
      "epoch: 4 step: 108 loss: 0.81860995 acc: 0.8801460266113281\n",
      "epoch: 4 step: 109 loss: 0.6775872 acc: 0.8934593200683594\n",
      "epoch: 4 step: 110 loss: 0.82323647 acc: 0.8736648559570312\n",
      "epoch: 4 step: 111 loss: 0.9219408 acc: 0.8531036376953125\n",
      "epoch: 4 step: 112 loss: 1.0862654 acc: 0.8679237365722656\n",
      "epoch: 4 step: 113 loss: 0.92062205 acc: 0.8580970764160156\n",
      "epoch: 4 step: 114 loss: 0.89587605 acc: 0.8410415649414062\n",
      "epoch: 4 step: 115 loss: 0.8683183 acc: 0.8752326965332031\n",
      "epoch: 4 step: 116 loss: 0.7637435 acc: 0.8602638244628906\n",
      "epoch: 4 step: 117 loss: 0.73871315 acc: 0.8737678527832031\n",
      "epoch: 4 step: 118 loss: 0.83830285 acc: 0.8630561828613281\n",
      "epoch: 4 step: 119 loss: 0.9323139 acc: 0.8468818664550781\n",
      "epoch: 4 step: 120 loss: 0.9185517 acc: 0.8512420654296875\n",
      "epoch: 4 step: 121 loss: 0.9994946 acc: 0.830535888671875\n",
      "epoch: 4 step: 122 loss: 0.8032314 acc: 0.8629608154296875\n",
      "epoch: 4 step: 123 loss: 1.0275196 acc: 0.8417396545410156\n",
      "epoch: 4 step: 124 loss: 1.0138695 acc: 0.8746773856026786\n",
      "epoch: 4 validation_loss: 1.107 validation_dice: 0.2598336949875769\n",
      "epoch: 4 test_dataset dice: 0.2433782589609818\n",
      "time cost 0.5357112089792887 min\n",
      "dice_best: 0\n",
      "******************************** epoch  4  is finished. *********************************\n",
      "epoch: 5 step: 1 loss: 0.92477465 acc: 0.8538246154785156\n",
      "epoch: 5 step: 2 loss: 0.79347175 acc: 0.864501953125\n",
      "epoch: 5 step: 3 loss: 0.89642394 acc: 0.8767280578613281\n",
      "epoch: 5 step: 4 loss: 0.8927867 acc: 0.8591232299804688\n",
      "epoch: 5 step: 5 loss: 0.9364879 acc: 0.8689994812011719\n",
      "epoch: 5 step: 6 loss: 0.9580037 acc: 0.8625373840332031\n",
      "epoch: 5 step: 7 loss: 0.81006515 acc: 0.8440628051757812\n",
      "epoch: 5 step: 8 loss: 0.84314334 acc: 0.8572921752929688\n",
      "epoch: 5 step: 9 loss: 0.67479646 acc: 0.8780975341796875\n",
      "epoch: 5 step: 10 loss: 0.78218687 acc: 0.8519706726074219\n",
      "epoch: 5 step: 11 loss: 0.7703808 acc: 0.85772705078125\n",
      "epoch: 5 step: 12 loss: 1.2966796 acc: 0.8384780883789062\n",
      "epoch: 5 step: 13 loss: 0.7747496 acc: 0.8785018920898438\n",
      "epoch: 5 step: 14 loss: 0.7911357 acc: 0.8761329650878906\n",
      "epoch: 5 step: 15 loss: 1.0144839 acc: 0.8567008972167969\n",
      "epoch: 5 step: 16 loss: 0.9156989 acc: 0.8656539916992188\n",
      "epoch: 5 step: 17 loss: 1.0472833 acc: 0.8531074523925781\n",
      "epoch: 5 step: 18 loss: 0.7519045 acc: 0.8827285766601562\n",
      "epoch: 5 step: 19 loss: 0.8462354 acc: 0.8788032531738281\n",
      "epoch: 5 step: 20 loss: 0.7110241 acc: 0.8761444091796875\n",
      "epoch: 5 step: 21 loss: 0.8805841 acc: 0.8431434631347656\n",
      "epoch: 5 step: 22 loss: 0.81477815 acc: 0.8808822631835938\n",
      "epoch: 5 step: 23 loss: 0.7015527 acc: 0.8834495544433594\n",
      "epoch: 5 step: 24 loss: 0.64755213 acc: 0.8891830444335938\n",
      "epoch: 5 step: 25 loss: 0.9204331 acc: 0.8551788330078125\n",
      "epoch: 5 step: 26 loss: 0.81966686 acc: 0.88916015625\n",
      "epoch: 5 step: 27 loss: 0.6825069 acc: 0.8907394409179688\n",
      "epoch: 5 step: 28 loss: 0.80401134 acc: 0.8747787475585938\n",
      "epoch: 5 step: 29 loss: 0.8123473 acc: 0.8847618103027344\n",
      "epoch: 5 step: 30 loss: 0.7615548 acc: 0.8734397888183594\n",
      "epoch: 5 step: 31 loss: 0.75388813 acc: 0.8948020935058594\n",
      "epoch: 5 step: 32 loss: 0.73895645 acc: 0.8687324523925781\n",
      "epoch: 5 step: 33 loss: 0.721164 acc: 0.9038505554199219\n",
      "epoch: 5 step: 34 loss: 0.8055473 acc: 0.8995513916015625\n",
      "epoch: 5 step: 35 loss: 0.87513757 acc: 0.8564872741699219\n",
      "epoch: 5 step: 36 loss: 0.71336645 acc: 0.8951988220214844\n",
      "epoch: 5 step: 37 loss: 0.7188494 acc: 0.8791465759277344\n",
      "epoch: 5 step: 38 loss: 0.57441235 acc: 0.8925323486328125\n",
      "epoch: 5 step: 39 loss: 0.9116053 acc: 0.8250923156738281\n",
      "epoch: 5 step: 40 loss: 0.6315686 acc: 0.88897705078125\n",
      "epoch: 5 step: 41 loss: 0.69064975 acc: 0.8675727844238281\n",
      "epoch: 5 step: 42 loss: 0.5643585 acc: 0.9059219360351562\n",
      "epoch: 5 step: 43 loss: 0.6834983 acc: 0.888641357421875\n",
      "epoch: 5 step: 44 loss: 0.6784033 acc: 0.8740997314453125\n",
      "epoch: 5 step: 45 loss: 0.7151086 acc: 0.8628463745117188\n",
      "epoch: 5 step: 46 loss: 0.63742137 acc: 0.8901596069335938\n",
      "epoch: 5 step: 47 loss: 0.7030531 acc: 0.8743095397949219\n",
      "epoch: 5 step: 48 loss: 0.8552601 acc: 0.8784408569335938\n",
      "epoch: 5 step: 49 loss: 0.65375453 acc: 0.8812141418457031\n",
      "epoch: 5 step: 50 loss: 0.8200218 acc: 0.8723983764648438\n",
      "epoch: 5 step: 51 loss: 0.65649885 acc: 0.8729171752929688\n",
      "epoch: 5 step: 52 loss: 0.58104897 acc: 0.8989028930664062\n",
      "epoch: 5 step: 53 loss: 0.5875812 acc: 0.8809318542480469\n",
      "epoch: 5 step: 54 loss: 0.6756988 acc: 0.9041519165039062\n",
      "epoch: 5 step: 55 loss: 0.6008004 acc: 0.8959388732910156\n",
      "epoch: 5 step: 56 loss: 0.73692787 acc: 0.8873367309570312\n",
      "epoch: 5 step: 57 loss: 0.5767069 acc: 0.9047431945800781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 58 loss: 0.6962293 acc: 0.8964271545410156\n",
      "epoch: 5 step: 59 loss: 0.71958077 acc: 0.8624038696289062\n",
      "epoch: 5 step: 60 loss: 0.7390132 acc: 0.862274169921875\n",
      "epoch: 5 step: 61 loss: 0.93282795 acc: 0.8542861938476562\n",
      "epoch: 5 step: 62 loss: 0.7263153 acc: 0.8544845581054688\n",
      "epoch: 5 step: 63 loss: 0.76541585 acc: 0.88507080078125\n",
      "epoch: 5 step: 64 loss: 0.7848899 acc: 0.8690223693847656\n",
      "epoch: 5 step: 65 loss: 0.7811931 acc: 0.8671112060546875\n",
      "epoch: 5 step: 66 loss: 0.93606067 acc: 0.8389396667480469\n",
      "epoch: 5 step: 67 loss: 0.8334258 acc: 0.8783035278320312\n",
      "epoch: 5 step: 68 loss: 0.8627169 acc: 0.8576583862304688\n",
      "epoch: 5 step: 69 loss: 0.69267565 acc: 0.8970184326171875\n",
      "epoch: 5 step: 70 loss: 0.6550018 acc: 0.9031219482421875\n",
      "epoch: 5 step: 71 loss: 0.64036554 acc: 0.89739990234375\n",
      "epoch: 5 step: 72 loss: 0.5789378 acc: 0.9101219177246094\n",
      "epoch: 5 step: 73 loss: 0.7296651 acc: 0.8789482116699219\n",
      "epoch: 5 step: 74 loss: 0.5955131 acc: 0.8974952697753906\n",
      "epoch: 5 step: 75 loss: 0.80043966 acc: 0.8703651428222656\n",
      "epoch: 5 step: 76 loss: 0.5929984 acc: 0.9035263061523438\n",
      "epoch: 5 step: 77 loss: 0.6812646 acc: 0.8823356628417969\n",
      "epoch: 5 step: 78 loss: 0.63748646 acc: 0.9176979064941406\n",
      "epoch: 5 step: 79 loss: 0.6031082 acc: 0.9011955261230469\n",
      "epoch: 5 step: 80 loss: 0.69786984 acc: 0.9039421081542969\n",
      "epoch: 5 step: 81 loss: 0.5538236 acc: 0.916259765625\n",
      "epoch: 5 step: 82 loss: 0.58265716 acc: 0.9006767272949219\n",
      "epoch: 5 step: 83 loss: 0.7975895 acc: 0.8816261291503906\n",
      "epoch: 5 step: 84 loss: 0.7075754 acc: 0.8951301574707031\n",
      "epoch: 5 step: 85 loss: 0.6783542 acc: 0.8905792236328125\n",
      "epoch: 5 step: 86 loss: 0.98822457 acc: 0.8591804504394531\n",
      "epoch: 5 step: 87 loss: 0.7335723 acc: 0.8900947570800781\n",
      "epoch: 5 step: 88 loss: 0.5190273 acc: 0.9042129516601562\n",
      "epoch: 5 step: 89 loss: 0.7522292 acc: 0.8654289245605469\n",
      "epoch: 5 step: 90 loss: 0.7504047 acc: 0.8604240417480469\n",
      "epoch: 5 step: 91 loss: 0.55762553 acc: 0.8930435180664062\n",
      "epoch: 5 step: 92 loss: 0.7042884 acc: 0.8901596069335938\n",
      "epoch: 5 step: 93 loss: 0.5842394 acc: 0.9001655578613281\n",
      "epoch: 5 step: 94 loss: 0.75373584 acc: 0.8796539306640625\n",
      "epoch: 5 step: 95 loss: 0.9923654 acc: 0.8622665405273438\n",
      "epoch: 5 step: 96 loss: 0.7524747 acc: 0.8781700134277344\n",
      "epoch: 5 step: 97 loss: 0.62078285 acc: 0.8723869323730469\n",
      "epoch: 5 step: 98 loss: 0.72032124 acc: 0.8733024597167969\n",
      "epoch: 5 step: 99 loss: 0.6708132 acc: 0.8469657897949219\n",
      "epoch: 5 step: 100 loss: 0.5639669 acc: 0.8924179077148438\n",
      "epoch: 5 step: 101 loss: 0.75341845 acc: 0.8564682006835938\n",
      "epoch: 5 step: 102 loss: 0.7694796 acc: 0.8260421752929688\n",
      "epoch: 5 step: 103 loss: 0.58257985 acc: 0.881317138671875\n",
      "epoch: 5 step: 104 loss: 0.75872564 acc: 0.8727264404296875\n",
      "epoch: 5 step: 105 loss: 0.54692715 acc: 0.895599365234375\n",
      "epoch: 5 step: 106 loss: 0.6841583 acc: 0.8871116638183594\n",
      "epoch: 5 step: 107 loss: 0.671307 acc: 0.9016227722167969\n",
      "epoch: 5 step: 108 loss: 0.84456134 acc: 0.8763313293457031\n",
      "epoch: 5 step: 109 loss: 0.67689925 acc: 0.8952064514160156\n",
      "epoch: 5 step: 110 loss: 0.7929342 acc: 0.8841476440429688\n",
      "epoch: 5 step: 111 loss: 0.7383522 acc: 0.8755340576171875\n",
      "epoch: 5 step: 112 loss: 0.6658516 acc: 0.8760871887207031\n",
      "epoch: 5 step: 113 loss: 0.706799 acc: 0.8641281127929688\n",
      "epoch: 5 step: 114 loss: 0.6434667 acc: 0.8644599914550781\n",
      "epoch: 5 step: 115 loss: 0.74422604 acc: 0.8564720153808594\n",
      "epoch: 5 step: 116 loss: 0.80042785 acc: 0.8406715393066406\n",
      "epoch: 5 step: 117 loss: 1.1404132 acc: 0.8452224731445312\n",
      "epoch: 5 step: 118 loss: 0.66910946 acc: 0.8790130615234375\n",
      "epoch: 5 step: 119 loss: 0.8073523 acc: 0.8643302917480469\n",
      "epoch: 5 step: 120 loss: 0.7590502 acc: 0.869873046875\n",
      "epoch: 5 step: 121 loss: 0.49916932 acc: 0.8840065002441406\n",
      "epoch: 5 step: 122 loss: 0.68584806 acc: 0.8942718505859375\n",
      "epoch: 5 step: 123 loss: 0.76260906 acc: 0.8687744140625\n",
      "epoch: 5 step: 124 loss: 0.7307726 acc: 0.8927089146205357\n",
      "epoch: 5 validation_loss: 0.795 validation_dice: 0.3984539419882461\n",
      "epoch: 5 test_dataset dice: 0.36591529442911885\n",
      "time cost 0.5362165133158366 min\n",
      "dice_best: 0\n",
      "******************************** epoch  5  is finished. *********************************\n",
      "epoch: 6 step: 1 loss: 0.6871301 acc: 0.8805084228515625\n",
      "epoch: 6 step: 2 loss: 0.6664181 acc: 0.8877792358398438\n",
      "epoch: 6 step: 3 loss: 0.70721644 acc: 0.8643951416015625\n",
      "epoch: 6 step: 4 loss: 0.704311 acc: 0.8926048278808594\n",
      "epoch: 6 step: 5 loss: 0.6285635 acc: 0.8892364501953125\n",
      "epoch: 6 step: 6 loss: 0.66453296 acc: 0.8983116149902344\n",
      "epoch: 6 step: 7 loss: 0.69845086 acc: 0.8852500915527344\n",
      "epoch: 6 step: 8 loss: 0.6819633 acc: 0.8860511779785156\n",
      "epoch: 6 step: 9 loss: 0.62396514 acc: 0.90386962890625\n",
      "epoch: 6 step: 10 loss: 0.63443816 acc: 0.8996467590332031\n",
      "epoch: 6 step: 11 loss: 0.56252927 acc: 0.9085578918457031\n",
      "epoch: 6 step: 12 loss: 0.7779945 acc: 0.8694000244140625\n",
      "epoch: 6 step: 13 loss: 0.6807613 acc: 0.8859710693359375\n",
      "epoch: 6 step: 14 loss: 0.6616236 acc: 0.8863983154296875\n",
      "epoch: 6 step: 15 loss: 0.62457687 acc: 0.9104194641113281\n",
      "epoch: 6 step: 16 loss: 0.6704637 acc: 0.8910865783691406\n",
      "epoch: 6 step: 17 loss: 0.55585325 acc: 0.9130630493164062\n",
      "epoch: 6 step: 18 loss: 0.72029763 acc: 0.8682937622070312\n",
      "epoch: 6 step: 19 loss: 0.59012717 acc: 0.9031906127929688\n",
      "epoch: 6 step: 20 loss: 0.7154853 acc: 0.8869285583496094\n",
      "epoch: 6 step: 21 loss: 0.49602938 acc: 0.8997077941894531\n",
      "epoch: 6 step: 22 loss: 0.5710554 acc: 0.8849945068359375\n",
      "epoch: 6 step: 23 loss: 0.5476325 acc: 0.890777587890625\n",
      "epoch: 6 step: 24 loss: 0.6089474 acc: 0.8798904418945312\n",
      "epoch: 6 step: 25 loss: 0.5803147 acc: 0.8914031982421875\n",
      "epoch: 6 step: 26 loss: 0.6167919 acc: 0.8970603942871094\n",
      "epoch: 6 step: 27 loss: 0.71699554 acc: 0.8718719482421875\n",
      "epoch: 6 step: 28 loss: 0.7617555 acc: 0.8720130920410156\n",
      "epoch: 6 step: 29 loss: 0.63508826 acc: 0.8904991149902344\n",
      "epoch: 6 step: 30 loss: 0.6301373 acc: 0.9117164611816406\n",
      "epoch: 6 step: 31 loss: 0.54248655 acc: 0.8973541259765625\n",
      "epoch: 6 step: 32 loss: 0.6619476 acc: 0.8844642639160156\n",
      "epoch: 6 step: 33 loss: 0.54610074 acc: 0.8895835876464844\n",
      "epoch: 6 step: 34 loss: 0.61905026 acc: 0.8948974609375\n",
      "epoch: 6 step: 35 loss: 0.6220685 acc: 0.8965988159179688\n",
      "epoch: 6 step: 36 loss: 0.48871696 acc: 0.9170722961425781\n",
      "epoch: 6 step: 37 loss: 0.76881427 acc: 0.8865966796875\n",
      "epoch: 6 step: 38 loss: 0.6591335 acc: 0.8900947570800781\n",
      "epoch: 6 step: 39 loss: 0.57843935 acc: 0.9012336730957031\n",
      "epoch: 6 step: 40 loss: 0.5477349 acc: 0.9175491333007812\n",
      "epoch: 6 step: 41 loss: 0.8714966 acc: 0.8766136169433594\n",
      "epoch: 6 step: 42 loss: 0.6140101 acc: 0.9115982055664062\n",
      "epoch: 6 step: 43 loss: 0.53485984 acc: 0.9135322570800781\n",
      "epoch: 6 step: 44 loss: 0.7013702 acc: 0.8720283508300781\n",
      "epoch: 6 step: 45 loss: 0.6799341 acc: 0.8900642395019531\n",
      "epoch: 6 step: 46 loss: 0.6584473 acc: 0.85418701171875\n",
      "epoch: 6 step: 47 loss: 0.51067144 acc: 0.9108810424804688\n",
      "epoch: 6 step: 48 loss: 0.60883427 acc: 0.8959732055664062\n",
      "epoch: 6 step: 49 loss: 0.65473735 acc: 0.8895988464355469\n",
      "epoch: 6 step: 50 loss: 0.5680775 acc: 0.8917579650878906\n",
      "epoch: 6 step: 51 loss: 0.5964268 acc: 0.8999481201171875\n",
      "epoch: 6 step: 52 loss: 0.7800113 acc: 0.890045166015625\n",
      "epoch: 6 step: 53 loss: 0.86147064 acc: 0.8625106811523438\n",
      "epoch: 6 step: 54 loss: 0.98543745 acc: 0.8615989685058594\n",
      "epoch: 6 step: 55 loss: 0.55917364 acc: 0.9048080444335938\n",
      "epoch: 6 step: 56 loss: 0.61865354 acc: 0.8994903564453125\n",
      "epoch: 6 step: 57 loss: 0.6777284 acc: 0.8946495056152344\n",
      "epoch: 6 step: 58 loss: 0.936187 acc: 0.8442230224609375\n",
      "epoch: 6 step: 59 loss: 0.49318132 acc: 0.9108352661132812\n",
      "epoch: 6 step: 60 loss: 0.7211369 acc: 0.8659439086914062\n",
      "epoch: 6 step: 61 loss: 0.67363286 acc: 0.8888626098632812\n",
      "epoch: 6 step: 62 loss: 0.8848906 acc: 0.8484344482421875\n",
      "epoch: 6 step: 63 loss: 0.5695069 acc: 0.8801765441894531\n",
      "epoch: 6 step: 64 loss: 0.6566326 acc: 0.8902206420898438\n",
      "epoch: 6 step: 65 loss: 0.65101445 acc: 0.8648605346679688\n",
      "epoch: 6 step: 66 loss: 0.59243304 acc: 0.8746681213378906\n",
      "epoch: 6 step: 67 loss: 0.6733743 acc: 0.8794136047363281\n",
      "epoch: 6 step: 68 loss: 0.7364952 acc: 0.88275146484375\n",
      "epoch: 6 step: 69 loss: 0.74228233 acc: 0.870941162109375\n",
      "epoch: 6 step: 70 loss: 0.6218945 acc: 0.8881263732910156\n",
      "epoch: 6 step: 71 loss: 0.86505705 acc: 0.8696174621582031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 72 loss: 0.49950606 acc: 0.9089813232421875\n",
      "epoch: 6 step: 73 loss: 0.5522736 acc: 0.9071502685546875\n",
      "epoch: 6 step: 74 loss: 0.6357811 acc: 0.8886871337890625\n",
      "epoch: 6 step: 75 loss: 0.5419837 acc: 0.9015998840332031\n",
      "epoch: 6 step: 76 loss: 0.5425372 acc: 0.9095802307128906\n",
      "epoch: 6 step: 77 loss: 0.575896 acc: 0.8942947387695312\n",
      "epoch: 6 step: 78 loss: 0.43938026 acc: 0.930267333984375\n",
      "epoch: 6 step: 79 loss: 0.48215067 acc: 0.9169349670410156\n",
      "epoch: 6 step: 80 loss: 0.667869 acc: 0.8990364074707031\n",
      "epoch: 6 step: 81 loss: 0.6422096 acc: 0.8888740539550781\n",
      "epoch: 6 step: 82 loss: 0.58956504 acc: 0.8901214599609375\n",
      "epoch: 6 step: 83 loss: 0.5882115 acc: 0.8890419006347656\n",
      "epoch: 6 step: 84 loss: 0.50899553 acc: 0.8921470642089844\n",
      "epoch: 6 step: 85 loss: 0.61515045 acc: 0.8864822387695312\n",
      "epoch: 6 step: 86 loss: 0.54918355 acc: 0.9004096984863281\n",
      "epoch: 6 step: 87 loss: 0.59864074 acc: 0.9030914306640625\n",
      "epoch: 6 step: 88 loss: 0.55321085 acc: 0.8946342468261719\n",
      "epoch: 6 step: 89 loss: 0.51696527 acc: 0.8949012756347656\n",
      "epoch: 6 step: 90 loss: 0.6155983 acc: 0.8872222900390625\n",
      "epoch: 6 step: 91 loss: 0.4681056 acc: 0.8967132568359375\n",
      "epoch: 6 step: 92 loss: 0.5051261 acc: 0.8922920227050781\n",
      "epoch: 6 step: 93 loss: 0.5889737 acc: 0.8782424926757812\n",
      "epoch: 6 step: 94 loss: 0.47252598 acc: 0.9075393676757812\n",
      "epoch: 6 step: 95 loss: 0.54642016 acc: 0.8971977233886719\n",
      "epoch: 6 step: 96 loss: 0.4499113 acc: 0.9186058044433594\n",
      "epoch: 6 step: 97 loss: 0.57500803 acc: 0.8945655822753906\n",
      "epoch: 6 step: 98 loss: 0.7350759 acc: 0.8703575134277344\n",
      "epoch: 6 step: 99 loss: 0.4942869 acc: 0.9136619567871094\n",
      "epoch: 6 step: 100 loss: 0.45388013 acc: 0.9205093383789062\n",
      "epoch: 6 step: 101 loss: 0.4918445 acc: 0.9240875244140625\n",
      "epoch: 6 step: 102 loss: 0.63037086 acc: 0.8875083923339844\n",
      "epoch: 6 step: 103 loss: 0.5833864 acc: 0.8935050964355469\n",
      "epoch: 6 step: 104 loss: 0.6350827 acc: 0.8998947143554688\n",
      "epoch: 6 step: 105 loss: 0.7283548 acc: 0.8607139587402344\n",
      "epoch: 6 step: 106 loss: 0.7603911 acc: 0.8611221313476562\n",
      "epoch: 6 step: 107 loss: 0.46518633 acc: 0.9043083190917969\n",
      "epoch: 6 step: 108 loss: 0.7816578 acc: 0.8406906127929688\n",
      "epoch: 6 step: 109 loss: 0.46498463 acc: 0.8990707397460938\n",
      "epoch: 6 step: 110 loss: 0.7384581 acc: 0.8641853332519531\n",
      "epoch: 6 step: 111 loss: 0.58150285 acc: 0.8887481689453125\n",
      "epoch: 6 step: 112 loss: 0.6743384 acc: 0.8859901428222656\n",
      "epoch: 6 step: 113 loss: 0.68940777 acc: 0.8703804016113281\n",
      "epoch: 6 step: 114 loss: 0.57119256 acc: 0.9096641540527344\n",
      "epoch: 6 step: 115 loss: 0.6024971 acc: 0.9071922302246094\n",
      "epoch: 6 step: 116 loss: 0.49914038 acc: 0.9012107849121094\n",
      "epoch: 6 step: 117 loss: 0.53173804 acc: 0.9097480773925781\n",
      "epoch: 6 step: 118 loss: 0.64039004 acc: 0.8804893493652344\n",
      "epoch: 6 step: 119 loss: 0.5754513 acc: 0.89251708984375\n",
      "epoch: 6 step: 120 loss: 0.48312935 acc: 0.9026718139648438\n",
      "epoch: 6 step: 121 loss: 0.7821591 acc: 0.8634300231933594\n",
      "epoch: 6 step: 122 loss: 0.6342008 acc: 0.8746719360351562\n",
      "epoch: 6 step: 123 loss: 0.5362087 acc: 0.9023475646972656\n",
      "epoch: 6 step: 124 loss: 0.7491015 acc: 0.8738752092633929\n",
      "epoch: 6 validation_loss: 0.757 validation_dice: 0.45861621073441994\n",
      "epoch: 6 test_dataset dice: 0.38001606041902525\n",
      "time cost 0.536007817586263 min\n",
      "dice_best: 0\n",
      "******************************** epoch  6  is finished. *********************************\n",
      "epoch: 7 step: 1 loss: 0.53890747 acc: 0.8937835693359375\n",
      "epoch: 7 step: 2 loss: 0.5925243 acc: 0.8788185119628906\n",
      "epoch: 7 step: 3 loss: 0.53212506 acc: 0.9089508056640625\n",
      "epoch: 7 step: 4 loss: 0.74118954 acc: 0.8640975952148438\n",
      "epoch: 7 step: 5 loss: 0.42967874 acc: 0.9159393310546875\n",
      "epoch: 7 step: 6 loss: 0.5166425 acc: 0.9027519226074219\n",
      "epoch: 7 step: 7 loss: 0.536534 acc: 0.9221916198730469\n",
      "epoch: 7 step: 8 loss: 0.6581898 acc: 0.9002723693847656\n",
      "epoch: 7 step: 9 loss: 0.6602951 acc: 0.8955307006835938\n",
      "epoch: 7 step: 10 loss: 0.55045617 acc: 0.9016647338867188\n",
      "epoch: 7 step: 11 loss: 0.6844967 acc: 0.8941116333007812\n",
      "epoch: 7 step: 12 loss: 0.57243985 acc: 0.8654327392578125\n",
      "epoch: 7 step: 13 loss: 0.53051907 acc: 0.8956947326660156\n",
      "epoch: 7 step: 14 loss: 0.6051156 acc: 0.8865547180175781\n",
      "epoch: 7 step: 15 loss: 0.63687223 acc: 0.8777427673339844\n",
      "epoch: 7 step: 16 loss: 0.58200836 acc: 0.8834304809570312\n",
      "epoch: 7 step: 17 loss: 0.6792523 acc: 0.8651046752929688\n",
      "epoch: 7 step: 18 loss: 0.6768806 acc: 0.8973731994628906\n",
      "epoch: 7 step: 19 loss: 0.7544846 acc: 0.8831634521484375\n",
      "epoch: 7 step: 20 loss: 0.5495519 acc: 0.9033927917480469\n",
      "epoch: 7 step: 21 loss: 0.530626 acc: 0.9023094177246094\n",
      "epoch: 7 step: 22 loss: 0.46147662 acc: 0.9036407470703125\n",
      "epoch: 7 step: 23 loss: 0.60229594 acc: 0.8917770385742188\n",
      "epoch: 7 step: 24 loss: 0.5327844 acc: 0.8930244445800781\n",
      "epoch: 7 step: 25 loss: 0.5341158 acc: 0.8838081359863281\n",
      "epoch: 7 step: 26 loss: 0.45505652 acc: 0.9175605773925781\n",
      "epoch: 7 step: 27 loss: 0.44432524 acc: 0.9126014709472656\n",
      "epoch: 7 step: 28 loss: 0.45341358 acc: 0.9098663330078125\n",
      "epoch: 7 step: 29 loss: 0.45146084 acc: 0.9035263061523438\n",
      "epoch: 7 step: 30 loss: 0.54521626 acc: 0.901458740234375\n",
      "epoch: 7 step: 31 loss: 0.5129311 acc: 0.9093704223632812\n",
      "epoch: 7 step: 32 loss: 0.51774573 acc: 0.906494140625\n",
      "epoch: 7 step: 33 loss: 0.46407962 acc: 0.9112167358398438\n",
      "epoch: 7 step: 34 loss: 0.55450857 acc: 0.9077415466308594\n",
      "epoch: 7 step: 35 loss: 0.5195101 acc: 0.9053306579589844\n",
      "epoch: 7 step: 36 loss: 0.64058095 acc: 0.8948402404785156\n",
      "epoch: 7 step: 37 loss: 0.4100021 acc: 0.9194297790527344\n",
      "epoch: 7 step: 38 loss: 0.64017564 acc: 0.8909721374511719\n",
      "epoch: 7 step: 39 loss: 0.5103196 acc: 0.9129829406738281\n",
      "epoch: 7 step: 40 loss: 0.5436916 acc: 0.8966712951660156\n",
      "epoch: 7 step: 41 loss: 0.69605064 acc: 0.9039154052734375\n",
      "epoch: 7 step: 42 loss: 0.51533586 acc: 0.9118194580078125\n",
      "epoch: 7 step: 43 loss: 0.73683137 acc: 0.8733787536621094\n",
      "epoch: 7 step: 44 loss: 0.595478 acc: 0.8831253051757812\n",
      "epoch: 7 step: 45 loss: 0.5084564 acc: 0.9011192321777344\n",
      "epoch: 7 step: 46 loss: 0.6619787 acc: 0.8726730346679688\n",
      "epoch: 7 step: 47 loss: 0.5612224 acc: 0.8874320983886719\n",
      "epoch: 7 step: 48 loss: 0.54215664 acc: 0.8986358642578125\n",
      "epoch: 7 step: 49 loss: 0.50182927 acc: 0.9094963073730469\n",
      "epoch: 7 step: 50 loss: 0.49229613 acc: 0.9062080383300781\n",
      "epoch: 7 step: 51 loss: 0.41050753 acc: 0.9323654174804688\n",
      "epoch: 7 step: 52 loss: 0.44602695 acc: 0.9079780578613281\n",
      "epoch: 7 step: 53 loss: 0.5270088 acc: 0.911041259765625\n",
      "epoch: 7 step: 54 loss: 0.7836189 acc: 0.8949317932128906\n",
      "epoch: 7 step: 55 loss: 0.492653 acc: 0.9139213562011719\n",
      "epoch: 7 step: 56 loss: 0.5079278 acc: 0.8924407958984375\n",
      "epoch: 7 step: 57 loss: 0.5385098 acc: 0.8936767578125\n",
      "epoch: 7 step: 58 loss: 0.61087817 acc: 0.8697128295898438\n",
      "epoch: 7 step: 59 loss: 0.6284691 acc: 0.8917388916015625\n",
      "epoch: 7 step: 60 loss: 0.5768294 acc: 0.902862548828125\n",
      "epoch: 7 step: 61 loss: 0.5995237 acc: 0.8833580017089844\n",
      "epoch: 7 step: 62 loss: 0.5854988 acc: 0.8907394409179688\n",
      "epoch: 7 step: 63 loss: 0.54797673 acc: 0.8952903747558594\n",
      "epoch: 7 step: 64 loss: 0.47222176 acc: 0.8933181762695312\n",
      "epoch: 7 step: 65 loss: 0.68808883 acc: 0.8655815124511719\n",
      "epoch: 7 step: 66 loss: 0.7682556 acc: 0.8368263244628906\n",
      "epoch: 7 step: 67 loss: 0.5038017 acc: 0.8974113464355469\n",
      "epoch: 7 step: 68 loss: 0.71647006 acc: 0.8639717102050781\n",
      "epoch: 7 step: 69 loss: 0.7237777 acc: 0.8728828430175781\n",
      "epoch: 7 step: 70 loss: 0.55966836 acc: 0.9002952575683594\n",
      "epoch: 7 step: 71 loss: 0.5383873 acc: 0.9021034240722656\n",
      "epoch: 7 step: 72 loss: 0.61897576 acc: 0.8728103637695312\n",
      "epoch: 7 step: 73 loss: 0.47292635 acc: 0.9112052917480469\n",
      "epoch: 7 step: 74 loss: 0.5639335 acc: 0.8928489685058594\n",
      "epoch: 7 step: 75 loss: 0.5376178 acc: 0.8987541198730469\n",
      "epoch: 7 step: 76 loss: 0.5655755 acc: 0.9041595458984375\n",
      "epoch: 7 step: 77 loss: 0.4331513 acc: 0.9276275634765625\n",
      "epoch: 7 step: 78 loss: 0.48867387 acc: 0.9021110534667969\n",
      "epoch: 7 step: 79 loss: 0.76098263 acc: 0.8712539672851562\n",
      "epoch: 7 step: 80 loss: 0.5054459 acc: 0.9001007080078125\n",
      "epoch: 7 step: 81 loss: 0.463697 acc: 0.9148750305175781\n",
      "epoch: 7 step: 82 loss: 0.454977 acc: 0.89324951171875\n",
      "epoch: 7 step: 83 loss: 0.53517073 acc: 0.9167022705078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 84 loss: 0.4299109 acc: 0.90838623046875\n",
      "epoch: 7 step: 85 loss: 0.52619195 acc: 0.8977088928222656\n",
      "epoch: 7 step: 86 loss: 0.5318534 acc: 0.9024200439453125\n",
      "epoch: 7 step: 87 loss: 0.50312847 acc: 0.9025917053222656\n",
      "epoch: 7 step: 88 loss: 0.407505 acc: 0.9213333129882812\n",
      "epoch: 7 step: 89 loss: 0.6678318 acc: 0.8648223876953125\n",
      "epoch: 7 step: 90 loss: 0.5722672 acc: 0.8867988586425781\n",
      "epoch: 7 step: 91 loss: 0.4686389 acc: 0.9084892272949219\n",
      "epoch: 7 step: 92 loss: 0.70840186 acc: 0.8922119140625\n",
      "epoch: 7 step: 93 loss: 0.52720004 acc: 0.9119148254394531\n",
      "epoch: 7 step: 94 loss: 0.47836852 acc: 0.9095268249511719\n",
      "epoch: 7 step: 95 loss: 0.40705284 acc: 0.9037704467773438\n",
      "epoch: 7 step: 96 loss: 0.5616196 acc: 0.8927879333496094\n",
      "epoch: 7 step: 97 loss: 0.5864795 acc: 0.8911628723144531\n",
      "epoch: 7 step: 98 loss: 0.7091575 acc: 0.885772705078125\n",
      "epoch: 7 step: 99 loss: 0.45125428 acc: 0.9196395874023438\n",
      "epoch: 7 step: 100 loss: 0.4605167 acc: 0.9217109680175781\n",
      "epoch: 7 step: 101 loss: 0.4804352 acc: 0.908111572265625\n",
      "epoch: 7 step: 102 loss: 0.4779049 acc: 0.9108543395996094\n",
      "epoch: 7 step: 103 loss: 0.5284828 acc: 0.8894424438476562\n",
      "epoch: 7 step: 104 loss: 0.4816151 acc: 0.9242706298828125\n",
      "epoch: 7 step: 105 loss: 0.53678864 acc: 0.8947219848632812\n",
      "epoch: 7 step: 106 loss: 0.50345236 acc: 0.9124946594238281\n",
      "epoch: 7 step: 107 loss: 0.4538078 acc: 0.9062347412109375\n",
      "epoch: 7 step: 108 loss: 0.47852838 acc: 0.8963470458984375\n",
      "epoch: 7 step: 109 loss: 1.0036565 acc: 0.8346748352050781\n",
      "epoch: 7 step: 110 loss: 0.43836564 acc: 0.9012222290039062\n",
      "epoch: 7 step: 111 loss: 0.56278104 acc: 0.8958473205566406\n",
      "epoch: 7 step: 112 loss: 0.5467748 acc: 0.8773689270019531\n",
      "epoch: 7 step: 113 loss: 0.56888187 acc: 0.867767333984375\n",
      "epoch: 7 step: 114 loss: 0.48422578 acc: 0.897918701171875\n",
      "epoch: 7 step: 115 loss: 0.4719412 acc: 0.9100761413574219\n",
      "epoch: 7 step: 116 loss: 0.43941632 acc: 0.9215660095214844\n",
      "epoch: 7 step: 117 loss: 0.48826858 acc: 0.9115371704101562\n",
      "epoch: 7 step: 118 loss: 0.5710362 acc: 0.9023551940917969\n",
      "epoch: 7 step: 119 loss: 0.5112921 acc: 0.9113197326660156\n",
      "epoch: 7 step: 120 loss: 0.46857172 acc: 0.9006767272949219\n",
      "epoch: 7 step: 121 loss: 0.45747426 acc: 0.9231910705566406\n",
      "epoch: 7 step: 122 loss: 0.5437804 acc: 0.8910331726074219\n",
      "epoch: 7 step: 123 loss: 0.5044052 acc: 0.9099617004394531\n",
      "epoch: 7 step: 124 loss: 0.6050328 acc: 0.9146990094866071\n",
      "epoch: 7 validation_loss: 0.522 validation_dice: 0.5588298299629263\n",
      "epoch: 7 test_dataset dice: 0.5007199907003173\n",
      "time cost 0.5364600022633871 min\n",
      "dice_best: 0\n",
      "******************************** epoch  7  is finished. *********************************\n",
      "epoch: 8 step: 1 loss: 0.514632 acc: 0.9083023071289062\n",
      "epoch: 8 step: 2 loss: 0.49789482 acc: 0.9027748107910156\n",
      "epoch: 8 step: 3 loss: 0.4763811 acc: 0.91033935546875\n",
      "epoch: 8 step: 4 loss: 0.6369354 acc: 0.8859138488769531\n",
      "epoch: 8 step: 5 loss: 0.45150658 acc: 0.899810791015625\n",
      "epoch: 8 step: 6 loss: 0.59287024 acc: 0.8841781616210938\n",
      "epoch: 8 step: 7 loss: 0.6262713 acc: 0.8895492553710938\n",
      "epoch: 8 step: 8 loss: 0.4524849 acc: 0.9080543518066406\n",
      "epoch: 8 step: 9 loss: 0.47419754 acc: 0.9074630737304688\n",
      "epoch: 8 step: 10 loss: 0.48847365 acc: 0.9209060668945312\n",
      "epoch: 8 step: 11 loss: 0.5682837 acc: 0.8939056396484375\n",
      "epoch: 8 step: 12 loss: 0.54038256 acc: 0.8890342712402344\n",
      "epoch: 8 step: 13 loss: 0.48095927 acc: 0.9126930236816406\n",
      "epoch: 8 step: 14 loss: 0.522869 acc: 0.8906631469726562\n",
      "epoch: 8 step: 15 loss: 0.4214167 acc: 0.9180488586425781\n",
      "epoch: 8 step: 16 loss: 0.4938039 acc: 0.9205436706542969\n",
      "epoch: 8 step: 17 loss: 0.5416705 acc: 0.8977737426757812\n",
      "epoch: 8 step: 18 loss: 0.52467984 acc: 0.9081573486328125\n",
      "epoch: 8 step: 19 loss: 0.4229196 acc: 0.917755126953125\n",
      "epoch: 8 step: 20 loss: 0.47819063 acc: 0.917144775390625\n",
      "epoch: 8 step: 21 loss: 0.4207707 acc: 0.9179344177246094\n",
      "epoch: 8 step: 22 loss: 0.406958 acc: 0.9229240417480469\n",
      "epoch: 8 step: 23 loss: 0.47933275 acc: 0.8852043151855469\n",
      "epoch: 8 step: 24 loss: 0.70470667 acc: 0.8628921508789062\n",
      "epoch: 8 step: 25 loss: 0.32930362 acc: 0.9245185852050781\n",
      "epoch: 8 step: 26 loss: 0.51202357 acc: 0.914306640625\n",
      "epoch: 8 step: 27 loss: 0.6160552 acc: 0.918487548828125\n",
      "epoch: 8 step: 28 loss: 0.4668692 acc: 0.908905029296875\n",
      "epoch: 8 step: 29 loss: 0.43441388 acc: 0.9331092834472656\n",
      "epoch: 8 step: 30 loss: 0.4879352 acc: 0.9145469665527344\n",
      "epoch: 8 step: 31 loss: 0.4289661 acc: 0.9132957458496094\n",
      "epoch: 8 step: 32 loss: 0.3957743 acc: 0.9051399230957031\n",
      "epoch: 8 step: 33 loss: 0.52215284 acc: 0.8846015930175781\n",
      "epoch: 8 step: 34 loss: 0.49230218 acc: 0.8897514343261719\n",
      "epoch: 8 step: 35 loss: 0.5344915 acc: 0.875732421875\n",
      "epoch: 8 step: 36 loss: 0.49717033 acc: 0.8833847045898438\n",
      "epoch: 8 step: 37 loss: 0.5913425 acc: 0.8822441101074219\n",
      "epoch: 8 step: 38 loss: 0.5663691 acc: 0.896820068359375\n",
      "epoch: 8 step: 39 loss: 0.60209966 acc: 0.89031982421875\n",
      "epoch: 8 step: 40 loss: 0.4478717 acc: 0.9027938842773438\n",
      "epoch: 8 step: 41 loss: 0.63014376 acc: 0.8837203979492188\n",
      "epoch: 8 step: 42 loss: 0.66441137 acc: 0.8696136474609375\n",
      "epoch: 8 step: 43 loss: 0.60628515 acc: 0.8874893188476562\n",
      "epoch: 8 step: 44 loss: 0.46612063 acc: 0.9006500244140625\n",
      "epoch: 8 step: 45 loss: 0.78519857 acc: 0.8673744201660156\n",
      "epoch: 8 step: 46 loss: 0.6169182 acc: 0.8773612976074219\n",
      "epoch: 8 step: 47 loss: 0.5371531 acc: 0.9076995849609375\n",
      "epoch: 8 step: 48 loss: 0.66191036 acc: 0.8934783935546875\n",
      "epoch: 8 step: 49 loss: 0.50519687 acc: 0.8871116638183594\n",
      "epoch: 8 step: 50 loss: 0.50461507 acc: 0.8900985717773438\n",
      "epoch: 8 step: 51 loss: 0.5120488 acc: 0.8927192687988281\n",
      "epoch: 8 step: 52 loss: 0.53249854 acc: 0.9052238464355469\n",
      "epoch: 8 step: 53 loss: 0.54968137 acc: 0.90228271484375\n",
      "epoch: 8 step: 54 loss: 0.52981067 acc: 0.9164085388183594\n",
      "epoch: 8 step: 55 loss: 0.52334243 acc: 0.8921394348144531\n",
      "epoch: 8 step: 56 loss: 0.5076381 acc: 0.9169578552246094\n",
      "epoch: 8 step: 57 loss: 0.5684902 acc: 0.9014167785644531\n",
      "epoch: 8 step: 58 loss: 0.47337276 acc: 0.9217491149902344\n",
      "epoch: 8 step: 59 loss: 0.48295543 acc: 0.9210281372070312\n",
      "epoch: 8 step: 60 loss: 0.45975474 acc: 0.9097747802734375\n",
      "epoch: 8 step: 61 loss: 0.6539871 acc: 0.8817367553710938\n",
      "epoch: 8 step: 62 loss: 0.4497759 acc: 0.8887710571289062\n",
      "epoch: 8 step: 63 loss: 0.51715314 acc: 0.8901863098144531\n",
      "epoch: 8 step: 64 loss: 0.37261438 acc: 0.9081954956054688\n",
      "epoch: 8 step: 65 loss: 0.47874615 acc: 0.9065628051757812\n",
      "epoch: 8 step: 66 loss: 0.55596215 acc: 0.9094161987304688\n",
      "epoch: 8 step: 67 loss: 0.55029833 acc: 0.8957443237304688\n",
      "epoch: 8 step: 68 loss: 0.42893463 acc: 0.8971138000488281\n",
      "epoch: 8 step: 69 loss: 0.5943425 acc: 0.894622802734375\n",
      "epoch: 8 step: 70 loss: 0.42231444 acc: 0.9066658020019531\n",
      "epoch: 8 step: 71 loss: 0.39649737 acc: 0.89385986328125\n",
      "epoch: 8 step: 72 loss: 0.89204216 acc: 0.8504638671875\n",
      "epoch: 8 step: 73 loss: 0.5783215 acc: 0.8652153015136719\n",
      "epoch: 8 step: 74 loss: 0.42655218 acc: 0.9068489074707031\n",
      "epoch: 8 step: 75 loss: 0.45615047 acc: 0.8989524841308594\n",
      "epoch: 8 step: 76 loss: 0.5363413 acc: 0.9028091430664062\n",
      "epoch: 8 step: 77 loss: 0.57087797 acc: 0.9067115783691406\n",
      "epoch: 8 step: 78 loss: 0.6854299 acc: 0.8941841125488281\n",
      "epoch: 8 step: 79 loss: 0.5564525 acc: 0.9037132263183594\n",
      "epoch: 8 step: 80 loss: 0.37152708 acc: 0.9292793273925781\n",
      "epoch: 8 step: 81 loss: 0.39292628 acc: 0.9236259460449219\n",
      "epoch: 8 step: 82 loss: 0.47579896 acc: 0.9152145385742188\n",
      "epoch: 8 step: 83 loss: 0.5593108 acc: 0.9050788879394531\n",
      "epoch: 8 step: 84 loss: 0.4982371 acc: 0.896331787109375\n",
      "epoch: 8 step: 85 loss: 0.4145576 acc: 0.923980712890625\n",
      "epoch: 8 step: 86 loss: 0.5517256 acc: 0.8843917846679688\n",
      "epoch: 8 step: 87 loss: 0.49663827 acc: 0.8960113525390625\n",
      "epoch: 8 step: 88 loss: 0.5091112 acc: 0.8986740112304688\n",
      "epoch: 8 step: 89 loss: 0.5700012 acc: 0.8938827514648438\n",
      "epoch: 8 step: 90 loss: 0.40782958 acc: 0.9144477844238281\n",
      "epoch: 8 step: 91 loss: 0.49340186 acc: 0.9144439697265625\n",
      "epoch: 8 step: 92 loss: 0.48174894 acc: 0.90240478515625\n",
      "epoch: 8 step: 93 loss: 0.54127705 acc: 0.8827629089355469\n",
      "epoch: 8 step: 94 loss: 0.38146684 acc: 0.9199562072753906\n",
      "epoch: 8 step: 95 loss: 0.4980536 acc: 0.9129409790039062\n",
      "epoch: 8 step: 96 loss: 0.47716773 acc: 0.9152183532714844\n",
      "epoch: 8 step: 97 loss: 0.5594023 acc: 0.9015579223632812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 98 loss: 0.5947597 acc: 0.8907356262207031\n",
      "epoch: 8 step: 99 loss: 0.39919978 acc: 0.9103546142578125\n",
      "epoch: 8 step: 100 loss: 0.45561948 acc: 0.9094467163085938\n",
      "epoch: 8 step: 101 loss: 0.49640054 acc: 0.921142578125\n",
      "epoch: 8 step: 102 loss: 0.34626752 acc: 0.9197235107421875\n",
      "epoch: 8 step: 103 loss: 0.47794235 acc: 0.8938102722167969\n",
      "epoch: 8 step: 104 loss: 0.49098015 acc: 0.879180908203125\n",
      "epoch: 8 step: 105 loss: 0.49686083 acc: 0.9076881408691406\n",
      "epoch: 8 step: 106 loss: 0.44633305 acc: 0.9215240478515625\n",
      "epoch: 8 step: 107 loss: 0.4504563 acc: 0.9193229675292969\n",
      "epoch: 8 step: 108 loss: 0.41675377 acc: 0.9189109802246094\n",
      "epoch: 8 step: 109 loss: 0.4489881 acc: 0.9125556945800781\n",
      "epoch: 8 step: 110 loss: 0.4773968 acc: 0.9139404296875\n",
      "epoch: 8 step: 111 loss: 0.41837132 acc: 0.9288444519042969\n",
      "epoch: 8 step: 112 loss: 0.38379616 acc: 0.9128265380859375\n",
      "epoch: 8 step: 113 loss: 0.4325255 acc: 0.9065628051757812\n",
      "epoch: 8 step: 114 loss: 0.39424586 acc: 0.9160308837890625\n",
      "epoch: 8 step: 115 loss: 0.46570674 acc: 0.892578125\n",
      "epoch: 8 step: 116 loss: 0.4057862 acc: 0.9110488891601562\n",
      "epoch: 8 step: 117 loss: 0.4015295 acc: 0.9166183471679688\n",
      "epoch: 8 step: 118 loss: 0.41801414 acc: 0.9240798950195312\n",
      "epoch: 8 step: 119 loss: 0.49172464 acc: 0.8950080871582031\n",
      "epoch: 8 step: 120 loss: 0.4823133 acc: 0.9027595520019531\n",
      "epoch: 8 step: 121 loss: 0.38190317 acc: 0.9150619506835938\n",
      "epoch: 8 step: 122 loss: 0.47169346 acc: 0.8826675415039062\n",
      "epoch: 8 step: 123 loss: 0.3995132 acc: 0.9033851623535156\n",
      "epoch: 8 step: 124 loss: 0.41523337 acc: 0.9220929827008929\n",
      "epoch: 8 validation_loss: 0.604 validation_dice: 0.5562237588029085\n",
      "epoch: 8 test_dataset dice: 0.4393691134411266\n",
      "time cost 0.5359890977541606 min\n",
      "dice_best: 0.5562237588029085\n",
      "******************************** epoch  8  is finished. *********************************\n",
      "epoch: 9 step: 1 loss: 0.41840458 acc: 0.9129486083984375\n",
      "epoch: 9 step: 2 loss: 0.44922557 acc: 0.9157485961914062\n",
      "epoch: 9 step: 3 loss: 0.39449757 acc: 0.9099922180175781\n",
      "epoch: 9 step: 4 loss: 0.33269826 acc: 0.9245223999023438\n",
      "epoch: 9 step: 5 loss: 0.4003897 acc: 0.9246482849121094\n",
      "epoch: 9 step: 6 loss: 0.31887743 acc: 0.9358634948730469\n",
      "epoch: 9 step: 7 loss: 0.45020422 acc: 0.9093017578125\n",
      "epoch: 9 step: 8 loss: 0.37826362 acc: 0.933380126953125\n",
      "epoch: 9 step: 9 loss: 0.32167083 acc: 0.9232749938964844\n",
      "epoch: 9 step: 10 loss: 0.39794546 acc: 0.9075546264648438\n",
      "epoch: 9 step: 11 loss: 0.334514 acc: 0.94744873046875\n",
      "epoch: 9 step: 12 loss: 0.5399579 acc: 0.9290046691894531\n",
      "epoch: 9 step: 13 loss: 0.4661593 acc: 0.90814208984375\n",
      "epoch: 9 step: 14 loss: 0.35275057 acc: 0.9277229309082031\n",
      "epoch: 9 step: 15 loss: 0.37670994 acc: 0.9153213500976562\n",
      "epoch: 9 step: 16 loss: 0.41226655 acc: 0.9082527160644531\n",
      "epoch: 9 step: 17 loss: 0.39091983 acc: 0.9084396362304688\n",
      "epoch: 9 step: 18 loss: 0.39648348 acc: 0.9223823547363281\n",
      "epoch: 9 step: 19 loss: 0.51980007 acc: 0.9247245788574219\n",
      "epoch: 9 step: 20 loss: 0.546292 acc: 0.8838729858398438\n",
      "epoch: 9 step: 21 loss: 0.4895217 acc: 0.9085655212402344\n",
      "epoch: 9 step: 22 loss: 0.5306054 acc: 0.9051094055175781\n",
      "epoch: 9 step: 23 loss: 0.4134191 acc: 0.9171142578125\n",
      "epoch: 9 step: 24 loss: 0.5527193 acc: 0.8903579711914062\n",
      "epoch: 9 step: 25 loss: 0.3545852 acc: 0.9123725891113281\n",
      "epoch: 9 step: 26 loss: 0.3831525 acc: 0.9208030700683594\n",
      "epoch: 9 step: 27 loss: 0.528084 acc: 0.9032096862792969\n",
      "epoch: 9 step: 28 loss: 0.35838935 acc: 0.9206771850585938\n",
      "epoch: 9 step: 29 loss: 0.38210806 acc: 0.9126434326171875\n",
      "epoch: 9 step: 30 loss: 0.3990669 acc: 0.9130935668945312\n",
      "epoch: 9 step: 31 loss: 0.32586768 acc: 0.9340286254882812\n",
      "epoch: 9 step: 32 loss: 0.4164708 acc: 0.9088821411132812\n",
      "epoch: 9 step: 33 loss: 0.311114 acc: 0.915496826171875\n",
      "epoch: 9 step: 34 loss: 0.4379752 acc: 0.9038619995117188\n",
      "epoch: 9 step: 35 loss: 0.38985494 acc: 0.9169349670410156\n",
      "epoch: 9 step: 36 loss: 0.46980357 acc: 0.8989105224609375\n",
      "epoch: 9 step: 37 loss: 0.56445295 acc: 0.8864593505859375\n",
      "epoch: 9 step: 38 loss: 0.44103405 acc: 0.8922500610351562\n",
      "epoch: 9 step: 39 loss: 0.42448986 acc: 0.9054718017578125\n",
      "epoch: 9 step: 40 loss: 0.36951882 acc: 0.8940505981445312\n",
      "epoch: 9 step: 41 loss: 0.366887 acc: 0.9120674133300781\n",
      "epoch: 9 step: 42 loss: 0.36917087 acc: 0.9142646789550781\n",
      "epoch: 9 step: 43 loss: 0.37062454 acc: 0.9093208312988281\n",
      "epoch: 9 step: 44 loss: 0.45222563 acc: 0.9179153442382812\n",
      "epoch: 9 step: 45 loss: 0.46787947 acc: 0.911163330078125\n",
      "epoch: 9 step: 46 loss: 0.4186679 acc: 0.9092216491699219\n",
      "epoch: 9 step: 47 loss: 0.37374276 acc: 0.9300918579101562\n",
      "epoch: 9 step: 48 loss: 0.40969434 acc: 0.9203834533691406\n",
      "epoch: 9 step: 49 loss: 0.28656262 acc: 0.9224090576171875\n",
      "epoch: 9 step: 50 loss: 0.35670578 acc: 0.9213447570800781\n",
      "epoch: 9 step: 51 loss: 0.39101833 acc: 0.9293327331542969\n",
      "epoch: 9 step: 52 loss: 0.42958993 acc: 0.9197998046875\n",
      "epoch: 9 step: 53 loss: 0.3947771 acc: 0.9127769470214844\n",
      "epoch: 9 step: 54 loss: 0.34926438 acc: 0.9208984375\n",
      "epoch: 9 step: 55 loss: 0.4062193 acc: 0.9183540344238281\n",
      "epoch: 9 step: 56 loss: 0.37462306 acc: 0.9168930053710938\n",
      "epoch: 9 step: 57 loss: 0.38799337 acc: 0.9129791259765625\n",
      "epoch: 9 step: 58 loss: 0.35927868 acc: 0.9255294799804688\n",
      "epoch: 9 step: 59 loss: 0.4032113 acc: 0.9208145141601562\n",
      "epoch: 9 step: 60 loss: 0.42608666 acc: 0.9187850952148438\n",
      "epoch: 9 step: 61 loss: 0.36527756 acc: 0.9114761352539062\n",
      "epoch: 9 step: 62 loss: 0.36263478 acc: 0.9051475524902344\n",
      "epoch: 9 step: 63 loss: 0.4491809 acc: 0.9027442932128906\n",
      "epoch: 9 step: 64 loss: 0.43143395 acc: 0.9013633728027344\n",
      "epoch: 9 step: 65 loss: 0.48721287 acc: 0.902130126953125\n",
      "epoch: 9 step: 66 loss: 0.38034177 acc: 0.9171791076660156\n",
      "epoch: 9 step: 67 loss: 0.38525712 acc: 0.9227867126464844\n",
      "epoch: 9 step: 68 loss: 0.50752944 acc: 0.8942451477050781\n",
      "epoch: 9 step: 69 loss: 0.4912783 acc: 0.8981170654296875\n",
      "epoch: 9 step: 70 loss: 0.4968016 acc: 0.9062843322753906\n",
      "epoch: 9 step: 71 loss: 0.50897825 acc: 0.9087295532226562\n",
      "epoch: 9 step: 72 loss: 0.45288524 acc: 0.9018287658691406\n",
      "epoch: 9 step: 73 loss: 0.51656014 acc: 0.8977775573730469\n",
      "epoch: 9 step: 74 loss: 0.47088814 acc: 0.9149742126464844\n",
      "epoch: 9 step: 75 loss: 0.4783312 acc: 0.9093894958496094\n",
      "epoch: 9 step: 76 loss: 0.35692668 acc: 0.9225311279296875\n",
      "epoch: 9 step: 77 loss: 0.37320837 acc: 0.9237251281738281\n",
      "epoch: 9 step: 78 loss: 0.4793556 acc: 0.9109535217285156\n",
      "epoch: 9 step: 79 loss: 0.42460796 acc: 0.9084930419921875\n",
      "epoch: 9 step: 80 loss: 0.34138417 acc: 0.9375648498535156\n",
      "epoch: 9 step: 81 loss: 0.36080706 acc: 0.9331817626953125\n",
      "epoch: 9 step: 82 loss: 0.5273668 acc: 0.8899040222167969\n",
      "epoch: 9 step: 83 loss: 0.43983245 acc: 0.9185829162597656\n",
      "epoch: 9 step: 84 loss: 0.4289539 acc: 0.903961181640625\n",
      "epoch: 9 step: 85 loss: 0.44515547 acc: 0.9033851623535156\n",
      "epoch: 9 step: 86 loss: 0.46852118 acc: 0.9255332946777344\n",
      "epoch: 9 step: 87 loss: 0.42217132 acc: 0.8926963806152344\n",
      "epoch: 9 step: 88 loss: 0.38005915 acc: 0.901397705078125\n",
      "epoch: 9 step: 89 loss: 0.37884995 acc: 0.8885307312011719\n",
      "epoch: 9 step: 90 loss: 0.4135508 acc: 0.9040260314941406\n",
      "epoch: 9 step: 91 loss: 0.5716426 acc: 0.8875389099121094\n",
      "epoch: 9 step: 92 loss: 0.47414118 acc: 0.9169273376464844\n",
      "epoch: 9 step: 93 loss: 0.44148925 acc: 0.9148178100585938\n",
      "epoch: 9 step: 94 loss: 0.44712657 acc: 0.9148216247558594\n",
      "epoch: 9 step: 95 loss: 0.48435864 acc: 0.9084930419921875\n",
      "epoch: 9 step: 96 loss: 0.48579141 acc: 0.9082489013671875\n",
      "epoch: 9 step: 97 loss: 0.49490812 acc: 0.910614013671875\n",
      "epoch: 9 step: 98 loss: 0.50683963 acc: 0.9160308837890625\n",
      "epoch: 9 step: 99 loss: 0.41930428 acc: 0.9124488830566406\n",
      "epoch: 9 step: 100 loss: 0.3983327 acc: 0.9025611877441406\n",
      "epoch: 9 step: 101 loss: 0.43731838 acc: 0.8931350708007812\n",
      "epoch: 9 step: 102 loss: 0.43915284 acc: 0.908477783203125\n",
      "epoch: 9 step: 103 loss: 0.63332653 acc: 0.889190673828125\n",
      "epoch: 9 step: 104 loss: 0.36373445 acc: 0.905303955078125\n",
      "epoch: 9 step: 105 loss: 0.50739574 acc: 0.8985977172851562\n",
      "epoch: 9 step: 106 loss: 0.4471836 acc: 0.9135665893554688\n",
      "epoch: 9 step: 107 loss: 0.4067859 acc: 0.9078254699707031\n",
      "epoch: 9 step: 108 loss: 0.44424173 acc: 0.9017372131347656\n",
      "epoch: 9 step: 109 loss: 0.45468146 acc: 0.905975341796875\n",
      "epoch: 9 step: 110 loss: 0.41511178 acc: 0.9081001281738281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 111 loss: 0.44852018 acc: 0.9146080017089844\n",
      "epoch: 9 step: 112 loss: 0.49778983 acc: 0.9055404663085938\n",
      "epoch: 9 step: 113 loss: 0.6119985 acc: 0.8998985290527344\n",
      "epoch: 9 step: 114 loss: 0.29776523 acc: 0.9390907287597656\n",
      "epoch: 9 step: 115 loss: 0.4901796 acc: 0.9258766174316406\n",
      "epoch: 9 step: 116 loss: 0.40151125 acc: 0.9332313537597656\n",
      "epoch: 9 step: 117 loss: 0.40419406 acc: 0.9207420349121094\n",
      "epoch: 9 step: 118 loss: 0.5494096 acc: 0.8945159912109375\n",
      "epoch: 9 step: 119 loss: 0.4120389 acc: 0.9183502197265625\n",
      "epoch: 9 step: 120 loss: 0.44555086 acc: 0.9130096435546875\n",
      "epoch: 9 step: 121 loss: 0.60646266 acc: 0.8832130432128906\n",
      "epoch: 9 step: 122 loss: 0.5754062 acc: 0.8764266967773438\n",
      "epoch: 9 step: 123 loss: 0.5027946 acc: 0.891632080078125\n",
      "epoch: 9 step: 124 loss: 0.5118822 acc: 0.8812430245535714\n",
      "epoch: 9 validation_loss: 0.587 validation_dice: 0.5135242197929796\n",
      "epoch: 9 test_dataset dice: 0.48021998017583933\n",
      "time cost 0.5339857657750448 min\n",
      "dice_best: 0.5562237588029085\n",
      "******************************** epoch  9  is finished. *********************************\n",
      "epoch: 10 step: 1 loss: 0.5467709 acc: 0.8781166076660156\n",
      "epoch: 10 step: 2 loss: 0.45491928 acc: 0.8911361694335938\n",
      "epoch: 10 step: 3 loss: 0.4952492 acc: 0.9144706726074219\n",
      "epoch: 10 step: 4 loss: 0.56322557 acc: 0.8936882019042969\n",
      "epoch: 10 step: 5 loss: 0.44532597 acc: 0.9160842895507812\n",
      "epoch: 10 step: 6 loss: 0.34495693 acc: 0.9158477783203125\n",
      "epoch: 10 step: 7 loss: 0.38783553 acc: 0.9027290344238281\n",
      "epoch: 10 step: 8 loss: 0.52583355 acc: 0.9028663635253906\n",
      "epoch: 10 step: 9 loss: 0.3630751 acc: 0.9275894165039062\n",
      "epoch: 10 step: 10 loss: 0.3208524 acc: 0.9200096130371094\n",
      "epoch: 10 step: 11 loss: 0.46639457 acc: 0.8978157043457031\n",
      "epoch: 10 step: 12 loss: 0.5016736 acc: 0.9159317016601562\n",
      "epoch: 10 step: 13 loss: 0.40339005 acc: 0.9008255004882812\n",
      "epoch: 10 step: 14 loss: 0.32405478 acc: 0.9302444458007812\n",
      "epoch: 10 step: 15 loss: 0.51441133 acc: 0.9149551391601562\n",
      "epoch: 10 step: 16 loss: 0.34771535 acc: 0.926727294921875\n",
      "epoch: 10 step: 17 loss: 0.6286204 acc: 0.90106201171875\n",
      "epoch: 10 step: 18 loss: 0.5200053 acc: 0.9123649597167969\n",
      "epoch: 10 step: 19 loss: 0.43883628 acc: 0.8921051025390625\n",
      "epoch: 10 step: 20 loss: 0.5790573 acc: 0.8667564392089844\n",
      "epoch: 10 step: 21 loss: 0.37935895 acc: 0.8978042602539062\n",
      "epoch: 10 step: 22 loss: 0.42357332 acc: 0.8999137878417969\n",
      "epoch: 10 step: 23 loss: 0.41536954 acc: 0.8818397521972656\n",
      "epoch: 10 step: 24 loss: 0.4104495 acc: 0.9014434814453125\n",
      "epoch: 10 step: 25 loss: 0.42195547 acc: 0.9099044799804688\n",
      "epoch: 10 step: 26 loss: 0.42200133 acc: 0.9133644104003906\n",
      "epoch: 10 step: 27 loss: 0.30084682 acc: 0.9244651794433594\n",
      "epoch: 10 step: 28 loss: 0.51414835 acc: 0.9105262756347656\n",
      "epoch: 10 step: 29 loss: 0.4408358 acc: 0.9235725402832031\n",
      "epoch: 10 step: 30 loss: 0.8414773 acc: 0.8658447265625\n",
      "epoch: 10 step: 31 loss: 0.47778502 acc: 0.920806884765625\n",
      "epoch: 10 step: 32 loss: 0.40348452 acc: 0.9291610717773438\n",
      "epoch: 10 step: 33 loss: 0.46695074 acc: 0.9229354858398438\n",
      "epoch: 10 step: 34 loss: 0.39775223 acc: 0.928558349609375\n",
      "epoch: 10 step: 35 loss: 0.45139542 acc: 0.9215126037597656\n",
      "epoch: 10 step: 36 loss: 0.5389275 acc: 0.9016609191894531\n",
      "epoch: 10 step: 37 loss: 0.39219594 acc: 0.9240646362304688\n",
      "epoch: 10 step: 38 loss: 0.49767086 acc: 0.9087257385253906\n",
      "epoch: 10 step: 39 loss: 0.4000151 acc: 0.9138526916503906\n",
      "epoch: 10 step: 40 loss: 0.45327163 acc: 0.9154434204101562\n",
      "epoch: 10 step: 41 loss: 0.5255853 acc: 0.8956146240234375\n",
      "epoch: 10 step: 42 loss: 0.46975622 acc: 0.8904037475585938\n",
      "epoch: 10 step: 43 loss: 0.42706713 acc: 0.9029197692871094\n",
      "epoch: 10 step: 44 loss: 0.39956465 acc: 0.9205818176269531\n",
      "epoch: 10 step: 45 loss: 0.36384326 acc: 0.9107170104980469\n",
      "epoch: 10 step: 46 loss: 0.4091869 acc: 0.9116020202636719\n",
      "epoch: 10 step: 47 loss: 0.5345115 acc: 0.8809432983398438\n",
      "epoch: 10 step: 48 loss: 0.438258 acc: 0.919342041015625\n",
      "epoch: 10 step: 49 loss: 0.4747831 acc: 0.90911865234375\n",
      "epoch: 10 step: 50 loss: 0.39108628 acc: 0.9288177490234375\n",
      "epoch: 10 step: 51 loss: 0.42670584 acc: 0.9149208068847656\n",
      "epoch: 10 step: 52 loss: 0.6137173 acc: 0.8966522216796875\n",
      "epoch: 10 step: 53 loss: 0.43144614 acc: 0.9123878479003906\n",
      "epoch: 10 step: 54 loss: 0.43397057 acc: 0.9116897583007812\n",
      "epoch: 10 step: 55 loss: 0.5036449 acc: 0.9243812561035156\n",
      "epoch: 10 step: 56 loss: 0.42230198 acc: 0.9055747985839844\n",
      "epoch: 10 step: 57 loss: 0.41211426 acc: 0.9144172668457031\n",
      "epoch: 10 step: 58 loss: 0.44426715 acc: 0.9217262268066406\n",
      "epoch: 10 step: 59 loss: 0.41854572 acc: 0.9178733825683594\n",
      "epoch: 10 step: 60 loss: 0.521917 acc: 0.9095878601074219\n",
      "epoch: 10 step: 61 loss: 0.40150186 acc: 0.9125595092773438\n",
      "epoch: 10 step: 62 loss: 0.391198 acc: 0.9237709045410156\n",
      "epoch: 10 step: 63 loss: 0.49176818 acc: 0.9014167785644531\n",
      "epoch: 10 step: 64 loss: 0.50127584 acc: 0.9013938903808594\n",
      "epoch: 10 step: 65 loss: 0.47721437 acc: 0.91033935546875\n",
      "epoch: 10 step: 66 loss: 0.3894749 acc: 0.91864013671875\n",
      "epoch: 10 step: 67 loss: 0.58569956 acc: 0.8889083862304688\n",
      "epoch: 10 step: 68 loss: 0.32262 acc: 0.9263076782226562\n",
      "epoch: 10 step: 69 loss: 0.45227998 acc: 0.9125823974609375\n",
      "epoch: 10 step: 70 loss: 0.45466313 acc: 0.9130859375\n",
      "epoch: 10 step: 71 loss: 0.5745317 acc: 0.8951263427734375\n",
      "epoch: 10 step: 72 loss: 0.49757645 acc: 0.9003829956054688\n",
      "epoch: 10 step: 73 loss: 0.53665423 acc: 0.9077262878417969\n",
      "epoch: 10 step: 74 loss: 0.47273883 acc: 0.9098701477050781\n",
      "epoch: 10 step: 75 loss: 0.38952553 acc: 0.9014244079589844\n",
      "epoch: 10 step: 76 loss: 0.36006227 acc: 0.9180259704589844\n",
      "epoch: 10 step: 77 loss: 0.3277218 acc: 0.9203300476074219\n",
      "epoch: 10 step: 78 loss: 0.5817772 acc: 0.8965530395507812\n",
      "epoch: 10 step: 79 loss: 0.42706534 acc: 0.9188270568847656\n",
      "epoch: 10 step: 80 loss: 0.42973232 acc: 0.92718505859375\n",
      "epoch: 10 step: 81 loss: 0.384779 acc: 0.9157142639160156\n",
      "epoch: 10 step: 82 loss: 0.3986772 acc: 0.9144859313964844\n",
      "epoch: 10 step: 83 loss: 0.37146413 acc: 0.9243736267089844\n",
      "epoch: 10 step: 84 loss: 0.40684822 acc: 0.8947181701660156\n",
      "epoch: 10 step: 85 loss: 0.52872914 acc: 0.8822975158691406\n",
      "epoch: 10 step: 86 loss: 0.40621918 acc: 0.9045639038085938\n",
      "epoch: 10 step: 87 loss: 0.37998772 acc: 0.9231300354003906\n",
      "epoch: 10 step: 88 loss: 0.31308284 acc: 0.9251976013183594\n",
      "epoch: 10 step: 89 loss: 0.3555349 acc: 0.932525634765625\n",
      "epoch: 10 step: 90 loss: 0.4135454 acc: 0.9244308471679688\n",
      "epoch: 10 step: 91 loss: 0.32322595 acc: 0.9292831420898438\n",
      "epoch: 10 step: 92 loss: 0.47587958 acc: 0.9111747741699219\n",
      "epoch: 10 step: 93 loss: 0.30466595 acc: 0.9285392761230469\n",
      "epoch: 10 step: 94 loss: 0.337808 acc: 0.9187240600585938\n",
      "epoch: 10 step: 95 loss: 0.4082991 acc: 0.9081611633300781\n",
      "epoch: 10 step: 96 loss: 0.418261 acc: 0.9306411743164062\n",
      "epoch: 10 step: 97 loss: 0.3083919 acc: 0.9194145202636719\n",
      "epoch: 10 step: 98 loss: 0.3161964 acc: 0.9202461242675781\n",
      "epoch: 10 step: 99 loss: 0.35091922 acc: 0.9152755737304688\n",
      "epoch: 10 step: 100 loss: 0.3207046 acc: 0.9191970825195312\n",
      "epoch: 10 step: 101 loss: 0.41035688 acc: 0.9069290161132812\n",
      "epoch: 10 step: 102 loss: 0.3959546 acc: 0.9175300598144531\n",
      "epoch: 10 step: 103 loss: 0.28710124 acc: 0.9316329956054688\n",
      "epoch: 10 step: 104 loss: 0.45742354 acc: 0.9100914001464844\n",
      "epoch: 10 step: 105 loss: 0.3582835 acc: 0.9295692443847656\n",
      "epoch: 10 step: 106 loss: 0.29132745 acc: 0.925506591796875\n",
      "epoch: 10 step: 107 loss: 0.34957778 acc: 0.9241485595703125\n",
      "epoch: 10 step: 108 loss: 0.39869416 acc: 0.9121170043945312\n",
      "epoch: 10 step: 109 loss: 0.48206863 acc: 0.9167404174804688\n",
      "epoch: 10 step: 110 loss: 0.39604893 acc: 0.9138755798339844\n",
      "epoch: 10 step: 111 loss: 0.32323807 acc: 0.9193916320800781\n",
      "epoch: 10 step: 112 loss: 0.47712928 acc: 0.9052619934082031\n",
      "epoch: 10 step: 113 loss: 0.3199014 acc: 0.917022705078125\n",
      "epoch: 10 step: 114 loss: 0.34255356 acc: 0.9213790893554688\n",
      "epoch: 10 step: 115 loss: 0.28045318 acc: 0.9397468566894531\n",
      "epoch: 10 step: 116 loss: 0.39415488 acc: 0.9260330200195312\n",
      "epoch: 10 step: 117 loss: 0.387537 acc: 0.9182777404785156\n",
      "epoch: 10 step: 118 loss: 0.27757153 acc: 0.9282035827636719\n",
      "epoch: 10 step: 119 loss: 0.37912422 acc: 0.9215850830078125\n",
      "epoch: 10 step: 120 loss: 0.39352536 acc: 0.9261283874511719\n",
      "epoch: 10 step: 121 loss: 0.3383611 acc: 0.9152069091796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 122 loss: 0.28060016 acc: 0.9179611206054688\n",
      "epoch: 10 step: 123 loss: 0.30706754 acc: 0.9306182861328125\n",
      "epoch: 10 step: 124 loss: 0.2874113 acc: 0.9238978794642857\n",
      "epoch: 10 validation_loss: 0.37 validation_dice: 0.6368300170690679\n",
      "epoch: 10 test_dataset dice: 0.5655035997288804\n",
      "time cost 0.5346432646115621 min\n",
      "dice_best: 0.6368300170690679\n",
      "******************************** epoch  10  is finished. *********************************\n",
      "epoch: 11 step: 1 loss: 0.34102574 acc: 0.9166679382324219\n",
      "epoch: 11 step: 2 loss: 0.44517007 acc: 0.9084968566894531\n",
      "epoch: 11 step: 3 loss: 0.2786949 acc: 0.9412345886230469\n",
      "epoch: 11 step: 4 loss: 0.28788146 acc: 0.9408378601074219\n",
      "epoch: 11 step: 5 loss: 0.3508002 acc: 0.9226226806640625\n",
      "epoch: 11 step: 6 loss: 0.31766123 acc: 0.9422645568847656\n",
      "epoch: 11 step: 7 loss: 0.25370768 acc: 0.938446044921875\n",
      "epoch: 11 step: 8 loss: 0.30589148 acc: 0.9227142333984375\n",
      "epoch: 11 step: 9 loss: 0.2841261 acc: 0.9272804260253906\n",
      "epoch: 11 step: 10 loss: 0.41761392 acc: 0.8986625671386719\n",
      "epoch: 11 step: 11 loss: 0.33628678 acc: 0.9212303161621094\n",
      "epoch: 11 step: 12 loss: 0.24551572 acc: 0.9313430786132812\n",
      "epoch: 11 step: 13 loss: 0.26946867 acc: 0.9360542297363281\n",
      "epoch: 11 step: 14 loss: 0.24707617 acc: 0.9374732971191406\n",
      "epoch: 11 step: 15 loss: 0.25469455 acc: 0.9381446838378906\n",
      "epoch: 11 step: 16 loss: 0.32536846 acc: 0.9217414855957031\n",
      "epoch: 11 step: 17 loss: 0.32060558 acc: 0.93902587890625\n",
      "epoch: 11 step: 18 loss: 0.33295873 acc: 0.9202690124511719\n",
      "epoch: 11 step: 19 loss: 0.24802282 acc: 0.9409751892089844\n",
      "epoch: 11 step: 20 loss: 0.46489215 acc: 0.9091072082519531\n",
      "epoch: 11 step: 21 loss: 0.32509208 acc: 0.9187850952148438\n",
      "epoch: 11 step: 22 loss: 0.3693774 acc: 0.9156837463378906\n",
      "epoch: 11 step: 23 loss: 0.28191936 acc: 0.9117355346679688\n",
      "epoch: 11 step: 24 loss: 0.29504132 acc: 0.9286117553710938\n",
      "epoch: 11 step: 25 loss: 0.40361944 acc: 0.8928871154785156\n",
      "epoch: 11 step: 26 loss: 0.28352538 acc: 0.9278488159179688\n",
      "epoch: 11 step: 27 loss: 0.31860113 acc: 0.9365806579589844\n",
      "epoch: 11 step: 28 loss: 0.3302148 acc: 0.9172744750976562\n",
      "epoch: 11 step: 29 loss: 0.31654263 acc: 0.9195785522460938\n",
      "epoch: 11 step: 30 loss: 0.3444948 acc: 0.9343338012695312\n",
      "epoch: 11 step: 31 loss: 0.2937356 acc: 0.9344673156738281\n",
      "epoch: 11 step: 32 loss: 0.33115596 acc: 0.8970603942871094\n",
      "epoch: 11 step: 33 loss: 0.37514564 acc: 0.9072990417480469\n",
      "epoch: 11 step: 34 loss: 0.26831907 acc: 0.9284248352050781\n",
      "epoch: 11 step: 35 loss: 0.3564215 acc: 0.924835205078125\n",
      "epoch: 11 step: 36 loss: 0.22826426 acc: 0.9500732421875\n",
      "epoch: 11 step: 37 loss: 0.33837196 acc: 0.9138526916503906\n",
      "epoch: 11 step: 38 loss: 0.3051852 acc: 0.9268531799316406\n",
      "epoch: 11 step: 39 loss: 0.3044255 acc: 0.9079933166503906\n",
      "epoch: 11 step: 40 loss: 0.28911513 acc: 0.9230461120605469\n",
      "epoch: 11 step: 41 loss: 0.23948693 acc: 0.9469184875488281\n",
      "epoch: 11 step: 42 loss: 0.26902685 acc: 0.9366493225097656\n",
      "epoch: 11 step: 43 loss: 0.35546347 acc: 0.9274978637695312\n",
      "epoch: 11 step: 44 loss: 0.218753 acc: 0.9352951049804688\n",
      "epoch: 11 step: 45 loss: 0.39583722 acc: 0.9063453674316406\n",
      "epoch: 11 step: 46 loss: 0.3878803 acc: 0.9140472412109375\n",
      "epoch: 11 step: 47 loss: 0.32504043 acc: 0.9095993041992188\n",
      "epoch: 11 step: 48 loss: 0.34237456 acc: 0.9189262390136719\n",
      "epoch: 11 step: 49 loss: 0.3132262 acc: 0.9105186462402344\n",
      "epoch: 11 step: 50 loss: 0.35631573 acc: 0.90875244140625\n",
      "epoch: 11 step: 51 loss: 0.31973976 acc: 0.9152107238769531\n",
      "epoch: 11 step: 52 loss: 0.31837678 acc: 0.9184074401855469\n",
      "epoch: 11 step: 53 loss: 0.32444528 acc: 0.9291877746582031\n",
      "epoch: 11 step: 54 loss: 0.29063943 acc: 0.9293441772460938\n",
      "epoch: 11 step: 55 loss: 0.33403897 acc: 0.9354133605957031\n",
      "epoch: 11 step: 56 loss: 0.30149963 acc: 0.9273033142089844\n",
      "epoch: 11 step: 57 loss: 0.2915378 acc: 0.9386940002441406\n",
      "epoch: 11 step: 58 loss: 0.2551168 acc: 0.9455718994140625\n",
      "epoch: 11 step: 59 loss: 0.50774807 acc: 0.917510986328125\n",
      "epoch: 11 step: 60 loss: 0.30255568 acc: 0.9185523986816406\n",
      "epoch: 11 step: 61 loss: 0.2545792 acc: 0.9361763000488281\n",
      "epoch: 11 step: 62 loss: 0.28095427 acc: 0.9361801147460938\n",
      "epoch: 11 step: 63 loss: 0.23704237 acc: 0.932098388671875\n",
      "epoch: 11 step: 64 loss: 0.31103697 acc: 0.9452285766601562\n",
      "epoch: 11 step: 65 loss: 0.29998067 acc: 0.9291572570800781\n",
      "epoch: 11 step: 66 loss: 0.3841229 acc: 0.9115524291992188\n",
      "epoch: 11 step: 67 loss: 0.3829324 acc: 0.9104461669921875\n",
      "epoch: 11 step: 68 loss: 0.35601306 acc: 0.9092597961425781\n",
      "epoch: 11 step: 69 loss: 0.33168492 acc: 0.9126930236816406\n",
      "epoch: 11 step: 70 loss: 0.27126482 acc: 0.9295234680175781\n",
      "epoch: 11 step: 71 loss: 0.22137216 acc: 0.9408149719238281\n",
      "epoch: 11 step: 72 loss: 0.28617856 acc: 0.9263954162597656\n",
      "epoch: 11 step: 73 loss: 0.4822346 acc: 0.9052162170410156\n",
      "epoch: 11 step: 74 loss: 0.33445403 acc: 0.9205970764160156\n",
      "epoch: 11 step: 75 loss: 0.38397953 acc: 0.9230308532714844\n",
      "epoch: 11 step: 76 loss: 0.34998962 acc: 0.9092903137207031\n",
      "epoch: 11 step: 77 loss: 0.3903366 acc: 0.9282035827636719\n",
      "epoch: 11 step: 78 loss: 0.3463321 acc: 0.9356651306152344\n",
      "epoch: 11 step: 79 loss: 0.45391148 acc: 0.8967399597167969\n",
      "epoch: 11 step: 80 loss: 0.3551124 acc: 0.9260177612304688\n",
      "epoch: 11 step: 81 loss: 0.35546535 acc: 0.9018783569335938\n",
      "epoch: 11 step: 82 loss: 0.28170744 acc: 0.9251365661621094\n",
      "epoch: 11 step: 83 loss: 0.37623602 acc: 0.9197425842285156\n",
      "epoch: 11 step: 84 loss: 0.27855018 acc: 0.9380149841308594\n",
      "epoch: 11 step: 85 loss: 0.35089567 acc: 0.9254074096679688\n",
      "epoch: 11 step: 86 loss: 0.3259155 acc: 0.9345207214355469\n",
      "epoch: 11 step: 87 loss: 0.3166948 acc: 0.9455490112304688\n",
      "epoch: 11 step: 88 loss: 0.31210783 acc: 0.9283599853515625\n",
      "epoch: 11 step: 89 loss: 0.31556642 acc: 0.9353446960449219\n",
      "epoch: 11 step: 90 loss: 0.4180969 acc: 0.9231185913085938\n",
      "epoch: 11 step: 91 loss: 0.32139793 acc: 0.93206787109375\n",
      "epoch: 11 step: 92 loss: 0.35549712 acc: 0.9239501953125\n",
      "epoch: 11 step: 93 loss: 0.35260144 acc: 0.9220352172851562\n",
      "epoch: 11 step: 94 loss: 0.3760202 acc: 0.9191246032714844\n",
      "epoch: 11 step: 95 loss: 0.37395215 acc: 0.8990135192871094\n",
      "epoch: 11 step: 96 loss: 0.39449102 acc: 0.9218597412109375\n",
      "epoch: 11 step: 97 loss: 0.3748444 acc: 0.9204597473144531\n",
      "epoch: 11 step: 98 loss: 0.30313268 acc: 0.9252853393554688\n",
      "epoch: 11 step: 99 loss: 0.48924604 acc: 0.8937721252441406\n",
      "epoch: 11 step: 100 loss: 0.37335727 acc: 0.9046745300292969\n",
      "epoch: 11 step: 101 loss: 0.34711877 acc: 0.9280052185058594\n",
      "epoch: 11 step: 102 loss: 0.39996204 acc: 0.9212684631347656\n",
      "epoch: 11 step: 103 loss: 0.44497842 acc: 0.9239616394042969\n",
      "epoch: 11 step: 104 loss: 0.35133502 acc: 0.908294677734375\n",
      "epoch: 11 step: 105 loss: 0.3596253 acc: 0.9188423156738281\n",
      "epoch: 11 step: 106 loss: 0.3290468 acc: 0.9273719787597656\n",
      "epoch: 11 step: 107 loss: 0.34696597 acc: 0.9042205810546875\n",
      "epoch: 11 step: 108 loss: 0.2926797 acc: 0.9348258972167969\n",
      "epoch: 11 step: 109 loss: 0.34868968 acc: 0.9086532592773438\n",
      "epoch: 11 step: 110 loss: 0.3053154 acc: 0.9281387329101562\n",
      "epoch: 11 step: 111 loss: 0.46505818 acc: 0.90185546875\n",
      "epoch: 11 step: 112 loss: 0.3039062 acc: 0.9209976196289062\n",
      "epoch: 11 step: 113 loss: 0.32967588 acc: 0.9409294128417969\n",
      "epoch: 11 step: 114 loss: 0.4484239 acc: 0.9291648864746094\n",
      "epoch: 11 step: 115 loss: 0.3678501 acc: 0.9275398254394531\n",
      "epoch: 11 step: 116 loss: 0.35317904 acc: 0.9104499816894531\n",
      "epoch: 11 step: 117 loss: 0.3111654 acc: 0.92919921875\n",
      "epoch: 11 step: 118 loss: 0.3060227 acc: 0.9223670959472656\n",
      "epoch: 11 step: 119 loss: 0.40584463 acc: 0.9067611694335938\n",
      "epoch: 11 step: 120 loss: 0.26368898 acc: 0.9227333068847656\n",
      "epoch: 11 step: 121 loss: 0.3411148 acc: 0.9061126708984375\n",
      "epoch: 11 step: 122 loss: 0.35002786 acc: 0.9049568176269531\n",
      "epoch: 11 step: 123 loss: 0.329737 acc: 0.9305381774902344\n",
      "epoch: 11 step: 124 loss: 0.3888594 acc: 0.9330008370535714\n",
      "epoch: 11 validation_loss: 0.32 validation_dice: 0.6790234323050488\n",
      "epoch: 11 test_dataset dice: 0.6157081530826211\n",
      "time cost 0.5341411828994751 min\n",
      "dice_best: 0.6790234323050488\n",
      "******************************** epoch  11  is finished. *********************************\n",
      "epoch: 12 step: 1 loss: 0.42733273 acc: 0.9077224731445312\n",
      "epoch: 12 step: 2 loss: 0.335994 acc: 0.926788330078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 step: 3 loss: 0.3986477 acc: 0.8999786376953125\n",
      "epoch: 12 step: 4 loss: 0.3544844 acc: 0.9159469604492188\n",
      "epoch: 12 step: 5 loss: 0.3838608 acc: 0.9210090637207031\n",
      "epoch: 12 step: 6 loss: 0.51949215 acc: 0.9164962768554688\n",
      "epoch: 12 step: 7 loss: 0.33268613 acc: 0.9163780212402344\n",
      "epoch: 12 step: 8 loss: 0.3544406 acc: 0.9268150329589844\n",
      "epoch: 12 step: 9 loss: 0.36132663 acc: 0.9227256774902344\n",
      "epoch: 12 step: 10 loss: 0.29474768 acc: 0.9252700805664062\n",
      "epoch: 12 step: 11 loss: 0.35182685 acc: 0.9197845458984375\n",
      "epoch: 12 step: 12 loss: 0.39638346 acc: 0.9074630737304688\n",
      "epoch: 12 step: 13 loss: 0.31667185 acc: 0.9376945495605469\n",
      "epoch: 12 step: 14 loss: 0.35552934 acc: 0.9234046936035156\n",
      "epoch: 12 step: 15 loss: 0.32469657 acc: 0.9269828796386719\n",
      "epoch: 12 step: 16 loss: 0.33185038 acc: 0.9353828430175781\n",
      "epoch: 12 step: 17 loss: 0.27842247 acc: 0.9387016296386719\n",
      "epoch: 12 step: 18 loss: 0.40179652 acc: 0.9104042053222656\n",
      "epoch: 12 step: 19 loss: 0.4335378 acc: 0.9106864929199219\n",
      "epoch: 12 step: 20 loss: 0.34305555 acc: 0.9138336181640625\n",
      "epoch: 12 step: 21 loss: 0.42023313 acc: 0.9055976867675781\n",
      "epoch: 12 step: 22 loss: 0.39113453 acc: 0.9041824340820312\n",
      "epoch: 12 step: 23 loss: 0.32837853 acc: 0.9225006103515625\n",
      "epoch: 12 step: 24 loss: 0.35084948 acc: 0.9178123474121094\n",
      "epoch: 12 step: 25 loss: 0.36312634 acc: 0.9189453125\n",
      "epoch: 12 step: 26 loss: 0.34826857 acc: 0.9034042358398438\n",
      "epoch: 12 step: 27 loss: 0.37669557 acc: 0.9032058715820312\n",
      "epoch: 12 step: 28 loss: 0.41969183 acc: 0.9174118041992188\n",
      "epoch: 12 step: 29 loss: 0.35274377 acc: 0.923126220703125\n",
      "epoch: 12 step: 30 loss: 0.3533488 acc: 0.9187889099121094\n",
      "epoch: 12 step: 31 loss: 0.2609911 acc: 0.9237327575683594\n",
      "epoch: 12 step: 32 loss: 0.3135374 acc: 0.9224624633789062\n",
      "epoch: 12 step: 33 loss: 0.353648 acc: 0.9070930480957031\n",
      "epoch: 12 step: 34 loss: 0.2985649 acc: 0.9161605834960938\n",
      "epoch: 12 step: 35 loss: 0.3614298 acc: 0.911346435546875\n",
      "epoch: 12 step: 36 loss: 0.35838684 acc: 0.923675537109375\n",
      "epoch: 12 step: 37 loss: 0.27430397 acc: 0.9448776245117188\n",
      "epoch: 12 step: 38 loss: 0.33619162 acc: 0.9365081787109375\n",
      "epoch: 12 step: 39 loss: 0.3295041 acc: 0.9278640747070312\n",
      "epoch: 12 step: 40 loss: 0.3248191 acc: 0.9248771667480469\n",
      "epoch: 12 step: 41 loss: 0.21964239 acc: 0.9452743530273438\n",
      "epoch: 12 step: 42 loss: 0.30682826 acc: 0.9252204895019531\n",
      "epoch: 12 step: 43 loss: 0.26206508 acc: 0.9344596862792969\n",
      "epoch: 12 step: 44 loss: 0.31073585 acc: 0.9338340759277344\n",
      "epoch: 12 step: 45 loss: 0.33530486 acc: 0.9301834106445312\n",
      "epoch: 12 step: 46 loss: 0.31282476 acc: 0.9118804931640625\n",
      "epoch: 12 step: 47 loss: 0.342347 acc: 0.922119140625\n",
      "epoch: 12 step: 48 loss: 0.27965122 acc: 0.9342422485351562\n",
      "epoch: 12 step: 49 loss: 0.37093842 acc: 0.9218673706054688\n",
      "epoch: 12 step: 50 loss: 0.26188108 acc: 0.932525634765625\n",
      "epoch: 12 step: 51 loss: 0.2636443 acc: 0.9325675964355469\n",
      "epoch: 12 step: 52 loss: 0.32106063 acc: 0.9263153076171875\n",
      "epoch: 12 step: 53 loss: 0.3065651 acc: 0.9278678894042969\n",
      "epoch: 12 step: 54 loss: 0.34163877 acc: 0.9226951599121094\n",
      "epoch: 12 step: 55 loss: 0.23782417 acc: 0.9409866333007812\n",
      "epoch: 12 step: 56 loss: 0.3169672 acc: 0.9230499267578125\n",
      "epoch: 12 step: 57 loss: 0.2708496 acc: 0.93048095703125\n",
      "epoch: 12 step: 58 loss: 0.28970903 acc: 0.9286117553710938\n",
      "epoch: 12 step: 59 loss: 0.2757603 acc: 0.9309654235839844\n",
      "epoch: 12 step: 60 loss: 0.20589973 acc: 0.9416656494140625\n",
      "epoch: 12 step: 61 loss: 0.2515619 acc: 0.9337806701660156\n",
      "epoch: 12 step: 62 loss: 0.19148348 acc: 0.9459877014160156\n",
      "epoch: 12 step: 63 loss: 0.2898704 acc: 0.9255752563476562\n",
      "epoch: 12 step: 64 loss: 0.36057815 acc: 0.9106330871582031\n",
      "epoch: 12 step: 65 loss: 0.27085322 acc: 0.9329376220703125\n",
      "epoch: 12 step: 66 loss: 0.3130247 acc: 0.9274826049804688\n",
      "epoch: 12 step: 67 loss: 0.26237622 acc: 0.9260444641113281\n",
      "epoch: 12 step: 68 loss: 0.25034955 acc: 0.931915283203125\n",
      "epoch: 12 step: 69 loss: 0.24355417 acc: 0.9451255798339844\n",
      "epoch: 12 step: 70 loss: 0.22617887 acc: 0.9403915405273438\n",
      "epoch: 12 step: 71 loss: 0.2542556 acc: 0.9351654052734375\n",
      "epoch: 12 step: 72 loss: 0.30997813 acc: 0.9170799255371094\n",
      "epoch: 12 step: 73 loss: 0.22984894 acc: 0.9317283630371094\n",
      "epoch: 12 step: 74 loss: 0.24147263 acc: 0.9213790893554688\n",
      "epoch: 12 step: 75 loss: 0.2480015 acc: 0.9371109008789062\n",
      "epoch: 12 step: 76 loss: 0.24825408 acc: 0.938873291015625\n",
      "epoch: 12 step: 77 loss: 0.24380887 acc: 0.950927734375\n",
      "epoch: 12 step: 78 loss: 0.2838076 acc: 0.935089111328125\n",
      "epoch: 12 step: 79 loss: 0.39040866 acc: 0.9119796752929688\n",
      "epoch: 12 step: 80 loss: 0.3021344 acc: 0.9268798828125\n",
      "epoch: 12 step: 81 loss: 0.3038891 acc: 0.9201469421386719\n",
      "epoch: 12 step: 82 loss: 0.21610767 acc: 0.9337158203125\n",
      "epoch: 12 step: 83 loss: 0.24307749 acc: 0.9320068359375\n",
      "epoch: 12 step: 84 loss: 0.28297573 acc: 0.9190216064453125\n",
      "epoch: 12 step: 85 loss: 0.299794 acc: 0.9230194091796875\n",
      "epoch: 12 step: 86 loss: 0.3318054 acc: 0.9257240295410156\n",
      "epoch: 12 step: 87 loss: 0.25237012 acc: 0.9390068054199219\n",
      "epoch: 12 step: 88 loss: 0.28033748 acc: 0.9470977783203125\n",
      "epoch: 12 step: 89 loss: 0.265468 acc: 0.9397697448730469\n",
      "epoch: 12 step: 90 loss: 0.26087728 acc: 0.9209938049316406\n",
      "epoch: 12 step: 91 loss: 0.28723842 acc: 0.9267654418945312\n",
      "epoch: 12 step: 92 loss: 0.3831986 acc: 0.9144363403320312\n",
      "epoch: 12 step: 93 loss: 0.32669577 acc: 0.9144935607910156\n",
      "epoch: 12 step: 94 loss: 0.27150273 acc: 0.9427909851074219\n",
      "epoch: 12 step: 95 loss: 0.27626416 acc: 0.9403076171875\n",
      "epoch: 12 step: 96 loss: 0.25050184 acc: 0.9358673095703125\n",
      "epoch: 12 step: 97 loss: 0.26769915 acc: 0.94281005859375\n",
      "epoch: 12 step: 98 loss: 0.270266 acc: 0.9498023986816406\n",
      "epoch: 12 step: 99 loss: 0.3308526 acc: 0.9311676025390625\n",
      "epoch: 12 step: 100 loss: 0.26313648 acc: 0.9355812072753906\n",
      "epoch: 12 step: 101 loss: 0.36555552 acc: 0.9158592224121094\n",
      "epoch: 12 step: 102 loss: 0.2500533 acc: 0.9346122741699219\n",
      "epoch: 12 step: 103 loss: 0.35221156 acc: 0.9239540100097656\n",
      "epoch: 12 step: 104 loss: 0.2965503 acc: 0.9354286193847656\n",
      "epoch: 12 step: 105 loss: 0.3651833 acc: 0.9154243469238281\n",
      "epoch: 12 step: 106 loss: 0.26014733 acc: 0.9262580871582031\n",
      "epoch: 12 step: 107 loss: 0.31212157 acc: 0.9186439514160156\n",
      "epoch: 12 step: 108 loss: 0.29042107 acc: 0.9353866577148438\n",
      "epoch: 12 step: 109 loss: 0.29049292 acc: 0.9242630004882812\n",
      "epoch: 12 step: 110 loss: 0.2884733 acc: 0.9387588500976562\n",
      "epoch: 12 step: 111 loss: 0.2529306 acc: 0.9358444213867188\n",
      "epoch: 12 step: 112 loss: 0.2924286 acc: 0.9344215393066406\n",
      "epoch: 12 step: 113 loss: 0.2820733 acc: 0.9374160766601562\n",
      "epoch: 12 step: 114 loss: 0.29572156 acc: 0.9271965026855469\n",
      "epoch: 12 step: 115 loss: 0.2237565 acc: 0.9331398010253906\n",
      "epoch: 12 step: 116 loss: 0.32862237 acc: 0.9196586608886719\n",
      "epoch: 12 step: 117 loss: 0.31085423 acc: 0.9151153564453125\n",
      "epoch: 12 step: 118 loss: 0.25897217 acc: 0.935211181640625\n",
      "epoch: 12 step: 119 loss: 0.2548023 acc: 0.9268112182617188\n",
      "epoch: 12 step: 120 loss: 0.27607453 acc: 0.9409904479980469\n",
      "epoch: 12 step: 121 loss: 0.32959425 acc: 0.9187278747558594\n",
      "epoch: 12 step: 122 loss: 0.23075277 acc: 0.9361343383789062\n",
      "epoch: 12 step: 123 loss: 0.28649458 acc: 0.9230384826660156\n",
      "epoch: 12 step: 124 loss: 0.32926056 acc: 0.9468994140625\n",
      "epoch: 12 validation_loss: 0.311 validation_dice: 0.66170711236485\n",
      "epoch: 12 test_dataset dice: 0.6397583056310745\n",
      "time cost 0.5340247829755147 min\n",
      "dice_best: 0.6790234323050488\n",
      "******************************** epoch  12  is finished. *********************************\n",
      "epoch: 13 step: 1 loss: 0.23701034 acc: 0.9321975708007812\n",
      "epoch: 13 step: 2 loss: 0.30762497 acc: 0.9196891784667969\n",
      "epoch: 13 step: 3 loss: 0.24012622 acc: 0.9297447204589844\n",
      "epoch: 13 step: 4 loss: 0.34794518 acc: 0.9163932800292969\n",
      "epoch: 13 step: 5 loss: 0.29693112 acc: 0.910736083984375\n",
      "epoch: 13 step: 6 loss: 0.29912105 acc: 0.9171562194824219\n",
      "epoch: 13 step: 7 loss: 0.27839103 acc: 0.9309310913085938\n",
      "epoch: 13 step: 8 loss: 0.25133732 acc: 0.9342002868652344\n",
      "epoch: 13 step: 9 loss: 0.29562905 acc: 0.9173660278320312\n",
      "epoch: 13 step: 10 loss: 0.33117706 acc: 0.9101638793945312\n",
      "epoch: 13 step: 11 loss: 0.27231035 acc: 0.9378204345703125\n",
      "epoch: 13 step: 12 loss: 0.21436048 acc: 0.9438056945800781\n",
      "epoch: 13 step: 13 loss: 0.23921812 acc: 0.9355583190917969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 step: 14 loss: 0.33673584 acc: 0.930633544921875\n",
      "epoch: 13 step: 15 loss: 0.21496959 acc: 0.9468002319335938\n",
      "epoch: 13 step: 16 loss: 0.28036946 acc: 0.9202461242675781\n",
      "epoch: 13 step: 17 loss: 0.23348998 acc: 0.9376373291015625\n",
      "epoch: 13 step: 18 loss: 0.26921976 acc: 0.9394035339355469\n",
      "epoch: 13 step: 19 loss: 0.35616627 acc: 0.9110794067382812\n",
      "epoch: 13 step: 20 loss: 0.22956036 acc: 0.928192138671875\n",
      "epoch: 13 step: 21 loss: 0.22374785 acc: 0.9249000549316406\n",
      "epoch: 13 step: 22 loss: 0.34480762 acc: 0.8982734680175781\n",
      "epoch: 13 step: 23 loss: 0.3766034 acc: 0.8990821838378906\n",
      "epoch: 13 step: 24 loss: 0.26738304 acc: 0.9281349182128906\n",
      "epoch: 13 step: 25 loss: 0.27680662 acc: 0.9273910522460938\n",
      "epoch: 13 step: 26 loss: 0.3199912 acc: 0.9210395812988281\n",
      "epoch: 13 step: 27 loss: 0.2519631 acc: 0.9262237548828125\n",
      "epoch: 13 step: 28 loss: 0.31189755 acc: 0.9276237487792969\n",
      "epoch: 13 step: 29 loss: 0.23040745 acc: 0.940460205078125\n",
      "epoch: 13 step: 30 loss: 0.29423624 acc: 0.9375762939453125\n",
      "epoch: 13 step: 31 loss: 0.2915896 acc: 0.94036865234375\n",
      "epoch: 13 step: 32 loss: 0.34379664 acc: 0.9346084594726562\n",
      "epoch: 13 step: 33 loss: 0.28788528 acc: 0.9334068298339844\n",
      "epoch: 13 step: 34 loss: 0.22731538 acc: 0.9357948303222656\n",
      "epoch: 13 step: 35 loss: 0.41643068 acc: 0.8953742980957031\n",
      "epoch: 13 step: 36 loss: 0.30035204 acc: 0.9103965759277344\n",
      "epoch: 13 step: 37 loss: 0.3033096 acc: 0.9084815979003906\n",
      "epoch: 13 step: 38 loss: 0.4709307 acc: 0.8866958618164062\n",
      "epoch: 13 step: 39 loss: 0.28968078 acc: 0.933349609375\n",
      "epoch: 13 step: 40 loss: 0.32256895 acc: 0.9311904907226562\n",
      "epoch: 13 step: 41 loss: 0.2976005 acc: 0.9220237731933594\n",
      "epoch: 13 step: 42 loss: 0.42448866 acc: 0.9191932678222656\n",
      "epoch: 13 step: 43 loss: 0.31588057 acc: 0.9239273071289062\n",
      "epoch: 13 step: 44 loss: 0.2707368 acc: 0.9387969970703125\n",
      "epoch: 13 step: 45 loss: 0.37839177 acc: 0.9301910400390625\n",
      "epoch: 13 step: 46 loss: 0.31673345 acc: 0.930908203125\n",
      "epoch: 13 step: 47 loss: 0.38357607 acc: 0.9196586608886719\n",
      "epoch: 13 step: 48 loss: 0.4069239 acc: 0.9226799011230469\n",
      "epoch: 13 step: 49 loss: 0.3398312 acc: 0.9136581420898438\n",
      "epoch: 13 step: 50 loss: 0.40040347 acc: 0.916534423828125\n",
      "epoch: 13 step: 51 loss: 0.32508272 acc: 0.9240493774414062\n",
      "epoch: 13 step: 52 loss: 0.31267694 acc: 0.9276657104492188\n",
      "epoch: 13 step: 53 loss: 0.40723142 acc: 0.9032783508300781\n",
      "epoch: 13 step: 54 loss: 0.35028017 acc: 0.9437675476074219\n",
      "epoch: 13 step: 55 loss: 0.26695877 acc: 0.9308395385742188\n",
      "epoch: 13 step: 56 loss: 0.27850354 acc: 0.9420204162597656\n",
      "epoch: 13 step: 57 loss: 0.32164067 acc: 0.9369773864746094\n",
      "epoch: 13 step: 58 loss: 0.37414077 acc: 0.9123191833496094\n",
      "epoch: 13 step: 59 loss: 0.33215272 acc: 0.9050064086914062\n",
      "epoch: 13 step: 60 loss: 0.24467266 acc: 0.9244918823242188\n",
      "epoch: 13 step: 61 loss: 0.33873755 acc: 0.9143333435058594\n",
      "epoch: 13 step: 62 loss: 0.3665539 acc: 0.9127349853515625\n",
      "epoch: 13 step: 63 loss: 0.3503358 acc: 0.9209022521972656\n",
      "epoch: 13 step: 64 loss: 0.36551705 acc: 0.9281196594238281\n",
      "epoch: 13 step: 65 loss: 0.34137198 acc: 0.939483642578125\n",
      "epoch: 13 step: 66 loss: 0.32644635 acc: 0.9265251159667969\n",
      "epoch: 13 step: 67 loss: 0.30938503 acc: 0.9242095947265625\n",
      "epoch: 13 step: 68 loss: 0.3033738 acc: 0.9347038269042969\n",
      "epoch: 13 step: 69 loss: 0.24774241 acc: 0.9244575500488281\n",
      "epoch: 13 step: 70 loss: 0.27676907 acc: 0.9217109680175781\n",
      "epoch: 13 step: 71 loss: 0.34907043 acc: 0.9178314208984375\n",
      "epoch: 13 step: 72 loss: 0.27108216 acc: 0.9222488403320312\n",
      "epoch: 13 step: 73 loss: 0.32705012 acc: 0.9165840148925781\n",
      "epoch: 13 step: 74 loss: 0.23797569 acc: 0.9349632263183594\n",
      "epoch: 13 step: 75 loss: 0.25513953 acc: 0.9404678344726562\n",
      "epoch: 13 step: 76 loss: 0.2543622 acc: 0.9438400268554688\n",
      "epoch: 13 step: 77 loss: 0.4221832 acc: 0.9243927001953125\n",
      "epoch: 13 step: 78 loss: 0.31516406 acc: 0.9340782165527344\n",
      "epoch: 13 step: 79 loss: 0.3251744 acc: 0.9345855712890625\n",
      "epoch: 13 step: 80 loss: 0.26853895 acc: 0.9376869201660156\n",
      "epoch: 13 step: 81 loss: 0.25058004 acc: 0.9233436584472656\n",
      "epoch: 13 step: 82 loss: 0.33256274 acc: 0.91192626953125\n",
      "epoch: 13 step: 83 loss: 0.27582404 acc: 0.9248771667480469\n",
      "epoch: 13 step: 84 loss: 0.30387917 acc: 0.9202232360839844\n",
      "epoch: 13 step: 85 loss: 0.33895093 acc: 0.9066696166992188\n",
      "epoch: 13 step: 86 loss: 0.33915323 acc: 0.9257659912109375\n",
      "epoch: 13 step: 87 loss: 0.2180202 acc: 0.9417152404785156\n",
      "epoch: 13 step: 88 loss: 0.27828085 acc: 0.9306907653808594\n",
      "epoch: 13 step: 89 loss: 0.29476538 acc: 0.9296989440917969\n",
      "epoch: 13 step: 90 loss: 0.22838792 acc: 0.9332275390625\n",
      "epoch: 13 step: 91 loss: 0.2721442 acc: 0.9324874877929688\n",
      "epoch: 13 step: 92 loss: 0.2241947 acc: 0.935516357421875\n",
      "epoch: 13 step: 93 loss: 0.36760095 acc: 0.9229240417480469\n",
      "epoch: 13 step: 94 loss: 0.3325528 acc: 0.9315605163574219\n",
      "epoch: 13 step: 95 loss: 0.35306698 acc: 0.9184799194335938\n",
      "epoch: 13 step: 96 loss: 0.2473673 acc: 0.9361190795898438\n",
      "epoch: 13 step: 97 loss: 0.40741956 acc: 0.9287452697753906\n",
      "epoch: 13 step: 98 loss: 0.33753744 acc: 0.9110565185546875\n",
      "epoch: 13 step: 99 loss: 0.32382733 acc: 0.9232139587402344\n",
      "epoch: 13 step: 100 loss: 0.32731152 acc: 0.9202919006347656\n",
      "epoch: 13 step: 101 loss: 0.31991976 acc: 0.9321937561035156\n",
      "epoch: 13 step: 102 loss: 0.36347944 acc: 0.9193344116210938\n",
      "epoch: 13 step: 103 loss: 0.27901486 acc: 0.9261970520019531\n",
      "epoch: 13 step: 104 loss: 0.2833682 acc: 0.9346542358398438\n",
      "epoch: 13 step: 105 loss: 0.40611494 acc: 0.9166603088378906\n",
      "epoch: 13 step: 106 loss: 0.4564871 acc: 0.8876457214355469\n",
      "epoch: 13 step: 107 loss: 0.31261715 acc: 0.9195137023925781\n",
      "epoch: 13 step: 108 loss: 0.3692293 acc: 0.9293899536132812\n",
      "epoch: 13 step: 109 loss: 0.29712892 acc: 0.9356727600097656\n",
      "epoch: 13 step: 110 loss: 0.31281984 acc: 0.9361114501953125\n",
      "epoch: 13 step: 111 loss: 0.34001532 acc: 0.9295768737792969\n",
      "epoch: 13 step: 112 loss: 0.4071795 acc: 0.9287376403808594\n",
      "epoch: 13 step: 113 loss: 0.26809826 acc: 0.93133544921875\n",
      "epoch: 13 step: 114 loss: 0.23327065 acc: 0.9327774047851562\n",
      "epoch: 13 step: 115 loss: 0.32180625 acc: 0.9217872619628906\n",
      "epoch: 13 step: 116 loss: 0.29242745 acc: 0.9297256469726562\n",
      "epoch: 13 step: 117 loss: 0.309703 acc: 0.9267311096191406\n",
      "epoch: 13 step: 118 loss: 0.29427838 acc: 0.9255638122558594\n",
      "epoch: 13 step: 119 loss: 0.3422068 acc: 0.9178810119628906\n",
      "epoch: 13 step: 120 loss: 0.27411702 acc: 0.93109130859375\n",
      "epoch: 13 step: 121 loss: 0.27931154 acc: 0.9272079467773438\n",
      "epoch: 13 step: 122 loss: 0.31905556 acc: 0.9333534240722656\n",
      "epoch: 13 step: 123 loss: 0.29954568 acc: 0.9169731140136719\n",
      "epoch: 13 step: 124 loss: 0.19325975 acc: 0.9426792689732143\n",
      "epoch: 13 validation_loss: 0.302 validation_dice: 0.6944907070675245\n",
      "epoch: 13 test_dataset dice: 0.5762808962745963\n",
      "time cost 0.5344645738601684 min\n",
      "dice_best: 0.6944907070675245\n",
      "******************************** epoch  13  is finished. *********************************\n",
      "epoch: 14 step: 1 loss: 0.29219875 acc: 0.9280319213867188\n",
      "epoch: 14 step: 2 loss: 0.31423995 acc: 0.9343223571777344\n",
      "epoch: 14 step: 3 loss: 0.2477972 acc: 0.9377937316894531\n",
      "epoch: 14 step: 4 loss: 0.2526452 acc: 0.9330253601074219\n",
      "epoch: 14 step: 5 loss: 0.267613 acc: 0.9337806701660156\n",
      "epoch: 14 step: 6 loss: 0.33472744 acc: 0.916595458984375\n",
      "epoch: 14 step: 7 loss: 0.28902793 acc: 0.9168586730957031\n",
      "epoch: 14 step: 8 loss: 0.2305094 acc: 0.933837890625\n",
      "epoch: 14 step: 9 loss: 0.23367207 acc: 0.9408149719238281\n",
      "epoch: 14 step: 10 loss: 0.30340245 acc: 0.9230995178222656\n",
      "epoch: 14 step: 11 loss: 0.33428952 acc: 0.9167861938476562\n",
      "epoch: 14 step: 12 loss: 0.27036 acc: 0.9328384399414062\n",
      "epoch: 14 step: 13 loss: 0.31025204 acc: 0.9197235107421875\n",
      "epoch: 14 step: 14 loss: 0.32103786 acc: 0.9081039428710938\n",
      "epoch: 14 step: 15 loss: 0.26949745 acc: 0.9357376098632812\n",
      "epoch: 14 step: 16 loss: 0.2759062 acc: 0.9253578186035156\n",
      "epoch: 14 step: 17 loss: 0.24707842 acc: 0.9332427978515625\n",
      "epoch: 14 step: 18 loss: 0.30502868 acc: 0.9374732971191406\n",
      "epoch: 14 step: 19 loss: 0.32462335 acc: 0.9306678771972656\n",
      "epoch: 14 step: 20 loss: 0.2978904 acc: 0.934600830078125\n",
      "epoch: 14 step: 21 loss: 0.27640155 acc: 0.9341392517089844\n",
      "epoch: 14 step: 22 loss: 0.24741551 acc: 0.9276924133300781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 step: 23 loss: 0.2783016 acc: 0.9239654541015625\n",
      "epoch: 14 step: 24 loss: 0.35421807 acc: 0.909881591796875\n",
      "epoch: 14 step: 25 loss: 0.22629699 acc: 0.9404029846191406\n",
      "epoch: 14 step: 26 loss: 0.28155005 acc: 0.92291259765625\n",
      "epoch: 14 step: 27 loss: 0.2447602 acc: 0.9365577697753906\n",
      "epoch: 14 step: 28 loss: 0.24321805 acc: 0.9542160034179688\n",
      "epoch: 14 step: 29 loss: 0.2224879 acc: 0.9433059692382812\n",
      "epoch: 14 step: 30 loss: 0.28463608 acc: 0.9409675598144531\n",
      "epoch: 14 step: 31 loss: 0.2942835 acc: 0.9367332458496094\n",
      "epoch: 14 step: 32 loss: 0.2338209 acc: 0.9473228454589844\n",
      "epoch: 14 step: 33 loss: 0.2981427 acc: 0.9386863708496094\n",
      "epoch: 14 step: 34 loss: 0.3031653 acc: 0.9387626647949219\n",
      "epoch: 14 step: 35 loss: 0.293768 acc: 0.9450798034667969\n",
      "epoch: 14 step: 36 loss: 0.31306598 acc: 0.927703857421875\n",
      "epoch: 14 step: 37 loss: 0.25122565 acc: 0.9270515441894531\n",
      "epoch: 14 step: 38 loss: 0.3262719 acc: 0.9274330139160156\n",
      "epoch: 14 step: 39 loss: 0.3006993 acc: 0.9353523254394531\n",
      "epoch: 14 step: 40 loss: 0.34036 acc: 0.9211158752441406\n",
      "epoch: 14 step: 41 loss: 0.3099207 acc: 0.9155845642089844\n",
      "epoch: 14 step: 42 loss: 0.23301779 acc: 0.9260673522949219\n",
      "epoch: 14 step: 43 loss: 0.22094819 acc: 0.9371109008789062\n",
      "epoch: 14 step: 44 loss: 0.3265051 acc: 0.9181098937988281\n",
      "epoch: 14 step: 45 loss: 0.37573284 acc: 0.9065322875976562\n",
      "epoch: 14 step: 46 loss: 0.2536204 acc: 0.9255752563476562\n",
      "epoch: 14 step: 47 loss: 0.2861782 acc: 0.9345436096191406\n",
      "epoch: 14 step: 48 loss: 0.30852765 acc: 0.9306373596191406\n",
      "epoch: 14 step: 49 loss: 0.25701284 acc: 0.9350852966308594\n",
      "epoch: 14 step: 50 loss: 0.29124224 acc: 0.9324836730957031\n",
      "epoch: 14 step: 51 loss: 0.28158018 acc: 0.9370307922363281\n",
      "epoch: 14 step: 52 loss: 0.2526085 acc: 0.9409942626953125\n",
      "epoch: 14 step: 53 loss: 0.26407683 acc: 0.9252052307128906\n",
      "epoch: 14 step: 54 loss: 0.2171437 acc: 0.9410324096679688\n",
      "epoch: 14 step: 55 loss: 0.24797072 acc: 0.9426002502441406\n",
      "epoch: 14 step: 56 loss: 0.29287636 acc: 0.9170188903808594\n",
      "epoch: 14 step: 57 loss: 0.24832842 acc: 0.9353065490722656\n",
      "epoch: 14 step: 58 loss: 0.2337225 acc: 0.9390220642089844\n",
      "epoch: 14 step: 59 loss: 0.23087227 acc: 0.9334487915039062\n",
      "epoch: 14 step: 60 loss: 0.31467095 acc: 0.9146385192871094\n",
      "epoch: 14 step: 61 loss: 0.29068363 acc: 0.9219322204589844\n",
      "epoch: 14 step: 62 loss: 0.20861095 acc: 0.9410820007324219\n",
      "epoch: 14 step: 63 loss: 0.23379605 acc: 0.93157958984375\n",
      "epoch: 14 step: 64 loss: 0.2418691 acc: 0.9393196105957031\n",
      "epoch: 14 step: 65 loss: 0.30789155 acc: 0.9407768249511719\n",
      "epoch: 14 step: 66 loss: 0.2581468 acc: 0.9307479858398438\n",
      "epoch: 14 step: 67 loss: 0.27751055 acc: 0.9374809265136719\n",
      "epoch: 14 step: 68 loss: 0.26071316 acc: 0.9381065368652344\n",
      "epoch: 14 step: 69 loss: 0.24666935 acc: 0.9334220886230469\n",
      "epoch: 14 step: 70 loss: 0.25655195 acc: 0.9405097961425781\n",
      "epoch: 14 step: 71 loss: 0.22409377 acc: 0.9438247680664062\n",
      "epoch: 14 step: 72 loss: 0.2947685 acc: 0.9371719360351562\n",
      "epoch: 14 step: 73 loss: 0.26629284 acc: 0.9413032531738281\n",
      "epoch: 14 step: 74 loss: 0.36146018 acc: 0.9298439025878906\n",
      "epoch: 14 step: 75 loss: 0.24676704 acc: 0.9344978332519531\n",
      "epoch: 14 step: 76 loss: 0.27157384 acc: 0.9237480163574219\n",
      "epoch: 14 step: 77 loss: 0.2074859 acc: 0.9338836669921875\n",
      "epoch: 14 step: 78 loss: 0.2708175 acc: 0.9282569885253906\n",
      "epoch: 14 step: 79 loss: 0.2755298 acc: 0.9237213134765625\n",
      "epoch: 14 step: 80 loss: 0.30863535 acc: 0.9326820373535156\n",
      "epoch: 14 step: 81 loss: 0.319518 acc: 0.9219512939453125\n",
      "epoch: 14 step: 82 loss: 0.29693577 acc: 0.9244155883789062\n",
      "epoch: 14 step: 83 loss: 0.31362417 acc: 0.9214248657226562\n",
      "epoch: 14 step: 84 loss: 0.2618851 acc: 0.9301567077636719\n",
      "epoch: 14 step: 85 loss: 0.29383725 acc: 0.9196281433105469\n",
      "epoch: 14 step: 86 loss: 0.18273541 acc: 0.9472274780273438\n",
      "epoch: 14 step: 87 loss: 0.30197275 acc: 0.9242362976074219\n",
      "epoch: 14 step: 88 loss: 0.25218588 acc: 0.9376487731933594\n",
      "epoch: 14 step: 89 loss: 0.27539143 acc: 0.9316215515136719\n",
      "epoch: 14 step: 90 loss: 0.26730758 acc: 0.9330520629882812\n",
      "epoch: 14 step: 91 loss: 0.24383843 acc: 0.9334754943847656\n",
      "epoch: 14 step: 92 loss: 0.27914107 acc: 0.9267082214355469\n",
      "epoch: 14 step: 93 loss: 0.19765463 acc: 0.9394454956054688\n",
      "epoch: 14 step: 94 loss: 0.22343054 acc: 0.9346847534179688\n",
      "epoch: 14 step: 95 loss: 0.23735863 acc: 0.936981201171875\n",
      "epoch: 14 step: 96 loss: 0.20394354 acc: 0.9421424865722656\n",
      "epoch: 14 step: 97 loss: 0.20641531 acc: 0.9396629333496094\n",
      "epoch: 14 step: 98 loss: 0.34931588 acc: 0.9230690002441406\n",
      "epoch: 14 step: 99 loss: 0.22969423 acc: 0.9448890686035156\n",
      "epoch: 14 step: 100 loss: 0.21406329 acc: 0.9422721862792969\n",
      "epoch: 14 step: 101 loss: 0.26906884 acc: 0.9293365478515625\n",
      "epoch: 14 step: 102 loss: 0.22681904 acc: 0.9476432800292969\n",
      "epoch: 14 step: 103 loss: 0.26935452 acc: 0.9297447204589844\n",
      "epoch: 14 step: 104 loss: 0.2102786 acc: 0.9411087036132812\n",
      "epoch: 14 step: 105 loss: 0.26517192 acc: 0.9371414184570312\n",
      "epoch: 14 step: 106 loss: 0.25053582 acc: 0.9348602294921875\n",
      "epoch: 14 step: 107 loss: 0.2613081 acc: 0.9418258666992188\n",
      "epoch: 14 step: 108 loss: 0.24734741 acc: 0.928009033203125\n",
      "epoch: 14 step: 109 loss: 0.24948066 acc: 0.9322128295898438\n",
      "epoch: 14 step: 110 loss: 0.17187314 acc: 0.9505844116210938\n",
      "epoch: 14 step: 111 loss: 0.24487683 acc: 0.9238395690917969\n",
      "epoch: 14 step: 112 loss: 0.25368163 acc: 0.9404144287109375\n",
      "epoch: 14 step: 113 loss: 0.30113706 acc: 0.912139892578125\n",
      "epoch: 14 step: 114 loss: 0.264637 acc: 0.9433021545410156\n",
      "epoch: 14 step: 115 loss: 0.2569778 acc: 0.933258056640625\n",
      "epoch: 14 step: 116 loss: 0.34067163 acc: 0.9326972961425781\n",
      "epoch: 14 step: 117 loss: 0.25033337 acc: 0.94024658203125\n",
      "epoch: 14 step: 118 loss: 0.2623227 acc: 0.941162109375\n",
      "epoch: 14 step: 119 loss: 0.29272908 acc: 0.9267158508300781\n",
      "epoch: 14 step: 120 loss: 0.34633553 acc: 0.9070854187011719\n",
      "epoch: 14 step: 121 loss: 0.22358933 acc: 0.9249420166015625\n",
      "epoch: 14 step: 122 loss: 0.25817475 acc: 0.9290771484375\n",
      "epoch: 14 step: 123 loss: 0.25091505 acc: 0.9467048645019531\n",
      "epoch: 14 step: 124 loss: 0.41566503 acc: 0.9188929966517857\n",
      "epoch: 14 validation_loss: 0.324 validation_dice: 0.68176701990543\n",
      "epoch: 14 test_dataset dice: 0.6202340277323635\n",
      "time cost 0.5365572094917297 min\n",
      "dice_best: 0.6944907070675245\n",
      "******************************** epoch  14  is finished. *********************************\n",
      "epoch: 15 step: 1 loss: 0.2741916 acc: 0.9148292541503906\n",
      "epoch: 15 step: 2 loss: 0.2978911 acc: 0.9225425720214844\n",
      "epoch: 15 step: 3 loss: 0.4541425 acc: 0.8947105407714844\n",
      "epoch: 15 step: 4 loss: 0.24315754 acc: 0.9273262023925781\n",
      "epoch: 15 step: 5 loss: 0.26954442 acc: 0.9219131469726562\n",
      "epoch: 15 step: 6 loss: 0.32914707 acc: 0.9241485595703125\n",
      "epoch: 15 step: 7 loss: 0.34100518 acc: 0.9215316772460938\n",
      "epoch: 15 step: 8 loss: 0.23482014 acc: 0.9233436584472656\n",
      "epoch: 15 step: 9 loss: 0.26523724 acc: 0.9351387023925781\n",
      "epoch: 15 step: 10 loss: 0.24525344 acc: 0.9435653686523438\n",
      "epoch: 15 step: 11 loss: 0.256066 acc: 0.9407196044921875\n",
      "epoch: 15 step: 12 loss: 0.32360515 acc: 0.9326515197753906\n",
      "epoch: 15 step: 13 loss: 0.25070256 acc: 0.9392738342285156\n",
      "epoch: 15 step: 14 loss: 0.2669779 acc: 0.9323272705078125\n",
      "epoch: 15 step: 15 loss: 0.31837815 acc: 0.9249229431152344\n",
      "epoch: 15 step: 16 loss: 0.26399377 acc: 0.9337120056152344\n",
      "epoch: 15 step: 17 loss: 0.28144115 acc: 0.9202842712402344\n",
      "epoch: 15 step: 18 loss: 0.27876747 acc: 0.9352035522460938\n",
      "epoch: 15 step: 19 loss: 0.20958361 acc: 0.9408721923828125\n",
      "epoch: 15 step: 20 loss: 0.22178355 acc: 0.9406318664550781\n",
      "epoch: 15 step: 21 loss: 0.24805416 acc: 0.9351730346679688\n",
      "epoch: 15 step: 22 loss: 0.22790577 acc: 0.942718505859375\n",
      "epoch: 15 step: 23 loss: 0.22115584 acc: 0.9466400146484375\n",
      "epoch: 15 step: 24 loss: 0.2901269 acc: 0.9289665222167969\n",
      "epoch: 15 step: 25 loss: 0.21367022 acc: 0.9371910095214844\n",
      "epoch: 15 step: 26 loss: 0.2482661 acc: 0.9356040954589844\n",
      "epoch: 15 step: 27 loss: 0.27912664 acc: 0.9277191162109375\n",
      "epoch: 15 step: 28 loss: 0.22726002 acc: 0.9368476867675781\n",
      "epoch: 15 step: 29 loss: 0.18934004 acc: 0.9465827941894531\n",
      "epoch: 15 step: 30 loss: 0.22275905 acc: 0.9320068359375\n",
      "epoch: 15 step: 31 loss: 0.32186496 acc: 0.9239273071289062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 step: 32 loss: 0.28287992 acc: 0.9433937072753906\n",
      "epoch: 15 step: 33 loss: 0.21258761 acc: 0.9385261535644531\n",
      "epoch: 15 step: 34 loss: 0.2478321 acc: 0.930908203125\n",
      "epoch: 15 step: 35 loss: 0.26669538 acc: 0.9310226440429688\n",
      "epoch: 15 step: 36 loss: 0.292279 acc: 0.9184455871582031\n",
      "epoch: 15 step: 37 loss: 0.21410728 acc: 0.9392776489257812\n",
      "epoch: 15 step: 38 loss: 0.24045463 acc: 0.9333381652832031\n",
      "epoch: 15 step: 39 loss: 0.26422736 acc: 0.936798095703125\n",
      "epoch: 15 step: 40 loss: 0.3548816 acc: 0.921173095703125\n",
      "epoch: 15 step: 41 loss: 0.2430441 acc: 0.9339179992675781\n",
      "epoch: 15 step: 42 loss: 0.226422 acc: 0.9389839172363281\n",
      "epoch: 15 step: 43 loss: 0.17378828 acc: 0.9557991027832031\n",
      "epoch: 15 step: 44 loss: 0.25122407 acc: 0.9277992248535156\n",
      "epoch: 15 step: 45 loss: 0.2516591 acc: 0.9255828857421875\n",
      "epoch: 15 step: 46 loss: 0.28776962 acc: 0.9217453002929688\n",
      "epoch: 15 step: 47 loss: 0.27212942 acc: 0.937652587890625\n",
      "epoch: 15 step: 48 loss: 0.3256804 acc: 0.9355697631835938\n",
      "epoch: 15 step: 49 loss: 0.24908365 acc: 0.9350662231445312\n",
      "epoch: 15 step: 50 loss: 0.26789722 acc: 0.948333740234375\n",
      "epoch: 15 step: 51 loss: 0.31769326 acc: 0.9290924072265625\n",
      "epoch: 15 step: 52 loss: 0.27109742 acc: 0.9281997680664062\n",
      "epoch: 15 step: 53 loss: 0.21288936 acc: 0.9295997619628906\n",
      "epoch: 15 step: 54 loss: 0.347316 acc: 0.911865234375\n",
      "epoch: 15 step: 55 loss: 0.31782722 acc: 0.9164772033691406\n",
      "epoch: 15 step: 56 loss: 0.29173964 acc: 0.9198074340820312\n",
      "epoch: 15 step: 57 loss: 0.27773142 acc: 0.9145240783691406\n",
      "epoch: 15 step: 58 loss: 0.22094955 acc: 0.9263420104980469\n",
      "epoch: 15 step: 59 loss: 0.28054577 acc: 0.935943603515625\n",
      "epoch: 15 step: 60 loss: 0.2222659 acc: 0.9375114440917969\n",
      "epoch: 15 step: 61 loss: 0.22362632 acc: 0.9368667602539062\n",
      "epoch: 15 step: 62 loss: 0.24881066 acc: 0.9422149658203125\n",
      "epoch: 15 step: 63 loss: 0.3027072 acc: 0.9350433349609375\n",
      "epoch: 15 step: 64 loss: 0.26864278 acc: 0.9430046081542969\n",
      "epoch: 15 step: 65 loss: 0.25809294 acc: 0.9425392150878906\n",
      "epoch: 15 step: 66 loss: 0.2316139 acc: 0.9429664611816406\n",
      "epoch: 15 step: 67 loss: 0.19726868 acc: 0.9445762634277344\n",
      "epoch: 15 step: 68 loss: 0.20165037 acc: 0.9489212036132812\n",
      "epoch: 15 step: 69 loss: 0.21281403 acc: 0.9468498229980469\n",
      "epoch: 15 step: 70 loss: 0.29245654 acc: 0.927093505859375\n",
      "epoch: 15 step: 71 loss: 0.20330086 acc: 0.9324874877929688\n",
      "epoch: 15 step: 72 loss: 0.2348798 acc: 0.9361305236816406\n",
      "epoch: 15 step: 73 loss: 0.20851833 acc: 0.944549560546875\n",
      "epoch: 15 step: 74 loss: 0.21720152 acc: 0.9435234069824219\n",
      "epoch: 15 step: 75 loss: 0.21269071 acc: 0.9387702941894531\n",
      "epoch: 15 step: 76 loss: 0.23133467 acc: 0.9315299987792969\n",
      "epoch: 15 step: 77 loss: 0.18958601 acc: 0.9449615478515625\n",
      "epoch: 15 step: 78 loss: 0.18548903 acc: 0.9548492431640625\n",
      "epoch: 15 step: 79 loss: 0.18722528 acc: 0.947235107421875\n",
      "epoch: 15 step: 80 loss: 0.20270868 acc: 0.9312324523925781\n",
      "epoch: 15 step: 81 loss: 0.19843301 acc: 0.9452133178710938\n",
      "epoch: 15 step: 82 loss: 0.18294218 acc: 0.9353485107421875\n",
      "epoch: 15 step: 83 loss: 0.21504077 acc: 0.9306716918945312\n",
      "epoch: 15 step: 84 loss: 0.20314693 acc: 0.9488716125488281\n",
      "epoch: 15 step: 85 loss: 0.2333969 acc: 0.9318466186523438\n",
      "epoch: 15 step: 86 loss: 0.26653025 acc: 0.9393577575683594\n",
      "epoch: 15 step: 87 loss: 0.27142328 acc: 0.924102783203125\n",
      "epoch: 15 step: 88 loss: 0.19276193 acc: 0.9484939575195312\n",
      "epoch: 15 step: 89 loss: 0.25320503 acc: 0.9306297302246094\n",
      "epoch: 15 step: 90 loss: 0.23153682 acc: 0.9412155151367188\n",
      "epoch: 15 step: 91 loss: 0.24667478 acc: 0.9337959289550781\n",
      "epoch: 15 step: 92 loss: 0.18901078 acc: 0.9470291137695312\n",
      "epoch: 15 step: 93 loss: 0.22165199 acc: 0.946014404296875\n",
      "epoch: 15 step: 94 loss: 0.22822452 acc: 0.9405250549316406\n",
      "epoch: 15 step: 95 loss: 0.23315696 acc: 0.9267196655273438\n",
      "epoch: 15 step: 96 loss: 0.22985837 acc: 0.9405059814453125\n",
      "epoch: 15 step: 97 loss: 0.26614243 acc: 0.9256744384765625\n",
      "epoch: 15 step: 98 loss: 0.19601521 acc: 0.94219970703125\n",
      "epoch: 15 step: 99 loss: 0.19635408 acc: 0.9495162963867188\n",
      "epoch: 15 step: 100 loss: 0.2308773 acc: 0.93939208984375\n",
      "epoch: 15 step: 101 loss: 0.21990752 acc: 0.9553070068359375\n",
      "epoch: 15 step: 102 loss: 0.22446167 acc: 0.9456062316894531\n",
      "epoch: 15 step: 103 loss: 0.22095385 acc: 0.9499893188476562\n",
      "epoch: 15 step: 104 loss: 0.21467738 acc: 0.9469032287597656\n",
      "epoch: 15 step: 105 loss: 0.2082038 acc: 0.9436149597167969\n",
      "epoch: 15 step: 106 loss: 0.2607878 acc: 0.9305839538574219\n",
      "epoch: 15 step: 107 loss: 0.19443883 acc: 0.9430389404296875\n",
      "epoch: 15 step: 108 loss: 0.19713461 acc: 0.9362716674804688\n",
      "epoch: 15 step: 109 loss: 0.1992763 acc: 0.9311637878417969\n",
      "epoch: 15 step: 110 loss: 0.20912038 acc: 0.9360733032226562\n",
      "epoch: 15 step: 111 loss: 0.2202704 acc: 0.9358711242675781\n",
      "epoch: 15 step: 112 loss: 0.19901265 acc: 0.939727783203125\n",
      "epoch: 15 step: 113 loss: 0.24182215 acc: 0.9334373474121094\n",
      "epoch: 15 step: 114 loss: 0.2053426 acc: 0.9431571960449219\n",
      "epoch: 15 step: 115 loss: 0.21867222 acc: 0.9476585388183594\n",
      "epoch: 15 step: 116 loss: 0.22535208 acc: 0.9441642761230469\n",
      "epoch: 15 step: 117 loss: 0.2008629 acc: 0.9541816711425781\n",
      "epoch: 15 step: 118 loss: 0.19763774 acc: 0.9491424560546875\n",
      "epoch: 15 step: 119 loss: 0.23223744 acc: 0.9454307556152344\n",
      "epoch: 15 step: 120 loss: 0.2782347 acc: 0.9263267517089844\n",
      "epoch: 15 step: 121 loss: 0.26400387 acc: 0.9246826171875\n",
      "epoch: 15 step: 122 loss: 0.22120728 acc: 0.9296607971191406\n",
      "epoch: 15 step: 123 loss: 0.190041 acc: 0.9330596923828125\n",
      "epoch: 15 step: 124 loss: 0.18730861 acc: 0.9534214564732143\n",
      "epoch: 15 validation_loss: 0.258 validation_dice: 0.6815402098517316\n",
      "epoch: 15 test_dataset dice: 0.6659724802998843\n",
      "time cost 0.5343721866607666 min\n",
      "dice_best: 0.6944907070675245\n",
      "******************************** epoch  15  is finished. *********************************\n",
      "epoch: 16 step: 1 loss: 0.23955241 acc: 0.9364089965820312\n",
      "epoch: 16 step: 2 loss: 0.25454515 acc: 0.9248695373535156\n",
      "epoch: 16 step: 3 loss: 0.23626097 acc: 0.9310722351074219\n",
      "epoch: 16 step: 4 loss: 0.18235736 acc: 0.9462814331054688\n",
      "epoch: 16 step: 5 loss: 0.21282639 acc: 0.941070556640625\n",
      "epoch: 16 step: 6 loss: 0.28864262 acc: 0.9419364929199219\n",
      "epoch: 16 step: 7 loss: 0.25680324 acc: 0.9258995056152344\n",
      "epoch: 16 step: 8 loss: 0.24466969 acc: 0.9429359436035156\n",
      "epoch: 16 step: 9 loss: 0.22964244 acc: 0.9338798522949219\n",
      "epoch: 16 step: 10 loss: 0.26480174 acc: 0.935455322265625\n",
      "epoch: 16 step: 11 loss: 0.2093138 acc: 0.9361724853515625\n",
      "epoch: 16 step: 12 loss: 0.22600602 acc: 0.94464111328125\n",
      "epoch: 16 step: 13 loss: 0.20284039 acc: 0.939239501953125\n",
      "epoch: 16 step: 14 loss: 0.22554457 acc: 0.9331817626953125\n",
      "epoch: 16 step: 15 loss: 0.24075808 acc: 0.9483489990234375\n",
      "epoch: 16 step: 16 loss: 0.18246453 acc: 0.9533462524414062\n",
      "epoch: 16 step: 17 loss: 0.20608954 acc: 0.9371147155761719\n",
      "epoch: 16 step: 18 loss: 0.2311941 acc: 0.9452629089355469\n",
      "epoch: 16 step: 19 loss: 0.20569363 acc: 0.9409332275390625\n",
      "epoch: 16 step: 20 loss: 0.26074117 acc: 0.9369583129882812\n",
      "epoch: 16 step: 21 loss: 0.24515231 acc: 0.943695068359375\n",
      "epoch: 16 step: 22 loss: 0.20368434 acc: 0.9334373474121094\n",
      "epoch: 16 step: 23 loss: 0.2707988 acc: 0.9082984924316406\n",
      "epoch: 16 step: 24 loss: 0.24889208 acc: 0.9205589294433594\n",
      "epoch: 16 step: 25 loss: 0.1783425 acc: 0.9395866394042969\n",
      "epoch: 16 step: 26 loss: 0.2001383 acc: 0.9425849914550781\n",
      "epoch: 16 step: 27 loss: 0.29307377 acc: 0.9340438842773438\n",
      "epoch: 16 step: 28 loss: 0.21739824 acc: 0.9403076171875\n",
      "epoch: 16 step: 29 loss: 0.23023824 acc: 0.942962646484375\n",
      "epoch: 16 step: 30 loss: 0.2215433 acc: 0.9445762634277344\n",
      "epoch: 16 step: 31 loss: 0.18260211 acc: 0.9501914978027344\n",
      "epoch: 16 step: 32 loss: 0.18157329 acc: 0.9494781494140625\n",
      "epoch: 16 step: 33 loss: 0.16866091 acc: 0.9434394836425781\n",
      "epoch: 16 step: 34 loss: 0.2164084 acc: 0.9373321533203125\n",
      "epoch: 16 step: 35 loss: 0.17429547 acc: 0.9441566467285156\n",
      "epoch: 16 step: 36 loss: 0.17950653 acc: 0.9471511840820312\n",
      "epoch: 16 step: 37 loss: 0.19167194 acc: 0.9429893493652344\n",
      "epoch: 16 step: 38 loss: 0.24205427 acc: 0.9355659484863281\n",
      "epoch: 16 step: 39 loss: 0.18605949 acc: 0.9413909912109375\n",
      "epoch: 16 step: 40 loss: 0.1470498 acc: 0.956451416015625\n",
      "epoch: 16 step: 41 loss: 0.20135525 acc: 0.9472389221191406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 step: 42 loss: 0.14881542 acc: 0.95111083984375\n",
      "epoch: 16 step: 43 loss: 0.1828671 acc: 0.9475669860839844\n",
      "epoch: 16 step: 44 loss: 0.14796928 acc: 0.9497795104980469\n",
      "epoch: 16 step: 45 loss: 0.18887691 acc: 0.9340248107910156\n",
      "epoch: 16 step: 46 loss: 0.22090462 acc: 0.9357032775878906\n",
      "epoch: 16 step: 47 loss: 0.17032942 acc: 0.9567909240722656\n",
      "epoch: 16 step: 48 loss: 0.18956459 acc: 0.9491958618164062\n",
      "epoch: 16 step: 49 loss: 0.21474236 acc: 0.9463539123535156\n",
      "epoch: 16 step: 50 loss: 0.16899712 acc: 0.9498176574707031\n",
      "epoch: 16 step: 51 loss: 0.20207548 acc: 0.9530105590820312\n",
      "epoch: 16 step: 52 loss: 0.22158475 acc: 0.9390487670898438\n",
      "epoch: 16 step: 53 loss: 0.18779436 acc: 0.9449958801269531\n",
      "epoch: 16 step: 54 loss: 0.3083851 acc: 0.91357421875\n",
      "epoch: 16 step: 55 loss: 0.25926003 acc: 0.9285316467285156\n",
      "epoch: 16 step: 56 loss: 0.22575712 acc: 0.9341316223144531\n",
      "epoch: 16 step: 57 loss: 0.28841835 acc: 0.9240989685058594\n",
      "epoch: 16 step: 58 loss: 0.24100554 acc: 0.939453125\n",
      "epoch: 16 step: 59 loss: 0.2638366 acc: 0.9467277526855469\n",
      "epoch: 16 step: 60 loss: 0.24157424 acc: 0.930694580078125\n",
      "epoch: 16 step: 61 loss: 0.25646198 acc: 0.9328956604003906\n",
      "epoch: 16 step: 62 loss: 0.26067245 acc: 0.9356651306152344\n",
      "epoch: 16 step: 63 loss: 0.2970814 acc: 0.9394683837890625\n",
      "epoch: 16 step: 64 loss: 0.24568474 acc: 0.9337615966796875\n",
      "epoch: 16 step: 65 loss: 0.3171843 acc: 0.938201904296875\n",
      "epoch: 16 step: 66 loss: 0.3342436 acc: 0.9252090454101562\n",
      "epoch: 16 step: 67 loss: 0.33247882 acc: 0.9202690124511719\n",
      "epoch: 16 step: 68 loss: 0.26627916 acc: 0.91802978515625\n",
      "epoch: 16 step: 69 loss: 0.23769608 acc: 0.9359016418457031\n",
      "epoch: 16 step: 70 loss: 0.27645558 acc: 0.9396247863769531\n",
      "epoch: 16 step: 71 loss: 0.40587005 acc: 0.9227256774902344\n",
      "epoch: 16 step: 72 loss: 0.27596563 acc: 0.939910888671875\n",
      "epoch: 16 step: 73 loss: 0.2552022 acc: 0.9302825927734375\n",
      "epoch: 16 step: 74 loss: 0.23965861 acc: 0.941864013671875\n",
      "epoch: 16 step: 75 loss: 0.28643885 acc: 0.9307937622070312\n",
      "epoch: 16 step: 76 loss: 0.33142152 acc: 0.9420356750488281\n",
      "epoch: 16 step: 77 loss: 0.4643678 acc: 0.9165878295898438\n",
      "epoch: 16 step: 78 loss: 0.2960396 acc: 0.938995361328125\n",
      "epoch: 16 step: 79 loss: 0.22397865 acc: 0.9444580078125\n",
      "epoch: 16 step: 80 loss: 0.23798485 acc: 0.9364967346191406\n",
      "epoch: 16 step: 81 loss: 0.31820354 acc: 0.9221382141113281\n",
      "epoch: 16 step: 82 loss: 0.2138105 acc: 0.9465141296386719\n",
      "epoch: 16 step: 83 loss: 0.37792003 acc: 0.9256553649902344\n",
      "epoch: 16 step: 84 loss: 0.35657924 acc: 0.9158897399902344\n",
      "epoch: 16 step: 85 loss: 0.27830753 acc: 0.9326362609863281\n",
      "epoch: 16 step: 86 loss: 0.32629132 acc: 0.9260902404785156\n",
      "epoch: 16 step: 87 loss: 0.27005354 acc: 0.9350471496582031\n",
      "epoch: 16 step: 88 loss: 0.31687397 acc: 0.9355735778808594\n",
      "epoch: 16 step: 89 loss: 0.2739616 acc: 0.9301071166992188\n",
      "epoch: 16 step: 90 loss: 0.24081713 acc: 0.9340133666992188\n",
      "epoch: 16 step: 91 loss: 0.42417258 acc: 0.9159660339355469\n",
      "epoch: 16 step: 92 loss: 0.25116917 acc: 0.9320144653320312\n",
      "epoch: 16 step: 93 loss: 0.2897772 acc: 0.9249267578125\n",
      "epoch: 16 step: 94 loss: 0.25413758 acc: 0.9367828369140625\n",
      "epoch: 16 step: 95 loss: 0.24386758 acc: 0.9248504638671875\n",
      "epoch: 16 step: 96 loss: 0.24013498 acc: 0.9311599731445312\n",
      "epoch: 16 step: 97 loss: 0.23755927 acc: 0.9371757507324219\n",
      "epoch: 16 step: 98 loss: 0.24062757 acc: 0.9359779357910156\n",
      "epoch: 16 step: 99 loss: 0.25014895 acc: 0.9338874816894531\n",
      "epoch: 16 step: 100 loss: 0.26779088 acc: 0.92193603515625\n",
      "epoch: 16 step: 101 loss: 0.27524954 acc: 0.9406547546386719\n",
      "epoch: 16 step: 102 loss: 0.22196531 acc: 0.9416732788085938\n",
      "epoch: 16 step: 103 loss: 0.1913155 acc: 0.9482307434082031\n",
      "epoch: 16 step: 104 loss: 0.24909137 acc: 0.9364395141601562\n",
      "epoch: 16 step: 105 loss: 0.22975396 acc: 0.9385871887207031\n",
      "epoch: 16 step: 106 loss: 0.21969625 acc: 0.9401741027832031\n",
      "epoch: 16 step: 107 loss: 0.22110027 acc: 0.951934814453125\n",
      "epoch: 16 step: 108 loss: 0.28084588 acc: 0.9336090087890625\n",
      "epoch: 16 step: 109 loss: 0.2902324 acc: 0.9330062866210938\n",
      "epoch: 16 step: 110 loss: 0.23017338 acc: 0.9248847961425781\n",
      "epoch: 16 step: 111 loss: 0.29254463 acc: 0.911376953125\n",
      "epoch: 16 step: 112 loss: 0.21932289 acc: 0.942352294921875\n",
      "epoch: 16 step: 113 loss: 0.2728379 acc: 0.9350357055664062\n",
      "epoch: 16 step: 114 loss: 0.22239651 acc: 0.93670654296875\n",
      "epoch: 16 step: 115 loss: 0.20891254 acc: 0.9454154968261719\n",
      "epoch: 16 step: 116 loss: 0.232946 acc: 0.93792724609375\n",
      "epoch: 16 step: 117 loss: 0.24276274 acc: 0.9319305419921875\n",
      "epoch: 16 step: 118 loss: 0.3129288 acc: 0.9238014221191406\n",
      "epoch: 16 step: 119 loss: 0.19374356 acc: 0.9510917663574219\n",
      "epoch: 16 step: 120 loss: 0.20324072 acc: 0.9479637145996094\n",
      "epoch: 16 step: 121 loss: 0.24104477 acc: 0.9314537048339844\n",
      "epoch: 16 step: 122 loss: 0.22330236 acc: 0.9384231567382812\n",
      "epoch: 16 step: 123 loss: 0.23336495 acc: 0.9327430725097656\n",
      "epoch: 16 step: 124 loss: 0.3371204 acc: 0.9220232282366071\n",
      "epoch: 16 validation_loss: 0.29 validation_dice: 0.6676358082850881\n",
      "epoch: 16 test_dataset dice: 0.6689801208900679\n",
      "time cost 0.534359089533488 min\n",
      "dice_best: 0.6944907070675245\n",
      "******************************** epoch  16  is finished. *********************************\n",
      "epoch: 17 step: 1 loss: 0.20871706 acc: 0.9371681213378906\n",
      "epoch: 17 step: 2 loss: 0.21710187 acc: 0.9380149841308594\n",
      "epoch: 17 step: 3 loss: 0.2745614 acc: 0.9354476928710938\n",
      "epoch: 17 step: 4 loss: 0.20261128 acc: 0.9514579772949219\n",
      "epoch: 17 step: 5 loss: 0.24769606 acc: 0.9380989074707031\n",
      "epoch: 17 step: 6 loss: 0.30879068 acc: 0.9450569152832031\n",
      "epoch: 17 step: 7 loss: 0.22510268 acc: 0.9441032409667969\n",
      "epoch: 17 step: 8 loss: 0.2530001 acc: 0.9372291564941406\n",
      "epoch: 17 step: 9 loss: 0.32456598 acc: 0.9199142456054688\n",
      "epoch: 17 step: 10 loss: 0.2659528 acc: 0.9275360107421875\n",
      "epoch: 17 step: 11 loss: 0.2369026 acc: 0.9221305847167969\n",
      "epoch: 17 step: 12 loss: 0.18826915 acc: 0.9405288696289062\n",
      "epoch: 17 step: 13 loss: 0.2282138 acc: 0.9322853088378906\n",
      "epoch: 17 step: 14 loss: 0.20162831 acc: 0.9470939636230469\n",
      "epoch: 17 step: 15 loss: 0.27352098 acc: 0.9458351135253906\n",
      "epoch: 17 step: 16 loss: 0.23133257 acc: 0.9375419616699219\n",
      "epoch: 17 step: 17 loss: 0.26891774 acc: 0.9415397644042969\n",
      "epoch: 17 step: 18 loss: 0.22388762 acc: 0.9471321105957031\n",
      "epoch: 17 step: 19 loss: 0.22289956 acc: 0.9364509582519531\n",
      "epoch: 17 step: 20 loss: 0.29790848 acc: 0.9438285827636719\n",
      "epoch: 17 step: 21 loss: 0.2053192 acc: 0.9390449523925781\n",
      "epoch: 17 step: 22 loss: 0.23858626 acc: 0.9349403381347656\n",
      "epoch: 17 step: 23 loss: 0.26605576 acc: 0.9384613037109375\n",
      "epoch: 17 step: 24 loss: 0.25188616 acc: 0.9307174682617188\n",
      "epoch: 17 step: 25 loss: 0.3573509 acc: 0.9145736694335938\n",
      "epoch: 17 step: 26 loss: 0.22867367 acc: 0.9383468627929688\n",
      "epoch: 17 step: 27 loss: 0.2892037 acc: 0.9421920776367188\n",
      "epoch: 17 step: 28 loss: 0.2304852 acc: 0.9423370361328125\n",
      "epoch: 17 step: 29 loss: 0.32180446 acc: 0.933868408203125\n",
      "epoch: 17 step: 30 loss: 0.3077876 acc: 0.9342727661132812\n",
      "epoch: 17 step: 31 loss: 0.29655632 acc: 0.9363021850585938\n",
      "epoch: 17 step: 32 loss: 0.23580532 acc: 0.9402732849121094\n",
      "epoch: 17 step: 33 loss: 0.2108674 acc: 0.9386444091796875\n",
      "epoch: 17 step: 34 loss: 0.28943512 acc: 0.9364738464355469\n",
      "epoch: 17 step: 35 loss: 0.24443024 acc: 0.9358558654785156\n",
      "epoch: 17 step: 36 loss: 0.22834176 acc: 0.9364395141601562\n",
      "epoch: 17 step: 37 loss: 0.2581373 acc: 0.9487838745117188\n",
      "epoch: 17 step: 38 loss: 0.27517444 acc: 0.9221992492675781\n",
      "epoch: 17 step: 39 loss: 0.23275958 acc: 0.9318962097167969\n",
      "epoch: 17 step: 40 loss: 0.28035957 acc: 0.9321136474609375\n",
      "epoch: 17 step: 41 loss: 0.31875655 acc: 0.9126930236816406\n",
      "epoch: 17 step: 42 loss: 0.28517178 acc: 0.921173095703125\n",
      "epoch: 17 step: 43 loss: 0.24167164 acc: 0.9381217956542969\n",
      "epoch: 17 step: 44 loss: 0.22571062 acc: 0.9408340454101562\n",
      "epoch: 17 step: 45 loss: 0.22449385 acc: 0.9454269409179688\n",
      "epoch: 17 step: 46 loss: 0.22830759 acc: 0.9396514892578125\n",
      "epoch: 17 step: 47 loss: 0.19667721 acc: 0.949554443359375\n",
      "epoch: 17 step: 48 loss: 0.21388435 acc: 0.9332809448242188\n",
      "epoch: 17 step: 49 loss: 0.19541013 acc: 0.9392929077148438\n",
      "epoch: 17 step: 50 loss: 0.21698038 acc: 0.9433670043945312\n",
      "epoch: 17 step: 51 loss: 0.2276077 acc: 0.9369392395019531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 step: 52 loss: 0.26932177 acc: 0.9298095703125\n",
      "epoch: 17 step: 53 loss: 0.19529034 acc: 0.9430999755859375\n",
      "epoch: 17 step: 54 loss: 0.20286302 acc: 0.9351539611816406\n",
      "epoch: 17 step: 55 loss: 0.17712119 acc: 0.9421463012695312\n",
      "epoch: 17 step: 56 loss: 0.25572014 acc: 0.9364128112792969\n",
      "epoch: 17 step: 57 loss: 0.22314367 acc: 0.9365806579589844\n",
      "epoch: 17 step: 58 loss: 0.21150926 acc: 0.9405059814453125\n",
      "epoch: 17 step: 59 loss: 0.2084482 acc: 0.9359703063964844\n",
      "epoch: 17 step: 60 loss: 0.25313625 acc: 0.9339408874511719\n",
      "epoch: 17 step: 61 loss: 0.15036018 acc: 0.9568557739257812\n",
      "epoch: 17 step: 62 loss: 0.22435217 acc: 0.9433097839355469\n",
      "epoch: 17 step: 63 loss: 0.21378511 acc: 0.9482765197753906\n",
      "epoch: 17 step: 64 loss: 0.23733784 acc: 0.9397315979003906\n",
      "epoch: 17 step: 65 loss: 0.25273174 acc: 0.9324760437011719\n",
      "epoch: 17 step: 66 loss: 0.19171697 acc: 0.9509468078613281\n",
      "epoch: 17 step: 67 loss: 0.2543366 acc: 0.9247550964355469\n",
      "epoch: 17 step: 68 loss: 0.22508554 acc: 0.9393234252929688\n",
      "epoch: 17 step: 69 loss: 0.15685654 acc: 0.9434852600097656\n",
      "epoch: 17 step: 70 loss: 0.2046565 acc: 0.9320449829101562\n",
      "epoch: 17 step: 71 loss: 0.20667788 acc: 0.9361763000488281\n",
      "epoch: 17 step: 72 loss: 0.21018809 acc: 0.9446907043457031\n",
      "epoch: 17 step: 73 loss: 0.18995996 acc: 0.9388618469238281\n",
      "epoch: 17 step: 74 loss: 0.19335043 acc: 0.945587158203125\n",
      "epoch: 17 step: 75 loss: 0.2588914 acc: 0.9338531494140625\n",
      "epoch: 17 step: 76 loss: 0.17818788 acc: 0.9424247741699219\n",
      "epoch: 17 step: 77 loss: 0.23107944 acc: 0.9402999877929688\n",
      "epoch: 17 step: 78 loss: 0.22145078 acc: 0.9425010681152344\n",
      "epoch: 17 step: 79 loss: 0.23128876 acc: 0.9402503967285156\n",
      "epoch: 17 step: 80 loss: 0.20171386 acc: 0.9428329467773438\n",
      "epoch: 17 step: 81 loss: 0.20355147 acc: 0.9361305236816406\n",
      "epoch: 17 step: 82 loss: 0.23809607 acc: 0.921905517578125\n",
      "epoch: 17 step: 83 loss: 0.17414093 acc: 0.9397811889648438\n",
      "epoch: 17 step: 84 loss: 0.16171312 acc: 0.9399261474609375\n",
      "epoch: 17 step: 85 loss: 0.27735868 acc: 0.93450927734375\n",
      "epoch: 17 step: 86 loss: 0.17167874 acc: 0.9472274780273438\n",
      "epoch: 17 step: 87 loss: 0.17900188 acc: 0.939300537109375\n",
      "epoch: 17 step: 88 loss: 0.20574021 acc: 0.9438514709472656\n",
      "epoch: 17 step: 89 loss: 0.21592349 acc: 0.93438720703125\n",
      "epoch: 17 step: 90 loss: 0.19922984 acc: 0.9509124755859375\n",
      "epoch: 17 step: 91 loss: 0.16308844 acc: 0.942718505859375\n",
      "epoch: 17 step: 92 loss: 0.20284507 acc: 0.9356880187988281\n",
      "epoch: 17 step: 93 loss: 0.25698438 acc: 0.945037841796875\n",
      "epoch: 17 step: 94 loss: 0.14881825 acc: 0.9559822082519531\n",
      "epoch: 17 step: 95 loss: 0.15092425 acc: 0.9473457336425781\n",
      "epoch: 17 step: 96 loss: 0.1723496 acc: 0.9480056762695312\n",
      "epoch: 17 step: 97 loss: 0.16661865 acc: 0.9385604858398438\n",
      "epoch: 17 step: 98 loss: 0.17626019 acc: 0.9455490112304688\n",
      "epoch: 17 step: 99 loss: 0.21512957 acc: 0.9388961791992188\n",
      "epoch: 17 step: 100 loss: 0.18777998 acc: 0.955078125\n",
      "epoch: 17 step: 101 loss: 0.21420477 acc: 0.9339218139648438\n",
      "epoch: 17 step: 102 loss: 0.18934 acc: 0.9422988891601562\n",
      "epoch: 17 step: 103 loss: 0.20947067 acc: 0.9369773864746094\n",
      "epoch: 17 step: 104 loss: 0.18915227 acc: 0.9430465698242188\n",
      "epoch: 17 step: 105 loss: 0.17713323 acc: 0.9501838684082031\n",
      "epoch: 17 step: 106 loss: 0.17259721 acc: 0.9507026672363281\n",
      "epoch: 17 step: 107 loss: 0.15484035 acc: 0.9513626098632812\n",
      "epoch: 17 step: 108 loss: 0.17208646 acc: 0.9464302062988281\n",
      "epoch: 17 step: 109 loss: 0.16620076 acc: 0.9531898498535156\n",
      "epoch: 17 step: 110 loss: 0.2370329 acc: 0.941741943359375\n",
      "epoch: 17 step: 111 loss: 0.24653983 acc: 0.9380645751953125\n",
      "epoch: 17 step: 112 loss: 0.16578944 acc: 0.9554786682128906\n",
      "epoch: 17 step: 113 loss: 0.21852235 acc: 0.9387626647949219\n",
      "epoch: 17 step: 114 loss: 0.17743808 acc: 0.9413681030273438\n",
      "epoch: 17 step: 115 loss: 0.20035452 acc: 0.9459152221679688\n",
      "epoch: 17 step: 116 loss: 0.17904781 acc: 0.9514045715332031\n",
      "epoch: 17 step: 117 loss: 0.25331563 acc: 0.934478759765625\n",
      "epoch: 17 step: 118 loss: 0.1705305 acc: 0.9457893371582031\n",
      "epoch: 17 step: 119 loss: 0.17979331 acc: 0.9409370422363281\n",
      "epoch: 17 step: 120 loss: 0.25464463 acc: 0.9347991943359375\n",
      "epoch: 17 step: 121 loss: 0.18453552 acc: 0.9401092529296875\n",
      "epoch: 17 step: 122 loss: 0.1882034 acc: 0.9496040344238281\n",
      "epoch: 17 step: 123 loss: 0.16703662 acc: 0.9547271728515625\n",
      "epoch: 17 step: 124 loss: 0.22282492 acc: 0.9375697544642857\n",
      "epoch: 17 validation_loss: 0.228 validation_dice: 0.7761289493951334\n",
      "epoch: 17 test_dataset dice: 0.6144464774418471\n",
      "time cost 0.5337121566136678 min\n",
      "dice_best: 0.7761289493951334\n",
      "******************************** epoch  17  is finished. *********************************\n",
      "epoch: 18 step: 1 loss: 0.21374694 acc: 0.9361038208007812\n",
      "epoch: 18 step: 2 loss: 0.19172978 acc: 0.9325370788574219\n",
      "epoch: 18 step: 3 loss: 0.15218714 acc: 0.9486312866210938\n",
      "epoch: 18 step: 4 loss: 0.23222601 acc: 0.9168701171875\n",
      "epoch: 18 step: 5 loss: 0.22990675 acc: 0.9372367858886719\n",
      "epoch: 18 step: 6 loss: 0.1889214 acc: 0.937469482421875\n",
      "epoch: 18 step: 7 loss: 0.19909875 acc: 0.9368629455566406\n",
      "epoch: 18 step: 8 loss: 0.23602945 acc: 0.9360542297363281\n",
      "epoch: 18 step: 9 loss: 0.257374 acc: 0.9412040710449219\n",
      "epoch: 18 step: 10 loss: 0.18781823 acc: 0.9493446350097656\n",
      "epoch: 18 step: 11 loss: 0.23851973 acc: 0.9470367431640625\n",
      "epoch: 18 step: 12 loss: 0.20642143 acc: 0.9359207153320312\n",
      "epoch: 18 step: 13 loss: 0.18825795 acc: 0.9492378234863281\n",
      "epoch: 18 step: 14 loss: 0.22002083 acc: 0.9243698120117188\n",
      "epoch: 18 step: 15 loss: 0.21898724 acc: 0.92626953125\n",
      "epoch: 18 step: 16 loss: 0.1658777 acc: 0.9492607116699219\n",
      "epoch: 18 step: 17 loss: 0.22519356 acc: 0.9368133544921875\n",
      "epoch: 18 step: 18 loss: 0.15895414 acc: 0.9469490051269531\n",
      "epoch: 18 step: 19 loss: 0.23969418 acc: 0.9436264038085938\n",
      "epoch: 18 step: 20 loss: 0.18168436 acc: 0.9470100402832031\n",
      "epoch: 18 step: 21 loss: 0.17008455 acc: 0.9486618041992188\n",
      "epoch: 18 step: 22 loss: 0.17932263 acc: 0.9395332336425781\n",
      "epoch: 18 step: 23 loss: 0.16775042 acc: 0.9446372985839844\n",
      "epoch: 18 step: 24 loss: 0.17286754 acc: 0.9347267150878906\n",
      "epoch: 18 step: 25 loss: 0.15701008 acc: 0.93914794921875\n",
      "epoch: 18 step: 26 loss: 0.22496982 acc: 0.9423408508300781\n",
      "epoch: 18 step: 27 loss: 0.14921531 acc: 0.9551162719726562\n",
      "epoch: 18 step: 28 loss: 0.18861866 acc: 0.9420585632324219\n",
      "epoch: 18 step: 29 loss: 0.16711023 acc: 0.9458236694335938\n",
      "epoch: 18 step: 30 loss: 0.21108724 acc: 0.9438514709472656\n",
      "epoch: 18 step: 31 loss: 0.1749647 acc: 0.9466743469238281\n",
      "epoch: 18 step: 32 loss: 0.21024458 acc: 0.9411582946777344\n",
      "epoch: 18 step: 33 loss: 0.18309554 acc: 0.9362564086914062\n",
      "epoch: 18 step: 34 loss: 0.1715117 acc: 0.9542694091796875\n",
      "epoch: 18 step: 35 loss: 0.25055736 acc: 0.9290504455566406\n",
      "epoch: 18 step: 36 loss: 0.20799617 acc: 0.9377899169921875\n",
      "epoch: 18 step: 37 loss: 0.21319409 acc: 0.936981201171875\n",
      "epoch: 18 step: 38 loss: 0.17468378 acc: 0.942352294921875\n",
      "epoch: 18 step: 39 loss: 0.2509311 acc: 0.9480361938476562\n",
      "epoch: 18 step: 40 loss: 0.1931123 acc: 0.9459304809570312\n",
      "epoch: 18 step: 41 loss: 0.17762381 acc: 0.9517669677734375\n",
      "epoch: 18 step: 42 loss: 0.15871243 acc: 0.9438400268554688\n",
      "epoch: 18 step: 43 loss: 0.24369489 acc: 0.9417915344238281\n",
      "epoch: 18 step: 44 loss: 0.27375567 acc: 0.9457550048828125\n",
      "epoch: 18 step: 45 loss: 0.26017123 acc: 0.93756103515625\n",
      "epoch: 18 step: 46 loss: 0.2142048 acc: 0.9503898620605469\n",
      "epoch: 18 step: 47 loss: 0.21862137 acc: 0.9547004699707031\n",
      "epoch: 18 step: 48 loss: 0.19971484 acc: 0.9437255859375\n",
      "epoch: 18 step: 49 loss: 0.22200522 acc: 0.9397315979003906\n",
      "epoch: 18 step: 50 loss: 0.20705613 acc: 0.9318351745605469\n",
      "epoch: 18 step: 51 loss: 0.23286659 acc: 0.9280891418457031\n",
      "epoch: 18 step: 52 loss: 0.2137143 acc: 0.93182373046875\n",
      "epoch: 18 step: 53 loss: 0.27752262 acc: 0.9373397827148438\n",
      "epoch: 18 step: 54 loss: 0.24458978 acc: 0.9287796020507812\n",
      "epoch: 18 step: 55 loss: 0.22415493 acc: 0.9348258972167969\n",
      "epoch: 18 step: 56 loss: 0.23090966 acc: 0.9393157958984375\n",
      "epoch: 18 step: 57 loss: 0.2590595 acc: 0.9384689331054688\n",
      "epoch: 18 step: 58 loss: 0.21084899 acc: 0.9499626159667969\n",
      "epoch: 18 step: 59 loss: 0.25270438 acc: 0.93914794921875\n",
      "epoch: 18 step: 60 loss: 0.23050177 acc: 0.9370193481445312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 61 loss: 0.3252319 acc: 0.9377708435058594\n",
      "epoch: 18 step: 62 loss: 0.20913167 acc: 0.9428787231445312\n",
      "epoch: 18 step: 63 loss: 0.21017608 acc: 0.9437446594238281\n",
      "epoch: 18 step: 64 loss: 0.20359613 acc: 0.9313201904296875\n",
      "epoch: 18 step: 65 loss: 0.3065869 acc: 0.9286651611328125\n",
      "epoch: 18 step: 66 loss: 0.26300246 acc: 0.9406318664550781\n",
      "epoch: 18 step: 67 loss: 0.27751797 acc: 0.941864013671875\n",
      "epoch: 18 step: 68 loss: 0.24413882 acc: 0.9201469421386719\n",
      "epoch: 18 step: 69 loss: 0.28749755 acc: 0.9310035705566406\n",
      "epoch: 18 step: 70 loss: 0.2510988 acc: 0.9407768249511719\n",
      "epoch: 18 step: 71 loss: 0.20935081 acc: 0.9418182373046875\n",
      "epoch: 18 step: 72 loss: 0.24198872 acc: 0.9397544860839844\n",
      "epoch: 18 step: 73 loss: 0.24918655 acc: 0.9386558532714844\n",
      "epoch: 18 step: 74 loss: 0.24641822 acc: 0.9239692687988281\n",
      "epoch: 18 step: 75 loss: 0.25857943 acc: 0.9208869934082031\n",
      "epoch: 18 step: 76 loss: 0.23167531 acc: 0.9300537109375\n",
      "epoch: 18 step: 77 loss: 0.27169952 acc: 0.89654541015625\n",
      "epoch: 18 step: 78 loss: 0.3232307 acc: 0.9288139343261719\n",
      "epoch: 18 step: 79 loss: 0.25010395 acc: 0.9329185485839844\n",
      "epoch: 18 step: 80 loss: 0.4316715 acc: 0.8877334594726562\n",
      "epoch: 18 step: 81 loss: 0.28045875 acc: 0.9273147583007812\n",
      "epoch: 18 step: 82 loss: 0.27220562 acc: 0.93609619140625\n",
      "epoch: 18 step: 83 loss: 0.2505359 acc: 0.9368705749511719\n",
      "epoch: 18 step: 84 loss: 0.32573482 acc: 0.932464599609375\n",
      "epoch: 18 step: 85 loss: 0.29803854 acc: 0.9351272583007812\n",
      "epoch: 18 step: 86 loss: 0.25225267 acc: 0.9336128234863281\n",
      "epoch: 18 step: 87 loss: 0.31791842 acc: 0.9268646240234375\n",
      "epoch: 18 step: 88 loss: 0.21813397 acc: 0.9416770935058594\n",
      "epoch: 18 step: 89 loss: 0.31991518 acc: 0.9211387634277344\n",
      "epoch: 18 step: 90 loss: 0.3310477 acc: 0.9280662536621094\n",
      "epoch: 18 step: 91 loss: 0.24485701 acc: 0.9285202026367188\n",
      "epoch: 18 step: 92 loss: 0.25360325 acc: 0.9252090454101562\n",
      "epoch: 18 step: 93 loss: 0.3167253 acc: 0.9214591979980469\n",
      "epoch: 18 step: 94 loss: 0.2506075 acc: 0.940765380859375\n",
      "epoch: 18 step: 95 loss: 0.20969342 acc: 0.9454803466796875\n",
      "epoch: 18 step: 96 loss: 0.31312492 acc: 0.9539527893066406\n",
      "epoch: 18 step: 97 loss: 0.18361214 acc: 0.9503173828125\n",
      "epoch: 18 step: 98 loss: 0.3153441 acc: 0.9274711608886719\n",
      "epoch: 18 step: 99 loss: 0.32192463 acc: 0.9348716735839844\n",
      "epoch: 18 step: 100 loss: 0.17617539 acc: 0.9546432495117188\n",
      "epoch: 18 step: 101 loss: 0.263976 acc: 0.9299201965332031\n",
      "epoch: 18 step: 102 loss: 0.31537095 acc: 0.9376029968261719\n",
      "epoch: 18 step: 103 loss: 0.21530364 acc: 0.9409523010253906\n",
      "epoch: 18 step: 104 loss: 0.31121868 acc: 0.934295654296875\n",
      "epoch: 18 step: 105 loss: 0.20619102 acc: 0.9418716430664062\n",
      "epoch: 18 step: 106 loss: 0.2922141 acc: 0.9088478088378906\n",
      "epoch: 18 step: 107 loss: 0.18157806 acc: 0.9528999328613281\n",
      "epoch: 18 step: 108 loss: 0.20710585 acc: 0.9483985900878906\n",
      "epoch: 18 step: 109 loss: 0.24183215 acc: 0.9371528625488281\n",
      "epoch: 18 step: 110 loss: 0.28181073 acc: 0.92999267578125\n",
      "epoch: 18 step: 111 loss: 0.21433969 acc: 0.9457588195800781\n",
      "epoch: 18 step: 112 loss: 0.31610474 acc: 0.9246368408203125\n",
      "epoch: 18 step: 113 loss: 0.26891342 acc: 0.9043006896972656\n",
      "epoch: 18 step: 114 loss: 0.2535579 acc: 0.9287338256835938\n",
      "epoch: 18 step: 115 loss: 0.28010467 acc: 0.922332763671875\n",
      "epoch: 18 step: 116 loss: 0.21500427 acc: 0.9352912902832031\n",
      "epoch: 18 step: 117 loss: 0.296587 acc: 0.9388084411621094\n",
      "epoch: 18 step: 118 loss: 0.21806163 acc: 0.93731689453125\n",
      "epoch: 18 step: 119 loss: 0.19569589 acc: 0.9487152099609375\n",
      "epoch: 18 step: 120 loss: 0.19629964 acc: 0.9401016235351562\n",
      "epoch: 18 step: 121 loss: 0.20490737 acc: 0.945343017578125\n",
      "epoch: 18 step: 122 loss: 0.25769186 acc: 0.9282188415527344\n",
      "epoch: 18 step: 123 loss: 0.20548452 acc: 0.9513893127441406\n",
      "epoch: 18 step: 124 loss: 0.25209498 acc: 0.9394967215401786\n",
      "epoch: 18 validation_loss: 0.215 validation_dice: 0.7241469358825969\n",
      "epoch: 18 test_dataset dice: 0.6531781642408738\n",
      "time cost 0.535199769337972 min\n",
      "dice_best: 0.7761289493951334\n",
      "******************************** epoch  18  is finished. *********************************\n",
      "epoch: 19 step: 1 loss: 0.21759707 acc: 0.9507598876953125\n",
      "epoch: 19 step: 2 loss: 0.24869615 acc: 0.9420051574707031\n",
      "epoch: 19 step: 3 loss: 0.22296447 acc: 0.9399337768554688\n",
      "epoch: 19 step: 4 loss: 0.19087417 acc: 0.9438438415527344\n",
      "epoch: 19 step: 5 loss: 0.22359139 acc: 0.9347305297851562\n",
      "epoch: 19 step: 6 loss: 0.24016643 acc: 0.9324798583984375\n",
      "epoch: 19 step: 7 loss: 0.22403784 acc: 0.9406967163085938\n",
      "epoch: 19 step: 8 loss: 0.19345684 acc: 0.9564743041992188\n",
      "epoch: 19 step: 9 loss: 0.1860767 acc: 0.9502639770507812\n",
      "epoch: 19 step: 10 loss: 0.18908373 acc: 0.9545326232910156\n",
      "epoch: 19 step: 11 loss: 0.19447324 acc: 0.9580841064453125\n",
      "epoch: 19 step: 12 loss: 0.22795407 acc: 0.9495010375976562\n",
      "epoch: 19 step: 13 loss: 0.2678585 acc: 0.9457969665527344\n",
      "epoch: 19 step: 14 loss: 0.13542774 acc: 0.9672470092773438\n",
      "epoch: 19 step: 15 loss: 0.17895107 acc: 0.9450874328613281\n",
      "epoch: 19 step: 16 loss: 0.21125244 acc: 0.9396400451660156\n",
      "epoch: 19 step: 17 loss: 0.1821347 acc: 0.9318275451660156\n",
      "epoch: 19 step: 18 loss: 0.2196037 acc: 0.94122314453125\n",
      "epoch: 19 step: 19 loss: 0.23883104 acc: 0.9420242309570312\n",
      "epoch: 19 step: 20 loss: 0.2371118 acc: 0.9313087463378906\n",
      "epoch: 19 step: 21 loss: 0.20732994 acc: 0.9433555603027344\n",
      "epoch: 19 step: 22 loss: 0.17500694 acc: 0.9435882568359375\n",
      "epoch: 19 step: 23 loss: 0.18737109 acc: 0.9466972351074219\n",
      "epoch: 19 step: 24 loss: 0.22016427 acc: 0.9366073608398438\n",
      "epoch: 19 step: 25 loss: 0.20057006 acc: 0.9422073364257812\n",
      "epoch: 19 step: 26 loss: 0.22866748 acc: 0.9392547607421875\n",
      "epoch: 19 step: 27 loss: 0.20089075 acc: 0.9438285827636719\n",
      "epoch: 19 step: 28 loss: 0.14548466 acc: 0.9510231018066406\n",
      "epoch: 19 step: 29 loss: 0.1793297 acc: 0.94647216796875\n",
      "epoch: 19 step: 30 loss: 0.22989538 acc: 0.9442558288574219\n",
      "epoch: 19 step: 31 loss: 0.1867392 acc: 0.944366455078125\n",
      "epoch: 19 step: 32 loss: 0.1612665 acc: 0.9544830322265625\n",
      "epoch: 19 step: 33 loss: 0.21301652 acc: 0.9371833801269531\n",
      "epoch: 19 step: 34 loss: 0.16540591 acc: 0.9430351257324219\n",
      "epoch: 19 step: 35 loss: 0.16981433 acc: 0.9518966674804688\n",
      "epoch: 19 step: 36 loss: 0.20466359 acc: 0.939910888671875\n",
      "epoch: 19 step: 37 loss: 0.18211669 acc: 0.9609298706054688\n",
      "epoch: 19 step: 38 loss: 0.19289866 acc: 0.9414634704589844\n",
      "epoch: 19 step: 39 loss: 0.17256716 acc: 0.9466400146484375\n",
      "epoch: 19 step: 40 loss: 0.20027354 acc: 0.9372291564941406\n",
      "epoch: 19 step: 41 loss: 0.1454751 acc: 0.954071044921875\n",
      "epoch: 19 step: 42 loss: 0.18563095 acc: 0.9368667602539062\n",
      "epoch: 19 step: 43 loss: 0.18617958 acc: 0.94268798828125\n",
      "epoch: 19 step: 44 loss: 0.18529181 acc: 0.9440193176269531\n",
      "epoch: 19 step: 45 loss: 0.19412363 acc: 0.9418678283691406\n",
      "epoch: 19 step: 46 loss: 0.17770103 acc: 0.9497756958007812\n",
      "epoch: 19 step: 47 loss: 0.18676312 acc: 0.9437408447265625\n",
      "epoch: 19 step: 48 loss: 0.20233154 acc: 0.9425086975097656\n",
      "epoch: 19 step: 49 loss: 0.14559013 acc: 0.9559135437011719\n",
      "epoch: 19 step: 50 loss: 0.17455839 acc: 0.9442405700683594\n",
      "epoch: 19 step: 51 loss: 0.1871041 acc: 0.9454307556152344\n",
      "epoch: 19 step: 52 loss: 0.16467969 acc: 0.9395828247070312\n",
      "epoch: 19 step: 53 loss: 0.16899672 acc: 0.9507522583007812\n",
      "epoch: 19 step: 54 loss: 0.18281846 acc: 0.9399757385253906\n",
      "epoch: 19 step: 55 loss: 0.2165414 acc: 0.9372825622558594\n",
      "epoch: 19 step: 56 loss: 0.16699241 acc: 0.9462852478027344\n",
      "epoch: 19 step: 57 loss: 0.16737998 acc: 0.9495162963867188\n",
      "epoch: 19 step: 58 loss: 0.15265594 acc: 0.9409446716308594\n",
      "epoch: 19 step: 59 loss: 0.18254296 acc: 0.9478302001953125\n",
      "epoch: 19 step: 60 loss: 0.20542435 acc: 0.9346389770507812\n",
      "epoch: 19 step: 61 loss: 0.16176865 acc: 0.9465141296386719\n",
      "epoch: 19 step: 62 loss: 0.14827882 acc: 0.9509849548339844\n",
      "epoch: 19 step: 63 loss: 0.15848486 acc: 0.9503402709960938\n",
      "epoch: 19 step: 64 loss: 0.19034678 acc: 0.9440460205078125\n",
      "epoch: 19 step: 65 loss: 0.15717198 acc: 0.9485206604003906\n",
      "epoch: 19 step: 66 loss: 0.16510935 acc: 0.9483108520507812\n",
      "epoch: 19 step: 67 loss: 0.19988212 acc: 0.9456596374511719\n",
      "epoch: 19 step: 68 loss: 0.15732443 acc: 0.9537239074707031\n",
      "epoch: 19 step: 69 loss: 0.18294692 acc: 0.9440460205078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 step: 70 loss: 0.17792523 acc: 0.9403266906738281\n",
      "epoch: 19 step: 71 loss: 0.12617613 acc: 0.9561080932617188\n",
      "epoch: 19 step: 72 loss: 0.20488721 acc: 0.9443359375\n",
      "epoch: 19 step: 73 loss: 0.1686863 acc: 0.9367294311523438\n",
      "epoch: 19 step: 74 loss: 0.19133301 acc: 0.9444961547851562\n",
      "epoch: 19 step: 75 loss: 0.1783637 acc: 0.9352493286132812\n",
      "epoch: 19 step: 76 loss: 0.19122684 acc: 0.9364471435546875\n",
      "epoch: 19 step: 77 loss: 0.18895788 acc: 0.9345169067382812\n",
      "epoch: 19 step: 78 loss: 0.20877191 acc: 0.9422187805175781\n",
      "epoch: 19 step: 79 loss: 0.16320348 acc: 0.9435386657714844\n",
      "epoch: 19 step: 80 loss: 0.14937317 acc: 0.9474334716796875\n",
      "epoch: 19 step: 81 loss: 0.16308975 acc: 0.9507827758789062\n",
      "epoch: 19 step: 82 loss: 0.17587921 acc: 0.9567146301269531\n",
      "epoch: 19 step: 83 loss: 0.1661128 acc: 0.95550537109375\n",
      "epoch: 19 step: 84 loss: 0.18448856 acc: 0.9560050964355469\n",
      "epoch: 19 step: 85 loss: 0.22298586 acc: 0.9402809143066406\n",
      "epoch: 19 step: 86 loss: 0.26418325 acc: 0.9336814880371094\n",
      "epoch: 19 step: 87 loss: 0.14666066 acc: 0.9552955627441406\n",
      "epoch: 19 step: 88 loss: 0.2312198 acc: 0.9322090148925781\n",
      "epoch: 19 step: 89 loss: 0.1838338 acc: 0.9383392333984375\n",
      "epoch: 19 step: 90 loss: 0.19847296 acc: 0.9380607604980469\n",
      "epoch: 19 step: 91 loss: 0.21506412 acc: 0.940399169921875\n",
      "epoch: 19 step: 92 loss: 0.18501228 acc: 0.9425926208496094\n",
      "epoch: 19 step: 93 loss: 0.16449362 acc: 0.9464874267578125\n",
      "epoch: 19 step: 94 loss: 0.1835547 acc: 0.9434394836425781\n",
      "epoch: 19 step: 95 loss: 0.15156431 acc: 0.9527244567871094\n",
      "epoch: 19 step: 96 loss: 0.17774945 acc: 0.9490623474121094\n",
      "epoch: 19 step: 97 loss: 0.1821104 acc: 0.9451828002929688\n",
      "epoch: 19 step: 98 loss: 0.13983399 acc: 0.9554595947265625\n",
      "epoch: 19 step: 99 loss: 0.14899094 acc: 0.9576683044433594\n",
      "epoch: 19 step: 100 loss: 0.17338002 acc: 0.9531784057617188\n",
      "epoch: 19 step: 101 loss: 0.1993068 acc: 0.939483642578125\n",
      "epoch: 19 step: 102 loss: 0.17824216 acc: 0.9432907104492188\n",
      "epoch: 19 step: 103 loss: 0.21340036 acc: 0.938323974609375\n",
      "epoch: 19 step: 104 loss: 0.21486822 acc: 0.9300422668457031\n",
      "epoch: 19 step: 105 loss: 0.1579066 acc: 0.9356613159179688\n",
      "epoch: 19 step: 106 loss: 0.1748008 acc: 0.922149658203125\n",
      "epoch: 19 step: 107 loss: 0.1815283 acc: 0.9410781860351562\n",
      "epoch: 19 step: 108 loss: 0.44608212 acc: 0.9170646667480469\n",
      "epoch: 19 step: 109 loss: 0.15812954 acc: 0.943084716796875\n",
      "epoch: 19 step: 110 loss: 0.22985144 acc: 0.9351119995117188\n",
      "epoch: 19 step: 111 loss: 0.39734438 acc: 0.9110679626464844\n",
      "epoch: 19 step: 112 loss: 0.20305508 acc: 0.9316329956054688\n",
      "epoch: 19 step: 113 loss: 0.2066035 acc: 0.9331321716308594\n",
      "epoch: 19 step: 114 loss: 0.2489811 acc: 0.934326171875\n",
      "epoch: 19 step: 115 loss: 0.3042512 acc: 0.9354362487792969\n",
      "epoch: 19 step: 116 loss: 0.20514356 acc: 0.95220947265625\n",
      "epoch: 19 step: 117 loss: 0.25751972 acc: 0.9383697509765625\n",
      "epoch: 19 step: 118 loss: 0.219469 acc: 0.9511489868164062\n",
      "epoch: 19 step: 119 loss: 0.20233357 acc: 0.932830810546875\n",
      "epoch: 19 step: 120 loss: 0.29097798 acc: 0.937957763671875\n",
      "epoch: 19 step: 121 loss: 0.20890564 acc: 0.9492301940917969\n",
      "epoch: 19 step: 122 loss: 0.29444999 acc: 0.92620849609375\n",
      "epoch: 19 step: 123 loss: 0.19213012 acc: 0.9485740661621094\n",
      "epoch: 19 step: 124 loss: 0.26637578 acc: 0.9526803152901786\n",
      "epoch: 19 validation_loss: 0.298 validation_dice: 0.7309169245482682\n",
      "epoch: 19 test_dataset dice: 0.5846358369940543\n",
      "time cost 0.5339701692263286 min\n",
      "dice_best: 0.7761289493951334\n",
      "******************************** epoch  19  is finished. *********************************\n",
      "epoch: 20 step: 1 loss: 0.22607411 acc: 0.9463119506835938\n",
      "epoch: 20 step: 2 loss: 0.28459144 acc: 0.9349632263183594\n",
      "epoch: 20 step: 3 loss: 0.2982644 acc: 0.9319229125976562\n",
      "epoch: 20 step: 4 loss: 0.2656215 acc: 0.9341201782226562\n",
      "epoch: 20 step: 5 loss: 0.2420119 acc: 0.9310493469238281\n",
      "epoch: 20 step: 6 loss: 0.3041899 acc: 0.939056396484375\n",
      "epoch: 20 step: 7 loss: 0.26474524 acc: 0.933868408203125\n",
      "epoch: 20 step: 8 loss: 0.23415852 acc: 0.9366302490234375\n",
      "epoch: 20 step: 9 loss: 0.2935247 acc: 0.9367942810058594\n",
      "epoch: 20 step: 10 loss: 0.24365295 acc: 0.9442634582519531\n",
      "epoch: 20 step: 11 loss: 0.23509571 acc: 0.9414825439453125\n",
      "epoch: 20 step: 12 loss: 0.28431413 acc: 0.933624267578125\n",
      "epoch: 20 step: 13 loss: 0.24551769 acc: 0.9261970520019531\n",
      "epoch: 20 step: 14 loss: 0.25622138 acc: 0.9238967895507812\n",
      "epoch: 20 step: 15 loss: 0.31633487 acc: 0.9170570373535156\n",
      "epoch: 20 step: 16 loss: 0.34657273 acc: 0.9176597595214844\n",
      "epoch: 20 step: 17 loss: 0.24082255 acc: 0.9206657409667969\n",
      "epoch: 20 step: 18 loss: 0.26533085 acc: 0.9286575317382812\n",
      "epoch: 20 step: 19 loss: 0.2942442 acc: 0.9349517822265625\n",
      "epoch: 20 step: 20 loss: 0.28527513 acc: 0.9312057495117188\n",
      "epoch: 20 step: 21 loss: 0.22543643 acc: 0.9388160705566406\n",
      "epoch: 20 step: 22 loss: 0.24672066 acc: 0.93585205078125\n",
      "epoch: 20 step: 23 loss: 0.35803342 acc: 0.92724609375\n",
      "epoch: 20 step: 24 loss: 0.19905065 acc: 0.9311561584472656\n",
      "epoch: 20 step: 25 loss: 0.303693 acc: 0.9385337829589844\n",
      "epoch: 20 step: 26 loss: 0.23389338 acc: 0.932891845703125\n",
      "epoch: 20 step: 27 loss: 0.23445936 acc: 0.9357986450195312\n",
      "epoch: 20 step: 28 loss: 0.26549095 acc: 0.9191551208496094\n",
      "epoch: 20 step: 29 loss: 0.19077897 acc: 0.938323974609375\n",
      "epoch: 20 step: 30 loss: 0.31737858 acc: 0.900360107421875\n",
      "epoch: 20 step: 31 loss: 0.24515058 acc: 0.9413261413574219\n",
      "epoch: 20 step: 32 loss: 0.30462292 acc: 0.9380760192871094\n",
      "epoch: 20 step: 33 loss: 0.30223092 acc: 0.9388008117675781\n",
      "epoch: 20 step: 34 loss: 0.32540366 acc: 0.942108154296875\n",
      "epoch: 20 step: 35 loss: 0.39838696 acc: 0.9291496276855469\n",
      "epoch: 20 step: 36 loss: 0.23784457 acc: 0.9356918334960938\n",
      "epoch: 20 step: 37 loss: 0.24336818 acc: 0.932952880859375\n",
      "epoch: 20 step: 38 loss: 0.38665637 acc: 0.9086532592773438\n",
      "epoch: 20 step: 39 loss: 0.31364703 acc: 0.9175682067871094\n",
      "epoch: 20 step: 40 loss: 0.26896194 acc: 0.9047508239746094\n",
      "epoch: 20 step: 41 loss: 0.2868145 acc: 0.919921875\n",
      "epoch: 20 step: 42 loss: 0.27709422 acc: 0.9190177917480469\n",
      "epoch: 20 step: 43 loss: 0.22244146 acc: 0.9384803771972656\n",
      "epoch: 20 step: 44 loss: 0.20462123 acc: 0.9431037902832031\n",
      "epoch: 20 step: 45 loss: 0.23900075 acc: 0.9486465454101562\n",
      "epoch: 20 step: 46 loss: 0.19431742 acc: 0.9468040466308594\n",
      "epoch: 20 step: 47 loss: 0.30259976 acc: 0.917938232421875\n",
      "epoch: 20 step: 48 loss: 0.2683857 acc: 0.93157958984375\n",
      "epoch: 20 step: 49 loss: 0.23100357 acc: 0.9370536804199219\n",
      "epoch: 20 step: 50 loss: 0.2178206 acc: 0.9344635009765625\n",
      "epoch: 20 step: 51 loss: 0.2095689 acc: 0.9388275146484375\n",
      "epoch: 20 step: 52 loss: 0.21976945 acc: 0.9378433227539062\n",
      "epoch: 20 step: 53 loss: 0.28602415 acc: 0.9289703369140625\n",
      "epoch: 20 step: 54 loss: 0.1758081 acc: 0.9456329345703125\n",
      "epoch: 20 step: 55 loss: 0.22163591 acc: 0.9214286804199219\n",
      "epoch: 20 step: 56 loss: 0.25294587 acc: 0.9433174133300781\n",
      "epoch: 20 step: 57 loss: 0.27701584 acc: 0.9094467163085938\n",
      "epoch: 20 step: 58 loss: 0.18615365 acc: 0.9484176635742188\n",
      "epoch: 20 step: 59 loss: 0.26586264 acc: 0.9289054870605469\n",
      "epoch: 20 step: 60 loss: 0.15642421 acc: 0.94903564453125\n",
      "epoch: 20 step: 61 loss: 0.16090111 acc: 0.9482612609863281\n",
      "epoch: 20 step: 62 loss: 0.20707671 acc: 0.9450569152832031\n",
      "epoch: 20 step: 63 loss: 0.1909909 acc: 0.94256591796875\n",
      "epoch: 20 step: 64 loss: 0.24677986 acc: 0.9397354125976562\n",
      "epoch: 20 step: 65 loss: 0.18428002 acc: 0.952545166015625\n",
      "epoch: 20 step: 66 loss: 0.18877208 acc: 0.9480438232421875\n",
      "epoch: 20 step: 67 loss: 0.19425522 acc: 0.9436264038085938\n",
      "epoch: 20 step: 68 loss: 0.20440386 acc: 0.9365310668945312\n",
      "epoch: 20 step: 69 loss: 0.19718818 acc: 0.9214248657226562\n",
      "epoch: 20 step: 70 loss: 0.20892982 acc: 0.9401931762695312\n",
      "epoch: 20 step: 71 loss: 0.2021661 acc: 0.9429664611816406\n",
      "epoch: 20 step: 72 loss: 0.1701704 acc: 0.9422035217285156\n",
      "epoch: 20 step: 73 loss: 0.22482257 acc: 0.9322586059570312\n",
      "epoch: 20 step: 74 loss: 0.18792906 acc: 0.9393272399902344\n",
      "epoch: 20 step: 75 loss: 0.18371616 acc: 0.9415855407714844\n",
      "epoch: 20 step: 76 loss: 0.17245759 acc: 0.9417686462402344\n",
      "epoch: 20 step: 77 loss: 0.13696659 acc: 0.9546928405761719\n",
      "epoch: 20 step: 78 loss: 0.21604556 acc: 0.943603515625\n",
      "epoch: 20 step: 79 loss: 0.19458449 acc: 0.9429702758789062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 step: 80 loss: 0.21482068 acc: 0.9471549987792969\n",
      "epoch: 20 step: 81 loss: 0.16411363 acc: 0.9514846801757812\n",
      "epoch: 20 step: 82 loss: 0.17224878 acc: 0.9500350952148438\n",
      "epoch: 20 step: 83 loss: 0.18109818 acc: 0.9388999938964844\n",
      "epoch: 20 step: 84 loss: 0.22322921 acc: 0.9371414184570312\n",
      "epoch: 20 step: 85 loss: 0.21048269 acc: 0.9382667541503906\n",
      "epoch: 20 step: 86 loss: 0.16954997 acc: 0.9482498168945312\n",
      "epoch: 20 step: 87 loss: 0.19412993 acc: 0.9395675659179688\n",
      "epoch: 20 step: 88 loss: 0.14355014 acc: 0.95001220703125\n",
      "epoch: 20 step: 89 loss: 0.18794873 acc: 0.9459609985351562\n",
      "epoch: 20 step: 90 loss: 0.16948359 acc: 0.9511260986328125\n",
      "epoch: 20 step: 91 loss: 0.16870046 acc: 0.9501800537109375\n",
      "epoch: 20 step: 92 loss: 0.20052722 acc: 0.9444770812988281\n",
      "epoch: 20 step: 93 loss: 0.1889345 acc: 0.9506797790527344\n",
      "epoch: 20 step: 94 loss: 0.1803408 acc: 0.9452705383300781\n",
      "epoch: 20 step: 95 loss: 0.18625416 acc: 0.9443588256835938\n",
      "epoch: 20 step: 96 loss: 0.15371545 acc: 0.9460716247558594\n",
      "epoch: 20 step: 97 loss: 0.15710288 acc: 0.9441261291503906\n",
      "epoch: 20 step: 98 loss: 0.14648324 acc: 0.9470977783203125\n",
      "epoch: 20 step: 99 loss: 0.15646544 acc: 0.9489860534667969\n",
      "epoch: 20 step: 100 loss: 0.15626286 acc: 0.9483489990234375\n",
      "epoch: 20 step: 101 loss: 0.14775391 acc: 0.9511375427246094\n",
      "epoch: 20 step: 102 loss: 0.15903956 acc: 0.9507331848144531\n",
      "epoch: 20 step: 103 loss: 0.15737876 acc: 0.9516067504882812\n",
      "epoch: 20 step: 104 loss: 0.15048182 acc: 0.9493598937988281\n",
      "epoch: 20 step: 105 loss: 0.27754626 acc: 0.947998046875\n",
      "epoch: 20 step: 106 loss: 0.16844374 acc: 0.9460639953613281\n",
      "epoch: 20 step: 107 loss: 0.16618408 acc: 0.9479866027832031\n",
      "epoch: 20 step: 108 loss: 0.17253785 acc: 0.9436569213867188\n",
      "epoch: 20 step: 109 loss: 0.17568083 acc: 0.9403762817382812\n",
      "epoch: 20 step: 110 loss: 0.17785639 acc: 0.9402999877929688\n",
      "epoch: 20 step: 111 loss: 0.1828813 acc: 0.9339027404785156\n",
      "epoch: 20 step: 112 loss: 0.17945515 acc: 0.94439697265625\n",
      "epoch: 20 step: 113 loss: 0.15982299 acc: 0.9416618347167969\n",
      "epoch: 20 step: 114 loss: 0.1922599 acc: 0.9352035522460938\n",
      "epoch: 20 step: 115 loss: 0.17391033 acc: 0.9457015991210938\n",
      "epoch: 20 step: 116 loss: 0.19120298 acc: 0.9416885375976562\n",
      "epoch: 20 step: 117 loss: 0.17140403 acc: 0.9446296691894531\n",
      "epoch: 20 step: 118 loss: 0.23677325 acc: 0.9345245361328125\n",
      "epoch: 20 step: 119 loss: 0.17539981 acc: 0.9605216979980469\n",
      "epoch: 20 step: 120 loss: 0.1557919 acc: 0.9449539184570312\n",
      "epoch: 20 step: 121 loss: 0.16450913 acc: 0.949310302734375\n",
      "epoch: 20 step: 122 loss: 0.14515272 acc: 0.956878662109375\n",
      "epoch: 20 step: 123 loss: 0.16767392 acc: 0.9527015686035156\n",
      "epoch: 20 step: 124 loss: 0.18591426 acc: 0.9460710797991071\n",
      "epoch: 20 validation_loss: 0.188 validation_dice: 0.7642640368983981\n",
      "epoch: 20 test_dataset dice: 0.6769835274837921\n",
      "time cost 0.5334524075190227 min\n",
      "dice_best: 0.7761289493951334\n",
      "******************************** epoch  20  is finished. *********************************\n",
      "epoch: 21 step: 1 loss: 0.17974137 acc: 0.9484291076660156\n",
      "epoch: 21 step: 2 loss: 0.15553097 acc: 0.94622802734375\n",
      "epoch: 21 step: 3 loss: 0.18241459 acc: 0.9388999938964844\n",
      "epoch: 21 step: 4 loss: 0.16407214 acc: 0.9414176940917969\n",
      "epoch: 21 step: 5 loss: 0.17364167 acc: 0.9380607604980469\n",
      "epoch: 21 step: 6 loss: 0.16436003 acc: 0.9365577697753906\n",
      "epoch: 21 step: 7 loss: 0.17044732 acc: 0.9472198486328125\n",
      "epoch: 21 step: 8 loss: 0.13972415 acc: 0.955291748046875\n",
      "epoch: 21 step: 9 loss: 0.15645501 acc: 0.9437599182128906\n",
      "epoch: 21 step: 10 loss: 0.17190897 acc: 0.9494743347167969\n",
      "epoch: 21 step: 11 loss: 0.1639998 acc: 0.9513397216796875\n",
      "epoch: 21 step: 12 loss: 0.14046761 acc: 0.9518203735351562\n",
      "epoch: 21 step: 13 loss: 0.14100608 acc: 0.9566383361816406\n",
      "epoch: 21 step: 14 loss: 0.17852414 acc: 0.9479408264160156\n",
      "epoch: 21 step: 15 loss: 0.1460203 acc: 0.951812744140625\n",
      "epoch: 21 step: 16 loss: 0.15731832 acc: 0.9497947692871094\n",
      "epoch: 21 step: 17 loss: 0.1752067 acc: 0.9533042907714844\n",
      "epoch: 21 step: 18 loss: 0.14570272 acc: 0.953643798828125\n",
      "epoch: 21 step: 19 loss: 0.1450995 acc: 0.9498214721679688\n",
      "epoch: 21 step: 20 loss: 0.14907837 acc: 0.9503364562988281\n",
      "epoch: 21 step: 21 loss: 0.16238388 acc: 0.9428787231445312\n",
      "epoch: 21 step: 22 loss: 0.12137846 acc: 0.9494819641113281\n",
      "epoch: 21 step: 23 loss: 0.1355742 acc: 0.9424247741699219\n",
      "epoch: 21 step: 24 loss: 0.14766225 acc: 0.9401016235351562\n",
      "epoch: 21 step: 25 loss: 0.17400983 acc: 0.9373588562011719\n",
      "epoch: 21 step: 26 loss: 0.17362784 acc: 0.9416961669921875\n",
      "epoch: 21 step: 27 loss: 0.1493463 acc: 0.9493637084960938\n",
      "epoch: 21 step: 28 loss: 0.13122512 acc: 0.961029052734375\n",
      "epoch: 21 step: 29 loss: 0.14850166 acc: 0.9553184509277344\n",
      "epoch: 21 step: 30 loss: 0.12504992 acc: 0.9595680236816406\n",
      "epoch: 21 step: 31 loss: 0.13457236 acc: 0.9548454284667969\n",
      "epoch: 21 step: 32 loss: 0.120512664 acc: 0.954376220703125\n",
      "epoch: 21 step: 33 loss: 0.16176194 acc: 0.9536895751953125\n",
      "epoch: 21 step: 34 loss: 0.124947816 acc: 0.954864501953125\n",
      "epoch: 21 step: 35 loss: 0.14576603 acc: 0.9471321105957031\n",
      "epoch: 21 step: 36 loss: 0.16844164 acc: 0.9473037719726562\n",
      "epoch: 21 step: 37 loss: 0.14776976 acc: 0.9386787414550781\n",
      "epoch: 21 step: 38 loss: 0.13383868 acc: 0.9499893188476562\n",
      "epoch: 21 step: 39 loss: 0.16356766 acc: 0.947418212890625\n",
      "epoch: 21 step: 40 loss: 0.13707855 acc: 0.9572181701660156\n",
      "epoch: 21 step: 41 loss: 0.15777887 acc: 0.9454345703125\n",
      "epoch: 21 step: 42 loss: 0.1474834 acc: 0.95037841796875\n",
      "epoch: 21 step: 43 loss: 0.12290006 acc: 0.9550285339355469\n",
      "epoch: 21 step: 44 loss: 0.15025657 acc: 0.9515762329101562\n",
      "epoch: 21 step: 45 loss: 0.15204602 acc: 0.962432861328125\n",
      "epoch: 21 step: 46 loss: 0.124838956 acc: 0.9566917419433594\n",
      "epoch: 21 step: 47 loss: 0.14003341 acc: 0.9488258361816406\n",
      "epoch: 21 step: 48 loss: 0.17534181 acc: 0.954986572265625\n",
      "epoch: 21 step: 49 loss: 0.12883501 acc: 0.9580154418945312\n",
      "epoch: 21 step: 50 loss: 0.16739248 acc: 0.9527702331542969\n",
      "epoch: 21 step: 51 loss: 0.14409778 acc: 0.9513130187988281\n",
      "epoch: 21 step: 52 loss: 0.14602973 acc: 0.9494895935058594\n",
      "epoch: 21 step: 53 loss: 0.16325441 acc: 0.9453277587890625\n",
      "epoch: 21 step: 54 loss: 0.13426213 acc: 0.9529571533203125\n",
      "epoch: 21 step: 55 loss: 0.14391507 acc: 0.9454498291015625\n",
      "epoch: 21 step: 56 loss: 0.16043551 acc: 0.9389381408691406\n",
      "epoch: 21 step: 57 loss: 0.14970641 acc: 0.9444389343261719\n",
      "epoch: 21 step: 58 loss: 0.15964378 acc: 0.941162109375\n",
      "epoch: 21 step: 59 loss: 0.12947439 acc: 0.9515953063964844\n",
      "epoch: 21 step: 60 loss: 0.11117833 acc: 0.958740234375\n",
      "epoch: 21 step: 61 loss: 0.15058975 acc: 0.9495277404785156\n",
      "epoch: 21 step: 62 loss: 0.14913684 acc: 0.9502487182617188\n",
      "epoch: 21 step: 63 loss: 0.20657285 acc: 0.9458808898925781\n",
      "epoch: 21 step: 64 loss: 0.13034616 acc: 0.9534645080566406\n",
      "epoch: 21 step: 65 loss: 0.14918657 acc: 0.9528884887695312\n",
      "epoch: 21 step: 66 loss: 0.14188087 acc: 0.9496002197265625\n",
      "epoch: 21 step: 67 loss: 0.16893215 acc: 0.9467964172363281\n",
      "epoch: 21 step: 68 loss: 0.19987033 acc: 0.9378547668457031\n",
      "epoch: 21 step: 69 loss: 0.17808978 acc: 0.9453620910644531\n",
      "epoch: 21 step: 70 loss: 0.13807805 acc: 0.9536781311035156\n",
      "epoch: 21 step: 71 loss: 0.12911937 acc: 0.9501304626464844\n",
      "epoch: 21 step: 72 loss: 0.1336726 acc: 0.9527053833007812\n",
      "epoch: 21 step: 73 loss: 0.14834186 acc: 0.9488983154296875\n",
      "epoch: 21 step: 74 loss: 0.14919221 acc: 0.9546737670898438\n",
      "epoch: 21 step: 75 loss: 0.2449873 acc: 0.934295654296875\n",
      "epoch: 21 step: 76 loss: 0.12912452 acc: 0.95440673828125\n",
      "epoch: 21 step: 77 loss: 0.17155659 acc: 0.9463768005371094\n",
      "epoch: 21 step: 78 loss: 0.13500258 acc: 0.9610099792480469\n",
      "epoch: 21 step: 79 loss: 0.16717947 acc: 0.944793701171875\n",
      "epoch: 21 step: 80 loss: 0.16502962 acc: 0.95513916015625\n",
      "epoch: 21 step: 81 loss: 0.15977354 acc: 0.9420623779296875\n",
      "epoch: 21 step: 82 loss: 0.16680476 acc: 0.9441986083984375\n",
      "epoch: 21 step: 83 loss: 0.16078238 acc: 0.941741943359375\n",
      "epoch: 21 step: 84 loss: 0.14078915 acc: 0.9486885070800781\n",
      "epoch: 21 step: 85 loss: 0.13131946 acc: 0.9523773193359375\n",
      "epoch: 21 step: 86 loss: 0.16214806 acc: 0.9529037475585938\n",
      "epoch: 21 step: 87 loss: 0.13640906 acc: 0.9597434997558594\n",
      "epoch: 21 step: 88 loss: 0.11388005 acc: 0.9622459411621094\n",
      "epoch: 21 step: 89 loss: 0.19298646 acc: 0.9463272094726562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 step: 90 loss: 0.1570833 acc: 0.9506263732910156\n",
      "epoch: 21 step: 91 loss: 0.15477219 acc: 0.951202392578125\n",
      "epoch: 21 step: 92 loss: 0.12565306 acc: 0.9580268859863281\n",
      "epoch: 21 step: 93 loss: 0.14110196 acc: 0.95849609375\n",
      "epoch: 21 step: 94 loss: 0.18977341 acc: 0.9445838928222656\n",
      "epoch: 21 step: 95 loss: 0.12241602 acc: 0.9544754028320312\n",
      "epoch: 21 step: 96 loss: 0.14155509 acc: 0.9471015930175781\n",
      "epoch: 21 step: 97 loss: 0.15297271 acc: 0.9442901611328125\n",
      "epoch: 21 step: 98 loss: 0.1404362 acc: 0.9564743041992188\n",
      "epoch: 21 step: 99 loss: 0.1954254 acc: 0.9416465759277344\n",
      "epoch: 21 step: 100 loss: 0.16710334 acc: 0.9397125244140625\n",
      "epoch: 21 step: 101 loss: 0.13176823 acc: 0.9508132934570312\n",
      "epoch: 21 step: 102 loss: 0.17412582 acc: 0.9426841735839844\n",
      "epoch: 21 step: 103 loss: 0.12089971 acc: 0.95758056640625\n",
      "epoch: 21 step: 104 loss: 0.14968865 acc: 0.9567184448242188\n",
      "epoch: 21 step: 105 loss: 0.16842102 acc: 0.9471626281738281\n",
      "epoch: 21 step: 106 loss: 0.15564612 acc: 0.9442214965820312\n",
      "epoch: 21 step: 107 loss: 0.13126512 acc: 0.95330810546875\n",
      "epoch: 21 step: 108 loss: 0.14087951 acc: 0.94598388671875\n",
      "epoch: 21 step: 109 loss: 0.15374139 acc: 0.9412994384765625\n",
      "epoch: 21 step: 110 loss: 0.13821496 acc: 0.9513702392578125\n",
      "epoch: 21 step: 111 loss: 0.15852937 acc: 0.9435272216796875\n",
      "epoch: 21 step: 112 loss: 0.14669667 acc: 0.950042724609375\n",
      "epoch: 21 step: 113 loss: 0.16533951 acc: 0.9409103393554688\n",
      "epoch: 21 step: 114 loss: 0.12425326 acc: 0.9510993957519531\n",
      "epoch: 21 step: 115 loss: 0.20959331 acc: 0.9435386657714844\n",
      "epoch: 21 step: 116 loss: 0.14726958 acc: 0.9558029174804688\n",
      "epoch: 21 step: 117 loss: 0.12866588 acc: 0.9591789245605469\n",
      "epoch: 21 step: 118 loss: 0.13032506 acc: 0.9561653137207031\n",
      "epoch: 21 step: 119 loss: 0.14693533 acc: 0.9474143981933594\n",
      "epoch: 21 step: 120 loss: 0.16280107 acc: 0.9521026611328125\n",
      "epoch: 21 step: 121 loss: 0.15471765 acc: 0.9557456970214844\n",
      "epoch: 21 step: 122 loss: 0.16782224 acc: 0.956756591796875\n",
      "epoch: 21 step: 123 loss: 0.18710522 acc: 0.9540367126464844\n",
      "epoch: 21 step: 124 loss: 0.13756365 acc: 0.9571969168526786\n",
      "epoch: 21 validation_loss: 0.175 validation_dice: 0.7815771303864456\n",
      "epoch: 21 test_dataset dice: 0.6939298716106377\n",
      "time cost 0.5338024179140727 min\n",
      "dice_best: 0.7815771303864456\n",
      "******************************** epoch  21  is finished. *********************************\n",
      "epoch: 22 step: 1 loss: 0.18963003 acc: 0.9487342834472656\n",
      "epoch: 22 step: 2 loss: 0.18560761 acc: 0.9509773254394531\n",
      "epoch: 22 step: 3 loss: 0.13354081 acc: 0.9529953002929688\n",
      "epoch: 22 step: 4 loss: 0.1842345 acc: 0.9434432983398438\n",
      "epoch: 22 step: 5 loss: 0.19862737 acc: 0.9380912780761719\n",
      "epoch: 22 step: 6 loss: 0.16487044 acc: 0.943511962890625\n",
      "epoch: 22 step: 7 loss: 0.1432172 acc: 0.9416389465332031\n",
      "epoch: 22 step: 8 loss: 0.16327964 acc: 0.9488105773925781\n",
      "epoch: 22 step: 9 loss: 0.23080413 acc: 0.9377212524414062\n",
      "epoch: 22 step: 10 loss: 0.14373963 acc: 0.9535140991210938\n",
      "epoch: 22 step: 11 loss: 0.17600003 acc: 0.9454765319824219\n",
      "epoch: 22 step: 12 loss: 0.2037134 acc: 0.9467048645019531\n",
      "epoch: 22 step: 13 loss: 0.23272468 acc: 0.9459075927734375\n",
      "epoch: 22 step: 14 loss: 0.14446788 acc: 0.9567832946777344\n",
      "epoch: 22 step: 15 loss: 0.16025525 acc: 0.9508590698242188\n",
      "epoch: 22 step: 16 loss: 0.15554559 acc: 0.95550537109375\n",
      "epoch: 22 step: 17 loss: 0.19515282 acc: 0.9518585205078125\n",
      "epoch: 22 step: 18 loss: 0.17465222 acc: 0.9494400024414062\n",
      "epoch: 22 step: 19 loss: 0.14525537 acc: 0.9530830383300781\n",
      "epoch: 22 step: 20 loss: 0.18984373 acc: 0.9495010375976562\n",
      "epoch: 22 step: 21 loss: 0.14030598 acc: 0.9525222778320312\n",
      "epoch: 22 step: 22 loss: 0.16156797 acc: 0.9468116760253906\n",
      "epoch: 22 step: 23 loss: 0.18352808 acc: 0.9381523132324219\n",
      "epoch: 22 step: 24 loss: 0.15425663 acc: 0.9509735107421875\n",
      "epoch: 22 step: 25 loss: 0.17807202 acc: 0.9370994567871094\n",
      "epoch: 22 step: 26 loss: 0.15699305 acc: 0.9484214782714844\n",
      "epoch: 22 step: 27 loss: 0.1677661 acc: 0.9568367004394531\n",
      "epoch: 22 step: 28 loss: 0.16417108 acc: 0.9471664428710938\n",
      "epoch: 22 step: 29 loss: 0.18931946 acc: 0.9423675537109375\n",
      "epoch: 22 step: 30 loss: 0.14251922 acc: 0.9493675231933594\n",
      "epoch: 22 step: 31 loss: 0.17146844 acc: 0.9465980529785156\n",
      "epoch: 22 step: 32 loss: 0.18782894 acc: 0.9473381042480469\n",
      "epoch: 22 step: 33 loss: 0.13221495 acc: 0.9518051147460938\n",
      "epoch: 22 step: 34 loss: 0.19324717 acc: 0.9416007995605469\n",
      "epoch: 22 step: 35 loss: 0.14772603 acc: 0.9471168518066406\n",
      "epoch: 22 step: 36 loss: 0.14609337 acc: 0.9551620483398438\n",
      "epoch: 22 step: 37 loss: 0.1495127 acc: 0.9494514465332031\n",
      "epoch: 22 step: 38 loss: 0.15669666 acc: 0.9540061950683594\n",
      "epoch: 22 step: 39 loss: 0.18394054 acc: 0.9464607238769531\n",
      "epoch: 22 step: 40 loss: 0.16995162 acc: 0.9446334838867188\n",
      "epoch: 22 step: 41 loss: 0.17025715 acc: 0.9503631591796875\n",
      "epoch: 22 step: 42 loss: 0.14107008 acc: 0.9523391723632812\n",
      "epoch: 22 step: 43 loss: 0.17861654 acc: 0.9552650451660156\n",
      "epoch: 22 step: 44 loss: 0.14813974 acc: 0.9530258178710938\n",
      "epoch: 22 step: 45 loss: 0.17537324 acc: 0.9485359191894531\n",
      "epoch: 22 step: 46 loss: 0.22219275 acc: 0.9425277709960938\n",
      "epoch: 22 step: 47 loss: 0.15952842 acc: 0.9510688781738281\n",
      "epoch: 22 step: 48 loss: 0.17985597 acc: 0.9339828491210938\n",
      "epoch: 22 step: 49 loss: 0.1460046 acc: 0.9461250305175781\n",
      "epoch: 22 step: 50 loss: 0.13759704 acc: 0.955474853515625\n",
      "epoch: 22 step: 51 loss: 0.17846954 acc: 0.9364662170410156\n",
      "epoch: 22 step: 52 loss: 0.15520152 acc: 0.9586601257324219\n",
      "epoch: 22 step: 53 loss: 0.14645107 acc: 0.9503669738769531\n",
      "epoch: 22 step: 54 loss: 0.18581252 acc: 0.9393806457519531\n",
      "epoch: 22 step: 55 loss: 0.17767622 acc: 0.9501953125\n",
      "epoch: 22 step: 56 loss: 0.14703661 acc: 0.9463310241699219\n",
      "epoch: 22 step: 57 loss: 0.13595867 acc: 0.9499168395996094\n",
      "epoch: 22 step: 58 loss: 0.13597764 acc: 0.9504127502441406\n",
      "epoch: 22 step: 59 loss: 0.14261839 acc: 0.959228515625\n",
      "epoch: 22 step: 60 loss: 0.13958637 acc: 0.9539375305175781\n",
      "epoch: 22 step: 61 loss: 0.19361012 acc: 0.9384346008300781\n",
      "epoch: 22 step: 62 loss: 0.1794363 acc: 0.941925048828125\n",
      "epoch: 22 step: 63 loss: 0.13113715 acc: 0.9589157104492188\n",
      "epoch: 22 step: 64 loss: 0.1495733 acc: 0.9500846862792969\n",
      "epoch: 22 step: 65 loss: 0.1396396 acc: 0.9539604187011719\n",
      "epoch: 22 step: 66 loss: 0.16090664 acc: 0.9535293579101562\n",
      "epoch: 22 step: 67 loss: 0.15300779 acc: 0.9485740661621094\n",
      "epoch: 22 step: 68 loss: 0.1736726 acc: 0.958465576171875\n",
      "epoch: 22 step: 69 loss: 0.17565088 acc: 0.9541358947753906\n",
      "epoch: 22 step: 70 loss: 0.1590418 acc: 0.9462318420410156\n",
      "epoch: 22 step: 71 loss: 0.18340862 acc: 0.9467697143554688\n",
      "epoch: 22 step: 72 loss: 0.15768935 acc: 0.9429397583007812\n",
      "epoch: 22 step: 73 loss: 0.20337608 acc: 0.9351768493652344\n",
      "epoch: 22 step: 74 loss: 0.13570115 acc: 0.9356040954589844\n",
      "epoch: 22 step: 75 loss: 0.1537104 acc: 0.9421920776367188\n",
      "epoch: 22 step: 76 loss: 0.1804571 acc: 0.9347267150878906\n",
      "epoch: 22 step: 77 loss: 0.16254374 acc: 0.9404029846191406\n",
      "epoch: 22 step: 78 loss: 0.16766304 acc: 0.9349250793457031\n",
      "epoch: 22 step: 79 loss: 0.16631088 acc: 0.9472389221191406\n",
      "epoch: 22 step: 80 loss: 0.15826592 acc: 0.9436187744140625\n",
      "epoch: 22 step: 81 loss: 0.16480379 acc: 0.9508781433105469\n",
      "epoch: 22 step: 82 loss: 0.21069546 acc: 0.9386253356933594\n",
      "epoch: 22 step: 83 loss: 0.19281763 acc: 0.9432220458984375\n",
      "epoch: 22 step: 84 loss: 0.14864957 acc: 0.9449195861816406\n",
      "epoch: 22 step: 85 loss: 0.20988564 acc: 0.9459762573242188\n",
      "epoch: 22 step: 86 loss: 0.216197 acc: 0.9559898376464844\n",
      "epoch: 22 step: 87 loss: 0.1447129 acc: 0.956878662109375\n",
      "epoch: 22 step: 88 loss: 0.18861893 acc: 0.9551315307617188\n",
      "epoch: 22 step: 89 loss: 0.1763755 acc: 0.9457626342773438\n",
      "epoch: 22 step: 90 loss: 0.17906952 acc: 0.9516754150390625\n",
      "epoch: 22 step: 91 loss: 0.17941783 acc: 0.9530677795410156\n",
      "epoch: 22 step: 92 loss: 0.12834187 acc: 0.9549789428710938\n",
      "epoch: 22 step: 93 loss: 0.17705323 acc: 0.9530296325683594\n",
      "epoch: 22 step: 94 loss: 0.17037345 acc: 0.9415397644042969\n",
      "epoch: 22 step: 95 loss: 0.15149957 acc: 0.9409980773925781\n",
      "epoch: 22 step: 96 loss: 0.16616838 acc: 0.9442405700683594\n",
      "epoch: 22 step: 97 loss: 0.1906334 acc: 0.934356689453125\n",
      "epoch: 22 step: 98 loss: 0.16007397 acc: 0.9441108703613281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 step: 99 loss: 0.16073968 acc: 0.94976806640625\n",
      "epoch: 22 step: 100 loss: 0.16798887 acc: 0.93988037109375\n",
      "epoch: 22 step: 101 loss: 0.16933757 acc: 0.941802978515625\n",
      "epoch: 22 step: 102 loss: 0.16939972 acc: 0.9515419006347656\n",
      "epoch: 22 step: 103 loss: 0.12632991 acc: 0.9535179138183594\n",
      "epoch: 22 step: 104 loss: 0.149272 acc: 0.9557533264160156\n",
      "epoch: 22 step: 105 loss: 0.14976524 acc: 0.9479331970214844\n",
      "epoch: 22 step: 106 loss: 0.1586709 acc: 0.9402389526367188\n",
      "epoch: 22 step: 107 loss: 0.15447026 acc: 0.9466285705566406\n",
      "epoch: 22 step: 108 loss: 0.19250816 acc: 0.9425888061523438\n",
      "epoch: 22 step: 109 loss: 0.120650455 acc: 0.9556503295898438\n",
      "epoch: 22 step: 110 loss: 0.14954914 acc: 0.9472007751464844\n",
      "epoch: 22 step: 111 loss: 0.18644232 acc: 0.9488983154296875\n",
      "epoch: 22 step: 112 loss: 0.16634691 acc: 0.9484329223632812\n",
      "epoch: 22 step: 113 loss: 0.14445981 acc: 0.961639404296875\n",
      "epoch: 22 step: 114 loss: 0.15687425 acc: 0.9553337097167969\n",
      "epoch: 22 step: 115 loss: 0.1378127 acc: 0.9470405578613281\n",
      "epoch: 22 step: 116 loss: 0.16142474 acc: 0.9500312805175781\n",
      "epoch: 22 step: 117 loss: 0.13298517 acc: 0.9612846374511719\n",
      "epoch: 22 step: 118 loss: 0.13091996 acc: 0.949432373046875\n",
      "epoch: 22 step: 119 loss: 0.1587409 acc: 0.9458847045898438\n",
      "epoch: 22 step: 120 loss: 0.15120184 acc: 0.9479598999023438\n",
      "epoch: 22 step: 121 loss: 0.14705366 acc: 0.94921875\n",
      "epoch: 22 step: 122 loss: 0.14741363 acc: 0.9523811340332031\n",
      "epoch: 22 step: 123 loss: 0.17596737 acc: 0.9439315795898438\n",
      "epoch: 22 step: 124 loss: 0.29377732 acc: 0.9302629743303571\n",
      "epoch: 22 validation_loss: 0.171 validation_dice: 0.7886073303901484\n",
      "epoch: 22 test_dataset dice: 0.7102358480301314\n",
      "time cost 0.5341352423032125 min\n",
      "dice_best: 0.7886073303901484\n",
      "******************************** epoch  22  is finished. *********************************\n",
      "epoch: 23 step: 1 loss: 0.17840402 acc: 0.9425735473632812\n",
      "epoch: 23 step: 2 loss: 0.2464505 acc: 0.94354248046875\n",
      "epoch: 23 step: 3 loss: 0.24429974 acc: 0.9456253051757812\n",
      "epoch: 23 step: 4 loss: 0.1757837 acc: 0.9506340026855469\n",
      "epoch: 23 step: 5 loss: 0.17134424 acc: 0.9517288208007812\n",
      "epoch: 23 step: 6 loss: 0.19648913 acc: 0.9372291564941406\n",
      "epoch: 23 step: 7 loss: 0.18228796 acc: 0.9495506286621094\n",
      "epoch: 23 step: 8 loss: 0.17252791 acc: 0.9524955749511719\n",
      "epoch: 23 step: 9 loss: 0.20445694 acc: 0.9472541809082031\n",
      "epoch: 23 step: 10 loss: 0.18836845 acc: 0.9381904602050781\n",
      "epoch: 23 step: 11 loss: 0.193704 acc: 0.9433479309082031\n",
      "epoch: 23 step: 12 loss: 0.22013088 acc: 0.94122314453125\n",
      "epoch: 23 step: 13 loss: 0.17565055 acc: 0.9466590881347656\n",
      "epoch: 23 step: 14 loss: 0.18091585 acc: 0.9522247314453125\n",
      "epoch: 23 step: 15 loss: 0.17479332 acc: 0.9452018737792969\n",
      "epoch: 23 step: 16 loss: 0.19889355 acc: 0.9430580139160156\n",
      "epoch: 23 step: 17 loss: 0.1710971 acc: 0.9509353637695312\n",
      "epoch: 23 step: 18 loss: 0.18106079 acc: 0.9519805908203125\n",
      "epoch: 23 step: 19 loss: 0.18272771 acc: 0.9495162963867188\n",
      "epoch: 23 step: 20 loss: 0.15899822 acc: 0.9627189636230469\n",
      "epoch: 23 step: 21 loss: 0.24281166 acc: 0.9454307556152344\n",
      "epoch: 23 step: 22 loss: 0.2070077 acc: 0.9461097717285156\n",
      "epoch: 23 step: 23 loss: 0.13504699 acc: 0.9595413208007812\n",
      "epoch: 23 step: 24 loss: 0.1467319 acc: 0.9456863403320312\n",
      "epoch: 23 step: 25 loss: 0.23748073 acc: 0.928497314453125\n",
      "epoch: 23 step: 26 loss: 0.16869411 acc: 0.9424018859863281\n",
      "epoch: 23 step: 27 loss: 0.16196404 acc: 0.9465408325195312\n",
      "epoch: 23 step: 28 loss: 0.22032708 acc: 0.9421920776367188\n",
      "epoch: 23 step: 29 loss: 0.19156477 acc: 0.9365310668945312\n",
      "epoch: 23 step: 30 loss: 0.19646539 acc: 0.9446601867675781\n",
      "epoch: 23 step: 31 loss: 0.18253255 acc: 0.9459762573242188\n",
      "epoch: 23 step: 32 loss: 0.16304682 acc: 0.9490928649902344\n",
      "epoch: 23 step: 33 loss: 0.19084613 acc: 0.9539070129394531\n",
      "epoch: 23 step: 34 loss: 0.18614227 acc: 0.9542503356933594\n",
      "epoch: 23 step: 35 loss: 0.1594027 acc: 0.9509658813476562\n",
      "epoch: 23 step: 36 loss: 0.1746822 acc: 0.9484977722167969\n",
      "epoch: 23 step: 37 loss: 0.17284998 acc: 0.9493560791015625\n",
      "epoch: 23 step: 38 loss: 0.14919108 acc: 0.9515190124511719\n",
      "epoch: 23 step: 39 loss: 0.16416964 acc: 0.9364013671875\n",
      "epoch: 23 step: 40 loss: 0.20208219 acc: 0.9371414184570312\n",
      "epoch: 23 step: 41 loss: 0.18478023 acc: 0.9420738220214844\n",
      "epoch: 23 step: 42 loss: 0.25493017 acc: 0.9226608276367188\n",
      "epoch: 23 step: 43 loss: 0.18146946 acc: 0.9327392578125\n",
      "epoch: 23 step: 44 loss: 0.16235891 acc: 0.9547653198242188\n",
      "epoch: 23 step: 45 loss: 0.23358305 acc: 0.9393386840820312\n",
      "epoch: 23 step: 46 loss: 0.1787671 acc: 0.951385498046875\n",
      "epoch: 23 step: 47 loss: 0.1981605 acc: 0.9489631652832031\n",
      "epoch: 23 step: 48 loss: 0.18330652 acc: 0.9572219848632812\n",
      "epoch: 23 step: 49 loss: 0.27772278 acc: 0.9364776611328125\n",
      "epoch: 23 step: 50 loss: 0.2012749 acc: 0.9413681030273438\n",
      "epoch: 23 step: 51 loss: 0.17491727 acc: 0.9452972412109375\n",
      "epoch: 23 step: 52 loss: 0.21557954 acc: 0.9311027526855469\n",
      "epoch: 23 step: 53 loss: 0.1864389 acc: 0.9330329895019531\n",
      "epoch: 23 step: 54 loss: 0.19670634 acc: 0.9187583923339844\n",
      "epoch: 23 step: 55 loss: 0.20307927 acc: 0.9295578002929688\n",
      "epoch: 23 step: 56 loss: 0.25371766 acc: 0.9325103759765625\n",
      "epoch: 23 step: 57 loss: 0.19373207 acc: 0.9438743591308594\n",
      "epoch: 23 step: 58 loss: 0.18220748 acc: 0.9366302490234375\n",
      "epoch: 23 step: 59 loss: 0.18865165 acc: 0.9365615844726562\n",
      "epoch: 23 step: 60 loss: 0.16859494 acc: 0.9440383911132812\n",
      "epoch: 23 step: 61 loss: 0.18418448 acc: 0.950836181640625\n",
      "epoch: 23 step: 62 loss: 0.18123521 acc: 0.9477424621582031\n",
      "epoch: 23 step: 63 loss: 0.18447454 acc: 0.9471359252929688\n",
      "epoch: 23 step: 64 loss: 0.1972207 acc: 0.9504547119140625\n",
      "epoch: 23 step: 65 loss: 0.16251129 acc: 0.9543533325195312\n",
      "epoch: 23 step: 66 loss: 0.20874408 acc: 0.9461174011230469\n",
      "epoch: 23 step: 67 loss: 0.13669728 acc: 0.9424819946289062\n",
      "epoch: 23 step: 68 loss: 0.15139022 acc: 0.9482040405273438\n",
      "epoch: 23 step: 69 loss: 0.15197669 acc: 0.9521827697753906\n",
      "epoch: 23 step: 70 loss: 0.14598629 acc: 0.94757080078125\n",
      "epoch: 23 step: 71 loss: 0.17258902 acc: 0.9465484619140625\n",
      "epoch: 23 step: 72 loss: 0.18445742 acc: 0.9292373657226562\n",
      "epoch: 23 step: 73 loss: 0.16529895 acc: 0.9482841491699219\n",
      "epoch: 23 step: 74 loss: 0.18717825 acc: 0.9403228759765625\n",
      "epoch: 23 step: 75 loss: 0.18079144 acc: 0.9426956176757812\n",
      "epoch: 23 step: 76 loss: 0.16075364 acc: 0.9511871337890625\n",
      "epoch: 23 step: 77 loss: 0.19860299 acc: 0.9506416320800781\n",
      "epoch: 23 step: 78 loss: 0.16203147 acc: 0.9506111145019531\n",
      "epoch: 23 step: 79 loss: 0.13889101 acc: 0.9591331481933594\n",
      "epoch: 23 step: 80 loss: 0.18020467 acc: 0.9465408325195312\n",
      "epoch: 23 step: 81 loss: 0.15227365 acc: 0.9489669799804688\n",
      "epoch: 23 step: 82 loss: 0.20384485 acc: 0.94390869140625\n",
      "epoch: 23 step: 83 loss: 0.15091607 acc: 0.9475364685058594\n",
      "epoch: 23 step: 84 loss: 0.19624455 acc: 0.9384193420410156\n",
      "epoch: 23 step: 85 loss: 0.14366674 acc: 0.9507102966308594\n",
      "epoch: 23 step: 86 loss: 0.17703603 acc: 0.9404983520507812\n",
      "epoch: 23 step: 87 loss: 0.1772088 acc: 0.9514122009277344\n",
      "epoch: 23 step: 88 loss: 0.13097036 acc: 0.9589195251464844\n",
      "epoch: 23 step: 89 loss: 0.17823808 acc: 0.936859130859375\n",
      "epoch: 23 step: 90 loss: 0.1661125 acc: 0.9409523010253906\n",
      "epoch: 23 step: 91 loss: 0.17884582 acc: 0.9449844360351562\n",
      "epoch: 23 step: 92 loss: 0.14649078 acc: 0.9561653137207031\n",
      "epoch: 23 step: 93 loss: 0.16720194 acc: 0.9417152404785156\n",
      "epoch: 23 step: 94 loss: 0.16998506 acc: 0.9519233703613281\n",
      "epoch: 23 step: 95 loss: 0.19381435 acc: 0.9444313049316406\n",
      "epoch: 23 step: 96 loss: 0.18175516 acc: 0.9434623718261719\n",
      "epoch: 23 step: 97 loss: 0.21590756 acc: 0.9431533813476562\n",
      "epoch: 23 step: 98 loss: 0.171486 acc: 0.949249267578125\n",
      "epoch: 23 step: 99 loss: 0.15821387 acc: 0.9484786987304688\n",
      "epoch: 23 step: 100 loss: 0.1960129 acc: 0.943634033203125\n",
      "epoch: 23 step: 101 loss: 0.16974123 acc: 0.9521293640136719\n",
      "epoch: 23 step: 102 loss: 0.17615375 acc: 0.9515876770019531\n",
      "epoch: 23 step: 103 loss: 0.21362123 acc: 0.9497184753417969\n",
      "epoch: 23 step: 104 loss: 0.14105965 acc: 0.9561042785644531\n",
      "epoch: 23 step: 105 loss: 0.20425394 acc: 0.935211181640625\n",
      "epoch: 23 step: 106 loss: 0.1677778 acc: 0.9478034973144531\n",
      "epoch: 23 step: 107 loss: 0.17723851 acc: 0.9411163330078125\n",
      "epoch: 23 step: 108 loss: 0.14877172 acc: 0.9572525024414062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 step: 109 loss: 0.14739823 acc: 0.9584693908691406\n",
      "epoch: 23 step: 110 loss: 0.17058034 acc: 0.9477081298828125\n",
      "epoch: 23 step: 111 loss: 0.17463417 acc: 0.9481086730957031\n",
      "epoch: 23 step: 112 loss: 0.17416187 acc: 0.9444236755371094\n",
      "epoch: 23 step: 113 loss: 0.16065364 acc: 0.9476280212402344\n",
      "epoch: 23 step: 114 loss: 0.16929789 acc: 0.9527778625488281\n",
      "epoch: 23 step: 115 loss: 0.1242933 acc: 0.9617691040039062\n",
      "epoch: 23 step: 116 loss: 0.17985883 acc: 0.9457015991210938\n",
      "epoch: 23 step: 117 loss: 0.16820824 acc: 0.9502182006835938\n",
      "epoch: 23 step: 118 loss: 0.13056259 acc: 0.9565467834472656\n",
      "epoch: 23 step: 119 loss: 0.14206663 acc: 0.9598922729492188\n",
      "epoch: 23 step: 120 loss: 0.2722567 acc: 0.9352874755859375\n",
      "epoch: 23 step: 121 loss: 0.16051324 acc: 0.9549484252929688\n",
      "epoch: 23 step: 122 loss: 0.19347191 acc: 0.9461402893066406\n",
      "epoch: 23 step: 123 loss: 0.20718402 acc: 0.9387092590332031\n",
      "epoch: 23 step: 124 loss: 0.16537087 acc: 0.9530203683035714\n",
      "epoch: 23 validation_loss: 0.259 validation_dice: 0.7530048614001353\n",
      "epoch: 23 test_dataset dice: 0.6563172078488916\n",
      "time cost 0.5339988311131795 min\n",
      "dice_best: 0.7886073303901484\n",
      "******************************** epoch  23  is finished. *********************************\n",
      "epoch: 24 step: 1 loss: 0.1899544 acc: 0.9557723999023438\n",
      "epoch: 24 step: 2 loss: 0.2136229 acc: 0.9363288879394531\n",
      "epoch: 24 step: 3 loss: 0.16580582 acc: 0.9426345825195312\n",
      "epoch: 24 step: 4 loss: 0.24983643 acc: 0.9408187866210938\n",
      "epoch: 24 step: 5 loss: 0.21404693 acc: 0.9460372924804688\n",
      "epoch: 24 step: 6 loss: 0.23780382 acc: 0.9277572631835938\n",
      "epoch: 24 step: 7 loss: 0.18518762 acc: 0.9448776245117188\n",
      "epoch: 24 step: 8 loss: 0.18349734 acc: 0.9517326354980469\n",
      "epoch: 24 step: 9 loss: 0.19190794 acc: 0.9454460144042969\n",
      "epoch: 24 step: 10 loss: 0.19184884 acc: 0.9417381286621094\n",
      "epoch: 24 step: 11 loss: 0.1905904 acc: 0.934722900390625\n",
      "epoch: 24 step: 12 loss: 0.23491168 acc: 0.9353408813476562\n",
      "epoch: 24 step: 13 loss: 0.23379026 acc: 0.9217643737792969\n",
      "epoch: 24 step: 14 loss: 0.18028289 acc: 0.9434738159179688\n",
      "epoch: 24 step: 15 loss: 0.17083038 acc: 0.9497795104980469\n",
      "epoch: 24 step: 16 loss: 0.19482583 acc: 0.9410057067871094\n",
      "epoch: 24 step: 17 loss: 0.20325789 acc: 0.942352294921875\n",
      "epoch: 24 step: 18 loss: 0.20556843 acc: 0.948822021484375\n",
      "epoch: 24 step: 19 loss: 0.17139749 acc: 0.9501380920410156\n",
      "epoch: 24 step: 20 loss: 0.1942146 acc: 0.9511680603027344\n",
      "epoch: 24 step: 21 loss: 0.18564543 acc: 0.952392578125\n",
      "epoch: 24 step: 22 loss: 0.14202963 acc: 0.9531440734863281\n",
      "epoch: 24 step: 23 loss: 0.16116981 acc: 0.9546165466308594\n",
      "epoch: 24 step: 24 loss: 0.17907815 acc: 0.942352294921875\n",
      "epoch: 24 step: 25 loss: 0.16155908 acc: 0.9553298950195312\n",
      "epoch: 24 step: 26 loss: 0.34307626 acc: 0.931640625\n",
      "epoch: 24 step: 27 loss: 0.21009181 acc: 0.9496612548828125\n",
      "epoch: 24 step: 28 loss: 0.15299763 acc: 0.9572486877441406\n",
      "epoch: 24 step: 29 loss: 0.23051587 acc: 0.945343017578125\n",
      "epoch: 24 step: 30 loss: 0.22509266 acc: 0.9386520385742188\n",
      "epoch: 24 step: 31 loss: 0.16429816 acc: 0.9553985595703125\n",
      "epoch: 24 step: 32 loss: 0.14794461 acc: 0.9571151733398438\n",
      "epoch: 24 step: 33 loss: 0.15237564 acc: 0.9526596069335938\n",
      "epoch: 24 step: 34 loss: 0.16479638 acc: 0.9541549682617188\n",
      "epoch: 24 step: 35 loss: 0.16528589 acc: 0.9523887634277344\n",
      "epoch: 24 step: 36 loss: 0.1899321 acc: 0.9549560546875\n",
      "epoch: 24 step: 37 loss: 0.17624466 acc: 0.9457168579101562\n",
      "epoch: 24 step: 38 loss: 0.19728963 acc: 0.9451103210449219\n",
      "epoch: 24 step: 39 loss: 0.18251102 acc: 0.9413719177246094\n",
      "epoch: 24 step: 40 loss: 0.19674248 acc: 0.9405136108398438\n",
      "epoch: 24 step: 41 loss: 0.24806735 acc: 0.9279632568359375\n",
      "epoch: 24 step: 42 loss: 0.17988412 acc: 0.943572998046875\n",
      "epoch: 24 step: 43 loss: 0.1718585 acc: 0.9492416381835938\n",
      "epoch: 24 step: 44 loss: 0.17086974 acc: 0.948822021484375\n",
      "epoch: 24 step: 45 loss: 0.19236495 acc: 0.951202392578125\n",
      "epoch: 24 step: 46 loss: 0.24066731 acc: 0.9494247436523438\n",
      "epoch: 24 step: 47 loss: 0.16913171 acc: 0.952117919921875\n",
      "epoch: 24 step: 48 loss: 0.165923 acc: 0.9502983093261719\n",
      "epoch: 24 step: 49 loss: 0.19518232 acc: 0.9350700378417969\n",
      "epoch: 24 step: 50 loss: 0.14404693 acc: 0.9461708068847656\n",
      "epoch: 24 step: 51 loss: 0.1741574 acc: 0.9440994262695312\n",
      "epoch: 24 step: 52 loss: 0.17100357 acc: 0.9401473999023438\n",
      "epoch: 24 step: 53 loss: 0.19481729 acc: 0.9443397521972656\n",
      "epoch: 24 step: 54 loss: 0.14628336 acc: 0.9479560852050781\n",
      "epoch: 24 step: 55 loss: 0.16634002 acc: 0.9505882263183594\n",
      "epoch: 24 step: 56 loss: 0.14051124 acc: 0.9518089294433594\n",
      "epoch: 24 step: 57 loss: 0.17512214 acc: 0.9398574829101562\n",
      "epoch: 24 step: 58 loss: 0.16899678 acc: 0.944000244140625\n",
      "epoch: 24 step: 59 loss: 0.1268628 acc: 0.9566764831542969\n",
      "epoch: 24 step: 60 loss: 0.17000511 acc: 0.9492874145507812\n",
      "epoch: 24 step: 61 loss: 0.18405695 acc: 0.9504127502441406\n",
      "epoch: 24 step: 62 loss: 0.1395657 acc: 0.9527702331542969\n",
      "epoch: 24 step: 63 loss: 0.1325845 acc: 0.9494514465332031\n",
      "epoch: 24 step: 64 loss: 0.16213472 acc: 0.9446563720703125\n",
      "epoch: 24 step: 65 loss: 0.17792593 acc: 0.9359817504882812\n",
      "epoch: 24 step: 66 loss: 0.195777 acc: 0.9408226013183594\n",
      "epoch: 24 step: 67 loss: 0.19739269 acc: 0.9469413757324219\n",
      "epoch: 24 step: 68 loss: 0.17430319 acc: 0.9400405883789062\n",
      "epoch: 24 step: 69 loss: 0.15895928 acc: 0.9414100646972656\n",
      "epoch: 24 step: 70 loss: 0.1506046 acc: 0.9500503540039062\n",
      "epoch: 24 step: 71 loss: 0.1524572 acc: 0.9427261352539062\n",
      "epoch: 24 step: 72 loss: 0.18729652 acc: 0.9386482238769531\n",
      "epoch: 24 step: 73 loss: 0.18671404 acc: 0.93988037109375\n",
      "epoch: 24 step: 74 loss: 0.16662747 acc: 0.9508705139160156\n",
      "epoch: 24 step: 75 loss: 0.18612367 acc: 0.9327125549316406\n",
      "epoch: 24 step: 76 loss: 0.1869561 acc: 0.9461479187011719\n",
      "epoch: 24 step: 77 loss: 0.15599082 acc: 0.9437179565429688\n",
      "epoch: 24 step: 78 loss: 0.121410124 acc: 0.9599838256835938\n",
      "epoch: 24 step: 79 loss: 0.14271232 acc: 0.9450874328613281\n",
      "epoch: 24 step: 80 loss: 0.16878757 acc: 0.9432373046875\n",
      "epoch: 24 step: 81 loss: 0.1453348 acc: 0.9466514587402344\n",
      "epoch: 24 step: 82 loss: 0.14841424 acc: 0.9553070068359375\n",
      "epoch: 24 step: 83 loss: 0.17404573 acc: 0.9410667419433594\n",
      "epoch: 24 step: 84 loss: 0.17695358 acc: 0.94268798828125\n",
      "epoch: 24 step: 85 loss: 0.18837029 acc: 0.9516334533691406\n",
      "epoch: 24 step: 86 loss: 0.18077357 acc: 0.9493560791015625\n",
      "epoch: 24 step: 87 loss: 0.17084672 acc: 0.9534721374511719\n",
      "epoch: 24 step: 88 loss: 0.13702637 acc: 0.9580230712890625\n",
      "epoch: 24 step: 89 loss: 0.13069813 acc: 0.9594192504882812\n",
      "epoch: 24 step: 90 loss: 0.17825046 acc: 0.9519996643066406\n",
      "epoch: 24 step: 91 loss: 0.15944481 acc: 0.9447708129882812\n",
      "epoch: 24 step: 92 loss: 0.14079343 acc: 0.9453964233398438\n",
      "epoch: 24 step: 93 loss: 0.15405679 acc: 0.9473495483398438\n",
      "epoch: 24 step: 94 loss: 0.15113987 acc: 0.9460334777832031\n",
      "epoch: 24 step: 95 loss: 0.17615636 acc: 0.9428138732910156\n",
      "epoch: 24 step: 96 loss: 0.15862234 acc: 0.9413986206054688\n",
      "epoch: 24 step: 97 loss: 0.13922852 acc: 0.9477462768554688\n",
      "epoch: 24 step: 98 loss: 0.18482897 acc: 0.9369926452636719\n",
      "epoch: 24 step: 99 loss: 0.1965875 acc: 0.9461784362792969\n",
      "epoch: 24 step: 100 loss: 0.12628403 acc: 0.9565353393554688\n",
      "epoch: 24 step: 101 loss: 0.19042778 acc: 0.9472846984863281\n",
      "epoch: 24 step: 102 loss: 0.15826492 acc: 0.9483146667480469\n",
      "epoch: 24 step: 103 loss: 0.1558276 acc: 0.94561767578125\n",
      "epoch: 24 step: 104 loss: 0.18052997 acc: 0.9491500854492188\n",
      "epoch: 24 step: 105 loss: 0.12857202 acc: 0.9582023620605469\n",
      "epoch: 24 step: 106 loss: 0.134336 acc: 0.9491691589355469\n",
      "epoch: 24 step: 107 loss: 0.16210932 acc: 0.9458732604980469\n",
      "epoch: 24 step: 108 loss: 0.14283629 acc: 0.94927978515625\n",
      "epoch: 24 step: 109 loss: 0.14148496 acc: 0.9516983032226562\n",
      "epoch: 24 step: 110 loss: 0.14410007 acc: 0.9533271789550781\n",
      "epoch: 24 step: 111 loss: 0.14477712 acc: 0.9528312683105469\n",
      "epoch: 24 step: 112 loss: 0.14452653 acc: 0.95526123046875\n",
      "epoch: 24 step: 113 loss: 0.12947126 acc: 0.95281982421875\n",
      "epoch: 24 step: 114 loss: 0.17994629 acc: 0.9455757141113281\n",
      "epoch: 24 step: 115 loss: 0.14905769 acc: 0.9475326538085938\n",
      "epoch: 24 step: 116 loss: 0.14219153 acc: 0.9543876647949219\n",
      "epoch: 24 step: 117 loss: 0.16756986 acc: 0.940216064453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 step: 118 loss: 0.13828515 acc: 0.9535369873046875\n",
      "epoch: 24 step: 119 loss: 0.14899857 acc: 0.9441184997558594\n",
      "epoch: 24 step: 120 loss: 0.1546884 acc: 0.9556770324707031\n",
      "epoch: 24 step: 121 loss: 0.15782274 acc: 0.9530715942382812\n",
      "epoch: 24 step: 122 loss: 0.14569767 acc: 0.9567031860351562\n",
      "epoch: 24 step: 123 loss: 0.16933832 acc: 0.9432220458984375\n",
      "epoch: 24 step: 124 loss: 0.12272507 acc: 0.9536045619419643\n",
      "epoch: 24 validation_loss: 0.153 validation_dice: 0.8054641565999802\n",
      "epoch: 24 test_dataset dice: 0.6866787488571501\n",
      "time cost 0.5369567433993022 min\n",
      "dice_best: 0.8054641565999802\n",
      "******************************** epoch  24  is finished. *********************************\n",
      "epoch: 25 step: 1 loss: 0.15617374 acc: 0.9545783996582031\n",
      "epoch: 25 step: 2 loss: 0.18986858 acc: 0.9426345825195312\n",
      "epoch: 25 step: 3 loss: 0.17104465 acc: 0.9553337097167969\n",
      "epoch: 25 step: 4 loss: 0.14565085 acc: 0.9518470764160156\n",
      "epoch: 25 step: 5 loss: 0.17917313 acc: 0.9553413391113281\n",
      "epoch: 25 step: 6 loss: 0.15867415 acc: 0.955108642578125\n",
      "epoch: 25 step: 7 loss: 0.14149334 acc: 0.9488945007324219\n",
      "epoch: 25 step: 8 loss: 0.13248833 acc: 0.9507408142089844\n",
      "epoch: 25 step: 9 loss: 0.121563114 acc: 0.9568290710449219\n",
      "epoch: 25 step: 10 loss: 0.15548651 acc: 0.9514122009277344\n",
      "epoch: 25 step: 11 loss: 0.15420179 acc: 0.9491767883300781\n",
      "epoch: 25 step: 12 loss: 0.1433649 acc: 0.9549064636230469\n",
      "epoch: 25 step: 13 loss: 0.14926372 acc: 0.9421501159667969\n",
      "epoch: 25 step: 14 loss: 0.15348391 acc: 0.941131591796875\n",
      "epoch: 25 step: 15 loss: 0.16818337 acc: 0.9459457397460938\n",
      "epoch: 25 step: 16 loss: 0.12388975 acc: 0.9476203918457031\n",
      "epoch: 25 step: 17 loss: 0.13722955 acc: 0.9492301940917969\n",
      "epoch: 25 step: 18 loss: 0.12365678 acc: 0.9474716186523438\n",
      "epoch: 25 step: 19 loss: 0.17674536 acc: 0.9438133239746094\n",
      "epoch: 25 step: 20 loss: 0.15685064 acc: 0.9541397094726562\n",
      "epoch: 25 step: 21 loss: 0.12628108 acc: 0.9570274353027344\n",
      "epoch: 25 step: 22 loss: 0.11554065 acc: 0.9607963562011719\n",
      "epoch: 25 step: 23 loss: 0.14445126 acc: 0.9517822265625\n",
      "epoch: 25 step: 24 loss: 0.16596352 acc: 0.9505462646484375\n",
      "epoch: 25 step: 25 loss: 0.13116829 acc: 0.9571304321289062\n",
      "epoch: 25 step: 26 loss: 0.14315665 acc: 0.9586906433105469\n",
      "epoch: 25 step: 27 loss: 0.1290953 acc: 0.9601821899414062\n",
      "epoch: 25 step: 28 loss: 0.16015945 acc: 0.9469528198242188\n",
      "epoch: 25 step: 29 loss: 0.116760984 acc: 0.9629974365234375\n",
      "epoch: 25 step: 30 loss: 0.14781632 acc: 0.9582405090332031\n",
      "epoch: 25 step: 31 loss: 0.15641542 acc: 0.9433631896972656\n",
      "epoch: 25 step: 32 loss: 0.13846853 acc: 0.9588813781738281\n",
      "epoch: 25 step: 33 loss: 0.12501334 acc: 0.9576873779296875\n",
      "epoch: 25 step: 34 loss: 0.1455526 acc: 0.9507980346679688\n",
      "epoch: 25 step: 35 loss: 0.1249204 acc: 0.9612350463867188\n",
      "epoch: 25 step: 36 loss: 0.11077017 acc: 0.9605751037597656\n",
      "epoch: 25 step: 37 loss: 0.1582308 acc: 0.9428520202636719\n",
      "epoch: 25 step: 38 loss: 0.12508565 acc: 0.9544944763183594\n",
      "epoch: 25 step: 39 loss: 0.13264073 acc: 0.9558181762695312\n",
      "epoch: 25 step: 40 loss: 0.18219337 acc: 0.9437599182128906\n",
      "epoch: 25 step: 41 loss: 0.14098412 acc: 0.9504547119140625\n",
      "epoch: 25 step: 42 loss: 0.13910122 acc: 0.9567832946777344\n",
      "epoch: 25 step: 43 loss: 0.13436432 acc: 0.9519271850585938\n",
      "epoch: 25 step: 44 loss: 0.1458423 acc: 0.9446678161621094\n",
      "epoch: 25 step: 45 loss: 0.13414226 acc: 0.9566421508789062\n",
      "epoch: 25 step: 46 loss: 0.15063025 acc: 0.9556121826171875\n",
      "epoch: 25 step: 47 loss: 0.1404476 acc: 0.9575462341308594\n",
      "epoch: 25 step: 48 loss: 0.17887394 acc: 0.9533348083496094\n",
      "epoch: 25 step: 49 loss: 0.14626639 acc: 0.9457511901855469\n",
      "epoch: 25 step: 50 loss: 0.1445457 acc: 0.9505157470703125\n",
      "epoch: 25 step: 51 loss: 0.13236931 acc: 0.9548797607421875\n",
      "epoch: 25 step: 52 loss: 0.14388628 acc: 0.95263671875\n",
      "epoch: 25 step: 53 loss: 0.14400622 acc: 0.9528579711914062\n",
      "epoch: 25 step: 54 loss: 0.19001509 acc: 0.9455413818359375\n",
      "epoch: 25 step: 55 loss: 0.14564219 acc: 0.9458770751953125\n",
      "epoch: 25 step: 56 loss: 0.12817737 acc: 0.9553298950195312\n",
      "epoch: 25 step: 57 loss: 0.14597292 acc: 0.9608879089355469\n",
      "epoch: 25 step: 58 loss: 0.19410923 acc: 0.9447174072265625\n",
      "epoch: 25 step: 59 loss: 0.12772554 acc: 0.9525184631347656\n",
      "epoch: 25 step: 60 loss: 0.14562479 acc: 0.9566078186035156\n",
      "epoch: 25 step: 61 loss: 0.14052348 acc: 0.9547615051269531\n",
      "epoch: 25 step: 62 loss: 0.20573512 acc: 0.9495964050292969\n",
      "epoch: 25 step: 63 loss: 0.1271086 acc: 0.9613800048828125\n",
      "epoch: 25 step: 64 loss: 0.13680176 acc: 0.9558143615722656\n",
      "epoch: 25 step: 65 loss: 0.120974995 acc: 0.9535408020019531\n",
      "epoch: 25 step: 66 loss: 0.17929013 acc: 0.9493598937988281\n",
      "epoch: 25 step: 67 loss: 0.15832931 acc: 0.9520988464355469\n",
      "epoch: 25 step: 68 loss: 0.16998056 acc: 0.9513893127441406\n",
      "epoch: 25 step: 69 loss: 0.1451386 acc: 0.952850341796875\n",
      "epoch: 25 step: 70 loss: 0.18278605 acc: 0.9396324157714844\n",
      "epoch: 25 step: 71 loss: 0.16689447 acc: 0.9355812072753906\n",
      "epoch: 25 step: 72 loss: 0.21113065 acc: 0.94287109375\n",
      "epoch: 25 step: 73 loss: 0.15411554 acc: 0.9471397399902344\n",
      "epoch: 25 step: 74 loss: 0.17039353 acc: 0.9477348327636719\n",
      "epoch: 25 step: 75 loss: 0.17174067 acc: 0.954254150390625\n",
      "epoch: 25 step: 76 loss: 0.1502167 acc: 0.9484367370605469\n",
      "epoch: 25 step: 77 loss: 0.21831837 acc: 0.9439735412597656\n",
      "epoch: 25 step: 78 loss: 0.148401 acc: 0.9542312622070312\n",
      "epoch: 25 step: 79 loss: 0.13797924 acc: 0.9502792358398438\n",
      "epoch: 25 step: 80 loss: 0.17356493 acc: 0.94970703125\n",
      "epoch: 25 step: 81 loss: 0.13140398 acc: 0.95703125\n",
      "epoch: 25 step: 82 loss: 0.21763511 acc: 0.9435920715332031\n",
      "epoch: 25 step: 83 loss: 0.22233279 acc: 0.9434127807617188\n",
      "epoch: 25 step: 84 loss: 0.18164814 acc: 0.9464836120605469\n",
      "epoch: 25 step: 85 loss: 0.16013803 acc: 0.9380226135253906\n",
      "epoch: 25 step: 86 loss: 0.20975596 acc: 0.9453544616699219\n",
      "epoch: 25 step: 87 loss: 0.26362425 acc: 0.9385757446289062\n",
      "epoch: 25 step: 88 loss: 0.24328203 acc: 0.9401702880859375\n",
      "epoch: 25 step: 89 loss: 0.17557517 acc: 0.9465980529785156\n",
      "epoch: 25 step: 90 loss: 0.17214213 acc: 0.9633827209472656\n",
      "epoch: 25 step: 91 loss: 0.16873814 acc: 0.9553337097167969\n",
      "epoch: 25 step: 92 loss: 0.16160256 acc: 0.9496994018554688\n",
      "epoch: 25 step: 93 loss: 0.22356388 acc: 0.9365882873535156\n",
      "epoch: 25 step: 94 loss: 0.21266045 acc: 0.9387359619140625\n",
      "epoch: 25 step: 95 loss: 0.19386017 acc: 0.934844970703125\n",
      "epoch: 25 step: 96 loss: 0.2312056 acc: 0.9267120361328125\n",
      "epoch: 25 step: 97 loss: 0.1794446 acc: 0.9409332275390625\n",
      "epoch: 25 step: 98 loss: 0.1870682 acc: 0.9386482238769531\n",
      "epoch: 25 step: 99 loss: 0.16744609 acc: 0.947265625\n",
      "epoch: 25 step: 100 loss: 0.17118269 acc: 0.9441795349121094\n",
      "epoch: 25 step: 101 loss: 0.17981058 acc: 0.9434814453125\n",
      "epoch: 25 step: 102 loss: 0.14142017 acc: 0.9568443298339844\n",
      "epoch: 25 step: 103 loss: 0.19420259 acc: 0.9504318237304688\n",
      "epoch: 25 step: 104 loss: 0.18034591 acc: 0.9514732360839844\n",
      "epoch: 25 step: 105 loss: 0.16275609 acc: 0.95458984375\n",
      "epoch: 25 step: 106 loss: 0.14624314 acc: 0.9516067504882812\n",
      "epoch: 25 step: 107 loss: 0.17584464 acc: 0.95025634765625\n",
      "epoch: 25 step: 108 loss: 0.16730033 acc: 0.9496536254882812\n",
      "epoch: 25 step: 109 loss: 0.161197 acc: 0.9498138427734375\n",
      "epoch: 25 step: 110 loss: 0.1323874 acc: 0.9546470642089844\n",
      "epoch: 25 step: 111 loss: 0.14981665 acc: 0.9513435363769531\n",
      "epoch: 25 step: 112 loss: 0.13410427 acc: 0.9529991149902344\n",
      "epoch: 25 step: 113 loss: 0.12348225 acc: 0.9536781311035156\n",
      "epoch: 25 step: 114 loss: 0.14533922 acc: 0.95556640625\n",
      "epoch: 25 step: 115 loss: 0.15695544 acc: 0.9513359069824219\n",
      "epoch: 25 step: 116 loss: 0.1550187 acc: 0.9516944885253906\n",
      "epoch: 25 step: 117 loss: 0.19222869 acc: 0.9496917724609375\n",
      "epoch: 25 step: 118 loss: 0.14630741 acc: 0.9430389404296875\n",
      "epoch: 25 step: 119 loss: 0.15538673 acc: 0.9419174194335938\n",
      "epoch: 25 step: 120 loss: 0.16100945 acc: 0.9519309997558594\n",
      "epoch: 25 step: 121 loss: 0.13796522 acc: 0.9531478881835938\n",
      "epoch: 25 step: 122 loss: 0.13875596 acc: 0.9519309997558594\n",
      "epoch: 25 step: 123 loss: 0.14564405 acc: 0.9518852233886719\n",
      "epoch: 25 step: 124 loss: 0.19761793 acc: 0.9398629324776786\n",
      "epoch: 25 validation_loss: 0.163 validation_dice: 0.8017082193688512\n",
      "epoch: 25 test_dataset dice: 0.7108543381289253\n",
      "time cost 0.538831353187561 min\n",
      "dice_best: 0.8054641565999802\n",
      "******************************** epoch  25  is finished. *********************************\n",
      "epoch: 26 step: 1 loss: 0.1470099 acc: 0.9529762268066406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 step: 2 loss: 0.13135272 acc: 0.9616050720214844\n",
      "epoch: 26 step: 3 loss: 0.16288318 acc: 0.9471664428710938\n",
      "epoch: 26 step: 4 loss: 0.19844042 acc: 0.9415168762207031\n",
      "epoch: 26 step: 5 loss: 0.14709842 acc: 0.9641456604003906\n",
      "epoch: 26 step: 6 loss: 0.14684437 acc: 0.9525604248046875\n",
      "epoch: 26 step: 7 loss: 0.14106415 acc: 0.9530601501464844\n",
      "epoch: 26 step: 8 loss: 0.17394204 acc: 0.947784423828125\n",
      "epoch: 26 step: 9 loss: 0.14926742 acc: 0.9512710571289062\n",
      "epoch: 26 step: 10 loss: 0.1337466 acc: 0.9492454528808594\n",
      "epoch: 26 step: 11 loss: 0.15429994 acc: 0.9434661865234375\n",
      "epoch: 26 step: 12 loss: 0.17181389 acc: 0.9466209411621094\n",
      "epoch: 26 step: 13 loss: 0.13473102 acc: 0.9538230895996094\n",
      "epoch: 26 step: 14 loss: 0.15311179 acc: 0.9561538696289062\n",
      "epoch: 26 step: 15 loss: 0.13708454 acc: 0.9472770690917969\n",
      "epoch: 26 step: 16 loss: 0.16529463 acc: 0.9383888244628906\n",
      "epoch: 26 step: 17 loss: 0.11419766 acc: 0.9589729309082031\n",
      "epoch: 26 step: 18 loss: 0.14280002 acc: 0.9612274169921875\n",
      "epoch: 26 step: 19 loss: 0.17222242 acc: 0.9524383544921875\n",
      "epoch: 26 step: 20 loss: 0.14834468 acc: 0.9551429748535156\n",
      "epoch: 26 step: 21 loss: 0.1562644 acc: 0.946197509765625\n",
      "epoch: 26 step: 22 loss: 0.1434442 acc: 0.94354248046875\n",
      "epoch: 26 step: 23 loss: 0.18436266 acc: 0.9511489868164062\n",
      "epoch: 26 step: 24 loss: 0.12631667 acc: 0.9478721618652344\n",
      "epoch: 26 step: 25 loss: 0.1384721 acc: 0.9433021545410156\n",
      "epoch: 26 step: 26 loss: 0.18468313 acc: 0.9303932189941406\n",
      "epoch: 26 step: 27 loss: 0.12823813 acc: 0.9484291076660156\n",
      "epoch: 26 step: 28 loss: 0.12046462 acc: 0.9548873901367188\n",
      "epoch: 26 step: 29 loss: 0.12292944 acc: 0.9499931335449219\n",
      "epoch: 26 step: 30 loss: 0.11264435 acc: 0.9578666687011719\n",
      "epoch: 26 step: 31 loss: 0.13592464 acc: 0.9504013061523438\n",
      "epoch: 26 step: 32 loss: 0.13729346 acc: 0.9475860595703125\n",
      "epoch: 26 step: 33 loss: 0.14603333 acc: 0.9461326599121094\n",
      "epoch: 26 step: 34 loss: 0.10784463 acc: 0.9671669006347656\n",
      "epoch: 26 step: 35 loss: 0.15427445 acc: 0.9584083557128906\n",
      "epoch: 26 step: 36 loss: 0.13803282 acc: 0.9560089111328125\n",
      "epoch: 26 step: 37 loss: 0.16113816 acc: 0.9487876892089844\n",
      "epoch: 26 step: 38 loss: 0.14219648 acc: 0.9503631591796875\n",
      "epoch: 26 step: 39 loss: 0.18178502 acc: 0.9509925842285156\n",
      "epoch: 26 step: 40 loss: 0.16191037 acc: 0.9556465148925781\n",
      "epoch: 26 step: 41 loss: 0.10951968 acc: 0.9562530517578125\n",
      "epoch: 26 step: 42 loss: 0.12103361 acc: 0.9566230773925781\n",
      "epoch: 26 step: 43 loss: 0.14957991 acc: 0.94586181640625\n",
      "epoch: 26 step: 44 loss: 0.17754456 acc: 0.9406394958496094\n",
      "epoch: 26 step: 45 loss: 0.15378045 acc: 0.9532852172851562\n",
      "epoch: 26 step: 46 loss: 0.15012676 acc: 0.9483718872070312\n",
      "epoch: 26 step: 47 loss: 0.14260663 acc: 0.9556655883789062\n",
      "epoch: 26 step: 48 loss: 0.15613364 acc: 0.9446907043457031\n",
      "epoch: 26 step: 49 loss: 0.14096266 acc: 0.9537277221679688\n",
      "epoch: 26 step: 50 loss: 0.12402272 acc: 0.9529457092285156\n",
      "epoch: 26 step: 51 loss: 0.14214687 acc: 0.956207275390625\n",
      "epoch: 26 step: 52 loss: 0.14187525 acc: 0.9547996520996094\n",
      "epoch: 26 step: 53 loss: 0.15070033 acc: 0.9508819580078125\n",
      "epoch: 26 step: 54 loss: 0.1818365 acc: 0.9452629089355469\n",
      "epoch: 26 step: 55 loss: 0.14627746 acc: 0.9490432739257812\n",
      "epoch: 26 step: 56 loss: 0.15069634 acc: 0.95361328125\n",
      "epoch: 26 step: 57 loss: 0.15959878 acc: 0.9467697143554688\n",
      "epoch: 26 step: 58 loss: 0.18933673 acc: 0.9391670227050781\n",
      "epoch: 26 step: 59 loss: 0.13825728 acc: 0.9580497741699219\n",
      "epoch: 26 step: 60 loss: 0.16544564 acc: 0.955810546875\n",
      "epoch: 26 step: 61 loss: 0.13977896 acc: 0.9559593200683594\n",
      "epoch: 26 step: 62 loss: 0.16888638 acc: 0.9438209533691406\n",
      "epoch: 26 step: 63 loss: 0.14578962 acc: 0.9502410888671875\n",
      "epoch: 26 step: 64 loss: 0.12930869 acc: 0.9504241943359375\n",
      "epoch: 26 step: 65 loss: 0.19453582 acc: 0.9439201354980469\n",
      "epoch: 26 step: 66 loss: 0.14617236 acc: 0.9512252807617188\n",
      "epoch: 26 step: 67 loss: 0.14679594 acc: 0.9569091796875\n",
      "epoch: 26 step: 68 loss: 0.13536766 acc: 0.9604415893554688\n",
      "epoch: 26 step: 69 loss: 0.14031006 acc: 0.958648681640625\n",
      "epoch: 26 step: 70 loss: 0.2728611 acc: 0.9388389587402344\n",
      "epoch: 26 step: 71 loss: 0.10148197 acc: 0.9620590209960938\n",
      "epoch: 26 step: 72 loss: 0.12312002 acc: 0.9668693542480469\n",
      "epoch: 26 step: 73 loss: 0.16543114 acc: 0.9630165100097656\n",
      "epoch: 26 step: 74 loss: 0.20426376 acc: 0.9395828247070312\n",
      "epoch: 26 step: 75 loss: 0.18418421 acc: 0.9432601928710938\n",
      "epoch: 26 step: 76 loss: 0.14680725 acc: 0.9515724182128906\n",
      "epoch: 26 step: 77 loss: 0.14833017 acc: 0.9440803527832031\n",
      "epoch: 26 step: 78 loss: 0.15572098 acc: 0.9478263854980469\n",
      "epoch: 26 step: 79 loss: 0.21222298 acc: 0.9428215026855469\n",
      "epoch: 26 step: 80 loss: 0.18004753 acc: 0.9465522766113281\n",
      "epoch: 26 step: 81 loss: 0.17449644 acc: 0.9525375366210938\n",
      "epoch: 26 step: 82 loss: 0.13194896 acc: 0.9562873840332031\n",
      "epoch: 26 step: 83 loss: 0.1971715 acc: 0.9416961669921875\n",
      "epoch: 26 step: 84 loss: 0.12848437 acc: 0.9611930847167969\n",
      "epoch: 26 step: 85 loss: 0.17773433 acc: 0.9475822448730469\n",
      "epoch: 26 step: 86 loss: 0.16998442 acc: 0.9478836059570312\n",
      "epoch: 26 step: 87 loss: 0.16343881 acc: 0.94659423828125\n",
      "epoch: 26 step: 88 loss: 0.14029807 acc: 0.9486083984375\n",
      "epoch: 26 step: 89 loss: 0.13901016 acc: 0.9520416259765625\n",
      "epoch: 26 step: 90 loss: 0.1505528 acc: 0.947906494140625\n",
      "epoch: 26 step: 91 loss: 0.16551928 acc: 0.9504432678222656\n",
      "epoch: 26 step: 92 loss: 0.15321037 acc: 0.94464111328125\n",
      "epoch: 26 step: 93 loss: 0.17810962 acc: 0.93682861328125\n",
      "epoch: 26 step: 94 loss: 0.16829354 acc: 0.9426231384277344\n",
      "epoch: 26 step: 95 loss: 0.1550673 acc: 0.9450416564941406\n",
      "epoch: 26 step: 96 loss: 0.13779034 acc: 0.9561920166015625\n",
      "epoch: 26 step: 97 loss: 0.1802526 acc: 0.942169189453125\n",
      "epoch: 26 step: 98 loss: 0.17853115 acc: 0.9441795349121094\n",
      "epoch: 26 step: 99 loss: 0.15910137 acc: 0.9533195495605469\n",
      "epoch: 26 step: 100 loss: 0.17674991 acc: 0.9486045837402344\n",
      "epoch: 26 step: 101 loss: 0.15576392 acc: 0.9524993896484375\n",
      "epoch: 26 step: 102 loss: 0.16746072 acc: 0.9463119506835938\n",
      "epoch: 26 step: 103 loss: 0.119831264 acc: 0.9544334411621094\n",
      "epoch: 26 step: 104 loss: 0.15060002 acc: 0.9512519836425781\n",
      "epoch: 26 step: 105 loss: 0.13151224 acc: 0.9550361633300781\n",
      "epoch: 26 step: 106 loss: 0.20094395 acc: 0.9401016235351562\n",
      "epoch: 26 step: 107 loss: 0.12842052 acc: 0.9508094787597656\n",
      "epoch: 26 step: 108 loss: 0.13206512 acc: 0.9499168395996094\n",
      "epoch: 26 step: 109 loss: 0.14590298 acc: 0.9549713134765625\n",
      "epoch: 26 step: 110 loss: 0.16253728 acc: 0.9529876708984375\n",
      "epoch: 26 step: 111 loss: 0.13494207 acc: 0.9572639465332031\n",
      "epoch: 26 step: 112 loss: 0.16977376 acc: 0.9494094848632812\n",
      "epoch: 26 step: 113 loss: 0.12579553 acc: 0.9551620483398438\n",
      "epoch: 26 step: 114 loss: 0.1576912 acc: 0.9483108520507812\n",
      "epoch: 26 step: 115 loss: 0.17982484 acc: 0.9457740783691406\n",
      "epoch: 26 step: 116 loss: 0.13271283 acc: 0.9447860717773438\n",
      "epoch: 26 step: 117 loss: 0.12651438 acc: 0.9561538696289062\n",
      "epoch: 26 step: 118 loss: 0.15108442 acc: 0.9307136535644531\n",
      "epoch: 26 step: 119 loss: 0.11520957 acc: 0.9501266479492188\n",
      "epoch: 26 step: 120 loss: 0.13624631 acc: 0.9439125061035156\n",
      "epoch: 26 step: 121 loss: 0.13824117 acc: 0.9504318237304688\n",
      "epoch: 26 step: 122 loss: 0.1279987 acc: 0.9520072937011719\n",
      "epoch: 26 step: 123 loss: 0.14165056 acc: 0.9377670288085938\n",
      "epoch: 26 step: 124 loss: 0.13657002 acc: 0.9540318080357143\n",
      "epoch: 26 validation_loss: 0.148 validation_dice: 0.8105984095001857\n",
      "epoch: 26 test_dataset dice: 0.6913943869998406\n",
      "time cost 0.5371244351069132 min\n",
      "dice_best: 0.8105984095001857\n",
      "******************************** epoch  26  is finished. *********************************\n",
      "epoch: 27 step: 1 loss: 0.17586838 acc: 0.9539527893066406\n",
      "epoch: 27 step: 2 loss: 0.11692273 acc: 0.9520149230957031\n",
      "epoch: 27 step: 3 loss: 0.14358674 acc: 0.9519805908203125\n",
      "epoch: 27 step: 4 loss: 0.16549094 acc: 0.949554443359375\n",
      "epoch: 27 step: 5 loss: 0.13785717 acc: 0.957000732421875\n",
      "epoch: 27 step: 6 loss: 0.11573754 acc: 0.9567718505859375\n",
      "epoch: 27 step: 7 loss: 0.132884 acc: 0.95391845703125\n",
      "epoch: 27 step: 8 loss: 0.12668175 acc: 0.9474296569824219\n",
      "epoch: 27 step: 9 loss: 0.1429036 acc: 0.9462852478027344\n",
      "epoch: 27 step: 10 loss: 0.16498633 acc: 0.945831298828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 step: 11 loss: 0.115046516 acc: 0.9552078247070312\n",
      "epoch: 27 step: 12 loss: 0.14189343 acc: 0.9538345336914062\n",
      "epoch: 27 step: 13 loss: 0.13332263 acc: 0.9580459594726562\n",
      "epoch: 27 step: 14 loss: 0.11789474 acc: 0.9590721130371094\n",
      "epoch: 27 step: 15 loss: 0.117006816 acc: 0.9551200866699219\n",
      "epoch: 27 step: 16 loss: 0.13197783 acc: 0.9503021240234375\n",
      "epoch: 27 step: 17 loss: 0.14625898 acc: 0.9527130126953125\n",
      "epoch: 27 step: 18 loss: 0.107919924 acc: 0.9647636413574219\n",
      "epoch: 27 step: 19 loss: 0.17710532 acc: 0.9507598876953125\n",
      "epoch: 27 step: 20 loss: 0.14480035 acc: 0.9593009948730469\n",
      "epoch: 27 step: 21 loss: 0.11958042 acc: 0.9568557739257812\n",
      "epoch: 27 step: 22 loss: 0.16348574 acc: 0.9428138732910156\n",
      "epoch: 27 step: 23 loss: 0.16959117 acc: 0.9416542053222656\n",
      "epoch: 27 step: 24 loss: 0.1421591 acc: 0.9463043212890625\n",
      "epoch: 27 step: 25 loss: 0.11132966 acc: 0.9572486877441406\n",
      "epoch: 27 step: 26 loss: 0.14123279 acc: 0.9486885070800781\n",
      "epoch: 27 step: 27 loss: 0.1261581 acc: 0.9515304565429688\n",
      "epoch: 27 step: 28 loss: 0.13825108 acc: 0.9522247314453125\n",
      "epoch: 27 step: 29 loss: 0.12922348 acc: 0.95635986328125\n",
      "epoch: 27 step: 30 loss: 0.16525748 acc: 0.9504318237304688\n",
      "epoch: 27 step: 31 loss: 0.18177271 acc: 0.9483299255371094\n",
      "epoch: 27 step: 32 loss: 0.1512456 acc: 0.9562568664550781\n",
      "epoch: 27 step: 33 loss: 0.11600395 acc: 0.9591789245605469\n",
      "epoch: 27 step: 34 loss: 0.12511972 acc: 0.9573326110839844\n",
      "epoch: 27 step: 35 loss: 0.15043108 acc: 0.9447288513183594\n",
      "epoch: 27 step: 36 loss: 0.12168814 acc: 0.9589614868164062\n",
      "epoch: 27 step: 37 loss: 0.14341222 acc: 0.9561233520507812\n",
      "epoch: 27 step: 38 loss: 0.14475438 acc: 0.9470634460449219\n",
      "epoch: 27 step: 39 loss: 0.1823469 acc: 0.9467964172363281\n",
      "epoch: 27 step: 40 loss: 0.17642933 acc: 0.9472846984863281\n",
      "epoch: 27 step: 41 loss: 0.1583846 acc: 0.9565238952636719\n",
      "epoch: 27 step: 42 loss: 0.10812561 acc: 0.9612884521484375\n",
      "epoch: 27 step: 43 loss: 0.14222273 acc: 0.9538955688476562\n",
      "epoch: 27 step: 44 loss: 0.13202658 acc: 0.9487648010253906\n",
      "epoch: 27 step: 45 loss: 0.15527122 acc: 0.9408607482910156\n",
      "epoch: 27 step: 46 loss: 0.15080169 acc: 0.9483871459960938\n",
      "epoch: 27 step: 47 loss: 0.14748915 acc: 0.9580459594726562\n",
      "epoch: 27 step: 48 loss: 0.13343908 acc: 0.9645843505859375\n",
      "epoch: 27 step: 49 loss: 0.1787943 acc: 0.9527931213378906\n",
      "epoch: 27 step: 50 loss: 0.14669523 acc: 0.9493598937988281\n",
      "epoch: 27 step: 51 loss: 0.18963446 acc: 0.94940185546875\n",
      "epoch: 27 step: 52 loss: 0.18338709 acc: 0.945068359375\n",
      "epoch: 27 step: 53 loss: 0.15479967 acc: 0.9494552612304688\n",
      "epoch: 27 step: 54 loss: 0.14378464 acc: 0.9575042724609375\n",
      "epoch: 27 step: 55 loss: 0.14183432 acc: 0.9519004821777344\n",
      "epoch: 27 step: 56 loss: 0.1545125 acc: 0.9486274719238281\n",
      "epoch: 27 step: 57 loss: 0.13848563 acc: 0.9534530639648438\n",
      "epoch: 27 step: 58 loss: 0.11222887 acc: 0.9579277038574219\n",
      "epoch: 27 step: 59 loss: 0.15512027 acc: 0.945709228515625\n",
      "epoch: 27 step: 60 loss: 0.15165405 acc: 0.9525260925292969\n",
      "epoch: 27 step: 61 loss: 0.17293417 acc: 0.9553031921386719\n",
      "epoch: 27 step: 62 loss: 0.1415125 acc: 0.9516716003417969\n",
      "epoch: 27 step: 63 loss: 0.11150564 acc: 0.9609947204589844\n",
      "epoch: 27 step: 64 loss: 0.15685084 acc: 0.9440994262695312\n",
      "epoch: 27 step: 65 loss: 0.11510184 acc: 0.9547233581542969\n",
      "epoch: 27 step: 66 loss: 0.14205508 acc: 0.94512939453125\n",
      "epoch: 27 step: 67 loss: 0.13326737 acc: 0.9456214904785156\n",
      "epoch: 27 step: 68 loss: 0.12628292 acc: 0.9512901306152344\n",
      "epoch: 27 step: 69 loss: 0.14377813 acc: 0.9508285522460938\n",
      "epoch: 27 step: 70 loss: 0.15762839 acc: 0.9537162780761719\n",
      "epoch: 27 step: 71 loss: 0.15820685 acc: 0.9531936645507812\n",
      "epoch: 27 step: 72 loss: 0.11813832 acc: 0.9522895812988281\n",
      "epoch: 27 step: 73 loss: 0.11452364 acc: 0.9631118774414062\n",
      "epoch: 27 step: 74 loss: 0.12356531 acc: 0.9647674560546875\n",
      "epoch: 27 step: 75 loss: 0.16090582 acc: 0.9442367553710938\n",
      "epoch: 27 step: 76 loss: 0.11788986 acc: 0.960235595703125\n",
      "epoch: 27 step: 77 loss: 0.10701135 acc: 0.9538688659667969\n",
      "epoch: 27 step: 78 loss: 0.12023422 acc: 0.9616661071777344\n",
      "epoch: 27 step: 79 loss: 0.15579283 acc: 0.9592514038085938\n",
      "epoch: 27 step: 80 loss: 0.1167493 acc: 0.9523353576660156\n",
      "epoch: 27 step: 81 loss: 0.1365437 acc: 0.9495697021484375\n",
      "epoch: 27 step: 82 loss: 0.18180852 acc: 0.9272842407226562\n",
      "epoch: 27 step: 83 loss: 0.12395267 acc: 0.9529190063476562\n",
      "epoch: 27 step: 84 loss: 0.14235224 acc: 0.9411201477050781\n",
      "epoch: 27 step: 85 loss: 0.16807175 acc: 0.9468307495117188\n",
      "epoch: 27 step: 86 loss: 0.15542693 acc: 0.950164794921875\n",
      "epoch: 27 step: 87 loss: 0.14323635 acc: 0.9473648071289062\n",
      "epoch: 27 step: 88 loss: 0.13191265 acc: 0.9486732482910156\n",
      "epoch: 27 step: 89 loss: 0.14458673 acc: 0.9517784118652344\n",
      "epoch: 27 step: 90 loss: 0.15885866 acc: 0.9506301879882812\n",
      "epoch: 27 step: 91 loss: 0.17353782 acc: 0.9510307312011719\n",
      "epoch: 27 step: 92 loss: 0.1512354 acc: 0.9505767822265625\n",
      "epoch: 27 step: 93 loss: 0.14795959 acc: 0.9517059326171875\n",
      "epoch: 27 step: 94 loss: 0.11124104 acc: 0.964508056640625\n",
      "epoch: 27 step: 95 loss: 0.15047807 acc: 0.9504547119140625\n",
      "epoch: 27 step: 96 loss: 0.11291555 acc: 0.9623947143554688\n",
      "epoch: 27 step: 97 loss: 0.14181298 acc: 0.9569015502929688\n",
      "epoch: 27 step: 98 loss: 0.13192713 acc: 0.9547080993652344\n",
      "epoch: 27 step: 99 loss: 0.11554482 acc: 0.9538307189941406\n",
      "epoch: 27 step: 100 loss: 0.17342678 acc: 0.9402847290039062\n",
      "epoch: 27 step: 101 loss: 0.14651483 acc: 0.9536895751953125\n",
      "epoch: 27 step: 102 loss: 0.12697874 acc: 0.9521827697753906\n",
      "epoch: 27 step: 103 loss: 0.15193862 acc: 0.9460105895996094\n",
      "epoch: 27 step: 104 loss: 0.15922222 acc: 0.9509925842285156\n",
      "epoch: 27 step: 105 loss: 0.1190971 acc: 0.9527473449707031\n",
      "epoch: 27 step: 106 loss: 0.10018692 acc: 0.9665069580078125\n",
      "epoch: 27 step: 107 loss: 0.13847868 acc: 0.9503097534179688\n",
      "epoch: 27 step: 108 loss: 0.13266383 acc: 0.9544219970703125\n",
      "epoch: 27 step: 109 loss: 0.1118298 acc: 0.9577865600585938\n",
      "epoch: 27 step: 110 loss: 0.13629906 acc: 0.960601806640625\n",
      "epoch: 27 step: 111 loss: 0.13352062 acc: 0.9525337219238281\n",
      "epoch: 27 step: 112 loss: 0.123983756 acc: 0.9578323364257812\n",
      "epoch: 27 step: 113 loss: 0.13156551 acc: 0.9510879516601562\n",
      "epoch: 27 step: 114 loss: 0.13862772 acc: 0.9528579711914062\n",
      "epoch: 27 step: 115 loss: 0.13313754 acc: 0.94549560546875\n",
      "epoch: 27 step: 116 loss: 0.1401873 acc: 0.9521865844726562\n",
      "epoch: 27 step: 117 loss: 0.113850065 acc: 0.9554977416992188\n",
      "epoch: 27 step: 118 loss: 0.16114461 acc: 0.9463462829589844\n",
      "epoch: 27 step: 119 loss: 0.14993839 acc: 0.9496078491210938\n",
      "epoch: 27 step: 120 loss: 0.13155001 acc: 0.9554901123046875\n",
      "epoch: 27 step: 121 loss: 0.11834056 acc: 0.9517478942871094\n",
      "epoch: 27 step: 122 loss: 0.14855331 acc: 0.9585380554199219\n",
      "epoch: 27 step: 123 loss: 0.11364302 acc: 0.954986572265625\n",
      "epoch: 27 step: 124 loss: 0.16480435 acc: 0.9496983119419643\n",
      "epoch: 27 validation_loss: 0.146 validation_dice: 0.8009451774594751\n",
      "epoch: 27 test_dataset dice: 0.6896946769281196\n",
      "time cost 0.5366619944572448 min\n",
      "dice_best: 0.8105984095001857\n",
      "******************************** epoch  27  is finished. *********************************\n",
      "epoch: 28 step: 1 loss: 0.12983596 acc: 0.9567718505859375\n",
      "epoch: 28 step: 2 loss: 0.11132654 acc: 0.9605636596679688\n",
      "epoch: 28 step: 3 loss: 0.13543133 acc: 0.9620819091796875\n",
      "epoch: 28 step: 4 loss: 0.12680073 acc: 0.9658317565917969\n",
      "epoch: 28 step: 5 loss: 0.1526376 acc: 0.9537086486816406\n",
      "epoch: 28 step: 6 loss: 0.13488694 acc: 0.9529876708984375\n",
      "epoch: 28 step: 7 loss: 0.11966834 acc: 0.9606094360351562\n",
      "epoch: 28 step: 8 loss: 0.10442905 acc: 0.9625587463378906\n",
      "epoch: 28 step: 9 loss: 0.14978911 acc: 0.9473800659179688\n",
      "epoch: 28 step: 10 loss: 0.13787657 acc: 0.944061279296875\n",
      "epoch: 28 step: 11 loss: 0.12710246 acc: 0.9564743041992188\n",
      "epoch: 28 step: 12 loss: 0.12321643 acc: 0.9501914978027344\n",
      "epoch: 28 step: 13 loss: 0.15063715 acc: 0.9487495422363281\n",
      "epoch: 28 step: 14 loss: 0.10426155 acc: 0.9550056457519531\n",
      "epoch: 28 step: 15 loss: 0.12887831 acc: 0.9560775756835938\n",
      "epoch: 28 step: 16 loss: 0.1349809 acc: 0.9580459594726562\n",
      "epoch: 28 step: 17 loss: 0.15605833 acc: 0.9603958129882812\n",
      "epoch: 28 step: 18 loss: 0.1420909 acc: 0.9488983154296875\n",
      "epoch: 28 step: 19 loss: 0.14416033 acc: 0.9456825256347656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 step: 20 loss: 0.14497916 acc: 0.9500312805175781\n",
      "epoch: 28 step: 21 loss: 0.16556825 acc: 0.9484596252441406\n",
      "epoch: 28 step: 22 loss: 0.15364777 acc: 0.9482460021972656\n",
      "epoch: 28 step: 23 loss: 0.1552394 acc: 0.9430580139160156\n",
      "epoch: 28 step: 24 loss: 0.14822318 acc: 0.9400787353515625\n",
      "epoch: 28 step: 25 loss: 0.16022392 acc: 0.9430732727050781\n",
      "epoch: 28 step: 26 loss: 0.20069528 acc: 0.9429168701171875\n",
      "epoch: 28 step: 27 loss: 0.1545317 acc: 0.9469146728515625\n",
      "epoch: 28 step: 28 loss: 0.19649814 acc: 0.9463081359863281\n",
      "epoch: 28 step: 29 loss: 0.1655678 acc: 0.9371414184570312\n",
      "epoch: 28 step: 30 loss: 0.13521367 acc: 0.9471054077148438\n",
      "epoch: 28 step: 31 loss: 0.16973725 acc: 0.93914794921875\n",
      "epoch: 28 step: 32 loss: 0.15392335 acc: 0.948150634765625\n",
      "epoch: 28 step: 33 loss: 0.23021087 acc: 0.949310302734375\n",
      "epoch: 28 step: 34 loss: 0.15143114 acc: 0.9515113830566406\n",
      "epoch: 28 step: 35 loss: 0.13896479 acc: 0.9485855102539062\n",
      "epoch: 28 step: 36 loss: 0.15550394 acc: 0.9354095458984375\n",
      "epoch: 28 step: 37 loss: 0.13807838 acc: 0.9506645202636719\n",
      "epoch: 28 step: 38 loss: 0.20159982 acc: 0.9551506042480469\n",
      "epoch: 28 step: 39 loss: 0.15384738 acc: 0.9441871643066406\n",
      "epoch: 28 step: 40 loss: 0.15320273 acc: 0.9507942199707031\n",
      "epoch: 28 step: 41 loss: 0.13303442 acc: 0.9554367065429688\n",
      "epoch: 28 step: 42 loss: 0.18556693 acc: 0.9444503784179688\n",
      "epoch: 28 step: 43 loss: 0.15752755 acc: 0.9566726684570312\n",
      "epoch: 28 step: 44 loss: 0.15381876 acc: 0.9556350708007812\n",
      "epoch: 28 step: 45 loss: 0.14476606 acc: 0.9553985595703125\n",
      "epoch: 28 step: 46 loss: 0.14563516 acc: 0.9495086669921875\n",
      "epoch: 28 step: 47 loss: 0.18241537 acc: 0.9538421630859375\n",
      "epoch: 28 step: 48 loss: 0.18224631 acc: 0.9502639770507812\n",
      "epoch: 28 step: 49 loss: 0.17286693 acc: 0.9513702392578125\n",
      "epoch: 28 step: 50 loss: 0.16725838 acc: 0.9533653259277344\n",
      "epoch: 28 step: 51 loss: 0.16114078 acc: 0.9395713806152344\n",
      "epoch: 28 step: 52 loss: 0.18711868 acc: 0.9425239562988281\n",
      "epoch: 28 step: 53 loss: 0.15473947 acc: 0.9517326354980469\n",
      "epoch: 28 step: 54 loss: 0.16311139 acc: 0.9355583190917969\n",
      "epoch: 28 step: 55 loss: 0.16339375 acc: 0.9464759826660156\n",
      "epoch: 28 step: 56 loss: 0.17556918 acc: 0.936767578125\n",
      "epoch: 28 step: 57 loss: 0.13320598 acc: 0.9511680603027344\n",
      "epoch: 28 step: 58 loss: 0.119853385 acc: 0.9537124633789062\n",
      "epoch: 28 step: 59 loss: 0.16826668 acc: 0.9481277465820312\n",
      "epoch: 28 step: 60 loss: 0.1893494 acc: 0.9453849792480469\n",
      "epoch: 28 step: 61 loss: 0.1432556 acc: 0.9489364624023438\n",
      "epoch: 28 step: 62 loss: 0.19050504 acc: 0.9558677673339844\n",
      "epoch: 28 step: 63 loss: 0.12764442 acc: 0.9502296447753906\n",
      "epoch: 28 step: 64 loss: 0.12967719 acc: 0.94525146484375\n",
      "epoch: 28 step: 65 loss: 0.1932013 acc: 0.9453468322753906\n",
      "epoch: 28 step: 66 loss: 0.13515598 acc: 0.9459724426269531\n",
      "epoch: 28 step: 67 loss: 0.13845158 acc: 0.9464073181152344\n",
      "epoch: 28 step: 68 loss: 0.14853466 acc: 0.9441909790039062\n",
      "epoch: 28 step: 69 loss: 0.1659299 acc: 0.951629638671875\n",
      "epoch: 28 step: 70 loss: 0.12675315 acc: 0.9595489501953125\n",
      "epoch: 28 step: 71 loss: 0.16383183 acc: 0.950225830078125\n",
      "epoch: 28 step: 72 loss: 0.18739781 acc: 0.9436988830566406\n",
      "epoch: 28 step: 73 loss: 0.15126573 acc: 0.9578514099121094\n",
      "epoch: 28 step: 74 loss: 0.13283485 acc: 0.9564323425292969\n",
      "epoch: 28 step: 75 loss: 0.13489985 acc: 0.9584465026855469\n",
      "epoch: 28 step: 76 loss: 0.15337202 acc: 0.9516105651855469\n",
      "epoch: 28 step: 77 loss: 0.15111727 acc: 0.947540283203125\n",
      "epoch: 28 step: 78 loss: 0.1490323 acc: 0.9538955688476562\n",
      "epoch: 28 step: 79 loss: 0.15511553 acc: 0.9528388977050781\n",
      "epoch: 28 step: 80 loss: 0.15945545 acc: 0.9531669616699219\n",
      "epoch: 28 step: 81 loss: 0.14823367 acc: 0.9514579772949219\n",
      "epoch: 28 step: 82 loss: 0.15053862 acc: 0.9521484375\n",
      "epoch: 28 step: 83 loss: 0.1254812 acc: 0.9560050964355469\n",
      "epoch: 28 step: 84 loss: 0.1347038 acc: 0.9503669738769531\n",
      "epoch: 28 step: 85 loss: 0.121902026 acc: 0.9554367065429688\n",
      "epoch: 28 step: 86 loss: 0.12985483 acc: 0.9560813903808594\n",
      "epoch: 28 step: 87 loss: 0.1585968 acc: 0.9493560791015625\n",
      "epoch: 28 step: 88 loss: 0.14403425 acc: 0.9517822265625\n",
      "epoch: 28 step: 89 loss: 0.16985328 acc: 0.9607620239257812\n",
      "epoch: 28 step: 90 loss: 0.13226636 acc: 0.9581222534179688\n",
      "epoch: 28 step: 91 loss: 0.1821645 acc: 0.9500808715820312\n",
      "epoch: 28 step: 92 loss: 0.19481343 acc: 0.9613723754882812\n",
      "epoch: 28 step: 93 loss: 0.16848181 acc: 0.9372482299804688\n",
      "epoch: 28 step: 94 loss: 0.13562024 acc: 0.9439926147460938\n",
      "epoch: 28 step: 95 loss: 0.14594604 acc: 0.9464530944824219\n",
      "epoch: 28 step: 96 loss: 0.14797273 acc: 0.93975830078125\n",
      "epoch: 28 step: 97 loss: 0.13491097 acc: 0.9546623229980469\n",
      "epoch: 28 step: 98 loss: 0.16849811 acc: 0.9357872009277344\n",
      "epoch: 28 step: 99 loss: 0.14390217 acc: 0.9505271911621094\n",
      "epoch: 28 step: 100 loss: 0.17192897 acc: 0.9422721862792969\n",
      "epoch: 28 step: 101 loss: 0.12143424 acc: 0.9574356079101562\n",
      "epoch: 28 step: 102 loss: 0.14511567 acc: 0.9543418884277344\n",
      "epoch: 28 step: 103 loss: 0.17687769 acc: 0.9454193115234375\n",
      "epoch: 28 step: 104 loss: 0.13741863 acc: 0.9561233520507812\n",
      "epoch: 28 step: 105 loss: 0.08672196 acc: 0.9706840515136719\n",
      "epoch: 28 step: 106 loss: 0.1841444 acc: 0.9495277404785156\n",
      "epoch: 28 step: 107 loss: 0.11574702 acc: 0.9628639221191406\n",
      "epoch: 28 step: 108 loss: 0.13514015 acc: 0.9577140808105469\n",
      "epoch: 28 step: 109 loss: 0.15300773 acc: 0.9512672424316406\n",
      "epoch: 28 step: 110 loss: 0.12335846 acc: 0.950714111328125\n",
      "epoch: 28 step: 111 loss: 0.15734865 acc: 0.9458160400390625\n",
      "epoch: 28 step: 112 loss: 0.13350675 acc: 0.9532394409179688\n",
      "epoch: 28 step: 113 loss: 0.14666903 acc: 0.9491195678710938\n",
      "epoch: 28 step: 114 loss: 0.10954959 acc: 0.9604988098144531\n",
      "epoch: 28 step: 115 loss: 0.123763666 acc: 0.9548797607421875\n",
      "epoch: 28 step: 116 loss: 0.14115092 acc: 0.9529304504394531\n",
      "epoch: 28 step: 117 loss: 0.13694178 acc: 0.9573020935058594\n",
      "epoch: 28 step: 118 loss: 0.14658 acc: 0.9536056518554688\n",
      "epoch: 28 step: 119 loss: 0.123430766 acc: 0.9638252258300781\n",
      "epoch: 28 step: 120 loss: 0.14728138 acc: 0.9474372863769531\n",
      "epoch: 28 step: 121 loss: 0.14334588 acc: 0.9519424438476562\n",
      "epoch: 28 step: 122 loss: 0.1564961 acc: 0.9421310424804688\n",
      "epoch: 28 step: 123 loss: 0.1469518 acc: 0.9485282897949219\n",
      "epoch: 28 step: 124 loss: 0.15947603 acc: 0.9455827985491071\n",
      "epoch: 28 validation_loss: 0.139 validation_dice: 0.7870749252546669\n",
      "epoch: 28 test_dataset dice: 0.7198725508302334\n",
      "time cost 0.5363332430521647 min\n",
      "dice_best: 0.8105984095001857\n",
      "******************************** epoch  28  is finished. *********************************\n",
      "epoch: 29 step: 1 loss: 0.12533711 acc: 0.9434776306152344\n",
      "epoch: 29 step: 2 loss: 0.13899083 acc: 0.9468536376953125\n",
      "epoch: 29 step: 3 loss: 0.19911322 acc: 0.930389404296875\n",
      "epoch: 29 step: 4 loss: 0.1540872 acc: 0.9431800842285156\n",
      "epoch: 29 step: 5 loss: 0.12689856 acc: 0.9643440246582031\n",
      "epoch: 29 step: 6 loss: 0.12863605 acc: 0.9655876159667969\n",
      "epoch: 29 step: 7 loss: 0.1859202 acc: 0.9508476257324219\n",
      "epoch: 29 step: 8 loss: 0.1540299 acc: 0.9571533203125\n",
      "epoch: 29 step: 9 loss: 0.1513223 acc: 0.9521102905273438\n",
      "epoch: 29 step: 10 loss: 0.14335607 acc: 0.9559402465820312\n",
      "epoch: 29 step: 11 loss: 0.12777679 acc: 0.9535102844238281\n",
      "epoch: 29 step: 12 loss: 0.14622039 acc: 0.9503364562988281\n",
      "epoch: 29 step: 13 loss: 0.13594598 acc: 0.9539375305175781\n",
      "epoch: 29 step: 14 loss: 0.12868156 acc: 0.9560279846191406\n",
      "epoch: 29 step: 15 loss: 0.13493103 acc: 0.9511871337890625\n",
      "epoch: 29 step: 16 loss: 0.11818779 acc: 0.9613151550292969\n",
      "epoch: 29 step: 17 loss: 0.109733135 acc: 0.9588127136230469\n",
      "epoch: 29 step: 18 loss: 0.13150431 acc: 0.9582710266113281\n",
      "epoch: 29 step: 19 loss: 0.15964708 acc: 0.9463577270507812\n",
      "epoch: 29 step: 20 loss: 0.16439383 acc: 0.9524765014648438\n",
      "epoch: 29 step: 21 loss: 0.13102461 acc: 0.9524574279785156\n",
      "epoch: 29 step: 22 loss: 0.13754609 acc: 0.9554557800292969\n",
      "epoch: 29 step: 23 loss: 0.15934528 acc: 0.9480934143066406\n",
      "epoch: 29 step: 24 loss: 0.13641153 acc: 0.9529190063476562\n",
      "epoch: 29 step: 25 loss: 0.14920706 acc: 0.9523162841796875\n",
      "epoch: 29 step: 26 loss: 0.122505315 acc: 0.956329345703125\n",
      "epoch: 29 step: 27 loss: 0.12622863 acc: 0.9614448547363281\n",
      "epoch: 29 step: 28 loss: 0.14832434 acc: 0.9476814270019531\n",
      "epoch: 29 step: 29 loss: 0.1499828 acc: 0.9540901184082031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 step: 30 loss: 0.121421 acc: 0.9480438232421875\n",
      "epoch: 29 step: 31 loss: 0.20178606 acc: 0.9433403015136719\n",
      "epoch: 29 step: 32 loss: 0.10999074 acc: 0.9593505859375\n",
      "epoch: 29 step: 33 loss: 0.13433667 acc: 0.9468269348144531\n",
      "epoch: 29 step: 34 loss: 0.13968268 acc: 0.9529876708984375\n",
      "epoch: 29 step: 35 loss: 0.22785203 acc: 0.9332962036132812\n",
      "epoch: 29 step: 36 loss: 0.12784642 acc: 0.9542503356933594\n",
      "epoch: 29 step: 37 loss: 0.15838812 acc: 0.9412803649902344\n",
      "epoch: 29 step: 38 loss: 0.1322109 acc: 0.9465065002441406\n",
      "epoch: 29 step: 39 loss: 0.14910838 acc: 0.9437789916992188\n",
      "epoch: 29 step: 40 loss: 0.14661497 acc: 0.942474365234375\n",
      "epoch: 29 step: 41 loss: 0.13287194 acc: 0.9511222839355469\n",
      "epoch: 29 step: 42 loss: 0.15916139 acc: 0.9459152221679688\n",
      "epoch: 29 step: 43 loss: 0.11864364 acc: 0.9592247009277344\n",
      "epoch: 29 step: 44 loss: 0.17842355 acc: 0.9449348449707031\n",
      "epoch: 29 step: 45 loss: 0.1478194 acc: 0.9458770751953125\n",
      "epoch: 29 step: 46 loss: 0.13815752 acc: 0.9508934020996094\n",
      "epoch: 29 step: 47 loss: 0.13061179 acc: 0.9628944396972656\n",
      "epoch: 29 step: 48 loss: 0.15063667 acc: 0.9560661315917969\n",
      "epoch: 29 step: 49 loss: 0.15781033 acc: 0.9611167907714844\n",
      "epoch: 29 step: 50 loss: 0.15570785 acc: 0.9588279724121094\n",
      "epoch: 29 step: 51 loss: 0.16183196 acc: 0.9386940002441406\n",
      "epoch: 29 step: 52 loss: 0.1154258 acc: 0.9536819458007812\n",
      "epoch: 29 step: 53 loss: 0.17927356 acc: 0.9427452087402344\n",
      "epoch: 29 step: 54 loss: 0.14806637 acc: 0.9412078857421875\n",
      "epoch: 29 step: 55 loss: 0.16030996 acc: 0.9419441223144531\n",
      "epoch: 29 step: 56 loss: 0.13821599 acc: 0.9488334655761719\n",
      "epoch: 29 step: 57 loss: 0.13373291 acc: 0.951995849609375\n",
      "epoch: 29 step: 58 loss: 0.16296934 acc: 0.9492454528808594\n",
      "epoch: 29 step: 59 loss: 0.15049964 acc: 0.9488677978515625\n",
      "epoch: 29 step: 60 loss: 0.120418206 acc: 0.9584007263183594\n",
      "epoch: 29 step: 61 loss: 0.15370145 acc: 0.9442977905273438\n",
      "epoch: 29 step: 62 loss: 0.13474898 acc: 0.9604835510253906\n",
      "epoch: 29 step: 63 loss: 0.14380698 acc: 0.9570236206054688\n",
      "epoch: 29 step: 64 loss: 0.11228827 acc: 0.9650764465332031\n",
      "epoch: 29 step: 65 loss: 0.1360945 acc: 0.9571075439453125\n",
      "epoch: 29 step: 66 loss: 0.14116977 acc: 0.9556999206542969\n",
      "epoch: 29 step: 67 loss: 0.14953987 acc: 0.9445610046386719\n",
      "epoch: 29 step: 68 loss: 0.13967898 acc: 0.9574470520019531\n",
      "epoch: 29 step: 69 loss: 0.17594665 acc: 0.9462661743164062\n",
      "epoch: 29 step: 70 loss: 0.18585399 acc: 0.9487152099609375\n",
      "epoch: 29 step: 71 loss: 0.11127821 acc: 0.9539680480957031\n",
      "epoch: 29 step: 72 loss: 0.13957547 acc: 0.9479827880859375\n",
      "epoch: 29 step: 73 loss: 0.13883677 acc: 0.9532928466796875\n",
      "epoch: 29 step: 74 loss: 0.14290959 acc: 0.9479484558105469\n",
      "epoch: 29 step: 75 loss: 0.20317939 acc: 0.9426231384277344\n",
      "epoch: 29 step: 76 loss: 0.15376702 acc: 0.9560089111328125\n",
      "epoch: 29 step: 77 loss: 0.11616174 acc: 0.962493896484375\n",
      "epoch: 29 step: 78 loss: 0.122473985 acc: 0.9575538635253906\n",
      "epoch: 29 step: 79 loss: 0.13966067 acc: 0.9582862854003906\n",
      "epoch: 29 step: 80 loss: 0.13485189 acc: 0.9533462524414062\n",
      "epoch: 29 step: 81 loss: 0.13900186 acc: 0.9586830139160156\n",
      "epoch: 29 step: 82 loss: 0.19284882 acc: 0.9465179443359375\n",
      "epoch: 29 step: 83 loss: 0.12918258 acc: 0.9533233642578125\n",
      "epoch: 29 step: 84 loss: 0.15379243 acc: 0.947723388671875\n",
      "epoch: 29 step: 85 loss: 0.12601116 acc: 0.9545211791992188\n",
      "epoch: 29 step: 86 loss: 0.14522776 acc: 0.9474716186523438\n",
      "epoch: 29 step: 87 loss: 0.16278622 acc: 0.9450759887695312\n",
      "epoch: 29 step: 88 loss: 0.11237986 acc: 0.9589958190917969\n",
      "epoch: 29 step: 89 loss: 0.15922217 acc: 0.9507484436035156\n",
      "epoch: 29 step: 90 loss: 0.19452278 acc: 0.9544105529785156\n",
      "epoch: 29 step: 91 loss: 0.15510073 acc: 0.9492645263671875\n",
      "epoch: 29 step: 92 loss: 0.14256816 acc: 0.9559097290039062\n",
      "epoch: 29 step: 93 loss: 0.16705717 acc: 0.9546661376953125\n",
      "epoch: 29 step: 94 loss: 0.15266217 acc: 0.9614944458007812\n",
      "epoch: 29 step: 95 loss: 0.1369587 acc: 0.9579696655273438\n",
      "epoch: 29 step: 96 loss: 0.13503462 acc: 0.9513397216796875\n",
      "epoch: 29 step: 97 loss: 0.20215766 acc: 0.9290733337402344\n",
      "epoch: 29 step: 98 loss: 0.12386328 acc: 0.9574546813964844\n",
      "epoch: 29 step: 99 loss: 0.14675118 acc: 0.9538688659667969\n",
      "epoch: 29 step: 100 loss: 0.12792172 acc: 0.9545173645019531\n",
      "epoch: 29 step: 101 loss: 0.14570227 acc: 0.9499931335449219\n",
      "epoch: 29 step: 102 loss: 0.12447205 acc: 0.9586029052734375\n",
      "epoch: 29 step: 103 loss: 0.14405969 acc: 0.9572486877441406\n",
      "epoch: 29 step: 104 loss: 0.15520132 acc: 0.9622116088867188\n",
      "epoch: 29 step: 105 loss: 0.13960007 acc: 0.9631881713867188\n",
      "epoch: 29 step: 106 loss: 0.16421139 acc: 0.9488983154296875\n",
      "epoch: 29 step: 107 loss: 0.1510611 acc: 0.9477500915527344\n",
      "epoch: 29 step: 108 loss: 0.17333604 acc: 0.948699951171875\n",
      "epoch: 29 step: 109 loss: 0.17805716 acc: 0.9492988586425781\n",
      "epoch: 29 step: 110 loss: 0.15863359 acc: 0.9504241943359375\n",
      "epoch: 29 step: 111 loss: 0.15561652 acc: 0.9498786926269531\n",
      "epoch: 29 step: 112 loss: 0.13620743 acc: 0.9514350891113281\n",
      "epoch: 29 step: 113 loss: 0.15599126 acc: 0.950958251953125\n",
      "epoch: 29 step: 114 loss: 0.22936758 acc: 0.945587158203125\n",
      "epoch: 29 step: 115 loss: 0.13995072 acc: 0.9420928955078125\n",
      "epoch: 29 step: 116 loss: 0.16198994 acc: 0.94769287109375\n",
      "epoch: 29 step: 117 loss: 0.14782356 acc: 0.9500885009765625\n",
      "epoch: 29 step: 118 loss: 0.20784342 acc: 0.9392280578613281\n",
      "epoch: 29 step: 119 loss: 0.16258326 acc: 0.9496040344238281\n",
      "epoch: 29 step: 120 loss: 0.16496523 acc: 0.9558181762695312\n",
      "epoch: 29 step: 121 loss: 0.13112071 acc: 0.9501380920410156\n",
      "epoch: 29 step: 122 loss: 0.19501056 acc: 0.9460220336914062\n",
      "epoch: 29 step: 123 loss: 0.14139658 acc: 0.9495353698730469\n",
      "epoch: 29 step: 124 loss: 0.162571 acc: 0.9510149274553571\n",
      "epoch: 29 validation_loss: 0.184 validation_dice: 0.7882976513412322\n",
      "epoch: 29 test_dataset dice: 0.6875097146852943\n",
      "time cost 0.5373212456703186 min\n",
      "dice_best: 0.8105984095001857\n",
      "******************************** epoch  29  is finished. *********************************\n",
      "epoch: 30 step: 1 loss: 0.16815512 acc: 0.9487876892089844\n",
      "epoch: 30 step: 2 loss: 0.14374053 acc: 0.95220947265625\n",
      "epoch: 30 step: 3 loss: 0.17087324 acc: 0.9452934265136719\n",
      "epoch: 30 step: 4 loss: 0.13736962 acc: 0.9462738037109375\n",
      "epoch: 30 step: 5 loss: 0.18577401 acc: 0.9471702575683594\n",
      "epoch: 30 step: 6 loss: 0.14159997 acc: 0.9469757080078125\n",
      "epoch: 30 step: 7 loss: 0.1450988 acc: 0.9496879577636719\n",
      "epoch: 30 step: 8 loss: 0.16601813 acc: 0.9456100463867188\n",
      "epoch: 30 step: 9 loss: 0.12956184 acc: 0.9488525390625\n",
      "epoch: 30 step: 10 loss: 0.13511875 acc: 0.9491729736328125\n",
      "epoch: 30 step: 11 loss: 0.14281829 acc: 0.9529685974121094\n",
      "epoch: 30 step: 12 loss: 0.123382665 acc: 0.9583663940429688\n",
      "epoch: 30 step: 13 loss: 0.161059 acc: 0.9482345581054688\n",
      "epoch: 30 step: 14 loss: 0.19142061 acc: 0.9519081115722656\n",
      "epoch: 30 step: 15 loss: 0.17224663 acc: 0.9464836120605469\n",
      "epoch: 30 step: 16 loss: 0.13726021 acc: 0.950927734375\n",
      "epoch: 30 step: 17 loss: 0.096373305 acc: 0.9559440612792969\n",
      "epoch: 30 step: 18 loss: 0.13631476 acc: 0.9531288146972656\n",
      "epoch: 30 step: 19 loss: 0.11904135 acc: 0.9644889831542969\n",
      "epoch: 30 step: 20 loss: 0.15201789 acc: 0.9543380737304688\n",
      "epoch: 30 step: 21 loss: 0.15556307 acc: 0.9470329284667969\n",
      "epoch: 30 step: 22 loss: 0.11080059 acc: 0.9608879089355469\n",
      "epoch: 30 step: 23 loss: 0.13226248 acc: 0.9539756774902344\n",
      "epoch: 30 step: 24 loss: 0.1544918 acc: 0.9403800964355469\n",
      "epoch: 30 step: 25 loss: 0.12625886 acc: 0.9621200561523438\n",
      "epoch: 30 step: 26 loss: 0.124345526 acc: 0.959320068359375\n",
      "epoch: 30 step: 27 loss: 0.1490013 acc: 0.950408935546875\n",
      "epoch: 30 step: 28 loss: 0.13112433 acc: 0.9582862854003906\n",
      "epoch: 30 step: 29 loss: 0.13752386 acc: 0.9463043212890625\n",
      "epoch: 30 step: 30 loss: 0.13040264 acc: 0.9520912170410156\n",
      "epoch: 30 step: 31 loss: 0.15998422 acc: 0.9535942077636719\n",
      "epoch: 30 step: 32 loss: 0.1383889 acc: 0.9493522644042969\n",
      "epoch: 30 step: 33 loss: 0.14291327 acc: 0.95196533203125\n",
      "epoch: 30 step: 34 loss: 0.12069397 acc: 0.9471054077148438\n",
      "epoch: 30 step: 35 loss: 0.12719803 acc: 0.9574813842773438\n",
      "epoch: 30 step: 36 loss: 0.12174747 acc: 0.9603347778320312\n",
      "epoch: 30 step: 37 loss: 0.11306994 acc: 0.9582862854003906\n",
      "epoch: 30 step: 38 loss: 0.15005074 acc: 0.9497337341308594\n",
      "epoch: 30 step: 39 loss: 0.13331418 acc: 0.958770751953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 step: 40 loss: 0.13207239 acc: 0.9563331604003906\n",
      "epoch: 30 step: 41 loss: 0.12629707 acc: 0.9537773132324219\n",
      "epoch: 30 step: 42 loss: 0.12413236 acc: 0.9609413146972656\n",
      "epoch: 30 step: 43 loss: 0.1080196 acc: 0.9620552062988281\n",
      "epoch: 30 step: 44 loss: 0.15720735 acc: 0.9529151916503906\n",
      "epoch: 30 step: 45 loss: 0.13817497 acc: 0.9567222595214844\n",
      "epoch: 30 step: 46 loss: 0.122380674 acc: 0.9580535888671875\n",
      "epoch: 30 step: 47 loss: 0.108738534 acc: 0.96270751953125\n",
      "epoch: 30 step: 48 loss: 0.1614019 acc: 0.9465217590332031\n",
      "epoch: 30 step: 49 loss: 0.119530566 acc: 0.9549751281738281\n",
      "epoch: 30 step: 50 loss: 0.13068959 acc: 0.9533576965332031\n",
      "epoch: 30 step: 51 loss: 0.11373154 acc: 0.9544792175292969\n",
      "epoch: 30 step: 52 loss: 0.1361925 acc: 0.9572296142578125\n",
      "epoch: 30 step: 53 loss: 0.13021778 acc: 0.9527473449707031\n",
      "epoch: 30 step: 54 loss: 0.12182675 acc: 0.9474601745605469\n",
      "epoch: 30 step: 55 loss: 0.117996335 acc: 0.9570732116699219\n",
      "epoch: 30 step: 56 loss: 0.15452048 acc: 0.933380126953125\n",
      "epoch: 30 step: 57 loss: 0.14993213 acc: 0.9507865905761719\n",
      "epoch: 30 step: 58 loss: 0.11808156 acc: 0.958587646484375\n",
      "epoch: 30 step: 59 loss: 0.11219273 acc: 0.9646949768066406\n",
      "epoch: 30 step: 60 loss: 0.12856074 acc: 0.9561805725097656\n",
      "epoch: 30 step: 61 loss: 0.1435946 acc: 0.9559288024902344\n",
      "epoch: 30 step: 62 loss: 0.17282963 acc: 0.9570503234863281\n",
      "epoch: 30 step: 63 loss: 0.13149695 acc: 0.9499778747558594\n",
      "epoch: 30 step: 64 loss: 0.12988995 acc: 0.9466667175292969\n",
      "epoch: 30 step: 65 loss: 0.12174108 acc: 0.9477310180664062\n",
      "epoch: 30 step: 66 loss: 0.119275056 acc: 0.9545021057128906\n",
      "epoch: 30 step: 67 loss: 0.13577269 acc: 0.9430313110351562\n",
      "epoch: 30 step: 68 loss: 0.11508621 acc: 0.9558067321777344\n",
      "epoch: 30 step: 69 loss: 0.14413372 acc: 0.9426956176757812\n",
      "epoch: 30 step: 70 loss: 0.12195426 acc: 0.9519195556640625\n",
      "epoch: 30 step: 71 loss: 0.15971057 acc: 0.9515800476074219\n",
      "epoch: 30 step: 72 loss: 0.14133757 acc: 0.9588546752929688\n",
      "epoch: 30 step: 73 loss: 0.1028606 acc: 0.9631576538085938\n",
      "epoch: 30 step: 74 loss: 0.12018586 acc: 0.958221435546875\n",
      "epoch: 30 step: 75 loss: 0.11537506 acc: 0.9621086120605469\n",
      "epoch: 30 step: 76 loss: 0.13452408 acc: 0.9594841003417969\n",
      "epoch: 30 step: 77 loss: 0.12967756 acc: 0.9566116333007812\n",
      "epoch: 30 step: 78 loss: 0.12881784 acc: 0.95587158203125\n",
      "epoch: 30 step: 79 loss: 0.1651262 acc: 0.9403495788574219\n",
      "epoch: 30 step: 80 loss: 0.11852443 acc: 0.9623451232910156\n",
      "epoch: 30 step: 81 loss: 0.14555788 acc: 0.9506950378417969\n",
      "epoch: 30 step: 82 loss: 0.13527836 acc: 0.9442520141601562\n",
      "epoch: 30 step: 83 loss: 0.09736684 acc: 0.9575729370117188\n",
      "epoch: 30 step: 84 loss: 0.14867574 acc: 0.9410438537597656\n",
      "epoch: 30 step: 85 loss: 0.1400856 acc: 0.9530105590820312\n",
      "epoch: 30 step: 86 loss: 0.1299843 acc: 0.9580078125\n",
      "epoch: 30 step: 87 loss: 0.18657494 acc: 0.9444656372070312\n",
      "epoch: 30 step: 88 loss: 0.12869434 acc: 0.9464340209960938\n",
      "epoch: 30 step: 89 loss: 0.13772278 acc: 0.9533042907714844\n",
      "epoch: 30 step: 90 loss: 0.126868 acc: 0.9594650268554688\n",
      "epoch: 30 step: 91 loss: 0.12697811 acc: 0.9617195129394531\n",
      "epoch: 30 step: 92 loss: 0.13787435 acc: 0.960357666015625\n",
      "epoch: 30 step: 93 loss: 0.14941731 acc: 0.9481582641601562\n",
      "epoch: 30 step: 94 loss: 0.121453896 acc: 0.95501708984375\n",
      "epoch: 30 step: 95 loss: 0.12612277 acc: 0.958709716796875\n",
      "epoch: 30 step: 96 loss: 0.14633544 acc: 0.9518661499023438\n",
      "epoch: 30 step: 97 loss: 0.14272484 acc: 0.9537010192871094\n",
      "epoch: 30 step: 98 loss: 0.10020065 acc: 0.9581832885742188\n",
      "epoch: 30 step: 99 loss: 0.123914234 acc: 0.9553718566894531\n",
      "epoch: 30 step: 100 loss: 0.10216442 acc: 0.9609909057617188\n",
      "epoch: 30 step: 101 loss: 0.11619737 acc: 0.9625320434570312\n",
      "epoch: 30 step: 102 loss: 0.1255415 acc: 0.958587646484375\n",
      "epoch: 30 step: 103 loss: 0.13439032 acc: 0.9586372375488281\n",
      "epoch: 30 step: 104 loss: 0.12229451 acc: 0.956451416015625\n",
      "epoch: 30 step: 105 loss: 0.10824192 acc: 0.962493896484375\n",
      "epoch: 30 step: 106 loss: 0.13760477 acc: 0.9510879516601562\n",
      "epoch: 30 step: 107 loss: 0.16528596 acc: 0.9434127807617188\n",
      "epoch: 30 step: 108 loss: 0.12255227 acc: 0.9531822204589844\n",
      "epoch: 30 step: 109 loss: 0.116619766 acc: 0.9529075622558594\n",
      "epoch: 30 step: 110 loss: 0.12422991 acc: 0.9471664428710938\n",
      "epoch: 30 step: 111 loss: 0.23652738 acc: 0.9240074157714844\n",
      "epoch: 30 step: 112 loss: 0.13628413 acc: 0.9527626037597656\n",
      "epoch: 30 step: 113 loss: 0.12017769 acc: 0.9570655822753906\n",
      "epoch: 30 step: 114 loss: 0.13180837 acc: 0.9532432556152344\n",
      "epoch: 30 step: 115 loss: 0.12794678 acc: 0.9544906616210938\n",
      "epoch: 30 step: 116 loss: 0.24867903 acc: 0.9461746215820312\n",
      "epoch: 30 step: 117 loss: 0.12349665 acc: 0.9500961303710938\n",
      "epoch: 30 step: 118 loss: 0.1465008 acc: 0.9498138427734375\n",
      "epoch: 30 step: 119 loss: 0.16210939 acc: 0.9471397399902344\n",
      "epoch: 30 step: 120 loss: 0.19683959 acc: 0.9408149719238281\n",
      "epoch: 30 step: 121 loss: 0.30630362 acc: 0.9285240173339844\n",
      "epoch: 30 step: 122 loss: 0.1681508 acc: 0.9435272216796875\n",
      "epoch: 30 step: 123 loss: 0.16554053 acc: 0.9458274841308594\n",
      "epoch: 30 step: 124 loss: 0.22897218 acc: 0.9503086635044643\n",
      "epoch: 30 validation_loss: 0.303 validation_dice: 0.7227704910887779\n",
      "epoch: 30 test_dataset dice: 0.5870086982166399\n",
      "time cost 0.5356436491012573 min\n",
      "dice_best: 0.8105984095001857\n",
      "******************************** epoch  30  is finished. *********************************\n",
      "epoch: 31 step: 1 loss: 0.20532289 acc: 0.9457015991210938\n",
      "epoch: 31 step: 2 loss: 0.20705779 acc: 0.9482002258300781\n",
      "epoch: 31 step: 3 loss: 0.19262086 acc: 0.9486198425292969\n",
      "epoch: 31 step: 4 loss: 0.18546592 acc: 0.9421310424804688\n",
      "epoch: 31 step: 5 loss: 0.23443234 acc: 0.94268798828125\n",
      "epoch: 31 step: 6 loss: 0.20157821 acc: 0.9242630004882812\n",
      "epoch: 31 step: 7 loss: 0.18393402 acc: 0.9458503723144531\n",
      "epoch: 31 step: 8 loss: 0.31379244 acc: 0.8933448791503906\n",
      "epoch: 31 step: 9 loss: 0.20202212 acc: 0.9239501953125\n",
      "epoch: 31 step: 10 loss: 0.21001326 acc: 0.930938720703125\n",
      "epoch: 31 step: 11 loss: 0.23264349 acc: 0.92840576171875\n",
      "epoch: 31 step: 12 loss: 0.20913664 acc: 0.9444770812988281\n",
      "epoch: 31 step: 13 loss: 0.24691729 acc: 0.9401206970214844\n",
      "epoch: 31 step: 14 loss: 0.2101649 acc: 0.9501113891601562\n",
      "epoch: 31 step: 15 loss: 0.21634685 acc: 0.9399375915527344\n",
      "epoch: 31 step: 16 loss: 0.19371448 acc: 0.9338798522949219\n",
      "epoch: 31 step: 17 loss: 0.20288864 acc: 0.9399337768554688\n",
      "epoch: 31 step: 18 loss: 0.21265632 acc: 0.9376373291015625\n",
      "epoch: 31 step: 19 loss: 0.17688568 acc: 0.9437713623046875\n",
      "epoch: 31 step: 20 loss: 0.2221865 acc: 0.943023681640625\n",
      "epoch: 31 step: 21 loss: 0.19478416 acc: 0.9460945129394531\n",
      "epoch: 31 step: 22 loss: 0.17711087 acc: 0.943206787109375\n",
      "epoch: 31 step: 23 loss: 0.17679942 acc: 0.9451942443847656\n",
      "epoch: 31 step: 24 loss: 0.22963469 acc: 0.9368705749511719\n",
      "epoch: 31 step: 25 loss: 0.15493973 acc: 0.9479103088378906\n",
      "epoch: 31 step: 26 loss: 0.17058168 acc: 0.9487838745117188\n",
      "epoch: 31 step: 27 loss: 0.24122192 acc: 0.9426155090332031\n",
      "epoch: 31 step: 28 loss: 0.16736105 acc: 0.9502716064453125\n",
      "epoch: 31 step: 29 loss: 0.18679456 acc: 0.9464378356933594\n",
      "epoch: 31 step: 30 loss: 0.17181727 acc: 0.9477081298828125\n",
      "epoch: 31 step: 31 loss: 0.16953354 acc: 0.9372024536132812\n",
      "epoch: 31 step: 32 loss: 0.18464044 acc: 0.9351158142089844\n",
      "epoch: 31 step: 33 loss: 0.19286112 acc: 0.9433326721191406\n",
      "epoch: 31 step: 34 loss: 0.18415916 acc: 0.9515762329101562\n",
      "epoch: 31 step: 35 loss: 0.14616679 acc: 0.9464836120605469\n",
      "epoch: 31 step: 36 loss: 0.13940053 acc: 0.9509429931640625\n",
      "epoch: 31 step: 37 loss: 0.1841015 acc: 0.9421195983886719\n",
      "epoch: 31 step: 38 loss: 0.16838272 acc: 0.9481544494628906\n",
      "epoch: 31 step: 39 loss: 0.16621055 acc: 0.9503822326660156\n",
      "epoch: 31 step: 40 loss: 0.1759754 acc: 0.946533203125\n",
      "epoch: 31 step: 41 loss: 0.16552843 acc: 0.950836181640625\n",
      "epoch: 31 step: 42 loss: 0.17877135 acc: 0.9578018188476562\n",
      "epoch: 31 step: 43 loss: 0.1914375 acc: 0.9402999877929688\n",
      "epoch: 31 step: 44 loss: 0.17413457 acc: 0.9619331359863281\n",
      "epoch: 31 step: 45 loss: 0.14957117 acc: 0.9534454345703125\n",
      "epoch: 31 step: 46 loss: 0.1581539 acc: 0.9398994445800781\n",
      "epoch: 31 step: 47 loss: 0.1347884 acc: 0.95458984375\n",
      "epoch: 31 step: 48 loss: 0.17244256 acc: 0.9454231262207031\n",
      "epoch: 31 step: 49 loss: 0.13567068 acc: 0.9548416137695312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31 step: 50 loss: 0.1327692 acc: 0.9525413513183594\n",
      "epoch: 31 step: 51 loss: 0.14938575 acc: 0.9453697204589844\n",
      "epoch: 31 step: 52 loss: 0.19854371 acc: 0.9365501403808594\n",
      "epoch: 31 step: 53 loss: 0.12857883 acc: 0.9509506225585938\n",
      "epoch: 31 step: 54 loss: 0.12845732 acc: 0.9570426940917969\n",
      "epoch: 31 step: 55 loss: 0.16622098 acc: 0.9469413757324219\n",
      "epoch: 31 step: 56 loss: 0.1507219 acc: 0.9533348083496094\n",
      "epoch: 31 step: 57 loss: 0.13950787 acc: 0.958709716796875\n",
      "epoch: 31 step: 58 loss: 0.15416871 acc: 0.9541168212890625\n",
      "epoch: 31 step: 59 loss: 0.13682753 acc: 0.9565048217773438\n",
      "epoch: 31 step: 60 loss: 0.12442689 acc: 0.9513740539550781\n",
      "epoch: 31 step: 61 loss: 0.11345283 acc: 0.9629478454589844\n",
      "epoch: 31 step: 62 loss: 0.16629931 acc: 0.9538116455078125\n",
      "epoch: 31 step: 63 loss: 0.12050752 acc: 0.9652595520019531\n",
      "epoch: 31 step: 64 loss: 0.15206666 acc: 0.9532890319824219\n",
      "epoch: 31 step: 65 loss: 0.15257539 acc: 0.9525794982910156\n",
      "epoch: 31 step: 66 loss: 0.12563838 acc: 0.9501075744628906\n",
      "epoch: 31 step: 67 loss: 0.1403382 acc: 0.9475898742675781\n",
      "epoch: 31 step: 68 loss: 0.1256129 acc: 0.9551773071289062\n",
      "epoch: 31 step: 69 loss: 0.14977963 acc: 0.9462203979492188\n",
      "epoch: 31 step: 70 loss: 0.18861869 acc: 0.9360160827636719\n",
      "epoch: 31 step: 71 loss: 0.114521556 acc: 0.951080322265625\n",
      "epoch: 31 step: 72 loss: 0.13859805 acc: 0.9445075988769531\n",
      "epoch: 31 step: 73 loss: 0.16399692 acc: 0.9505119323730469\n",
      "epoch: 31 step: 74 loss: 0.16212502 acc: 0.95501708984375\n",
      "epoch: 31 step: 75 loss: 0.11797746 acc: 0.9587669372558594\n",
      "epoch: 31 step: 76 loss: 0.1525387 acc: 0.954254150390625\n",
      "epoch: 31 step: 77 loss: 0.12831746 acc: 0.9597511291503906\n",
      "epoch: 31 step: 78 loss: 0.11670058 acc: 0.9603919982910156\n",
      "epoch: 31 step: 79 loss: 0.15189178 acc: 0.9491043090820312\n",
      "epoch: 31 step: 80 loss: 0.12689374 acc: 0.9622039794921875\n",
      "epoch: 31 step: 81 loss: 0.15087968 acc: 0.9491004943847656\n",
      "epoch: 31 step: 82 loss: 0.16961029 acc: 0.9487724304199219\n",
      "epoch: 31 step: 83 loss: 0.13067059 acc: 0.9573249816894531\n",
      "epoch: 31 step: 84 loss: 0.15929306 acc: 0.9369354248046875\n",
      "epoch: 31 step: 85 loss: 0.11215923 acc: 0.947967529296875\n",
      "epoch: 31 step: 86 loss: 0.11759189 acc: 0.9528617858886719\n",
      "epoch: 31 step: 87 loss: 0.14914961 acc: 0.9515647888183594\n",
      "epoch: 31 step: 88 loss: 0.12582192 acc: 0.9533576965332031\n",
      "epoch: 31 step: 89 loss: 0.15263124 acc: 0.9514503479003906\n",
      "epoch: 31 step: 90 loss: 0.124161705 acc: 0.9573936462402344\n",
      "epoch: 31 step: 91 loss: 0.16346298 acc: 0.9541549682617188\n",
      "epoch: 31 step: 92 loss: 0.16677043 acc: 0.9537925720214844\n",
      "epoch: 31 step: 93 loss: 0.1706533 acc: 0.9508056640625\n",
      "epoch: 31 step: 94 loss: 0.11934158 acc: 0.9576225280761719\n",
      "epoch: 31 step: 95 loss: 0.1347252 acc: 0.9468193054199219\n",
      "epoch: 31 step: 96 loss: 0.11904396 acc: 0.9581947326660156\n",
      "epoch: 31 step: 97 loss: 0.13769631 acc: 0.9489707946777344\n",
      "epoch: 31 step: 98 loss: 0.15676416 acc: 0.9426994323730469\n",
      "epoch: 31 step: 99 loss: 0.17137885 acc: 0.9283676147460938\n",
      "epoch: 31 step: 100 loss: 0.1610896 acc: 0.9417572021484375\n",
      "epoch: 31 step: 101 loss: 0.13308199 acc: 0.9511985778808594\n",
      "epoch: 31 step: 102 loss: 0.12631495 acc: 0.9554824829101562\n",
      "epoch: 31 step: 103 loss: 0.14802213 acc: 0.9599761962890625\n",
      "epoch: 31 step: 104 loss: 0.13875833 acc: 0.9484100341796875\n",
      "epoch: 31 step: 105 loss: 0.11702768 acc: 0.9606361389160156\n",
      "epoch: 31 step: 106 loss: 0.163376 acc: 0.9536933898925781\n",
      "epoch: 31 step: 107 loss: 0.18133524 acc: 0.9530715942382812\n",
      "epoch: 31 step: 108 loss: 0.11984601 acc: 0.9605865478515625\n",
      "epoch: 31 step: 109 loss: 0.11656646 acc: 0.9639816284179688\n",
      "epoch: 31 step: 110 loss: 0.1366912 acc: 0.9604644775390625\n",
      "epoch: 31 step: 111 loss: 0.15251826 acc: 0.955078125\n",
      "epoch: 31 step: 112 loss: 0.15655614 acc: 0.9575157165527344\n",
      "epoch: 31 step: 113 loss: 0.13644539 acc: 0.9535598754882812\n",
      "epoch: 31 step: 114 loss: 0.14495395 acc: 0.9535751342773438\n",
      "epoch: 31 step: 115 loss: 0.1338805 acc: 0.9575996398925781\n",
      "epoch: 31 step: 116 loss: 0.12616603 acc: 0.9505081176757812\n",
      "epoch: 31 step: 117 loss: 0.15344007 acc: 0.9402389526367188\n",
      "epoch: 31 step: 118 loss: 0.11512539 acc: 0.9570999145507812\n",
      "epoch: 31 step: 119 loss: 0.120855115 acc: 0.961822509765625\n",
      "epoch: 31 step: 120 loss: 0.15155743 acc: 0.9588966369628906\n",
      "epoch: 31 step: 121 loss: 0.12746568 acc: 0.9578399658203125\n",
      "epoch: 31 step: 122 loss: 0.101790555 acc: 0.9591827392578125\n",
      "epoch: 31 step: 123 loss: 0.16179551 acc: 0.946685791015625\n",
      "epoch: 31 step: 124 loss: 0.17021433 acc: 0.9617222377232143\n",
      "epoch: 31 validation_loss: 0.138 validation_dice: 0.8068373482213387\n",
      "epoch: 31 test_dataset dice: 0.7244481283521096\n",
      "time cost 0.5368191440900166 min\n",
      "dice_best: 0.8105984095001857\n",
      "******************************** epoch  31  is finished. *********************************\n",
      "epoch: 32 step: 1 loss: 0.11807725 acc: 0.9492378234863281\n",
      "epoch: 32 step: 2 loss: 0.13316196 acc: 0.9434013366699219\n",
      "epoch: 32 step: 3 loss: 0.13529858 acc: 0.9400444030761719\n",
      "epoch: 32 step: 4 loss: 0.12093321 acc: 0.9486770629882812\n",
      "epoch: 32 step: 5 loss: 0.117466316 acc: 0.9473991394042969\n",
      "epoch: 32 step: 6 loss: 0.10930398 acc: 0.9554977416992188\n",
      "epoch: 32 step: 7 loss: 0.1275615 acc: 0.9540138244628906\n",
      "epoch: 32 step: 8 loss: 0.11909815 acc: 0.9518814086914062\n",
      "epoch: 32 step: 9 loss: 0.14088407 acc: 0.94830322265625\n",
      "epoch: 32 step: 10 loss: 0.11251643 acc: 0.9543609619140625\n",
      "epoch: 32 step: 11 loss: 0.09840335 acc: 0.9625015258789062\n",
      "epoch: 32 step: 12 loss: 0.14536373 acc: 0.9555168151855469\n",
      "epoch: 32 step: 13 loss: 0.12554839 acc: 0.9572067260742188\n",
      "epoch: 32 step: 14 loss: 0.11323588 acc: 0.9556503295898438\n",
      "epoch: 32 step: 15 loss: 0.12070604 acc: 0.9603996276855469\n",
      "epoch: 32 step: 16 loss: 0.09781886 acc: 0.9655532836914062\n",
      "epoch: 32 step: 17 loss: 0.11724261 acc: 0.9587364196777344\n",
      "epoch: 32 step: 18 loss: 0.15221137 acc: 0.9521331787109375\n",
      "epoch: 32 step: 19 loss: 0.10186342 acc: 0.9594383239746094\n",
      "epoch: 32 step: 20 loss: 0.12431907 acc: 0.9560661315917969\n",
      "epoch: 32 step: 21 loss: 0.11966647 acc: 0.9574470520019531\n",
      "epoch: 32 step: 22 loss: 0.16579847 acc: 0.9476318359375\n",
      "epoch: 32 step: 23 loss: 0.14807157 acc: 0.9514198303222656\n",
      "epoch: 32 step: 24 loss: 0.14315034 acc: 0.9567222595214844\n",
      "epoch: 32 step: 25 loss: 0.16311778 acc: 0.9521713256835938\n",
      "epoch: 32 step: 26 loss: 0.10902489 acc: 0.9617462158203125\n",
      "epoch: 32 step: 27 loss: 0.11353748 acc: 0.9587821960449219\n",
      "epoch: 32 step: 28 loss: 0.1606932 acc: 0.9507560729980469\n",
      "epoch: 32 step: 29 loss: 0.12912144 acc: 0.9493484497070312\n",
      "epoch: 32 step: 30 loss: 0.11609816 acc: 0.9560585021972656\n",
      "epoch: 32 step: 31 loss: 0.11880564 acc: 0.9431610107421875\n",
      "epoch: 32 step: 32 loss: 0.12931168 acc: 0.9476890563964844\n",
      "epoch: 32 step: 33 loss: 0.14261167 acc: 0.9488601684570312\n",
      "epoch: 32 step: 34 loss: 0.09255975 acc: 0.960479736328125\n",
      "epoch: 32 step: 35 loss: 0.14208904 acc: 0.959686279296875\n",
      "epoch: 32 step: 36 loss: 0.14464743 acc: 0.961578369140625\n",
      "epoch: 32 step: 37 loss: 0.12262064 acc: 0.9541702270507812\n",
      "epoch: 32 step: 38 loss: 0.122249424 acc: 0.9571113586425781\n",
      "epoch: 32 step: 39 loss: 0.12046771 acc: 0.9577560424804688\n",
      "epoch: 32 step: 40 loss: 0.14149083 acc: 0.9531784057617188\n",
      "epoch: 32 step: 41 loss: 0.13787547 acc: 0.9577178955078125\n",
      "epoch: 32 step: 42 loss: 0.1082602 acc: 0.9568557739257812\n",
      "epoch: 32 step: 43 loss: 0.111971654 acc: 0.9622726440429688\n",
      "epoch: 32 step: 44 loss: 0.11829029 acc: 0.954315185546875\n",
      "epoch: 32 step: 45 loss: 0.12372109 acc: 0.9538726806640625\n",
      "epoch: 32 step: 46 loss: 0.13040982 acc: 0.9534072875976562\n",
      "epoch: 32 step: 47 loss: 0.15007907 acc: 0.9538040161132812\n",
      "epoch: 32 step: 48 loss: 0.13598636 acc: 0.9524307250976562\n",
      "epoch: 32 step: 49 loss: 0.10520808 acc: 0.9589042663574219\n",
      "epoch: 32 step: 50 loss: 0.13392821 acc: 0.9630546569824219\n",
      "epoch: 32 step: 51 loss: 0.14246513 acc: 0.9517059326171875\n",
      "epoch: 32 step: 52 loss: 0.123100705 acc: 0.9503364562988281\n",
      "epoch: 32 step: 53 loss: 0.12140299 acc: 0.9552383422851562\n",
      "epoch: 32 step: 54 loss: 0.13746485 acc: 0.9506568908691406\n",
      "epoch: 32 step: 55 loss: 0.14867947 acc: 0.9526748657226562\n",
      "epoch: 32 step: 56 loss: 0.107094854 acc: 0.9547576904296875\n",
      "epoch: 32 step: 57 loss: 0.1270984 acc: 0.953125\n",
      "epoch: 32 step: 58 loss: 0.11731339 acc: 0.9522590637207031\n",
      "epoch: 32 step: 59 loss: 0.14121753 acc: 0.9513282775878906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32 step: 60 loss: 0.13015485 acc: 0.9550743103027344\n",
      "epoch: 32 step: 61 loss: 0.13158305 acc: 0.9493408203125\n",
      "epoch: 32 step: 62 loss: 0.13818528 acc: 0.9484596252441406\n",
      "epoch: 32 step: 63 loss: 0.12784824 acc: 0.9528656005859375\n",
      "epoch: 32 step: 64 loss: 0.12118428 acc: 0.9528427124023438\n",
      "epoch: 32 step: 65 loss: 0.11673385 acc: 0.9568595886230469\n",
      "epoch: 32 step: 66 loss: 0.15056317 acc: 0.9615592956542969\n",
      "epoch: 32 step: 67 loss: 0.107054375 acc: 0.9585609436035156\n",
      "epoch: 32 step: 68 loss: 0.13152766 acc: 0.9531936645507812\n",
      "epoch: 32 step: 69 loss: 0.12661919 acc: 0.9654769897460938\n",
      "epoch: 32 step: 70 loss: 0.15029103 acc: 0.9618949890136719\n",
      "epoch: 32 step: 71 loss: 0.09552766 acc: 0.959442138671875\n",
      "epoch: 32 step: 72 loss: 0.14578027 acc: 0.9551849365234375\n",
      "epoch: 32 step: 73 loss: 0.113547854 acc: 0.962310791015625\n",
      "epoch: 32 step: 74 loss: 0.12904812 acc: 0.9550361633300781\n",
      "epoch: 32 step: 75 loss: 0.13694583 acc: 0.9591751098632812\n",
      "epoch: 32 step: 76 loss: 0.13751262 acc: 0.9549102783203125\n",
      "epoch: 32 step: 77 loss: 0.14535065 acc: 0.9572792053222656\n",
      "epoch: 32 step: 78 loss: 0.1439616 acc: 0.950531005859375\n",
      "epoch: 32 step: 79 loss: 0.13913849 acc: 0.9536857604980469\n",
      "epoch: 32 step: 80 loss: 0.13920216 acc: 0.9448051452636719\n",
      "epoch: 32 step: 81 loss: 0.12319511 acc: 0.9537925720214844\n",
      "epoch: 32 step: 82 loss: 0.13210195 acc: 0.9546928405761719\n",
      "epoch: 32 step: 83 loss: 0.13301449 acc: 0.9512367248535156\n",
      "epoch: 32 step: 84 loss: 0.13616042 acc: 0.9576568603515625\n",
      "epoch: 32 step: 85 loss: 0.11427777 acc: 0.9531021118164062\n",
      "epoch: 32 step: 86 loss: 0.11716792 acc: 0.9565773010253906\n",
      "epoch: 32 step: 87 loss: 0.16241966 acc: 0.9545669555664062\n",
      "epoch: 32 step: 88 loss: 0.13005204 acc: 0.9597511291503906\n",
      "epoch: 32 step: 89 loss: 0.12431246 acc: 0.9587974548339844\n",
      "epoch: 32 step: 90 loss: 0.1157355 acc: 0.956329345703125\n",
      "epoch: 32 step: 91 loss: 0.1411033 acc: 0.9463996887207031\n",
      "epoch: 32 step: 92 loss: 0.10682613 acc: 0.9597320556640625\n",
      "epoch: 32 step: 93 loss: 0.12740603 acc: 0.9511222839355469\n",
      "epoch: 32 step: 94 loss: 0.1502985 acc: 0.9423713684082031\n",
      "epoch: 32 step: 95 loss: 0.12050526 acc: 0.9515724182128906\n",
      "epoch: 32 step: 96 loss: 0.109988205 acc: 0.9580116271972656\n",
      "epoch: 32 step: 97 loss: 0.14216089 acc: 0.9455757141113281\n",
      "epoch: 32 step: 98 loss: 0.13923822 acc: 0.953582763671875\n",
      "epoch: 32 step: 99 loss: 0.1051063 acc: 0.9580459594726562\n",
      "epoch: 32 step: 100 loss: 0.11995861 acc: 0.9558753967285156\n",
      "epoch: 32 step: 101 loss: 0.13050146 acc: 0.9595870971679688\n",
      "epoch: 32 step: 102 loss: 0.12540442 acc: 0.9549674987792969\n",
      "epoch: 32 step: 103 loss: 0.122430526 acc: 0.956207275390625\n",
      "epoch: 32 step: 104 loss: 0.123726405 acc: 0.9534759521484375\n",
      "epoch: 32 step: 105 loss: 0.12605676 acc: 0.9494171142578125\n",
      "epoch: 32 step: 106 loss: 0.16470446 acc: 0.9541091918945312\n",
      "epoch: 32 step: 107 loss: 0.11120737 acc: 0.9525909423828125\n",
      "epoch: 32 step: 108 loss: 0.10276303 acc: 0.9571571350097656\n",
      "epoch: 32 step: 109 loss: 0.1145855 acc: 0.9582939147949219\n",
      "epoch: 32 step: 110 loss: 0.121044695 acc: 0.9516983032226562\n",
      "epoch: 32 step: 111 loss: 0.10899827 acc: 0.9582633972167969\n",
      "epoch: 32 step: 112 loss: 0.150427 acc: 0.9498634338378906\n",
      "epoch: 32 step: 113 loss: 0.14443305 acc: 0.9510269165039062\n",
      "epoch: 32 step: 114 loss: 0.120131984 acc: 0.9546661376953125\n",
      "epoch: 32 step: 115 loss: 0.11421703 acc: 0.9599380493164062\n",
      "epoch: 32 step: 116 loss: 0.115594886 acc: 0.9644775390625\n",
      "epoch: 32 step: 117 loss: 0.13905506 acc: 0.9521408081054688\n",
      "epoch: 32 step: 118 loss: 0.1612142 acc: 0.9479904174804688\n",
      "epoch: 32 step: 119 loss: 0.12586911 acc: 0.9540023803710938\n",
      "epoch: 32 step: 120 loss: 0.10875366 acc: 0.9548416137695312\n",
      "epoch: 32 step: 121 loss: 0.12358716 acc: 0.9538993835449219\n",
      "epoch: 32 step: 122 loss: 0.12250319 acc: 0.9546356201171875\n",
      "epoch: 32 step: 123 loss: 0.10344293 acc: 0.9547920227050781\n",
      "epoch: 32 step: 124 loss: 0.10917326 acc: 0.9663260323660714\n",
      "epoch: 32 validation_loss: 0.142 validation_dice: 0.7930951901059504\n",
      "epoch: 32 test_dataset dice: 0.7166288758135784\n",
      "time cost 0.5372400959332784 min\n",
      "dice_best: 0.8105984095001857\n",
      "******************************** epoch  32  is finished. *********************************\n",
      "epoch: 33 step: 1 loss: 0.1115769 acc: 0.9544029235839844\n",
      "epoch: 33 step: 2 loss: 0.12477164 acc: 0.9539566040039062\n",
      "epoch: 33 step: 3 loss: 0.12115268 acc: 0.9589309692382812\n",
      "epoch: 33 step: 4 loss: 0.13125777 acc: 0.9621353149414062\n",
      "epoch: 33 step: 5 loss: 0.11749161 acc: 0.9566841125488281\n",
      "epoch: 33 step: 6 loss: 0.14489897 acc: 0.9558944702148438\n",
      "epoch: 33 step: 7 loss: 0.1294381 acc: 0.9536895751953125\n",
      "epoch: 33 step: 8 loss: 0.12204433 acc: 0.9489593505859375\n",
      "epoch: 33 step: 9 loss: 0.08301484 acc: 0.9670219421386719\n",
      "epoch: 33 step: 10 loss: 0.10955582 acc: 0.9516944885253906\n",
      "epoch: 33 step: 11 loss: 0.14845501 acc: 0.9469032287597656\n",
      "epoch: 33 step: 12 loss: 0.11829935 acc: 0.9476509094238281\n",
      "epoch: 33 step: 13 loss: 0.11308488 acc: 0.9510421752929688\n",
      "epoch: 33 step: 14 loss: 0.11701501 acc: 0.9591026306152344\n",
      "epoch: 33 step: 15 loss: 0.13173424 acc: 0.9575004577636719\n",
      "epoch: 33 step: 16 loss: 0.14680246 acc: 0.9527626037597656\n",
      "epoch: 33 step: 17 loss: 0.120197706 acc: 0.9614105224609375\n",
      "epoch: 33 step: 18 loss: 0.09437965 acc: 0.9638824462890625\n",
      "epoch: 33 step: 19 loss: 0.1328042 acc: 0.9514312744140625\n",
      "epoch: 33 step: 20 loss: 0.11720393 acc: 0.9569320678710938\n",
      "epoch: 33 step: 21 loss: 0.115861855 acc: 0.9556655883789062\n",
      "epoch: 33 step: 22 loss: 0.12081492 acc: 0.9579887390136719\n",
      "epoch: 33 step: 23 loss: 0.17113529 acc: 0.9415435791015625\n",
      "epoch: 33 step: 24 loss: 0.12320295 acc: 0.9533767700195312\n",
      "epoch: 33 step: 25 loss: 0.11658388 acc: 0.9540786743164062\n",
      "epoch: 33 step: 26 loss: 0.1439972 acc: 0.9470176696777344\n",
      "epoch: 33 step: 27 loss: 0.09489578 acc: 0.954498291015625\n",
      "epoch: 33 step: 28 loss: 0.11681407 acc: 0.9511566162109375\n",
      "epoch: 33 step: 29 loss: 0.12668446 acc: 0.9421348571777344\n",
      "epoch: 33 step: 30 loss: 0.1249725 acc: 0.9544219970703125\n",
      "epoch: 33 step: 31 loss: 0.12176901 acc: 0.9546546936035156\n",
      "epoch: 33 step: 32 loss: 0.11400219 acc: 0.9634323120117188\n",
      "epoch: 33 step: 33 loss: 0.12164236 acc: 0.9658393859863281\n",
      "epoch: 33 step: 34 loss: 0.12851879 acc: 0.9607582092285156\n",
      "epoch: 33 step: 35 loss: 0.12112789 acc: 0.958709716796875\n",
      "epoch: 33 step: 36 loss: 0.1524695 acc: 0.9531898498535156\n",
      "epoch: 33 step: 37 loss: 0.13453268 acc: 0.9569053649902344\n",
      "epoch: 33 step: 38 loss: 0.10529577 acc: 0.9669914245605469\n",
      "epoch: 33 step: 39 loss: 0.11461876 acc: 0.9534721374511719\n",
      "epoch: 33 step: 40 loss: 0.093832 acc: 0.9557228088378906\n",
      "epoch: 33 step: 41 loss: 0.10830309 acc: 0.9559516906738281\n",
      "epoch: 33 step: 42 loss: 0.11643229 acc: 0.9441757202148438\n",
      "epoch: 33 step: 43 loss: 0.13144813 acc: 0.9445610046386719\n",
      "epoch: 33 step: 44 loss: 0.12038064 acc: 0.9580001831054688\n",
      "epoch: 33 step: 45 loss: 0.124241345 acc: 0.9586067199707031\n",
      "epoch: 33 step: 46 loss: 0.11235516 acc: 0.9618148803710938\n",
      "epoch: 33 step: 47 loss: 0.11299134 acc: 0.9636764526367188\n",
      "epoch: 33 step: 48 loss: 0.10556759 acc: 0.9678421020507812\n",
      "epoch: 33 step: 49 loss: 0.10068498 acc: 0.9699783325195312\n",
      "epoch: 33 step: 50 loss: 0.10410929 acc: 0.9663047790527344\n",
      "epoch: 33 step: 51 loss: 0.112168066 acc: 0.9591712951660156\n",
      "epoch: 33 step: 52 loss: 0.11693521 acc: 0.9617233276367188\n",
      "epoch: 33 step: 53 loss: 0.1293144 acc: 0.9551200866699219\n",
      "epoch: 33 step: 54 loss: 0.1193584 acc: 0.9590415954589844\n",
      "epoch: 33 step: 55 loss: 0.12653089 acc: 0.9535255432128906\n",
      "epoch: 33 step: 56 loss: 0.10648675 acc: 0.9539947509765625\n",
      "epoch: 33 step: 57 loss: 0.10896463 acc: 0.9531784057617188\n",
      "epoch: 33 step: 58 loss: 0.12167666 acc: 0.9513397216796875\n",
      "epoch: 33 step: 59 loss: 0.10760698 acc: 0.9608306884765625\n",
      "epoch: 33 step: 60 loss: 0.10079017 acc: 0.9519844055175781\n",
      "epoch: 33 step: 61 loss: 0.103964016 acc: 0.9561004638671875\n",
      "epoch: 33 step: 62 loss: 0.103320874 acc: 0.9616355895996094\n",
      "epoch: 33 step: 63 loss: 0.13419172 acc: 0.9552345275878906\n",
      "epoch: 33 step: 64 loss: 0.10626363 acc: 0.9671287536621094\n",
      "epoch: 33 step: 65 loss: 0.0846749 acc: 0.9664573669433594\n",
      "epoch: 33 step: 66 loss: 0.112002164 acc: 0.9631118774414062\n",
      "epoch: 33 step: 67 loss: 0.16639058 acc: 0.9590530395507812\n",
      "epoch: 33 step: 68 loss: 0.14725474 acc: 0.9661674499511719\n",
      "epoch: 33 step: 69 loss: 0.11201527 acc: 0.9620704650878906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33 step: 70 loss: 0.12670986 acc: 0.9605293273925781\n",
      "epoch: 33 step: 71 loss: 0.115763344 acc: 0.9580154418945312\n",
      "epoch: 33 step: 72 loss: 0.13690685 acc: 0.9448738098144531\n",
      "epoch: 33 step: 73 loss: 0.13729203 acc: 0.9473037719726562\n",
      "epoch: 33 step: 74 loss: 0.11977184 acc: 0.9472541809082031\n",
      "epoch: 33 step: 75 loss: 0.1408141 acc: 0.9561958312988281\n",
      "epoch: 33 step: 76 loss: 0.102705136 acc: 0.9580650329589844\n",
      "epoch: 33 step: 77 loss: 0.13805169 acc: 0.9562950134277344\n",
      "epoch: 33 step: 78 loss: 0.10049582 acc: 0.96435546875\n",
      "epoch: 33 step: 79 loss: 0.13217309 acc: 0.9526100158691406\n",
      "epoch: 33 step: 80 loss: 0.11784358 acc: 0.9547767639160156\n",
      "epoch: 33 step: 81 loss: 0.1562235 acc: 0.9471473693847656\n",
      "epoch: 33 step: 82 loss: 0.12237551 acc: 0.9532699584960938\n",
      "epoch: 33 step: 83 loss: 0.11372783 acc: 0.9518280029296875\n",
      "epoch: 33 step: 84 loss: 0.12871005 acc: 0.9488067626953125\n",
      "epoch: 33 step: 85 loss: 0.15275684 acc: 0.9473800659179688\n",
      "epoch: 33 step: 86 loss: 0.11538574 acc: 0.9555778503417969\n",
      "epoch: 33 step: 87 loss: 0.12107827 acc: 0.9503059387207031\n",
      "epoch: 33 step: 88 loss: 0.12692691 acc: 0.9538078308105469\n",
      "epoch: 33 step: 89 loss: 0.09502174 acc: 0.9626045227050781\n",
      "epoch: 33 step: 90 loss: 0.1251563 acc: 0.9548492431640625\n",
      "epoch: 33 step: 91 loss: 0.121027514 acc: 0.957183837890625\n",
      "epoch: 33 step: 92 loss: 0.11347512 acc: 0.9621505737304688\n",
      "epoch: 33 step: 93 loss: 0.12604868 acc: 0.9534034729003906\n",
      "epoch: 33 step: 94 loss: 0.094413064 acc: 0.9635162353515625\n",
      "epoch: 33 step: 95 loss: 0.12748706 acc: 0.9599418640136719\n",
      "epoch: 33 step: 96 loss: 0.12866695 acc: 0.9548187255859375\n",
      "epoch: 33 step: 97 loss: 0.12882991 acc: 0.9511833190917969\n",
      "epoch: 33 step: 98 loss: 0.12795924 acc: 0.95477294921875\n",
      "epoch: 33 step: 99 loss: 0.11253914 acc: 0.9549369812011719\n",
      "epoch: 33 step: 100 loss: 0.12691395 acc: 0.9581794738769531\n",
      "epoch: 33 step: 101 loss: 0.13049495 acc: 0.9520835876464844\n",
      "epoch: 33 step: 102 loss: 0.12897171 acc: 0.9519882202148438\n",
      "epoch: 33 step: 103 loss: 0.111216724 acc: 0.9550361633300781\n",
      "epoch: 33 step: 104 loss: 0.11914958 acc: 0.9589614868164062\n",
      "epoch: 33 step: 105 loss: 0.12408698 acc: 0.9539871215820312\n",
      "epoch: 33 step: 106 loss: 0.15156938 acc: 0.9499168395996094\n",
      "epoch: 33 step: 107 loss: 0.109103836 acc: 0.9608306884765625\n",
      "epoch: 33 step: 108 loss: 0.1358461 acc: 0.9576225280761719\n",
      "epoch: 33 step: 109 loss: 0.10910005 acc: 0.9613838195800781\n",
      "epoch: 33 step: 110 loss: 0.10851639 acc: 0.9625511169433594\n",
      "epoch: 33 step: 111 loss: 0.10562812 acc: 0.9626541137695312\n",
      "epoch: 33 step: 112 loss: 0.12411668 acc: 0.9612236022949219\n",
      "epoch: 33 step: 113 loss: 0.13036667 acc: 0.9669113159179688\n",
      "epoch: 33 step: 114 loss: 0.1333723 acc: 0.9449539184570312\n",
      "epoch: 33 step: 115 loss: 0.12379496 acc: 0.9451751708984375\n",
      "epoch: 33 step: 116 loss: 0.1115805 acc: 0.9552650451660156\n",
      "epoch: 33 step: 117 loss: 0.122232124 acc: 0.9565391540527344\n",
      "epoch: 33 step: 118 loss: 0.11310368 acc: 0.9518661499023438\n",
      "epoch: 33 step: 119 loss: 0.14210072 acc: 0.9472770690917969\n",
      "epoch: 33 step: 120 loss: 0.12804121 acc: 0.9484138488769531\n",
      "epoch: 33 step: 121 loss: 0.13387877 acc: 0.9573097229003906\n",
      "epoch: 33 step: 122 loss: 0.101757914 acc: 0.9620170593261719\n",
      "epoch: 33 step: 123 loss: 0.12711285 acc: 0.9498634338378906\n",
      "epoch: 33 step: 124 loss: 0.16832599 acc: 0.9512416294642857\n",
      "epoch: 33 validation_loss: 0.119 validation_dice: 0.8274300650157543\n",
      "epoch: 33 test_dataset dice: 0.7375534932146514\n",
      "time cost 0.5384130001068115 min\n",
      "dice_best: 0.8274300650157543\n",
      "******************************** epoch  33  is finished. *********************************\n",
      "epoch: 34 step: 1 loss: 0.103128344 acc: 0.9537467956542969\n",
      "epoch: 34 step: 2 loss: 0.122328505 acc: 0.9531402587890625\n",
      "epoch: 34 step: 3 loss: 0.124879114 acc: 0.9509429931640625\n",
      "epoch: 34 step: 4 loss: 0.114048295 acc: 0.9500274658203125\n",
      "epoch: 34 step: 5 loss: 0.13026989 acc: 0.9544029235839844\n",
      "epoch: 34 step: 6 loss: 0.12335187 acc: 0.95513916015625\n",
      "epoch: 34 step: 7 loss: 0.102090865 acc: 0.9528884887695312\n",
      "epoch: 34 step: 8 loss: 0.14397046 acc: 0.9455184936523438\n",
      "epoch: 34 step: 9 loss: 0.102810495 acc: 0.9629325866699219\n",
      "epoch: 34 step: 10 loss: 0.09558465 acc: 0.9605216979980469\n",
      "epoch: 34 step: 11 loss: 0.17301169 acc: 0.9471321105957031\n",
      "epoch: 34 step: 12 loss: 0.097059384 acc: 0.9607429504394531\n",
      "epoch: 34 step: 13 loss: 0.1168037 acc: 0.9586524963378906\n",
      "epoch: 34 step: 14 loss: 0.1131759 acc: 0.9596481323242188\n",
      "epoch: 34 step: 15 loss: 0.12379617 acc: 0.9603157043457031\n",
      "epoch: 34 step: 16 loss: 0.10399702 acc: 0.9575653076171875\n",
      "epoch: 34 step: 17 loss: 0.12477883 acc: 0.9548301696777344\n",
      "epoch: 34 step: 18 loss: 0.10928649 acc: 0.9555130004882812\n",
      "epoch: 34 step: 19 loss: 0.11388806 acc: 0.9607276916503906\n",
      "epoch: 34 step: 20 loss: 0.095300876 acc: 0.9596366882324219\n",
      "epoch: 34 step: 21 loss: 0.13481386 acc: 0.9544334411621094\n",
      "epoch: 34 step: 22 loss: 0.11478237 acc: 0.9540786743164062\n",
      "epoch: 34 step: 23 loss: 0.113155164 acc: 0.9502105712890625\n",
      "epoch: 34 step: 24 loss: 0.14318596 acc: 0.9484786987304688\n",
      "epoch: 34 step: 25 loss: 0.11977596 acc: 0.9547195434570312\n",
      "epoch: 34 step: 26 loss: 0.09951862 acc: 0.9651565551757812\n",
      "epoch: 34 step: 27 loss: 0.10458442 acc: 0.959930419921875\n",
      "epoch: 34 step: 28 loss: 0.12738621 acc: 0.9558296203613281\n",
      "epoch: 34 step: 29 loss: 0.106920086 acc: 0.96044921875\n",
      "epoch: 34 step: 30 loss: 0.114104554 acc: 0.9672775268554688\n",
      "epoch: 34 step: 31 loss: 0.10836016 acc: 0.9540176391601562\n",
      "epoch: 34 step: 32 loss: 0.10671047 acc: 0.9563064575195312\n",
      "epoch: 34 step: 33 loss: 0.13239056 acc: 0.9511184692382812\n",
      "epoch: 34 step: 34 loss: 0.14496654 acc: 0.9452629089355469\n",
      "epoch: 34 step: 35 loss: 0.10378969 acc: 0.956817626953125\n",
      "epoch: 34 step: 36 loss: 0.124584176 acc: 0.9491500854492188\n",
      "epoch: 34 step: 37 loss: 0.10958878 acc: 0.9586410522460938\n",
      "epoch: 34 step: 38 loss: 0.11316873 acc: 0.9609184265136719\n",
      "epoch: 34 step: 39 loss: 0.13327605 acc: 0.9592170715332031\n",
      "epoch: 34 step: 40 loss: 0.11010141 acc: 0.962249755859375\n",
      "epoch: 34 step: 41 loss: 0.1429199 acc: 0.9496040344238281\n",
      "epoch: 34 step: 42 loss: 0.116807505 acc: 0.9579925537109375\n",
      "epoch: 34 step: 43 loss: 0.12800562 acc: 0.9498138427734375\n",
      "epoch: 34 step: 44 loss: 0.13219105 acc: 0.9474258422851562\n",
      "epoch: 34 step: 45 loss: 0.13676673 acc: 0.9526824951171875\n",
      "epoch: 34 step: 46 loss: 0.11934051 acc: 0.9499931335449219\n",
      "epoch: 34 step: 47 loss: 0.12790695 acc: 0.9602203369140625\n",
      "epoch: 34 step: 48 loss: 0.1058834 acc: 0.9546279907226562\n",
      "epoch: 34 step: 49 loss: 0.12646522 acc: 0.9605522155761719\n",
      "epoch: 34 step: 50 loss: 0.116066225 acc: 0.9520378112792969\n",
      "epoch: 34 step: 51 loss: 0.1496875 acc: 0.9549713134765625\n",
      "epoch: 34 step: 52 loss: 0.116210304 acc: 0.9594459533691406\n",
      "epoch: 34 step: 53 loss: 0.1375573 acc: 0.9618682861328125\n",
      "epoch: 34 step: 54 loss: 0.102230646 acc: 0.9686317443847656\n",
      "epoch: 34 step: 55 loss: 0.12413663 acc: 0.9572982788085938\n",
      "epoch: 34 step: 56 loss: 0.15133236 acc: 0.952117919921875\n",
      "epoch: 34 step: 57 loss: 0.16445906 acc: 0.9443244934082031\n",
      "epoch: 34 step: 58 loss: 0.12864566 acc: 0.9448432922363281\n",
      "epoch: 34 step: 59 loss: 0.1350108 acc: 0.957733154296875\n",
      "epoch: 34 step: 60 loss: 0.135169 acc: 0.9457054138183594\n",
      "epoch: 34 step: 61 loss: 0.12835312 acc: 0.9539566040039062\n",
      "epoch: 34 step: 62 loss: 0.119792454 acc: 0.9515113830566406\n",
      "epoch: 34 step: 63 loss: 0.121895805 acc: 0.9492111206054688\n",
      "epoch: 34 step: 64 loss: 0.14197606 acc: 0.9523048400878906\n",
      "epoch: 34 step: 65 loss: 0.13850182 acc: 0.9496231079101562\n",
      "epoch: 34 step: 66 loss: 0.113191314 acc: 0.9489021301269531\n",
      "epoch: 34 step: 67 loss: 0.13050085 acc: 0.9554023742675781\n",
      "epoch: 34 step: 68 loss: 0.14075845 acc: 0.9518165588378906\n",
      "epoch: 34 step: 69 loss: 0.11787022 acc: 0.9568862915039062\n",
      "epoch: 34 step: 70 loss: 0.11742756 acc: 0.9632148742675781\n",
      "epoch: 34 step: 71 loss: 0.118147604 acc: 0.9592819213867188\n",
      "epoch: 34 step: 72 loss: 0.10276031 acc: 0.957763671875\n",
      "epoch: 34 step: 73 loss: 0.12940687 acc: 0.9581832885742188\n",
      "epoch: 34 step: 74 loss: 0.11444318 acc: 0.9561843872070312\n",
      "epoch: 34 step: 75 loss: 0.11178648 acc: 0.9590721130371094\n",
      "epoch: 34 step: 76 loss: 0.119953744 acc: 0.9499244689941406\n",
      "epoch: 34 step: 77 loss: 0.116735995 acc: 0.9511947631835938\n",
      "epoch: 34 step: 78 loss: 0.10169176 acc: 0.9559974670410156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34 step: 79 loss: 0.124671124 acc: 0.9473419189453125\n",
      "epoch: 34 step: 80 loss: 0.14127588 acc: 0.9440383911132812\n",
      "epoch: 34 step: 81 loss: 0.12631817 acc: 0.9472885131835938\n",
      "epoch: 34 step: 82 loss: 0.10995955 acc: 0.9535064697265625\n",
      "epoch: 34 step: 83 loss: 0.13247807 acc: 0.9574356079101562\n",
      "epoch: 34 step: 84 loss: 0.1174798 acc: 0.9544029235839844\n",
      "epoch: 34 step: 85 loss: 0.10855995 acc: 0.9636650085449219\n",
      "epoch: 34 step: 86 loss: 0.11834676 acc: 0.9525222778320312\n",
      "epoch: 34 step: 87 loss: 0.11550548 acc: 0.9576301574707031\n",
      "epoch: 34 step: 88 loss: 0.119716465 acc: 0.9655914306640625\n",
      "epoch: 34 step: 89 loss: 0.14025481 acc: 0.9636154174804688\n",
      "epoch: 34 step: 90 loss: 0.1499234 acc: 0.9560432434082031\n",
      "epoch: 34 step: 91 loss: 0.13148206 acc: 0.9520797729492188\n",
      "epoch: 34 step: 92 loss: 0.09245113 acc: 0.9652786254882812\n",
      "epoch: 34 step: 93 loss: 0.1277476 acc: 0.9586715698242188\n",
      "epoch: 34 step: 94 loss: 0.11081149 acc: 0.9568595886230469\n",
      "epoch: 34 step: 95 loss: 0.11304818 acc: 0.9462890625\n",
      "epoch: 34 step: 96 loss: 0.12459929 acc: 0.9544219970703125\n",
      "epoch: 34 step: 97 loss: 0.09405185 acc: 0.9592094421386719\n",
      "epoch: 34 step: 98 loss: 0.10848704 acc: 0.9566535949707031\n",
      "epoch: 34 step: 99 loss: 0.100204386 acc: 0.9552764892578125\n",
      "epoch: 34 step: 100 loss: 0.11695702 acc: 0.9577598571777344\n",
      "epoch: 34 step: 101 loss: 0.14469717 acc: 0.9439239501953125\n",
      "epoch: 34 step: 102 loss: 0.124521606 acc: 0.9529647827148438\n",
      "epoch: 34 step: 103 loss: 0.1033058 acc: 0.9616584777832031\n",
      "epoch: 34 step: 104 loss: 0.10794062 acc: 0.9597549438476562\n",
      "epoch: 34 step: 105 loss: 0.1435938 acc: 0.9652862548828125\n",
      "epoch: 34 step: 106 loss: 0.1326359 acc: 0.95166015625\n",
      "epoch: 34 step: 107 loss: 0.11025409 acc: 0.956817626953125\n",
      "epoch: 34 step: 108 loss: 0.08345579 acc: 0.9627304077148438\n",
      "epoch: 34 step: 109 loss: 0.10865135 acc: 0.9636421203613281\n",
      "epoch: 34 step: 110 loss: 0.099779755 acc: 0.9659919738769531\n",
      "epoch: 34 step: 111 loss: 0.10527693 acc: 0.9607353210449219\n",
      "epoch: 34 step: 112 loss: 0.16762102 acc: 0.9455986022949219\n",
      "epoch: 34 step: 113 loss: 0.1073506 acc: 0.948394775390625\n",
      "epoch: 34 step: 114 loss: 0.13875146 acc: 0.9488906860351562\n",
      "epoch: 34 step: 115 loss: 0.10707417 acc: 0.9494972229003906\n",
      "epoch: 34 step: 116 loss: 0.13506532 acc: 0.9461479187011719\n",
      "epoch: 34 step: 117 loss: 0.13557288 acc: 0.9450263977050781\n",
      "epoch: 34 step: 118 loss: 0.12223862 acc: 0.9536933898925781\n",
      "epoch: 34 step: 119 loss: 0.1274064 acc: 0.9508209228515625\n",
      "epoch: 34 step: 120 loss: 0.1170111 acc: 0.9570579528808594\n",
      "epoch: 34 step: 121 loss: 0.115771614 acc: 0.9544410705566406\n",
      "epoch: 34 step: 122 loss: 0.115877375 acc: 0.9644966125488281\n",
      "epoch: 34 step: 123 loss: 0.12936229 acc: 0.9547615051269531\n",
      "epoch: 34 step: 124 loss: 0.11883451 acc: 0.9562203543526786\n",
      "epoch: 34 validation_loss: 0.122 validation_dice: 0.8265277634483152\n",
      "epoch: 34 test_dataset dice: 0.7450460066907216\n",
      "time cost 0.5374167084693908 min\n",
      "dice_best: 0.8274300650157543\n",
      "******************************** epoch  34  is finished. *********************************\n",
      "epoch: 35 step: 1 loss: 0.13830446 acc: 0.9544792175292969\n",
      "epoch: 35 step: 2 loss: 0.094776034 acc: 0.9601860046386719\n",
      "epoch: 35 step: 3 loss: 0.107753396 acc: 0.9533462524414062\n",
      "epoch: 35 step: 4 loss: 0.11268404 acc: 0.9557838439941406\n",
      "epoch: 35 step: 5 loss: 0.10494907 acc: 0.9548149108886719\n",
      "epoch: 35 step: 6 loss: 0.124257244 acc: 0.9541549682617188\n",
      "epoch: 35 step: 7 loss: 0.12641115 acc: 0.9681587219238281\n",
      "epoch: 35 step: 8 loss: 0.1126918 acc: 0.961029052734375\n",
      "epoch: 35 step: 9 loss: 0.11072928 acc: 0.9576797485351562\n",
      "epoch: 35 step: 10 loss: 0.10508862 acc: 0.9601249694824219\n",
      "epoch: 35 step: 11 loss: 0.124444954 acc: 0.9595184326171875\n",
      "epoch: 35 step: 12 loss: 0.117708884 acc: 0.9613456726074219\n",
      "epoch: 35 step: 13 loss: 0.1252171 acc: 0.9543495178222656\n",
      "epoch: 35 step: 14 loss: 0.14011212 acc: 0.9543304443359375\n",
      "epoch: 35 step: 15 loss: 0.13576429 acc: 0.9556884765625\n",
      "epoch: 35 step: 16 loss: 0.13142252 acc: 0.9511795043945312\n",
      "epoch: 35 step: 17 loss: 0.16629095 acc: 0.9504852294921875\n",
      "epoch: 35 step: 18 loss: 0.14892782 acc: 0.9476509094238281\n",
      "epoch: 35 step: 19 loss: 0.10633142 acc: 0.9614639282226562\n",
      "epoch: 35 step: 20 loss: 0.10863213 acc: 0.9626502990722656\n",
      "epoch: 35 step: 21 loss: 0.108417094 acc: 0.9630966186523438\n",
      "epoch: 35 step: 22 loss: 0.15845057 acc: 0.9629859924316406\n",
      "epoch: 35 step: 23 loss: 0.14032152 acc: 0.9555015563964844\n",
      "epoch: 35 step: 24 loss: 0.13364203 acc: 0.9468154907226562\n",
      "epoch: 35 step: 25 loss: 0.13233706 acc: 0.954254150390625\n",
      "epoch: 35 step: 26 loss: 0.13449505 acc: 0.9472160339355469\n",
      "epoch: 35 step: 27 loss: 0.12986568 acc: 0.9599189758300781\n",
      "epoch: 35 step: 28 loss: 0.1367084 acc: 0.9435615539550781\n",
      "epoch: 35 step: 29 loss: 0.1240377 acc: 0.9506340026855469\n",
      "epoch: 35 step: 30 loss: 0.17359486 acc: 0.9368095397949219\n",
      "epoch: 35 step: 31 loss: 0.12491058 acc: 0.9523811340332031\n",
      "epoch: 35 step: 32 loss: 0.14181785 acc: 0.94195556640625\n",
      "epoch: 35 step: 33 loss: 0.14043431 acc: 0.9583282470703125\n",
      "epoch: 35 step: 34 loss: 0.124124944 acc: 0.9563026428222656\n",
      "epoch: 35 step: 35 loss: 0.10324652 acc: 0.9597396850585938\n",
      "epoch: 35 step: 36 loss: 0.08623838 acc: 0.9639816284179688\n",
      "epoch: 35 step: 37 loss: 0.1012485 acc: 0.9611854553222656\n",
      "epoch: 35 step: 38 loss: 0.102938965 acc: 0.9580345153808594\n",
      "epoch: 35 step: 39 loss: 0.14639384 acc: 0.9579963684082031\n",
      "epoch: 35 step: 40 loss: 0.11189471 acc: 0.9586105346679688\n",
      "epoch: 35 step: 41 loss: 0.11480786 acc: 0.9566535949707031\n",
      "epoch: 35 step: 42 loss: 0.14145738 acc: 0.961029052734375\n",
      "epoch: 35 step: 43 loss: 0.122350916 acc: 0.9515647888183594\n",
      "epoch: 35 step: 44 loss: 0.11144205 acc: 0.9586448669433594\n",
      "epoch: 35 step: 45 loss: 0.14149165 acc: 0.9577102661132812\n",
      "epoch: 35 step: 46 loss: 0.112657644 acc: 0.9515571594238281\n",
      "epoch: 35 step: 47 loss: 0.16167645 acc: 0.9554786682128906\n",
      "epoch: 35 step: 48 loss: 0.14246464 acc: 0.9513053894042969\n",
      "epoch: 35 step: 49 loss: 0.13725735 acc: 0.9560165405273438\n",
      "epoch: 35 step: 50 loss: 0.16707473 acc: 0.9544754028320312\n",
      "epoch: 35 step: 51 loss: 0.14440419 acc: 0.9516639709472656\n",
      "epoch: 35 step: 52 loss: 0.12340271 acc: 0.9635086059570312\n",
      "epoch: 35 step: 53 loss: 0.13601907 acc: 0.951873779296875\n",
      "epoch: 35 step: 54 loss: 0.13885304 acc: 0.9515190124511719\n",
      "epoch: 35 step: 55 loss: 0.16078033 acc: 0.9546127319335938\n",
      "epoch: 35 step: 56 loss: 0.19396041 acc: 0.9407157897949219\n",
      "epoch: 35 step: 57 loss: 0.13250674 acc: 0.946868896484375\n",
      "epoch: 35 step: 58 loss: 0.13088258 acc: 0.9484367370605469\n",
      "epoch: 35 step: 59 loss: 0.15419693 acc: 0.9491310119628906\n",
      "epoch: 35 step: 60 loss: 0.14314626 acc: 0.9489631652832031\n",
      "epoch: 35 step: 61 loss: 0.15334937 acc: 0.9486618041992188\n",
      "epoch: 35 step: 62 loss: 0.16306598 acc: 0.9513702392578125\n",
      "epoch: 35 step: 63 loss: 0.1409527 acc: 0.9549217224121094\n",
      "epoch: 35 step: 64 loss: 0.16653328 acc: 0.9553375244140625\n",
      "epoch: 35 step: 65 loss: 0.1367189 acc: 0.9613075256347656\n",
      "epoch: 35 step: 66 loss: 0.12630871 acc: 0.9589385986328125\n",
      "epoch: 35 step: 67 loss: 0.20411043 acc: 0.939056396484375\n",
      "epoch: 35 step: 68 loss: 0.11218065 acc: 0.9562644958496094\n",
      "epoch: 35 step: 69 loss: 0.1631882 acc: 0.9423942565917969\n",
      "epoch: 35 step: 70 loss: 0.13134275 acc: 0.951446533203125\n",
      "epoch: 35 step: 71 loss: 0.120345525 acc: 0.9523696899414062\n",
      "epoch: 35 step: 72 loss: 0.18063399 acc: 0.9423332214355469\n",
      "epoch: 35 step: 73 loss: 0.16652937 acc: 0.9586296081542969\n",
      "epoch: 35 step: 74 loss: 0.1356236 acc: 0.9533233642578125\n",
      "epoch: 35 step: 75 loss: 0.149935 acc: 0.95648193359375\n",
      "epoch: 35 step: 76 loss: 0.14896198 acc: 0.9570732116699219\n",
      "epoch: 35 step: 77 loss: 0.11892013 acc: 0.959014892578125\n",
      "epoch: 35 step: 78 loss: 0.12945014 acc: 0.9563369750976562\n",
      "epoch: 35 step: 79 loss: 0.12547265 acc: 0.9569053649902344\n",
      "epoch: 35 step: 80 loss: 0.110217586 acc: 0.9592742919921875\n",
      "epoch: 35 step: 81 loss: 0.17385772 acc: 0.9459991455078125\n",
      "epoch: 35 step: 82 loss: 0.13925056 acc: 0.9490013122558594\n",
      "epoch: 35 step: 83 loss: 0.18454024 acc: 0.9387969970703125\n",
      "epoch: 35 step: 84 loss: 0.11524747 acc: 0.9505996704101562\n",
      "epoch: 35 step: 85 loss: 0.14083794 acc: 0.9364852905273438\n",
      "epoch: 35 step: 86 loss: 0.13612312 acc: 0.9543609619140625\n",
      "epoch: 35 step: 87 loss: 0.101843 acc: 0.95599365234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35 step: 88 loss: 0.17062284 acc: 0.9435691833496094\n",
      "epoch: 35 step: 89 loss: 0.16929613 acc: 0.9462165832519531\n",
      "epoch: 35 step: 90 loss: 0.13005175 acc: 0.9563751220703125\n",
      "epoch: 35 step: 91 loss: 0.16184184 acc: 0.9475250244140625\n",
      "epoch: 35 step: 92 loss: 0.12321929 acc: 0.9562873840332031\n",
      "epoch: 35 step: 93 loss: 0.13957866 acc: 0.9552078247070312\n",
      "epoch: 35 step: 94 loss: 0.14827017 acc: 0.9475135803222656\n",
      "epoch: 35 step: 95 loss: 0.13434738 acc: 0.9559288024902344\n",
      "epoch: 35 step: 96 loss: 0.16387579 acc: 0.9530410766601562\n",
      "epoch: 35 step: 97 loss: 0.13475224 acc: 0.9560165405273438\n",
      "epoch: 35 step: 98 loss: 0.1466767 acc: 0.95721435546875\n",
      "epoch: 35 step: 99 loss: 0.1436501 acc: 0.9431533813476562\n",
      "epoch: 35 step: 100 loss: 0.14936772 acc: 0.9394111633300781\n",
      "epoch: 35 step: 101 loss: 0.1407708 acc: 0.9409675598144531\n",
      "epoch: 35 step: 102 loss: 0.13907231 acc: 0.9501571655273438\n",
      "epoch: 35 step: 103 loss: 0.1276655 acc: 0.9452171325683594\n",
      "epoch: 35 step: 104 loss: 0.10556704 acc: 0.9621200561523438\n",
      "epoch: 35 step: 105 loss: 0.12174137 acc: 0.9514846801757812\n",
      "epoch: 35 step: 106 loss: 0.14475991 acc: 0.9449577331542969\n",
      "epoch: 35 step: 107 loss: 0.12767462 acc: 0.9566192626953125\n",
      "epoch: 35 step: 108 loss: 0.16259564 acc: 0.9527740478515625\n",
      "epoch: 35 step: 109 loss: 0.12937617 acc: 0.9609794616699219\n",
      "epoch: 35 step: 110 loss: 0.13338052 acc: 0.963470458984375\n",
      "epoch: 35 step: 111 loss: 0.13622282 acc: 0.9588966369628906\n",
      "epoch: 35 step: 112 loss: 0.18419966 acc: 0.9468421936035156\n",
      "epoch: 35 step: 113 loss: 0.16109991 acc: 0.9485206604003906\n",
      "epoch: 35 step: 114 loss: 0.1134603 acc: 0.95831298828125\n",
      "epoch: 35 step: 115 loss: 0.14291172 acc: 0.9446449279785156\n",
      "epoch: 35 step: 116 loss: 0.14449745 acc: 0.9477920532226562\n",
      "epoch: 35 step: 117 loss: 0.11063609 acc: 0.9587135314941406\n",
      "epoch: 35 step: 118 loss: 0.15489638 acc: 0.94921875\n",
      "epoch: 35 step: 119 loss: 0.113114536 acc: 0.9534339904785156\n",
      "epoch: 35 step: 120 loss: 0.15940854 acc: 0.9470252990722656\n",
      "epoch: 35 step: 121 loss: 0.12267001 acc: 0.9563560485839844\n",
      "epoch: 35 step: 122 loss: 0.15636148 acc: 0.9471664428710938\n",
      "epoch: 35 step: 123 loss: 0.13698664 acc: 0.9495124816894531\n",
      "epoch: 35 step: 124 loss: 0.13331701 acc: 0.9629952566964286\n",
      "epoch: 35 validation_loss: 0.142 validation_dice: 0.8138850670374345\n",
      "epoch: 35 test_dataset dice: 0.6981559045460182\n",
      "time cost 0.5365663250287374 min\n",
      "dice_best: 0.8274300650157543\n",
      "******************************** epoch  35  is finished. *********************************\n",
      "epoch: 36 step: 1 loss: 0.12717167 acc: 0.9502182006835938\n",
      "epoch: 36 step: 2 loss: 0.19428839 acc: 0.9455909729003906\n",
      "epoch: 36 step: 3 loss: 0.11206681 acc: 0.9564285278320312\n",
      "epoch: 36 step: 4 loss: 0.11925537 acc: 0.9528388977050781\n",
      "epoch: 36 step: 5 loss: 0.14946042 acc: 0.9515876770019531\n",
      "epoch: 36 step: 6 loss: 0.12263991 acc: 0.9544677734375\n",
      "epoch: 36 step: 7 loss: 0.11402091 acc: 0.9606208801269531\n",
      "epoch: 36 step: 8 loss: 0.10725628 acc: 0.9548530578613281\n",
      "epoch: 36 step: 9 loss: 0.1936845 acc: 0.935394287109375\n",
      "epoch: 36 step: 10 loss: 0.10976964 acc: 0.9625015258789062\n",
      "epoch: 36 step: 11 loss: 0.11539831 acc: 0.9487762451171875\n",
      "epoch: 36 step: 12 loss: 0.11557781 acc: 0.9594535827636719\n",
      "epoch: 36 step: 13 loss: 0.14071621 acc: 0.9515419006347656\n",
      "epoch: 36 step: 14 loss: 0.12360636 acc: 0.9566574096679688\n",
      "epoch: 36 step: 15 loss: 0.1645076 acc: 0.9526596069335938\n",
      "epoch: 36 step: 16 loss: 0.114547074 acc: 0.9690170288085938\n",
      "epoch: 36 step: 17 loss: 0.14211714 acc: 0.9528884887695312\n",
      "epoch: 36 step: 18 loss: 0.12299617 acc: 0.951446533203125\n",
      "epoch: 36 step: 19 loss: 0.13055971 acc: 0.9486579895019531\n",
      "epoch: 36 step: 20 loss: 0.10782253 acc: 0.9556694030761719\n",
      "epoch: 36 step: 21 loss: 0.13060088 acc: 0.9426383972167969\n",
      "epoch: 36 step: 22 loss: 0.11487704 acc: 0.9549446105957031\n",
      "epoch: 36 step: 23 loss: 0.12069162 acc: 0.95172119140625\n",
      "epoch: 36 step: 24 loss: 0.13600637 acc: 0.9496307373046875\n",
      "epoch: 36 step: 25 loss: 0.11320432 acc: 0.9597740173339844\n",
      "epoch: 36 step: 26 loss: 0.10882404 acc: 0.9561309814453125\n",
      "epoch: 36 step: 27 loss: 0.12406181 acc: 0.9542503356933594\n",
      "epoch: 36 step: 28 loss: 0.16426207 acc: 0.9467124938964844\n",
      "epoch: 36 step: 29 loss: 0.10919668 acc: 0.959136962890625\n",
      "epoch: 36 step: 30 loss: 0.14848325 acc: 0.9476814270019531\n",
      "epoch: 36 step: 31 loss: 0.12095174 acc: 0.9628257751464844\n",
      "epoch: 36 step: 32 loss: 0.14002924 acc: 0.9551239013671875\n",
      "epoch: 36 step: 33 loss: 0.11414796 acc: 0.9478569030761719\n",
      "epoch: 36 step: 34 loss: 0.12634242 acc: 0.9545402526855469\n",
      "epoch: 36 step: 35 loss: 0.12566382 acc: 0.9544830322265625\n",
      "epoch: 36 step: 36 loss: 0.115230426 acc: 0.9664382934570312\n",
      "epoch: 36 step: 37 loss: 0.11357476 acc: 0.9536361694335938\n",
      "epoch: 36 step: 38 loss: 0.14657356 acc: 0.9579544067382812\n",
      "epoch: 36 step: 39 loss: 0.125147 acc: 0.9581680297851562\n",
      "epoch: 36 step: 40 loss: 0.12326785 acc: 0.9606666564941406\n",
      "epoch: 36 step: 41 loss: 0.14619736 acc: 0.9515533447265625\n",
      "epoch: 36 step: 42 loss: 0.13724682 acc: 0.953369140625\n",
      "epoch: 36 step: 43 loss: 0.123223305 acc: 0.9519691467285156\n",
      "epoch: 36 step: 44 loss: 0.111784264 acc: 0.9524459838867188\n",
      "epoch: 36 step: 45 loss: 0.11939546 acc: 0.9573936462402344\n",
      "epoch: 36 step: 46 loss: 0.118858136 acc: 0.9530868530273438\n",
      "epoch: 36 step: 47 loss: 0.11993392 acc: 0.9442977905273438\n",
      "epoch: 36 step: 48 loss: 0.12907945 acc: 0.949951171875\n",
      "epoch: 36 step: 49 loss: 0.120581485 acc: 0.95574951171875\n",
      "epoch: 36 step: 50 loss: 0.113237165 acc: 0.9604072570800781\n",
      "epoch: 36 step: 51 loss: 0.12614895 acc: 0.9496116638183594\n",
      "epoch: 36 step: 52 loss: 0.13342045 acc: 0.947357177734375\n",
      "epoch: 36 step: 53 loss: 0.15596363 acc: 0.9550895690917969\n",
      "epoch: 36 step: 54 loss: 0.12300186 acc: 0.957000732421875\n",
      "epoch: 36 step: 55 loss: 0.12317961 acc: 0.9639472961425781\n",
      "epoch: 36 step: 56 loss: 0.084939264 acc: 0.9669189453125\n",
      "epoch: 36 step: 57 loss: 0.12265619 acc: 0.9566268920898438\n",
      "epoch: 36 step: 58 loss: 0.1672912 acc: 0.94732666015625\n",
      "epoch: 36 step: 59 loss: 0.12551676 acc: 0.9533576965332031\n",
      "epoch: 36 step: 60 loss: 0.11389566 acc: 0.9574737548828125\n",
      "epoch: 36 step: 61 loss: 0.14677428 acc: 0.95709228515625\n",
      "epoch: 36 step: 62 loss: 0.12178118 acc: 0.9590873718261719\n",
      "epoch: 36 step: 63 loss: 0.13665546 acc: 0.9537086486816406\n",
      "epoch: 36 step: 64 loss: 0.13530077 acc: 0.9513626098632812\n",
      "epoch: 36 step: 65 loss: 0.1362008 acc: 0.9499053955078125\n",
      "epoch: 36 step: 66 loss: 0.1478983 acc: 0.9461822509765625\n",
      "epoch: 36 step: 67 loss: 0.12876026 acc: 0.9542121887207031\n",
      "epoch: 36 step: 68 loss: 0.14429009 acc: 0.9560012817382812\n",
      "epoch: 36 step: 69 loss: 0.15245186 acc: 0.9525871276855469\n",
      "epoch: 36 step: 70 loss: 0.13777375 acc: 0.9579505920410156\n",
      "epoch: 36 step: 71 loss: 0.1162456 acc: 0.9522476196289062\n",
      "epoch: 36 step: 72 loss: 0.112062916 acc: 0.9546585083007812\n",
      "epoch: 36 step: 73 loss: 0.15491337 acc: 0.9499053955078125\n",
      "epoch: 36 step: 74 loss: 0.14489481 acc: 0.9435882568359375\n",
      "epoch: 36 step: 75 loss: 0.12087104 acc: 0.9508552551269531\n",
      "epoch: 36 step: 76 loss: 0.12129988 acc: 0.9611587524414062\n",
      "epoch: 36 step: 77 loss: 0.15757066 acc: 0.9502449035644531\n",
      "epoch: 36 step: 78 loss: 0.14082718 acc: 0.9577827453613281\n",
      "epoch: 36 step: 79 loss: 0.115560286 acc: 0.9652137756347656\n",
      "epoch: 36 step: 80 loss: 0.25183284 acc: 0.94390869140625\n",
      "epoch: 36 step: 81 loss: 0.13108882 acc: 0.9470443725585938\n",
      "epoch: 36 step: 82 loss: 0.10094375 acc: 0.957550048828125\n",
      "epoch: 36 step: 83 loss: 0.15119432 acc: 0.9441337585449219\n",
      "epoch: 36 step: 84 loss: 0.14197266 acc: 0.9496688842773438\n",
      "epoch: 36 step: 85 loss: 0.13680632 acc: 0.9508171081542969\n",
      "epoch: 36 step: 86 loss: 0.13648044 acc: 0.9480667114257812\n",
      "epoch: 36 step: 87 loss: 0.17040375 acc: 0.9543533325195312\n",
      "epoch: 36 step: 88 loss: 0.14469732 acc: 0.9585685729980469\n",
      "epoch: 36 step: 89 loss: 0.1475702 acc: 0.953643798828125\n",
      "epoch: 36 step: 90 loss: 0.17320348 acc: 0.9650650024414062\n",
      "epoch: 36 step: 91 loss: 0.1710301 acc: 0.9519386291503906\n",
      "epoch: 36 step: 92 loss: 0.19708517 acc: 0.9483757019042969\n",
      "epoch: 36 step: 93 loss: 0.16039838 acc: 0.9537124633789062\n",
      "epoch: 36 step: 94 loss: 0.15439281 acc: 0.9492568969726562\n",
      "epoch: 36 step: 95 loss: 0.1465655 acc: 0.947235107421875\n",
      "epoch: 36 step: 96 loss: 0.17873694 acc: 0.9419822692871094\n",
      "epoch: 36 step: 97 loss: 0.14556065 acc: 0.9509353637695312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36 step: 98 loss: 0.15804714 acc: 0.9509429931640625\n",
      "epoch: 36 step: 99 loss: 0.13186663 acc: 0.9609298706054688\n",
      "epoch: 36 step: 100 loss: 0.15059066 acc: 0.9478034973144531\n",
      "epoch: 36 step: 101 loss: 0.15842858 acc: 0.9509162902832031\n",
      "epoch: 36 step: 102 loss: 0.20085198 acc: 0.9461517333984375\n",
      "epoch: 36 step: 103 loss: 0.1306612 acc: 0.9613418579101562\n",
      "epoch: 36 step: 104 loss: 0.15199503 acc: 0.9575309753417969\n",
      "epoch: 36 step: 105 loss: 0.14963564 acc: 0.9540023803710938\n",
      "epoch: 36 step: 106 loss: 0.17017046 acc: 0.9405021667480469\n",
      "epoch: 36 step: 107 loss: 0.14653534 acc: 0.9627647399902344\n",
      "epoch: 36 step: 108 loss: 0.12733784 acc: 0.9519996643066406\n",
      "epoch: 36 step: 109 loss: 0.19405846 acc: 0.9510498046875\n",
      "epoch: 36 step: 110 loss: 0.18598613 acc: 0.9523048400878906\n",
      "epoch: 36 step: 111 loss: 0.15712988 acc: 0.9582672119140625\n",
      "epoch: 36 step: 112 loss: 0.17080405 acc: 0.9488525390625\n",
      "epoch: 36 step: 113 loss: 0.115560256 acc: 0.9559402465820312\n",
      "epoch: 36 step: 114 loss: 0.16072018 acc: 0.9450645446777344\n",
      "epoch: 36 step: 115 loss: 0.14167601 acc: 0.950958251953125\n",
      "epoch: 36 step: 116 loss: 0.12434303 acc: 0.9553947448730469\n",
      "epoch: 36 step: 117 loss: 0.20013961 acc: 0.9374198913574219\n",
      "epoch: 36 step: 118 loss: 0.1485248 acc: 0.9538612365722656\n",
      "epoch: 36 step: 119 loss: 0.16188578 acc: 0.940643310546875\n",
      "epoch: 36 step: 120 loss: 0.1350277 acc: 0.9519615173339844\n",
      "epoch: 36 step: 121 loss: 0.12936743 acc: 0.9580116271972656\n",
      "epoch: 36 step: 122 loss: 0.15361202 acc: 0.9492454528808594\n",
      "epoch: 36 step: 123 loss: 0.12966518 acc: 0.9550743103027344\n",
      "epoch: 36 step: 124 loss: 0.14702092 acc: 0.9543805803571429\n",
      "epoch: 36 validation_loss: 0.167 validation_dice: 0.7735504682705012\n",
      "epoch: 36 test_dataset dice: 0.6856655423081247\n",
      "time cost 0.5372343341509501 min\n",
      "dice_best: 0.8274300650157543\n",
      "******************************** epoch  36  is finished. *********************************\n",
      "epoch: 37 step: 1 loss: 0.1465391 acc: 0.9569244384765625\n",
      "epoch: 37 step: 2 loss: 0.15724911 acc: 0.9542617797851562\n",
      "epoch: 37 step: 3 loss: 0.13485092 acc: 0.9580879211425781\n",
      "epoch: 37 step: 4 loss: 0.13258366 acc: 0.9575004577636719\n",
      "epoch: 37 step: 5 loss: 0.119959645 acc: 0.957427978515625\n",
      "epoch: 37 step: 6 loss: 0.14883475 acc: 0.9517097473144531\n",
      "epoch: 37 step: 7 loss: 0.13103533 acc: 0.9495124816894531\n",
      "epoch: 37 step: 8 loss: 0.18115905 acc: 0.9461555480957031\n",
      "epoch: 37 step: 9 loss: 0.13127439 acc: 0.9631843566894531\n",
      "epoch: 37 step: 10 loss: 0.14170258 acc: 0.9595985412597656\n",
      "epoch: 37 step: 11 loss: 0.16241942 acc: 0.958953857421875\n",
      "epoch: 37 step: 12 loss: 0.12952803 acc: 0.9450645446777344\n",
      "epoch: 37 step: 13 loss: 0.1453478 acc: 0.9567146301269531\n",
      "epoch: 37 step: 14 loss: 0.16860174 acc: 0.9490470886230469\n",
      "epoch: 37 step: 15 loss: 0.14035992 acc: 0.9453773498535156\n",
      "epoch: 37 step: 16 loss: 0.14174052 acc: 0.9525375366210938\n",
      "epoch: 37 step: 17 loss: 0.12608461 acc: 0.95794677734375\n",
      "epoch: 37 step: 18 loss: 0.1554809 acc: 0.9452972412109375\n",
      "epoch: 37 step: 19 loss: 0.1832072 acc: 0.947296142578125\n",
      "epoch: 37 step: 20 loss: 0.13154575 acc: 0.9584274291992188\n",
      "epoch: 37 step: 21 loss: 0.16509818 acc: 0.9560928344726562\n",
      "epoch: 37 step: 22 loss: 0.14319696 acc: 0.9519577026367188\n",
      "epoch: 37 step: 23 loss: 0.14864615 acc: 0.9482002258300781\n",
      "epoch: 37 step: 24 loss: 0.1701469 acc: 0.9481124877929688\n",
      "epoch: 37 step: 25 loss: 0.1567309 acc: 0.9406623840332031\n",
      "epoch: 37 step: 26 loss: 0.14373893 acc: 0.9445457458496094\n",
      "epoch: 37 step: 27 loss: 0.11180149 acc: 0.9579963684082031\n",
      "epoch: 37 step: 28 loss: 0.12625381 acc: 0.949462890625\n",
      "epoch: 37 step: 29 loss: 0.1159962 acc: 0.9561843872070312\n",
      "epoch: 37 step: 30 loss: 0.13420789 acc: 0.9597816467285156\n",
      "epoch: 37 step: 31 loss: 0.16071247 acc: 0.9484710693359375\n",
      "epoch: 37 step: 32 loss: 0.12636012 acc: 0.9636306762695312\n",
      "epoch: 37 step: 33 loss: 0.14123277 acc: 0.9542770385742188\n",
      "epoch: 37 step: 34 loss: 0.13344015 acc: 0.9528274536132812\n",
      "epoch: 37 step: 35 loss: 0.10277881 acc: 0.96435546875\n",
      "epoch: 37 step: 36 loss: 0.14825334 acc: 0.9568061828613281\n",
      "epoch: 37 step: 37 loss: 0.13741653 acc: 0.9660568237304688\n",
      "epoch: 37 step: 38 loss: 0.17026944 acc: 0.9605674743652344\n",
      "epoch: 37 step: 39 loss: 0.117803775 acc: 0.9569931030273438\n",
      "epoch: 37 step: 40 loss: 0.13281357 acc: 0.9566726684570312\n",
      "epoch: 37 step: 41 loss: 0.14294012 acc: 0.94378662109375\n",
      "epoch: 37 step: 42 loss: 0.13841413 acc: 0.9453506469726562\n",
      "epoch: 37 step: 43 loss: 0.1479821 acc: 0.9480133056640625\n",
      "epoch: 37 step: 44 loss: 0.13514991 acc: 0.9394798278808594\n",
      "epoch: 37 step: 45 loss: 0.119587034 acc: 0.9439125061035156\n",
      "epoch: 37 step: 46 loss: 0.14779407 acc: 0.9436264038085938\n",
      "epoch: 37 step: 47 loss: 0.13075471 acc: 0.9531326293945312\n",
      "epoch: 37 step: 48 loss: 0.1295655 acc: 0.9517974853515625\n",
      "epoch: 37 step: 49 loss: 0.16202748 acc: 0.9572792053222656\n",
      "epoch: 37 step: 50 loss: 0.15589918 acc: 0.9537010192871094\n",
      "epoch: 37 step: 51 loss: 0.11088108 acc: 0.9603118896484375\n",
      "epoch: 37 step: 52 loss: 0.12498907 acc: 0.95501708984375\n",
      "epoch: 37 step: 53 loss: 0.1290445 acc: 0.9527359008789062\n",
      "epoch: 37 step: 54 loss: 0.123417325 acc: 0.9610671997070312\n",
      "epoch: 37 step: 55 loss: 0.12502968 acc: 0.9529304504394531\n",
      "epoch: 37 step: 56 loss: 0.1264502 acc: 0.9574432373046875\n",
      "epoch: 37 step: 57 loss: 0.14320326 acc: 0.9519271850585938\n",
      "epoch: 37 step: 58 loss: 0.13037275 acc: 0.9576644897460938\n",
      "epoch: 37 step: 59 loss: 0.11799008 acc: 0.9614677429199219\n",
      "epoch: 37 step: 60 loss: 0.13591751 acc: 0.9564323425292969\n",
      "epoch: 37 step: 61 loss: 0.11971401 acc: 0.9601516723632812\n",
      "epoch: 37 step: 62 loss: 0.1422883 acc: 0.9487533569335938\n",
      "epoch: 37 step: 63 loss: 0.1519727 acc: 0.9388236999511719\n",
      "epoch: 37 step: 64 loss: 0.13094969 acc: 0.9500541687011719\n",
      "epoch: 37 step: 65 loss: 0.101839475 acc: 0.9551887512207031\n",
      "epoch: 37 step: 66 loss: 0.12968165 acc: 0.9447364807128906\n",
      "epoch: 37 step: 67 loss: 0.1324545 acc: 0.9462203979492188\n",
      "epoch: 37 step: 68 loss: 0.14847536 acc: 0.9519691467285156\n",
      "epoch: 37 step: 69 loss: 0.11228193 acc: 0.9559555053710938\n",
      "epoch: 37 step: 70 loss: 0.14598875 acc: 0.9553985595703125\n",
      "epoch: 37 step: 71 loss: 0.14184459 acc: 0.9497299194335938\n",
      "epoch: 37 step: 72 loss: 0.12615295 acc: 0.9541244506835938\n",
      "epoch: 37 step: 73 loss: 0.16370963 acc: 0.9496116638183594\n",
      "epoch: 37 step: 74 loss: 0.11866911 acc: 0.9631080627441406\n",
      "epoch: 37 step: 75 loss: 0.12542173 acc: 0.9481658935546875\n",
      "epoch: 37 step: 76 loss: 0.13598147 acc: 0.9499397277832031\n",
      "epoch: 37 step: 77 loss: 0.123044394 acc: 0.9541282653808594\n",
      "epoch: 37 step: 78 loss: 0.12291068 acc: 0.9525260925292969\n",
      "epoch: 37 step: 79 loss: 0.14420871 acc: 0.9486885070800781\n",
      "epoch: 37 step: 80 loss: 0.10918519 acc: 0.9597091674804688\n",
      "epoch: 37 step: 81 loss: 0.115348764 acc: 0.951416015625\n",
      "epoch: 37 step: 82 loss: 0.11768226 acc: 0.9500846862792969\n",
      "epoch: 37 step: 83 loss: 0.14595653 acc: 0.9557647705078125\n",
      "epoch: 37 step: 84 loss: 0.10279997 acc: 0.9576873779296875\n",
      "epoch: 37 step: 85 loss: 0.106450886 acc: 0.9573249816894531\n",
      "epoch: 37 step: 86 loss: 0.13935348 acc: 0.966461181640625\n",
      "epoch: 37 step: 87 loss: 0.13280617 acc: 0.9594879150390625\n",
      "epoch: 37 step: 88 loss: 0.14492416 acc: 0.950531005859375\n",
      "epoch: 37 step: 89 loss: 0.15799555 acc: 0.9474029541015625\n",
      "epoch: 37 step: 90 loss: 0.109506726 acc: 0.9576225280761719\n",
      "epoch: 37 step: 91 loss: 0.122810945 acc: 0.9526023864746094\n",
      "epoch: 37 step: 92 loss: 0.120702304 acc: 0.9535293579101562\n",
      "epoch: 37 step: 93 loss: 0.111381926 acc: 0.9549217224121094\n",
      "epoch: 37 step: 94 loss: 0.13917547 acc: 0.9441719055175781\n",
      "epoch: 37 step: 95 loss: 0.116960436 acc: 0.9520187377929688\n",
      "epoch: 37 step: 96 loss: 0.123656176 acc: 0.9524116516113281\n",
      "epoch: 37 step: 97 loss: 0.112721585 acc: 0.9489593505859375\n",
      "epoch: 37 step: 98 loss: 0.129688 acc: 0.9618873596191406\n",
      "epoch: 37 step: 99 loss: 0.11325549 acc: 0.9562339782714844\n",
      "epoch: 37 step: 100 loss: 0.13209926 acc: 0.9535942077636719\n",
      "epoch: 37 step: 101 loss: 0.11274465 acc: 0.9520645141601562\n",
      "epoch: 37 step: 102 loss: 0.1746825 acc: 0.9404258728027344\n",
      "epoch: 37 step: 103 loss: 0.107899174 acc: 0.9520187377929688\n",
      "epoch: 37 step: 104 loss: 0.10401298 acc: 0.9575271606445312\n",
      "epoch: 37 step: 105 loss: 0.1310212 acc: 0.9486808776855469\n",
      "epoch: 37 step: 106 loss: 0.103461094 acc: 0.9622688293457031\n",
      "epoch: 37 step: 107 loss: 0.1312945 acc: 0.9601211547851562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37 step: 108 loss: 0.11079829 acc: 0.9623794555664062\n",
      "epoch: 37 step: 109 loss: 0.11793419 acc: 0.9552841186523438\n",
      "epoch: 37 step: 110 loss: 0.11835859 acc: 0.9590187072753906\n",
      "epoch: 37 step: 111 loss: 0.13136388 acc: 0.9507789611816406\n",
      "epoch: 37 step: 112 loss: 0.1074891 acc: 0.953948974609375\n",
      "epoch: 37 step: 113 loss: 0.121791914 acc: 0.9446029663085938\n",
      "epoch: 37 step: 114 loss: 0.12056935 acc: 0.9528350830078125\n",
      "epoch: 37 step: 115 loss: 0.12029433 acc: 0.9542121887207031\n",
      "epoch: 37 step: 116 loss: 0.10004694 acc: 0.9541740417480469\n",
      "epoch: 37 step: 117 loss: 0.12934023 acc: 0.9514122009277344\n",
      "epoch: 37 step: 118 loss: 0.13245487 acc: 0.9542427062988281\n",
      "epoch: 37 step: 119 loss: 0.12759334 acc: 0.9566879272460938\n",
      "epoch: 37 step: 120 loss: 0.1524092 acc: 0.9508895874023438\n",
      "epoch: 37 step: 121 loss: 0.11733528 acc: 0.9590835571289062\n",
      "epoch: 37 step: 122 loss: 0.15448582 acc: 0.9527511596679688\n",
      "epoch: 37 step: 123 loss: 0.11325181 acc: 0.9591712951660156\n",
      "epoch: 37 step: 124 loss: 0.12732932 acc: 0.9617571149553571\n",
      "epoch: 37 validation_loss: 0.13 validation_dice: 0.8061151189130684\n",
      "epoch: 37 test_dataset dice: 0.7243238319788422\n",
      "time cost 0.5367594798405965 min\n",
      "dice_best: 0.8274300650157543\n",
      "******************************** epoch  37  is finished. *********************************\n",
      "epoch: 38 step: 1 loss: 0.12161701 acc: 0.9522514343261719\n",
      "epoch: 38 step: 2 loss: 0.10670673 acc: 0.9576492309570312\n",
      "epoch: 38 step: 3 loss: 0.14476664 acc: 0.9545440673828125\n",
      "epoch: 38 step: 4 loss: 0.120754324 acc: 0.9631690979003906\n",
      "epoch: 38 step: 5 loss: 0.122214824 acc: 0.9594802856445312\n",
      "epoch: 38 step: 6 loss: 0.16833048 acc: 0.9518699645996094\n",
      "epoch: 38 step: 7 loss: 0.11923245 acc: 0.9550437927246094\n",
      "epoch: 38 step: 8 loss: 0.15010828 acc: 0.9543914794921875\n",
      "epoch: 38 step: 9 loss: 0.20279874 acc: 0.9491958618164062\n",
      "epoch: 38 step: 10 loss: 0.14689253 acc: 0.952667236328125\n",
      "epoch: 38 step: 11 loss: 0.124610975 acc: 0.956451416015625\n",
      "epoch: 38 step: 12 loss: 0.1289282 acc: 0.9591560363769531\n",
      "epoch: 38 step: 13 loss: 0.13636702 acc: 0.9533195495605469\n",
      "epoch: 38 step: 14 loss: 0.10849949 acc: 0.9683189392089844\n",
      "epoch: 38 step: 15 loss: 0.1271778 acc: 0.9536323547363281\n",
      "epoch: 38 step: 16 loss: 0.14347029 acc: 0.9497261047363281\n",
      "epoch: 38 step: 17 loss: 0.12222598 acc: 0.9493217468261719\n",
      "epoch: 38 step: 18 loss: 0.14190303 acc: 0.9551277160644531\n",
      "epoch: 38 step: 19 loss: 0.14146268 acc: 0.9525718688964844\n",
      "epoch: 38 step: 20 loss: 0.13835406 acc: 0.9597053527832031\n",
      "epoch: 38 step: 21 loss: 0.13088258 acc: 0.9478759765625\n",
      "epoch: 38 step: 22 loss: 0.11254358 acc: 0.9559364318847656\n",
      "epoch: 38 step: 23 loss: 0.1437862 acc: 0.9505119323730469\n",
      "epoch: 38 step: 24 loss: 0.14145948 acc: 0.9465522766113281\n",
      "epoch: 38 step: 25 loss: 0.12732363 acc: 0.95098876953125\n",
      "epoch: 38 step: 26 loss: 0.12428258 acc: 0.9531059265136719\n",
      "epoch: 38 step: 27 loss: 0.12070061 acc: 0.9531135559082031\n",
      "epoch: 38 step: 28 loss: 0.12115111 acc: 0.9468612670898438\n",
      "epoch: 38 step: 29 loss: 0.12515566 acc: 0.9551811218261719\n",
      "epoch: 38 step: 30 loss: 0.09538329 acc: 0.9628143310546875\n",
      "epoch: 38 step: 31 loss: 0.1472365 acc: 0.9535636901855469\n",
      "epoch: 38 step: 32 loss: 0.1413553 acc: 0.9545974731445312\n",
      "epoch: 38 step: 33 loss: 0.13615337 acc: 0.9637908935546875\n",
      "epoch: 38 step: 34 loss: 0.10494776 acc: 0.9628181457519531\n",
      "epoch: 38 step: 35 loss: 0.14277159 acc: 0.9607620239257812\n",
      "epoch: 38 step: 36 loss: 0.12681352 acc: 0.9571762084960938\n",
      "epoch: 38 step: 37 loss: 0.15223372 acc: 0.9544219970703125\n",
      "epoch: 38 step: 38 loss: 0.12548056 acc: 0.9548377990722656\n",
      "epoch: 38 step: 39 loss: 0.11650585 acc: 0.9576492309570312\n",
      "epoch: 38 step: 40 loss: 0.11904663 acc: 0.9518089294433594\n",
      "epoch: 38 step: 41 loss: 0.13161257 acc: 0.9440383911132812\n",
      "epoch: 38 step: 42 loss: 0.115213834 acc: 0.9538040161132812\n",
      "epoch: 38 step: 43 loss: 0.123336166 acc: 0.9546012878417969\n",
      "epoch: 38 step: 44 loss: 0.1106875 acc: 0.9538383483886719\n",
      "epoch: 38 step: 45 loss: 0.13307863 acc: 0.9487953186035156\n",
      "epoch: 38 step: 46 loss: 0.11722848 acc: 0.9589958190917969\n",
      "epoch: 38 step: 47 loss: 0.12939684 acc: 0.9561653137207031\n",
      "epoch: 38 step: 48 loss: 0.10092845 acc: 0.9602699279785156\n",
      "epoch: 38 step: 49 loss: 0.10009231 acc: 0.9576416015625\n",
      "epoch: 38 step: 50 loss: 0.10503133 acc: 0.9638137817382812\n",
      "epoch: 38 step: 51 loss: 0.116854064 acc: 0.9542579650878906\n",
      "epoch: 38 step: 52 loss: 0.11471893 acc: 0.954620361328125\n",
      "epoch: 38 step: 53 loss: 0.123689465 acc: 0.9606246948242188\n",
      "epoch: 38 step: 54 loss: 0.107579075 acc: 0.9634742736816406\n",
      "epoch: 38 step: 55 loss: 0.117253855 acc: 0.96185302734375\n",
      "epoch: 38 step: 56 loss: 0.11888823 acc: 0.9522323608398438\n",
      "epoch: 38 step: 57 loss: 0.15166736 acc: 0.9520797729492188\n",
      "epoch: 38 step: 58 loss: 0.1379337 acc: 0.9462356567382812\n",
      "epoch: 38 step: 59 loss: 0.097720094 acc: 0.9551277160644531\n",
      "epoch: 38 step: 60 loss: 0.117814764 acc: 0.95855712890625\n",
      "epoch: 38 step: 61 loss: 0.104545645 acc: 0.9577369689941406\n",
      "epoch: 38 step: 62 loss: 0.1012663 acc: 0.9625434875488281\n",
      "epoch: 38 step: 63 loss: 0.09979059 acc: 0.9636039733886719\n",
      "epoch: 38 step: 64 loss: 0.09993384 acc: 0.96044921875\n",
      "epoch: 38 step: 65 loss: 0.11731157 acc: 0.9574851989746094\n",
      "epoch: 38 step: 66 loss: 0.12865537 acc: 0.9475440979003906\n",
      "epoch: 38 step: 67 loss: 0.1018095 acc: 0.956695556640625\n",
      "epoch: 38 step: 68 loss: 0.10515606 acc: 0.9560585021972656\n",
      "epoch: 38 step: 69 loss: 0.13947 acc: 0.9554214477539062\n",
      "epoch: 38 step: 70 loss: 0.120089255 acc: 0.9554481506347656\n",
      "epoch: 38 step: 71 loss: 0.11058055 acc: 0.9582328796386719\n",
      "epoch: 38 step: 72 loss: 0.09267632 acc: 0.9679298400878906\n",
      "epoch: 38 step: 73 loss: 0.122608006 acc: 0.9493942260742188\n",
      "epoch: 38 step: 74 loss: 0.1002748 acc: 0.9559669494628906\n",
      "epoch: 38 step: 75 loss: 0.10959042 acc: 0.9596519470214844\n",
      "epoch: 38 step: 76 loss: 0.149507 acc: 0.9526519775390625\n",
      "epoch: 38 step: 77 loss: 0.110277094 acc: 0.9537124633789062\n",
      "epoch: 38 step: 78 loss: 0.12812603 acc: 0.9504051208496094\n",
      "epoch: 38 step: 79 loss: 0.11179239 acc: 0.9613227844238281\n",
      "epoch: 38 step: 80 loss: 0.12735653 acc: 0.9557571411132812\n",
      "epoch: 38 step: 81 loss: 0.104262315 acc: 0.9599151611328125\n",
      "epoch: 38 step: 82 loss: 0.12854011 acc: 0.9484748840332031\n",
      "epoch: 38 step: 83 loss: 0.11471556 acc: 0.9499702453613281\n",
      "epoch: 38 step: 84 loss: 0.120580025 acc: 0.9539871215820312\n",
      "epoch: 38 step: 85 loss: 0.111087315 acc: 0.956451416015625\n",
      "epoch: 38 step: 86 loss: 0.129886 acc: 0.9501266479492188\n",
      "epoch: 38 step: 87 loss: 0.13170767 acc: 0.9526634216308594\n",
      "epoch: 38 step: 88 loss: 0.11635383 acc: 0.9506187438964844\n",
      "epoch: 38 step: 89 loss: 0.101112954 acc: 0.9549331665039062\n",
      "epoch: 38 step: 90 loss: 0.12912753 acc: 0.9558753967285156\n",
      "epoch: 38 step: 91 loss: 0.14950976 acc: 0.9465789794921875\n",
      "epoch: 38 step: 92 loss: 0.105331406 acc: 0.9582710266113281\n",
      "epoch: 38 step: 93 loss: 0.114776604 acc: 0.9553298950195312\n",
      "epoch: 38 step: 94 loss: 0.10131226 acc: 0.9575347900390625\n",
      "epoch: 38 step: 95 loss: 0.10862336 acc: 0.9591026306152344\n",
      "epoch: 38 step: 96 loss: 0.1419712 acc: 0.946533203125\n",
      "epoch: 38 step: 97 loss: 0.11445444 acc: 0.9550933837890625\n",
      "epoch: 38 step: 98 loss: 0.10763421 acc: 0.9588584899902344\n",
      "epoch: 38 step: 99 loss: 0.122316405 acc: 0.9515342712402344\n",
      "epoch: 38 step: 100 loss: 0.10010298 acc: 0.9600753784179688\n",
      "epoch: 38 step: 101 loss: 0.1259682 acc: 0.9633979797363281\n",
      "epoch: 38 step: 102 loss: 0.13771895 acc: 0.9547004699707031\n",
      "epoch: 38 step: 103 loss: 0.0848106 acc: 0.9657745361328125\n",
      "epoch: 38 step: 104 loss: 0.11493983 acc: 0.9590301513671875\n",
      "epoch: 38 step: 105 loss: 0.12257729 acc: 0.9530143737792969\n",
      "epoch: 38 step: 106 loss: 0.11795186 acc: 0.95526123046875\n",
      "epoch: 38 step: 107 loss: 0.15284677 acc: 0.9493026733398438\n",
      "epoch: 38 step: 108 loss: 0.1315078 acc: 0.9512939453125\n",
      "epoch: 38 step: 109 loss: 0.111468546 acc: 0.9537353515625\n",
      "epoch: 38 step: 110 loss: 0.11563071 acc: 0.9546699523925781\n",
      "epoch: 38 step: 111 loss: 0.10690616 acc: 0.9506492614746094\n",
      "epoch: 38 step: 112 loss: 0.13051231 acc: 0.9545555114746094\n",
      "epoch: 38 step: 113 loss: 0.13653788 acc: 0.9572868347167969\n",
      "epoch: 38 step: 114 loss: 0.093184635 acc: 0.9588584899902344\n",
      "epoch: 38 step: 115 loss: 0.15000272 acc: 0.9556922912597656\n",
      "epoch: 38 step: 116 loss: 0.12413051 acc: 0.9576873779296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38 step: 117 loss: 0.12659736 acc: 0.9630928039550781\n",
      "epoch: 38 step: 118 loss: 0.10352591 acc: 0.9649734497070312\n",
      "epoch: 38 step: 119 loss: 0.13228893 acc: 0.9555854797363281\n",
      "epoch: 38 step: 120 loss: 0.11209538 acc: 0.95611572265625\n",
      "epoch: 38 step: 121 loss: 0.13455012 acc: 0.95098876953125\n",
      "epoch: 38 step: 122 loss: 0.11700106 acc: 0.9564933776855469\n",
      "epoch: 38 step: 123 loss: 0.10709683 acc: 0.9548492431640625\n",
      "epoch: 38 step: 124 loss: 0.21980864 acc: 0.9560372488839286\n",
      "epoch: 38 validation_loss: 0.139 validation_dice: 0.8246087084111221\n",
      "epoch: 38 test_dataset dice: 0.7140752337636889\n",
      "time cost 0.5368270635604858 min\n",
      "dice_best: 0.8274300650157543\n",
      "******************************** epoch  38  is finished. *********************************\n",
      "epoch: 39 step: 1 loss: 0.13451105 acc: 0.9514312744140625\n",
      "epoch: 39 step: 2 loss: 0.1103509 acc: 0.9460716247558594\n",
      "epoch: 39 step: 3 loss: 0.17578731 acc: 0.9444541931152344\n",
      "epoch: 39 step: 4 loss: 0.16518642 acc: 0.945770263671875\n",
      "epoch: 39 step: 5 loss: 0.11957023 acc: 0.9549598693847656\n",
      "epoch: 39 step: 6 loss: 0.1517326 acc: 0.9492874145507812\n",
      "epoch: 39 step: 7 loss: 0.13480037 acc: 0.9522247314453125\n",
      "epoch: 39 step: 8 loss: 0.15318838 acc: 0.952850341796875\n",
      "epoch: 39 step: 9 loss: 0.1420839 acc: 0.9491729736328125\n",
      "epoch: 39 step: 10 loss: 0.15234025 acc: 0.9359626770019531\n",
      "epoch: 39 step: 11 loss: 0.157416 acc: 0.9309005737304688\n",
      "epoch: 39 step: 12 loss: 0.15953788 acc: 0.9461326599121094\n",
      "epoch: 39 step: 13 loss: 0.123684935 acc: 0.9500045776367188\n",
      "epoch: 39 step: 14 loss: 0.14566943 acc: 0.9461822509765625\n",
      "epoch: 39 step: 15 loss: 0.15688117 acc: 0.9467697143554688\n",
      "epoch: 39 step: 16 loss: 0.14638145 acc: 0.9463005065917969\n",
      "epoch: 39 step: 17 loss: 0.1410698 acc: 0.9487762451171875\n",
      "epoch: 39 step: 18 loss: 0.14014539 acc: 0.9547500610351562\n",
      "epoch: 39 step: 19 loss: 0.13185023 acc: 0.9592628479003906\n",
      "epoch: 39 step: 20 loss: 0.14207509 acc: 0.950164794921875\n",
      "epoch: 39 step: 21 loss: 0.14423501 acc: 0.9544448852539062\n",
      "epoch: 39 step: 22 loss: 0.1155362 acc: 0.9532279968261719\n",
      "epoch: 39 step: 23 loss: 0.13960215 acc: 0.9593467712402344\n",
      "epoch: 39 step: 24 loss: 0.1208272 acc: 0.959442138671875\n",
      "epoch: 39 step: 25 loss: 0.13601416 acc: 0.9517059326171875\n",
      "epoch: 39 step: 26 loss: 0.116004996 acc: 0.9502372741699219\n",
      "epoch: 39 step: 27 loss: 0.12579487 acc: 0.9537277221679688\n",
      "epoch: 39 step: 28 loss: 0.105935186 acc: 0.9627456665039062\n",
      "epoch: 39 step: 29 loss: 0.1255815 acc: 0.9566993713378906\n",
      "epoch: 39 step: 30 loss: 0.122812964 acc: 0.9521942138671875\n",
      "epoch: 39 step: 31 loss: 0.12538072 acc: 0.9510917663574219\n",
      "epoch: 39 step: 32 loss: 0.112396546 acc: 0.9587974548339844\n",
      "epoch: 39 step: 33 loss: 0.12687881 acc: 0.95526123046875\n",
      "epoch: 39 step: 34 loss: 0.12964268 acc: 0.95367431640625\n",
      "epoch: 39 step: 35 loss: 0.13342346 acc: 0.9595909118652344\n",
      "epoch: 39 step: 36 loss: 0.1287997 acc: 0.9512252807617188\n",
      "epoch: 39 step: 37 loss: 0.134115 acc: 0.9576034545898438\n",
      "epoch: 39 step: 38 loss: 0.11883194 acc: 0.9585342407226562\n",
      "epoch: 39 step: 39 loss: 0.14124392 acc: 0.9567146301269531\n",
      "epoch: 39 step: 40 loss: 0.12208353 acc: 0.9604339599609375\n",
      "epoch: 39 step: 41 loss: 0.12077626 acc: 0.9585037231445312\n",
      "epoch: 39 step: 42 loss: 0.122028604 acc: 0.9504051208496094\n",
      "epoch: 39 step: 43 loss: 0.13581586 acc: 0.9463653564453125\n",
      "epoch: 39 step: 44 loss: 0.13369957 acc: 0.960235595703125\n",
      "epoch: 39 step: 45 loss: 0.14395893 acc: 0.9515800476074219\n",
      "epoch: 39 step: 46 loss: 0.14006065 acc: 0.9518165588378906\n",
      "epoch: 39 step: 47 loss: 0.14233746 acc: 0.9561729431152344\n",
      "epoch: 39 step: 48 loss: 0.12773614 acc: 0.9580650329589844\n",
      "epoch: 39 step: 49 loss: 0.16988961 acc: 0.9502372741699219\n",
      "epoch: 39 step: 50 loss: 0.116009735 acc: 0.9610862731933594\n",
      "epoch: 39 step: 51 loss: 0.10096358 acc: 0.9626312255859375\n",
      "epoch: 39 step: 52 loss: 0.17696868 acc: 0.9474411010742188\n",
      "epoch: 39 step: 53 loss: 0.11390871 acc: 0.9615745544433594\n",
      "epoch: 39 step: 54 loss: 0.14125244 acc: 0.9557380676269531\n",
      "epoch: 39 step: 55 loss: 0.15464956 acc: 0.9449653625488281\n",
      "epoch: 39 step: 56 loss: 0.14311275 acc: 0.9434585571289062\n",
      "epoch: 39 step: 57 loss: 0.12441681 acc: 0.9440383911132812\n",
      "epoch: 39 step: 58 loss: 0.122611225 acc: 0.9430923461914062\n",
      "epoch: 39 step: 59 loss: 0.11171319 acc: 0.9512252807617188\n",
      "epoch: 39 step: 60 loss: 0.12511149 acc: 0.9496116638183594\n",
      "epoch: 39 step: 61 loss: 0.16254936 acc: 0.9423942565917969\n",
      "epoch: 39 step: 62 loss: 0.12931812 acc: 0.9547080993652344\n",
      "epoch: 39 step: 63 loss: 0.111821115 acc: 0.9556121826171875\n",
      "epoch: 39 step: 64 loss: 0.15684782 acc: 0.9469871520996094\n",
      "epoch: 39 step: 65 loss: 0.12566175 acc: 0.9532814025878906\n",
      "epoch: 39 step: 66 loss: 0.147707 acc: 0.9587783813476562\n",
      "epoch: 39 step: 67 loss: 0.14921376 acc: 0.956298828125\n",
      "epoch: 39 step: 68 loss: 0.11064942 acc: 0.9586029052734375\n",
      "epoch: 39 step: 69 loss: 0.12778465 acc: 0.9616813659667969\n",
      "epoch: 39 step: 70 loss: 0.14085141 acc: 0.94830322265625\n",
      "epoch: 39 step: 71 loss: 0.14093873 acc: 0.9527854919433594\n",
      "epoch: 39 step: 72 loss: 0.12160283 acc: 0.9521598815917969\n",
      "epoch: 39 step: 73 loss: 0.11362099 acc: 0.9522171020507812\n",
      "epoch: 39 step: 74 loss: 0.11546891 acc: 0.9558868408203125\n",
      "epoch: 39 step: 75 loss: 0.12639873 acc: 0.9523086547851562\n",
      "epoch: 39 step: 76 loss: 0.13149562 acc: 0.9582748413085938\n",
      "epoch: 39 step: 77 loss: 0.15668118 acc: 0.9548416137695312\n",
      "epoch: 39 step: 78 loss: 0.11364812 acc: 0.9612617492675781\n",
      "epoch: 39 step: 79 loss: 0.122084126 acc: 0.9591178894042969\n",
      "epoch: 39 step: 80 loss: 0.119097225 acc: 0.9589424133300781\n",
      "epoch: 39 step: 81 loss: 0.13082251 acc: 0.9557533264160156\n",
      "epoch: 39 step: 82 loss: 0.10380968 acc: 0.9632797241210938\n",
      "epoch: 39 step: 83 loss: 0.11676349 acc: 0.9630165100097656\n",
      "epoch: 39 step: 84 loss: 0.14007509 acc: 0.9603958129882812\n",
      "epoch: 39 step: 85 loss: 0.115162306 acc: 0.9584770202636719\n",
      "epoch: 39 step: 86 loss: 0.1744781 acc: 0.9397926330566406\n",
      "epoch: 39 step: 87 loss: 0.12465863 acc: 0.9622077941894531\n",
      "epoch: 39 step: 88 loss: 0.14731959 acc: 0.9445686340332031\n",
      "epoch: 39 step: 89 loss: 0.11809859 acc: 0.9552764892578125\n",
      "epoch: 39 step: 90 loss: 0.10968624 acc: 0.9580650329589844\n",
      "epoch: 39 step: 91 loss: 0.11193392 acc: 0.9598731994628906\n",
      "epoch: 39 step: 92 loss: 0.114137806 acc: 0.953460693359375\n",
      "epoch: 39 step: 93 loss: 0.1306879 acc: 0.9447288513183594\n",
      "epoch: 39 step: 94 loss: 0.11488497 acc: 0.9553985595703125\n",
      "epoch: 39 step: 95 loss: 0.1351733 acc: 0.9515647888183594\n",
      "epoch: 39 step: 96 loss: 0.12784751 acc: 0.9600944519042969\n",
      "epoch: 39 step: 97 loss: 0.109746546 acc: 0.9557876586914062\n",
      "epoch: 39 step: 98 loss: 0.11506843 acc: 0.9533195495605469\n",
      "epoch: 39 step: 99 loss: 0.11227724 acc: 0.9629135131835938\n",
      "epoch: 39 step: 100 loss: 0.1195259 acc: 0.9491043090820312\n",
      "epoch: 39 step: 101 loss: 0.10500422 acc: 0.9561576843261719\n",
      "epoch: 39 step: 102 loss: 0.13826893 acc: 0.9508552551269531\n",
      "epoch: 39 step: 103 loss: 0.14792198 acc: 0.9480514526367188\n",
      "epoch: 39 step: 104 loss: 0.119376354 acc: 0.9648551940917969\n",
      "epoch: 39 step: 105 loss: 0.1347793 acc: 0.9571037292480469\n",
      "epoch: 39 step: 106 loss: 0.13724431 acc: 0.9555244445800781\n",
      "epoch: 39 step: 107 loss: 0.12454082 acc: 0.957916259765625\n",
      "epoch: 39 step: 108 loss: 0.1103557 acc: 0.9539794921875\n",
      "epoch: 39 step: 109 loss: 0.105912924 acc: 0.9580154418945312\n",
      "epoch: 39 step: 110 loss: 0.11409931 acc: 0.9577674865722656\n",
      "epoch: 39 step: 111 loss: 0.1163502 acc: 0.9530258178710938\n",
      "epoch: 39 step: 112 loss: 0.11884143 acc: 0.956451416015625\n",
      "epoch: 39 step: 113 loss: 0.089614056 acc: 0.9574775695800781\n",
      "epoch: 39 step: 114 loss: 0.11308908 acc: 0.9622611999511719\n",
      "epoch: 39 step: 115 loss: 0.100937866 acc: 0.96240234375\n",
      "epoch: 39 step: 116 loss: 0.09812068 acc: 0.9645309448242188\n",
      "epoch: 39 step: 117 loss: 0.13276811 acc: 0.9620513916015625\n",
      "epoch: 39 step: 118 loss: 0.103536054 acc: 0.9666786193847656\n",
      "epoch: 39 step: 119 loss: 0.09930839 acc: 0.961273193359375\n",
      "epoch: 39 step: 120 loss: 0.1201808 acc: 0.9616928100585938\n",
      "epoch: 39 step: 121 loss: 0.106346525 acc: 0.9540901184082031\n",
      "epoch: 39 step: 122 loss: 0.11701442 acc: 0.9586448669433594\n",
      "epoch: 39 step: 123 loss: 0.10723955 acc: 0.9598770141601562\n",
      "epoch: 39 step: 124 loss: 0.10390826 acc: 0.9596383231026786\n",
      "epoch: 39 validation_loss: 0.118 validation_dice: 0.8142030336879581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39 test_dataset dice: 0.7301736323246983\n",
      "time cost 0.5358774304389954 min\n",
      "dice_best: 0.8274300650157543\n",
      "******************************** epoch  39  is finished. *********************************\n",
      "epoch: 40 step: 1 loss: 0.11776897 acc: 0.9541549682617188\n",
      "epoch: 40 step: 2 loss: 0.09666862 acc: 0.9645805358886719\n",
      "epoch: 40 step: 3 loss: 0.1166813 acc: 0.9542579650878906\n",
      "epoch: 40 step: 4 loss: 0.11111437 acc: 0.9569129943847656\n",
      "epoch: 40 step: 5 loss: 0.12506959 acc: 0.9598121643066406\n",
      "epoch: 40 step: 6 loss: 0.1395313 acc: 0.9488563537597656\n",
      "epoch: 40 step: 7 loss: 0.12756203 acc: 0.9574432373046875\n",
      "epoch: 40 step: 8 loss: 0.08637896 acc: 0.9628028869628906\n",
      "epoch: 40 step: 9 loss: 0.11626071 acc: 0.9546928405761719\n",
      "epoch: 40 step: 10 loss: 0.13735232 acc: 0.9473800659179688\n",
      "epoch: 40 step: 11 loss: 0.09478159 acc: 0.9620704650878906\n",
      "epoch: 40 step: 12 loss: 0.13530694 acc: 0.9504966735839844\n",
      "epoch: 40 step: 13 loss: 0.11836133 acc: 0.9554824829101562\n",
      "epoch: 40 step: 14 loss: 0.11538512 acc: 0.9554901123046875\n",
      "epoch: 40 step: 15 loss: 0.10074024 acc: 0.9572944641113281\n",
      "epoch: 40 step: 16 loss: 0.087736994 acc: 0.9637260437011719\n",
      "epoch: 40 step: 17 loss: 0.12346726 acc: 0.9540519714355469\n",
      "epoch: 40 step: 18 loss: 0.095686786 acc: 0.9615859985351562\n",
      "epoch: 40 step: 19 loss: 0.1176897 acc: 0.9587020874023438\n",
      "epoch: 40 step: 20 loss: 0.10682438 acc: 0.9566841125488281\n",
      "epoch: 40 step: 21 loss: 0.10257332 acc: 0.964813232421875\n",
      "epoch: 40 step: 22 loss: 0.09832019 acc: 0.9603462219238281\n",
      "epoch: 40 step: 23 loss: 0.103108734 acc: 0.9607429504394531\n",
      "epoch: 40 step: 24 loss: 0.10935844 acc: 0.9575347900390625\n",
      "epoch: 40 step: 25 loss: 0.10986227 acc: 0.9591407775878906\n",
      "epoch: 40 step: 26 loss: 0.098636374 acc: 0.9649581909179688\n",
      "epoch: 40 step: 27 loss: 0.120600596 acc: 0.9528350830078125\n",
      "epoch: 40 step: 28 loss: 0.12166806 acc: 0.9531784057617188\n",
      "epoch: 40 step: 29 loss: 0.14493154 acc: 0.9465713500976562\n",
      "epoch: 40 step: 30 loss: 0.1077683 acc: 0.9541244506835938\n",
      "epoch: 40 step: 31 loss: 0.12265035 acc: 0.954986572265625\n",
      "epoch: 40 step: 32 loss: 0.12530424 acc: 0.9460182189941406\n",
      "epoch: 40 step: 33 loss: 0.121328235 acc: 0.9575462341308594\n",
      "epoch: 40 step: 34 loss: 0.12131341 acc: 0.94696044921875\n",
      "epoch: 40 step: 35 loss: 0.11034989 acc: 0.9557418823242188\n",
      "epoch: 40 step: 36 loss: 0.117851876 acc: 0.9480628967285156\n",
      "epoch: 40 step: 37 loss: 0.112176165 acc: 0.9624061584472656\n",
      "epoch: 40 step: 38 loss: 0.09170648 acc: 0.958587646484375\n",
      "epoch: 40 step: 39 loss: 0.11992938 acc: 0.9487342834472656\n",
      "epoch: 40 step: 40 loss: 0.11321528 acc: 0.9598579406738281\n",
      "epoch: 40 step: 41 loss: 0.09565907 acc: 0.9701805114746094\n",
      "epoch: 40 step: 42 loss: 0.11233668 acc: 0.9694404602050781\n",
      "epoch: 40 step: 43 loss: 0.09845936 acc: 0.9573516845703125\n",
      "epoch: 40 step: 44 loss: 0.11924279 acc: 0.94915771484375\n",
      "epoch: 40 step: 45 loss: 0.12834233 acc: 0.9610557556152344\n",
      "epoch: 40 step: 46 loss: 0.112273134 acc: 0.9560737609863281\n",
      "epoch: 40 step: 47 loss: 0.09756996 acc: 0.9598159790039062\n",
      "epoch: 40 step: 48 loss: 0.09398987 acc: 0.9625015258789062\n",
      "epoch: 40 step: 49 loss: 0.16200627 acc: 0.9563446044921875\n",
      "epoch: 40 step: 50 loss: 0.12364997 acc: 0.9589004516601562\n",
      "epoch: 40 step: 51 loss: 0.13972475 acc: 0.9524612426757812\n",
      "epoch: 40 step: 52 loss: 0.10136071 acc: 0.9663543701171875\n",
      "epoch: 40 step: 53 loss: 0.102970816 acc: 0.9650382995605469\n",
      "epoch: 40 step: 54 loss: 0.1264041 acc: 0.9515800476074219\n",
      "epoch: 40 step: 55 loss: 0.098687515 acc: 0.9658699035644531\n",
      "epoch: 40 step: 56 loss: 0.09990145 acc: 0.962677001953125\n",
      "epoch: 40 step: 57 loss: 0.12611009 acc: 0.959442138671875\n",
      "epoch: 40 step: 58 loss: 0.13965982 acc: 0.9518623352050781\n",
      "epoch: 40 step: 59 loss: 0.11484372 acc: 0.957794189453125\n",
      "epoch: 40 step: 60 loss: 0.12651345 acc: 0.952911376953125\n",
      "epoch: 40 step: 61 loss: 0.1371838 acc: 0.9464073181152344\n",
      "epoch: 40 step: 62 loss: 0.12080078 acc: 0.9521293640136719\n",
      "epoch: 40 step: 63 loss: 0.14451036 acc: 0.9514122009277344\n",
      "epoch: 40 step: 64 loss: 0.11279233 acc: 0.9563941955566406\n",
      "epoch: 40 step: 65 loss: 0.11071589 acc: 0.9646034240722656\n",
      "epoch: 40 step: 66 loss: 0.11289078 acc: 0.9630508422851562\n",
      "epoch: 40 step: 67 loss: 0.1253215 acc: 0.9514617919921875\n",
      "epoch: 40 step: 68 loss: 0.12872161 acc: 0.9534454345703125\n",
      "epoch: 40 step: 69 loss: 0.09789882 acc: 0.9641227722167969\n",
      "epoch: 40 step: 70 loss: 0.11896924 acc: 0.958251953125\n",
      "epoch: 40 step: 71 loss: 0.13382065 acc: 0.9538154602050781\n",
      "epoch: 40 step: 72 loss: 0.11134737 acc: 0.9530563354492188\n",
      "epoch: 40 step: 73 loss: 0.11856794 acc: 0.9577102661132812\n",
      "epoch: 40 step: 74 loss: 0.15599276 acc: 0.9479789733886719\n",
      "epoch: 40 step: 75 loss: 0.13086453 acc: 0.94610595703125\n",
      "epoch: 40 step: 76 loss: 0.11293227 acc: 0.9512290954589844\n",
      "epoch: 40 step: 77 loss: 0.118180074 acc: 0.949615478515625\n",
      "epoch: 40 step: 78 loss: 0.1358172 acc: 0.9435195922851562\n",
      "epoch: 40 step: 79 loss: 0.11458799 acc: 0.9532012939453125\n",
      "epoch: 40 step: 80 loss: 0.10925723 acc: 0.9552803039550781\n",
      "epoch: 40 step: 81 loss: 0.13290282 acc: 0.9557991027832031\n",
      "epoch: 40 step: 82 loss: 0.12212809 acc: 0.9626007080078125\n",
      "epoch: 40 step: 83 loss: 0.15938553 acc: 0.9582290649414062\n",
      "epoch: 40 step: 84 loss: 0.13539821 acc: 0.9516181945800781\n",
      "epoch: 40 step: 85 loss: 0.12183345 acc: 0.9524993896484375\n",
      "epoch: 40 step: 86 loss: 0.13839398 acc: 0.9433631896972656\n",
      "epoch: 40 step: 87 loss: 0.115776375 acc: 0.951690673828125\n",
      "epoch: 40 step: 88 loss: 0.111858375 acc: 0.9542312622070312\n",
      "epoch: 40 step: 89 loss: 0.14273788 acc: 0.9515571594238281\n",
      "epoch: 40 step: 90 loss: 0.12930062 acc: 0.9586906433105469\n",
      "epoch: 40 step: 91 loss: 0.10695196 acc: 0.9617538452148438\n",
      "epoch: 40 step: 92 loss: 0.11663164 acc: 0.9544181823730469\n",
      "epoch: 40 step: 93 loss: 0.11801463 acc: 0.9552497863769531\n",
      "epoch: 40 step: 94 loss: 0.13252401 acc: 0.9555740356445312\n",
      "epoch: 40 step: 95 loss: 0.10692314 acc: 0.9521293640136719\n",
      "epoch: 40 step: 96 loss: 0.11979719 acc: 0.9529914855957031\n",
      "epoch: 40 step: 97 loss: 0.11176193 acc: 0.9536018371582031\n",
      "epoch: 40 step: 98 loss: 0.15004723 acc: 0.95513916015625\n",
      "epoch: 40 step: 99 loss: 0.12411927 acc: 0.9580154418945312\n",
      "epoch: 40 step: 100 loss: 0.11283863 acc: 0.9593391418457031\n",
      "epoch: 40 step: 101 loss: 0.12434645 acc: 0.9577445983886719\n",
      "epoch: 40 step: 102 loss: 0.12924872 acc: 0.9613723754882812\n",
      "epoch: 40 step: 103 loss: 0.117360175 acc: 0.9626960754394531\n",
      "epoch: 40 step: 104 loss: 0.13086812 acc: 0.9553260803222656\n",
      "epoch: 40 step: 105 loss: 0.111541115 acc: 0.9520454406738281\n",
      "epoch: 40 step: 106 loss: 0.110214986 acc: 0.9501724243164062\n",
      "epoch: 40 step: 107 loss: 0.12887475 acc: 0.9495582580566406\n",
      "epoch: 40 step: 108 loss: 0.14299293 acc: 0.9548568725585938\n",
      "epoch: 40 step: 109 loss: 0.1298618 acc: 0.9554710388183594\n",
      "epoch: 40 step: 110 loss: 0.11835336 acc: 0.9568443298339844\n",
      "epoch: 40 step: 111 loss: 0.11122646 acc: 0.9591865539550781\n",
      "epoch: 40 step: 112 loss: 0.13122104 acc: 0.9529342651367188\n",
      "epoch: 40 step: 113 loss: 0.10840598 acc: 0.9640655517578125\n",
      "epoch: 40 step: 114 loss: 0.12654018 acc: 0.9606590270996094\n",
      "epoch: 40 step: 115 loss: 0.12076427 acc: 0.9673194885253906\n",
      "epoch: 40 step: 116 loss: 0.15111887 acc: 0.9638290405273438\n",
      "epoch: 40 step: 117 loss: 0.10325361 acc: 0.967803955078125\n",
      "epoch: 40 step: 118 loss: 0.14793047 acc: 0.949981689453125\n",
      "epoch: 40 step: 119 loss: 0.13943642 acc: 0.9540252685546875\n",
      "epoch: 40 step: 120 loss: 0.13978098 acc: 0.9423065185546875\n",
      "epoch: 40 step: 121 loss: 0.12089849 acc: 0.9552536010742188\n",
      "epoch: 40 step: 122 loss: 0.144002 acc: 0.9442634582519531\n",
      "epoch: 40 step: 123 loss: 0.12165094 acc: 0.9473838806152344\n",
      "epoch: 40 step: 124 loss: 0.15921734 acc: 0.9307163783482143\n",
      "epoch: 40 validation_loss: 0.214 validation_dice: 0.7651920546413927\n",
      "epoch: 40 test_dataset dice: 0.6819543984060388\n",
      "time cost 0.5381566365559896 min\n",
      "dice_best: 0.8274300650157543\n",
      "******************************** epoch  40  is finished. *********************************\n",
      "epoch: 41 step: 1 loss: 0.13409661 acc: 0.9563560485839844\n",
      "epoch: 41 step: 2 loss: 0.1349124 acc: 0.9649391174316406\n",
      "epoch: 41 step: 3 loss: 0.13203345 acc: 0.9585456848144531\n",
      "epoch: 41 step: 4 loss: 0.2182282 acc: 0.9638137817382812\n",
      "epoch: 41 step: 5 loss: 0.20203508 acc: 0.9568824768066406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41 step: 6 loss: 0.14929269 acc: 0.9552497863769531\n",
      "epoch: 41 step: 7 loss: 0.16650835 acc: 0.9515609741210938\n",
      "epoch: 41 step: 8 loss: 0.15386997 acc: 0.9417877197265625\n",
      "epoch: 41 step: 9 loss: 0.19020696 acc: 0.9547843933105469\n",
      "epoch: 41 step: 10 loss: 0.18588777 acc: 0.9335708618164062\n",
      "epoch: 41 step: 11 loss: 0.16464433 acc: 0.9421348571777344\n",
      "epoch: 41 step: 12 loss: 0.15737641 acc: 0.948944091796875\n",
      "epoch: 41 step: 13 loss: 0.14523809 acc: 0.9560165405273438\n",
      "epoch: 41 step: 14 loss: 0.22090955 acc: 0.9421005249023438\n",
      "epoch: 41 step: 15 loss: 0.15621567 acc: 0.9441108703613281\n",
      "epoch: 41 step: 16 loss: 0.14163847 acc: 0.9412307739257812\n",
      "epoch: 41 step: 17 loss: 0.17071746 acc: 0.9516448974609375\n",
      "epoch: 41 step: 18 loss: 0.14177273 acc: 0.9501838684082031\n",
      "epoch: 41 step: 19 loss: 0.20716421 acc: 0.9404258728027344\n",
      "epoch: 41 step: 20 loss: 0.17613874 acc: 0.9477806091308594\n",
      "epoch: 41 step: 21 loss: 0.11878509 acc: 0.9594383239746094\n",
      "epoch: 41 step: 22 loss: 0.15751174 acc: 0.9537696838378906\n",
      "epoch: 41 step: 23 loss: 0.119908206 acc: 0.9602394104003906\n",
      "epoch: 41 step: 24 loss: 0.13680689 acc: 0.9587135314941406\n",
      "epoch: 41 step: 25 loss: 0.12461679 acc: 0.9609031677246094\n",
      "epoch: 41 step: 26 loss: 0.18545157 acc: 0.9448165893554688\n",
      "epoch: 41 step: 27 loss: 0.15850943 acc: 0.9516716003417969\n",
      "epoch: 41 step: 28 loss: 0.1523766 acc: 0.9482192993164062\n",
      "epoch: 41 step: 29 loss: 0.14062306 acc: 0.943511962890625\n",
      "epoch: 41 step: 30 loss: 0.15219858 acc: 0.9415664672851562\n",
      "epoch: 41 step: 31 loss: 0.14090718 acc: 0.9473304748535156\n",
      "epoch: 41 step: 32 loss: 0.12427771 acc: 0.9477958679199219\n",
      "epoch: 41 step: 33 loss: 0.12792638 acc: 0.9528045654296875\n",
      "epoch: 41 step: 34 loss: 0.11293367 acc: 0.967315673828125\n",
      "epoch: 41 step: 35 loss: 0.13614228 acc: 0.947479248046875\n",
      "epoch: 41 step: 36 loss: 0.13732243 acc: 0.9597511291503906\n",
      "epoch: 41 step: 37 loss: 0.13479216 acc: 0.9603385925292969\n",
      "epoch: 41 step: 38 loss: 0.110803135 acc: 0.96209716796875\n",
      "epoch: 41 step: 39 loss: 0.13108 acc: 0.9568557739257812\n",
      "epoch: 41 step: 40 loss: 0.13873899 acc: 0.9570465087890625\n",
      "epoch: 41 step: 41 loss: 0.13908394 acc: 0.9561386108398438\n",
      "epoch: 41 step: 42 loss: 0.1059052 acc: 0.9580039978027344\n",
      "epoch: 41 step: 43 loss: 0.12488674 acc: 0.9484024047851562\n",
      "epoch: 41 step: 44 loss: 0.12773538 acc: 0.9537734985351562\n",
      "epoch: 41 step: 45 loss: 0.12279891 acc: 0.9627265930175781\n",
      "epoch: 41 step: 46 loss: 0.14347512 acc: 0.9492454528808594\n",
      "epoch: 41 step: 47 loss: 0.12894991 acc: 0.9534797668457031\n",
      "epoch: 41 step: 48 loss: 0.12821212 acc: 0.9465065002441406\n",
      "epoch: 41 step: 49 loss: 0.09992223 acc: 0.9600067138671875\n",
      "epoch: 41 step: 50 loss: 0.114046946 acc: 0.9523048400878906\n",
      "epoch: 41 step: 51 loss: 0.09759877 acc: 0.9564704895019531\n",
      "epoch: 41 step: 52 loss: 0.13850376 acc: 0.9515533447265625\n",
      "epoch: 41 step: 53 loss: 0.11437268 acc: 0.9525375366210938\n",
      "epoch: 41 step: 54 loss: 0.15453506 acc: 0.9544944763183594\n",
      "epoch: 41 step: 55 loss: 0.10326792 acc: 0.9677925109863281\n",
      "epoch: 41 step: 56 loss: 0.14292681 acc: 0.9566307067871094\n",
      "epoch: 41 step: 57 loss: 0.116070084 acc: 0.9530410766601562\n",
      "epoch: 41 step: 58 loss: 0.15010609 acc: 0.9537696838378906\n",
      "epoch: 41 step: 59 loss: 0.12589248 acc: 0.950592041015625\n",
      "epoch: 41 step: 60 loss: 0.10247233 acc: 0.9556846618652344\n",
      "epoch: 41 step: 61 loss: 0.11376734 acc: 0.9555892944335938\n",
      "epoch: 41 step: 62 loss: 0.11118693 acc: 0.9557304382324219\n",
      "epoch: 41 step: 63 loss: 0.1159602 acc: 0.9552078247070312\n",
      "epoch: 41 step: 64 loss: 0.11259883 acc: 0.9559974670410156\n",
      "epoch: 41 step: 65 loss: 0.11936845 acc: 0.9595375061035156\n",
      "epoch: 41 step: 66 loss: 0.1578879 acc: 0.9484291076660156\n",
      "epoch: 41 step: 67 loss: 0.13055038 acc: 0.9532814025878906\n",
      "epoch: 41 step: 68 loss: 0.11505329 acc: 0.9562835693359375\n",
      "epoch: 41 step: 69 loss: 0.10660405 acc: 0.9548721313476562\n",
      "epoch: 41 step: 70 loss: 0.09287273 acc: 0.95458984375\n",
      "epoch: 41 step: 71 loss: 0.10686136 acc: 0.9529342651367188\n",
      "epoch: 41 step: 72 loss: 0.09713437 acc: 0.9595985412597656\n",
      "epoch: 41 step: 73 loss: 0.13295819 acc: 0.9500999450683594\n",
      "epoch: 41 step: 74 loss: 0.14294565 acc: 0.9433746337890625\n",
      "epoch: 41 step: 75 loss: 0.12074702 acc: 0.9526252746582031\n",
      "epoch: 41 step: 76 loss: 0.10915213 acc: 0.9624214172363281\n",
      "epoch: 41 step: 77 loss: 0.11828376 acc: 0.958099365234375\n",
      "epoch: 41 step: 78 loss: 0.13570821 acc: 0.9521942138671875\n",
      "epoch: 41 step: 79 loss: 0.113102034 acc: 0.9590911865234375\n",
      "epoch: 41 step: 80 loss: 0.10317342 acc: 0.9613037109375\n",
      "epoch: 41 step: 81 loss: 0.13007148 acc: 0.9488716125488281\n",
      "epoch: 41 step: 82 loss: 0.12527512 acc: 0.9505767822265625\n",
      "epoch: 41 step: 83 loss: 0.11210736 acc: 0.9590301513671875\n",
      "epoch: 41 step: 84 loss: 0.11306767 acc: 0.9626045227050781\n",
      "epoch: 41 step: 85 loss: 0.11829828 acc: 0.955841064453125\n",
      "epoch: 41 step: 86 loss: 0.11804255 acc: 0.9526901245117188\n",
      "epoch: 41 step: 87 loss: 0.109164536 acc: 0.958770751953125\n",
      "epoch: 41 step: 88 loss: 0.13851294 acc: 0.9511756896972656\n",
      "epoch: 41 step: 89 loss: 0.13453345 acc: 0.9447898864746094\n",
      "epoch: 41 step: 90 loss: 0.101701654 acc: 0.9531059265136719\n",
      "epoch: 41 step: 91 loss: 0.10181809 acc: 0.9561767578125\n",
      "epoch: 41 step: 92 loss: 0.104669 acc: 0.9585609436035156\n",
      "epoch: 41 step: 93 loss: 0.12289842 acc: 0.950408935546875\n",
      "epoch: 41 step: 94 loss: 0.11220147 acc: 0.95648193359375\n",
      "epoch: 41 step: 95 loss: 0.09929787 acc: 0.9614219665527344\n",
      "epoch: 41 step: 96 loss: 0.12997746 acc: 0.9533462524414062\n",
      "epoch: 41 step: 97 loss: 0.121206045 acc: 0.9598846435546875\n",
      "epoch: 41 step: 98 loss: 0.14281625 acc: 0.9531593322753906\n",
      "epoch: 41 step: 99 loss: 0.112954386 acc: 0.9567489624023438\n",
      "epoch: 41 step: 100 loss: 0.10707669 acc: 0.953033447265625\n",
      "epoch: 41 step: 101 loss: 0.14213593 acc: 0.9455223083496094\n",
      "epoch: 41 step: 102 loss: 0.12933736 acc: 0.9538345336914062\n",
      "epoch: 41 step: 103 loss: 0.1459191 acc: 0.9454116821289062\n",
      "epoch: 41 step: 104 loss: 0.10702285 acc: 0.9568901062011719\n",
      "epoch: 41 step: 105 loss: 0.11832015 acc: 0.9448204040527344\n",
      "epoch: 41 step: 106 loss: 0.11464386 acc: 0.953399658203125\n",
      "epoch: 41 step: 107 loss: 0.097066015 acc: 0.9550628662109375\n",
      "epoch: 41 step: 108 loss: 0.10037361 acc: 0.9560356140136719\n",
      "epoch: 41 step: 109 loss: 0.11656772 acc: 0.9488906860351562\n",
      "epoch: 41 step: 110 loss: 0.10178001 acc: 0.9672470092773438\n",
      "epoch: 41 step: 111 loss: 0.10878332 acc: 0.9553108215332031\n",
      "epoch: 41 step: 112 loss: 0.10772084 acc: 0.9659767150878906\n",
      "epoch: 41 step: 113 loss: 0.09703782 acc: 0.9667015075683594\n",
      "epoch: 41 step: 114 loss: 0.13008115 acc: 0.9554595947265625\n",
      "epoch: 41 step: 115 loss: 0.11010779 acc: 0.9609489440917969\n",
      "epoch: 41 step: 116 loss: 0.123612754 acc: 0.9680442810058594\n",
      "epoch: 41 step: 117 loss: 0.10190647 acc: 0.96923828125\n",
      "epoch: 41 step: 118 loss: 0.100701906 acc: 0.968292236328125\n",
      "epoch: 41 step: 119 loss: 0.09263443 acc: 0.9627914428710938\n",
      "epoch: 41 step: 120 loss: 0.13858949 acc: 0.9539527893066406\n",
      "epoch: 41 step: 121 loss: 0.119966835 acc: 0.9525909423828125\n",
      "epoch: 41 step: 122 loss: 0.10512322 acc: 0.9501686096191406\n",
      "epoch: 41 step: 123 loss: 0.10530402 acc: 0.9503135681152344\n",
      "epoch: 41 step: 124 loss: 0.11729616 acc: 0.9546421595982143\n",
      "epoch: 41 validation_loss: 0.112 validation_dice: 0.8007549126156951\n",
      "epoch: 41 test_dataset dice: 0.7435337683661506\n",
      "time cost 0.5365829547246297 min\n",
      "dice_best: 0.8274300650157543\n",
      "******************************** epoch  41  is finished. *********************************\n",
      "epoch: 42 step: 1 loss: 0.10195375 acc: 0.9602699279785156\n",
      "epoch: 42 step: 2 loss: 0.11462695 acc: 0.949371337890625\n",
      "epoch: 42 step: 3 loss: 0.09390366 acc: 0.9571647644042969\n",
      "epoch: 42 step: 4 loss: 0.14268929 acc: 0.9530181884765625\n",
      "epoch: 42 step: 5 loss: 0.115380734 acc: 0.9525680541992188\n",
      "epoch: 42 step: 6 loss: 0.12139188 acc: 0.9547805786132812\n",
      "epoch: 42 step: 7 loss: 0.10860992 acc: 0.9603080749511719\n",
      "epoch: 42 step: 8 loss: 0.10168631 acc: 0.9639663696289062\n",
      "epoch: 42 step: 9 loss: 0.10635827 acc: 0.9629554748535156\n",
      "epoch: 42 step: 10 loss: 0.12032695 acc: 0.96026611328125\n",
      "epoch: 42 step: 11 loss: 0.11893721 acc: 0.9600372314453125\n",
      "epoch: 42 step: 12 loss: 0.0955733 acc: 0.9623985290527344\n",
      "epoch: 42 step: 13 loss: 0.12638786 acc: 0.9587059020996094\n",
      "epoch: 42 step: 14 loss: 0.08852778 acc: 0.9652519226074219\n",
      "epoch: 42 step: 15 loss: 0.1031322 acc: 0.9582252502441406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42 step: 16 loss: 0.11674918 acc: 0.9586868286132812\n",
      "epoch: 42 step: 17 loss: 0.09585406 acc: 0.9565658569335938\n",
      "epoch: 42 step: 18 loss: 0.10545368 acc: 0.9610252380371094\n",
      "epoch: 42 step: 19 loss: 0.10794025 acc: 0.9516487121582031\n",
      "epoch: 42 step: 20 loss: 0.107186265 acc: 0.9552955627441406\n",
      "epoch: 42 step: 21 loss: 0.12637062 acc: 0.9565315246582031\n",
      "epoch: 42 step: 22 loss: 0.11648814 acc: 0.9610061645507812\n",
      "epoch: 42 step: 23 loss: 0.11521017 acc: 0.9499626159667969\n",
      "epoch: 42 step: 24 loss: 0.10991148 acc: 0.9538803100585938\n",
      "epoch: 42 step: 25 loss: 0.11059635 acc: 0.9515953063964844\n",
      "epoch: 42 step: 26 loss: 0.106575534 acc: 0.9574508666992188\n",
      "epoch: 42 step: 27 loss: 0.12305637 acc: 0.9522361755371094\n",
      "epoch: 42 step: 28 loss: 0.12917252 acc: 0.9510612487792969\n",
      "epoch: 42 step: 29 loss: 0.095723584 acc: 0.96466064453125\n",
      "epoch: 42 step: 30 loss: 0.08305338 acc: 0.9675102233886719\n",
      "epoch: 42 step: 31 loss: 0.13002135 acc: 0.9590072631835938\n",
      "epoch: 42 step: 32 loss: 0.10855837 acc: 0.965850830078125\n",
      "epoch: 42 step: 33 loss: 0.10199667 acc: 0.9623985290527344\n",
      "epoch: 42 step: 34 loss: 0.10917779 acc: 0.9644622802734375\n",
      "epoch: 42 step: 35 loss: 0.09708121 acc: 0.9602928161621094\n",
      "epoch: 42 step: 36 loss: 0.09932231 acc: 0.9613189697265625\n",
      "epoch: 42 step: 37 loss: 0.11080348 acc: 0.9558143615722656\n",
      "epoch: 42 step: 38 loss: 0.12103469 acc: 0.9515037536621094\n",
      "epoch: 42 step: 39 loss: 0.098264135 acc: 0.9575920104980469\n",
      "epoch: 42 step: 40 loss: 0.09824629 acc: 0.9571266174316406\n",
      "epoch: 42 step: 41 loss: 0.10432263 acc: 0.9518280029296875\n",
      "epoch: 42 step: 42 loss: 0.10894799 acc: 0.9533958435058594\n",
      "epoch: 42 step: 43 loss: 0.14618096 acc: 0.9609336853027344\n",
      "epoch: 42 step: 44 loss: 0.12994382 acc: 0.95428466796875\n",
      "epoch: 42 step: 45 loss: 0.10759675 acc: 0.9601669311523438\n",
      "epoch: 42 step: 46 loss: 0.09339183 acc: 0.9589767456054688\n",
      "epoch: 42 step: 47 loss: 0.09672617 acc: 0.9649543762207031\n",
      "epoch: 42 step: 48 loss: 0.12601669 acc: 0.9601860046386719\n",
      "epoch: 42 step: 49 loss: 0.07705463 acc: 0.9698829650878906\n",
      "epoch: 42 step: 50 loss: 0.10614299 acc: 0.9580917358398438\n",
      "epoch: 42 step: 51 loss: 0.12760615 acc: 0.9598388671875\n",
      "epoch: 42 step: 52 loss: 0.089967005 acc: 0.9643363952636719\n",
      "epoch: 42 step: 53 loss: 0.11325993 acc: 0.9546585083007812\n",
      "epoch: 42 step: 54 loss: 0.10214598 acc: 0.9535751342773438\n",
      "epoch: 42 step: 55 loss: 0.09559355 acc: 0.9560623168945312\n",
      "epoch: 42 step: 56 loss: 0.14212677 acc: 0.9506149291992188\n",
      "epoch: 42 step: 57 loss: 0.08765536 acc: 0.9599952697753906\n",
      "epoch: 42 step: 58 loss: 0.11624173 acc: 0.9553298950195312\n",
      "epoch: 42 step: 59 loss: 0.11098957 acc: 0.9556388854980469\n",
      "epoch: 42 step: 60 loss: 0.09311336 acc: 0.9644660949707031\n",
      "epoch: 42 step: 61 loss: 0.10042531 acc: 0.9628753662109375\n",
      "epoch: 42 step: 62 loss: 0.112596884 acc: 0.9523849487304688\n",
      "epoch: 42 step: 63 loss: 0.1115334 acc: 0.9661026000976562\n",
      "epoch: 42 step: 64 loss: 0.10452868 acc: 0.9590568542480469\n",
      "epoch: 42 step: 65 loss: 0.12665673 acc: 0.9547538757324219\n",
      "epoch: 42 step: 66 loss: 0.10036664 acc: 0.9628334045410156\n",
      "epoch: 42 step: 67 loss: 0.089117154 acc: 0.962493896484375\n",
      "epoch: 42 step: 68 loss: 0.11298106 acc: 0.9528007507324219\n",
      "epoch: 42 step: 69 loss: 0.08650979 acc: 0.9590072631835938\n",
      "epoch: 42 step: 70 loss: 0.10865709 acc: 0.9526748657226562\n",
      "epoch: 42 step: 71 loss: 0.116698794 acc: 0.947998046875\n",
      "epoch: 42 step: 72 loss: 0.105022736 acc: 0.9532279968261719\n",
      "epoch: 42 step: 73 loss: 0.10513996 acc: 0.955474853515625\n",
      "epoch: 42 step: 74 loss: 0.10625984 acc: 0.9560050964355469\n",
      "epoch: 42 step: 75 loss: 0.10311694 acc: 0.9688758850097656\n",
      "epoch: 42 step: 76 loss: 0.11687406 acc: 0.9637641906738281\n",
      "epoch: 42 step: 77 loss: 0.1201603 acc: 0.9637908935546875\n",
      "epoch: 42 step: 78 loss: 0.11128428 acc: 0.9616851806640625\n",
      "epoch: 42 step: 79 loss: 0.09272249 acc: 0.9686508178710938\n",
      "epoch: 42 step: 80 loss: 0.086347334 acc: 0.9650230407714844\n",
      "epoch: 42 step: 81 loss: 0.10185283 acc: 0.95867919921875\n",
      "epoch: 42 step: 82 loss: 0.10039628 acc: 0.9605293273925781\n",
      "epoch: 42 step: 83 loss: 0.10887567 acc: 0.9602851867675781\n",
      "epoch: 42 step: 84 loss: 0.09803024 acc: 0.9555320739746094\n",
      "epoch: 42 step: 85 loss: 0.10917379 acc: 0.9535560607910156\n",
      "epoch: 42 step: 86 loss: 0.10522192 acc: 0.9568748474121094\n",
      "epoch: 42 step: 87 loss: 0.10534271 acc: 0.9564323425292969\n",
      "epoch: 42 step: 88 loss: 0.10050738 acc: 0.9636268615722656\n",
      "epoch: 42 step: 89 loss: 0.114785455 acc: 0.9513931274414062\n",
      "epoch: 42 step: 90 loss: 0.09566721 acc: 0.9596519470214844\n",
      "epoch: 42 step: 91 loss: 0.111356266 acc: 0.9538154602050781\n",
      "epoch: 42 step: 92 loss: 0.10482601 acc: 0.9604911804199219\n",
      "epoch: 42 step: 93 loss: 0.09443893 acc: 0.9615287780761719\n",
      "epoch: 42 step: 94 loss: 0.106354095 acc: 0.9606170654296875\n",
      "epoch: 42 step: 95 loss: 0.11766437 acc: 0.9549713134765625\n",
      "epoch: 42 step: 96 loss: 0.089002736 acc: 0.9645347595214844\n",
      "epoch: 42 step: 97 loss: 0.10066074 acc: 0.9600868225097656\n",
      "epoch: 42 step: 98 loss: 0.08894176 acc: 0.963592529296875\n",
      "epoch: 42 step: 99 loss: 0.10595042 acc: 0.9638595581054688\n",
      "epoch: 42 step: 100 loss: 0.11007477 acc: 0.953094482421875\n",
      "epoch: 42 step: 101 loss: 0.09182551 acc: 0.9598617553710938\n",
      "epoch: 42 step: 102 loss: 0.08936529 acc: 0.9622306823730469\n",
      "epoch: 42 step: 103 loss: 0.10440585 acc: 0.9502029418945312\n",
      "epoch: 42 step: 104 loss: 0.11214263 acc: 0.9530487060546875\n",
      "epoch: 42 step: 105 loss: 0.10592538 acc: 0.9523735046386719\n",
      "epoch: 42 step: 106 loss: 0.09310443 acc: 0.9546890258789062\n",
      "epoch: 42 step: 107 loss: 0.11101466 acc: 0.9582366943359375\n",
      "epoch: 42 step: 108 loss: 0.116483584 acc: 0.9516372680664062\n",
      "epoch: 42 step: 109 loss: 0.081035115 acc: 0.9645652770996094\n",
      "epoch: 42 step: 110 loss: 0.106920436 acc: 0.9548072814941406\n",
      "epoch: 42 step: 111 loss: 0.10827222 acc: 0.9504623413085938\n",
      "epoch: 42 step: 112 loss: 0.08705254 acc: 0.9681549072265625\n",
      "epoch: 42 step: 113 loss: 0.08383885 acc: 0.9592742919921875\n",
      "epoch: 42 step: 114 loss: 0.12084862 acc: 0.9529495239257812\n",
      "epoch: 42 step: 115 loss: 0.10493453 acc: 0.9542694091796875\n",
      "epoch: 42 step: 116 loss: 0.12933548 acc: 0.9448890686035156\n",
      "epoch: 42 step: 117 loss: 0.10783316 acc: 0.9523773193359375\n",
      "epoch: 42 step: 118 loss: 0.12449502 acc: 0.94659423828125\n",
      "epoch: 42 step: 119 loss: 0.09881056 acc: 0.9521865844726562\n",
      "epoch: 42 step: 120 loss: 0.09632644 acc: 0.9593124389648438\n",
      "epoch: 42 step: 121 loss: 0.097145565 acc: 0.9609603881835938\n",
      "epoch: 42 step: 122 loss: 0.13114013 acc: 0.9466972351074219\n",
      "epoch: 42 step: 123 loss: 0.09570274 acc: 0.9608192443847656\n",
      "epoch: 42 step: 124 loss: 0.15914348 acc: 0.9603533063616071\n",
      "epoch: 42 validation_loss: 0.104 validation_dice: 0.824840665897746\n",
      "epoch: 42 test_dataset dice: 0.7070325811962277\n",
      "time cost 0.5380159656206767 min\n",
      "dice_best: 0.8274300650157543\n",
      "******************************** epoch  42  is finished. *********************************\n",
      "epoch: 43 step: 1 loss: 0.105167344 acc: 0.9585914611816406\n",
      "epoch: 43 step: 2 loss: 0.099067606 acc: 0.9630928039550781\n",
      "epoch: 43 step: 3 loss: 0.113617375 acc: 0.9620590209960938\n",
      "epoch: 43 step: 4 loss: 0.08587823 acc: 0.9682540893554688\n",
      "epoch: 43 step: 5 loss: 0.121135056 acc: 0.9554481506347656\n",
      "epoch: 43 step: 6 loss: 0.08719459 acc: 0.9692039489746094\n",
      "epoch: 43 step: 7 loss: 0.10517247 acc: 0.9613609313964844\n",
      "epoch: 43 step: 8 loss: 0.11248911 acc: 0.9553451538085938\n",
      "epoch: 43 step: 9 loss: 0.09259627 acc: 0.9569778442382812\n",
      "epoch: 43 step: 10 loss: 0.0959479 acc: 0.9599723815917969\n",
      "epoch: 43 step: 11 loss: 0.09919657 acc: 0.9563255310058594\n",
      "epoch: 43 step: 12 loss: 0.118172675 acc: 0.9499893188476562\n",
      "epoch: 43 step: 13 loss: 0.11302968 acc: 0.9586410522460938\n",
      "epoch: 43 step: 14 loss: 0.112956055 acc: 0.9522514343261719\n",
      "epoch: 43 step: 15 loss: 0.09697387 acc: 0.9603500366210938\n",
      "epoch: 43 step: 16 loss: 0.09369499 acc: 0.9618339538574219\n",
      "epoch: 43 step: 17 loss: 0.10902721 acc: 0.9586868286132812\n",
      "epoch: 43 step: 18 loss: 0.11258593 acc: 0.954345703125\n",
      "epoch: 43 step: 19 loss: 0.13220799 acc: 0.9489479064941406\n",
      "epoch: 43 step: 20 loss: 0.107421555 acc: 0.95068359375\n",
      "epoch: 43 step: 21 loss: 0.10177616 acc: 0.9589424133300781\n",
      "epoch: 43 step: 22 loss: 0.10880475 acc: 0.9604911804199219\n",
      "epoch: 43 step: 23 loss: 0.104307495 acc: 0.9545211791992188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43 step: 24 loss: 0.122574694 acc: 0.9565925598144531\n",
      "epoch: 43 step: 25 loss: 0.096695215 acc: 0.9627113342285156\n",
      "epoch: 43 step: 26 loss: 0.090266556 acc: 0.9594650268554688\n",
      "epoch: 43 step: 27 loss: 0.123931356 acc: 0.9467353820800781\n",
      "epoch: 43 step: 28 loss: 0.1042315 acc: 0.9610061645507812\n",
      "epoch: 43 step: 29 loss: 0.09050707 acc: 0.9616813659667969\n",
      "epoch: 43 step: 30 loss: 0.0832597 acc: 0.9644737243652344\n",
      "epoch: 43 step: 31 loss: 0.11232223 acc: 0.9601097106933594\n",
      "epoch: 43 step: 32 loss: 0.10675694 acc: 0.9607086181640625\n",
      "epoch: 43 step: 33 loss: 0.08879818 acc: 0.9678573608398438\n",
      "epoch: 43 step: 34 loss: 0.10261753 acc: 0.9615669250488281\n",
      "epoch: 43 step: 35 loss: 0.10204594 acc: 0.9584732055664062\n",
      "epoch: 43 step: 36 loss: 0.1147998 acc: 0.9632911682128906\n",
      "epoch: 43 step: 37 loss: 0.124717474 acc: 0.9580307006835938\n",
      "epoch: 43 step: 38 loss: 0.09751613 acc: 0.9642066955566406\n",
      "epoch: 43 step: 39 loss: 0.10413701 acc: 0.9625282287597656\n",
      "epoch: 43 step: 40 loss: 0.10703625 acc: 0.9562950134277344\n",
      "epoch: 43 step: 41 loss: 0.0947131 acc: 0.9610061645507812\n",
      "epoch: 43 step: 42 loss: 0.10449005 acc: 0.960662841796875\n",
      "epoch: 43 step: 43 loss: 0.10618057 acc: 0.9611320495605469\n",
      "epoch: 43 step: 44 loss: 0.10375857 acc: 0.9550094604492188\n",
      "epoch: 43 step: 45 loss: 0.109732 acc: 0.9659271240234375\n",
      "epoch: 43 step: 46 loss: 0.15786754 acc: 0.9449310302734375\n",
      "epoch: 43 step: 47 loss: 0.10113919 acc: 0.9560470581054688\n",
      "epoch: 43 step: 48 loss: 0.11119606 acc: 0.9509429931640625\n",
      "epoch: 43 step: 49 loss: 0.11193898 acc: 0.9544677734375\n",
      "epoch: 43 step: 50 loss: 0.09914487 acc: 0.9556808471679688\n",
      "epoch: 43 step: 51 loss: 0.100005046 acc: 0.9581336975097656\n",
      "epoch: 43 step: 52 loss: 0.109127164 acc: 0.9571914672851562\n",
      "epoch: 43 step: 53 loss: 0.09692427 acc: 0.961517333984375\n",
      "epoch: 43 step: 54 loss: 0.1072624 acc: 0.9633865356445312\n",
      "epoch: 43 step: 55 loss: 0.10290703 acc: 0.9606285095214844\n",
      "epoch: 43 step: 56 loss: 0.097420424 acc: 0.9650001525878906\n",
      "epoch: 43 step: 57 loss: 0.11569829 acc: 0.9542007446289062\n",
      "epoch: 43 step: 58 loss: 0.11261479 acc: 0.9615211486816406\n",
      "epoch: 43 step: 59 loss: 0.10538237 acc: 0.9500045776367188\n",
      "epoch: 43 step: 60 loss: 0.10541954 acc: 0.9536285400390625\n",
      "epoch: 43 step: 61 loss: 0.10881154 acc: 0.9579315185546875\n",
      "epoch: 43 step: 62 loss: 0.09303656 acc: 0.967559814453125\n",
      "epoch: 43 step: 63 loss: 0.08252729 acc: 0.9657974243164062\n",
      "epoch: 43 step: 64 loss: 0.1252417 acc: 0.9541587829589844\n",
      "epoch: 43 step: 65 loss: 0.09171218 acc: 0.9626960754394531\n",
      "epoch: 43 step: 66 loss: 0.08657746 acc: 0.9674110412597656\n",
      "epoch: 43 step: 67 loss: 0.0934573 acc: 0.9697151184082031\n",
      "epoch: 43 step: 68 loss: 0.08851338 acc: 0.960906982421875\n",
      "epoch: 43 step: 69 loss: 0.1136287 acc: 0.9488754272460938\n",
      "epoch: 43 step: 70 loss: 0.1053557 acc: 0.9619789123535156\n",
      "epoch: 43 step: 71 loss: 0.091056064 acc: 0.9646835327148438\n",
      "epoch: 43 step: 72 loss: 0.12415806 acc: 0.948516845703125\n",
      "epoch: 43 step: 73 loss: 0.12508196 acc: 0.9558944702148438\n",
      "epoch: 43 step: 74 loss: 0.1101111 acc: 0.9564704895019531\n",
      "epoch: 43 step: 75 loss: 0.10593813 acc: 0.9650497436523438\n",
      "epoch: 43 step: 76 loss: 0.10820552 acc: 0.9568901062011719\n",
      "epoch: 43 step: 77 loss: 0.107465416 acc: 0.9618492126464844\n",
      "epoch: 43 step: 78 loss: 0.09930959 acc: 0.9577217102050781\n",
      "epoch: 43 step: 79 loss: 0.11554686 acc: 0.9502143859863281\n",
      "epoch: 43 step: 80 loss: 0.09837201 acc: 0.9617843627929688\n",
      "epoch: 43 step: 81 loss: 0.088432476 acc: 0.9592475891113281\n",
      "epoch: 43 step: 82 loss: 0.090961844 acc: 0.9577369689941406\n",
      "epoch: 43 step: 83 loss: 0.095265396 acc: 0.9676971435546875\n",
      "epoch: 43 step: 84 loss: 0.08135203 acc: 0.968170166015625\n",
      "epoch: 43 step: 85 loss: 0.10614952 acc: 0.9617958068847656\n",
      "epoch: 43 step: 86 loss: 0.10042541 acc: 0.9698753356933594\n",
      "epoch: 43 step: 87 loss: 0.08537207 acc: 0.9633865356445312\n",
      "epoch: 43 step: 88 loss: 0.09999028 acc: 0.9600906372070312\n",
      "epoch: 43 step: 89 loss: 0.0930421 acc: 0.9647789001464844\n",
      "epoch: 43 step: 90 loss: 0.11464496 acc: 0.9587974548339844\n",
      "epoch: 43 step: 91 loss: 0.108643 acc: 0.9626274108886719\n",
      "epoch: 43 step: 92 loss: 0.09526772 acc: 0.9603958129882812\n",
      "epoch: 43 step: 93 loss: 0.11442896 acc: 0.9563407897949219\n",
      "epoch: 43 step: 94 loss: 0.095035404 acc: 0.9544181823730469\n",
      "epoch: 43 step: 95 loss: 0.09343205 acc: 0.9585533142089844\n",
      "epoch: 43 step: 96 loss: 0.0866321 acc: 0.9591026306152344\n",
      "epoch: 43 step: 97 loss: 0.10071646 acc: 0.9521217346191406\n",
      "epoch: 43 step: 98 loss: 0.10980536 acc: 0.9589042663574219\n",
      "epoch: 43 step: 99 loss: 0.12104698 acc: 0.9621162414550781\n",
      "epoch: 43 step: 100 loss: 0.08611771 acc: 0.9636154174804688\n",
      "epoch: 43 step: 101 loss: 0.11706602 acc: 0.9584922790527344\n",
      "epoch: 43 step: 102 loss: 0.11076134 acc: 0.9643936157226562\n",
      "epoch: 43 step: 103 loss: 0.12962392 acc: 0.9550514221191406\n",
      "epoch: 43 step: 104 loss: 0.09416623 acc: 0.9631004333496094\n",
      "epoch: 43 step: 105 loss: 0.10753913 acc: 0.9527320861816406\n",
      "epoch: 43 step: 106 loss: 0.12185496 acc: 0.9524307250976562\n",
      "epoch: 43 step: 107 loss: 0.11639305 acc: 0.9485816955566406\n",
      "epoch: 43 step: 108 loss: 0.11797139 acc: 0.957855224609375\n",
      "epoch: 43 step: 109 loss: 0.09461101 acc: 0.9576644897460938\n",
      "epoch: 43 step: 110 loss: 0.12825169 acc: 0.9505081176757812\n",
      "epoch: 43 step: 111 loss: 0.10052449 acc: 0.9645233154296875\n",
      "epoch: 43 step: 112 loss: 0.0884764 acc: 0.9619636535644531\n",
      "epoch: 43 step: 113 loss: 0.107852586 acc: 0.9585762023925781\n",
      "epoch: 43 step: 114 loss: 0.09200499 acc: 0.9684410095214844\n",
      "epoch: 43 step: 115 loss: 0.099768594 acc: 0.9583358764648438\n",
      "epoch: 43 step: 116 loss: 0.10066288 acc: 0.9560813903808594\n",
      "epoch: 43 step: 117 loss: 0.13665646 acc: 0.9535560607910156\n",
      "epoch: 43 step: 118 loss: 0.09542329 acc: 0.9642219543457031\n",
      "epoch: 43 step: 119 loss: 0.098357335 acc: 0.9618453979492188\n",
      "epoch: 43 step: 120 loss: 0.10692529 acc: 0.9568977355957031\n",
      "epoch: 43 step: 121 loss: 0.08319775 acc: 0.9664497375488281\n",
      "epoch: 43 step: 122 loss: 0.10201996 acc: 0.9578704833984375\n",
      "epoch: 43 step: 123 loss: 0.11647392 acc: 0.9540328979492188\n",
      "epoch: 43 step: 124 loss: 0.11866736 acc: 0.9455392020089286\n",
      "epoch: 43 validation_loss: 0.107 validation_dice: 0.8208636687960612\n",
      "epoch: 43 test_dataset dice: 0.7367107587307664\n",
      "time cost 0.5399032791455587 min\n",
      "dice_best: 0.8274300650157543\n",
      "******************************** epoch  43  is finished. *********************************\n",
      "epoch: 44 step: 1 loss: 0.09507395 acc: 0.9488449096679688\n",
      "epoch: 44 step: 2 loss: 0.09435857 acc: 0.9529342651367188\n",
      "epoch: 44 step: 3 loss: 0.110775016 acc: 0.9510574340820312\n",
      "epoch: 44 step: 4 loss: 0.09958223 acc: 0.9575576782226562\n",
      "epoch: 44 step: 5 loss: 0.11860344 acc: 0.9573936462402344\n",
      "epoch: 44 step: 6 loss: 0.093608126 acc: 0.9646835327148438\n",
      "epoch: 44 step: 7 loss: 0.08740514 acc: 0.9638175964355469\n",
      "epoch: 44 step: 8 loss: 0.13327093 acc: 0.9496803283691406\n",
      "epoch: 44 step: 9 loss: 0.098984554 acc: 0.9660186767578125\n",
      "epoch: 44 step: 10 loss: 0.104712196 acc: 0.961669921875\n",
      "epoch: 44 step: 11 loss: 0.11288045 acc: 0.9624862670898438\n",
      "epoch: 44 step: 12 loss: 0.09137508 acc: 0.9639472961425781\n",
      "epoch: 44 step: 13 loss: 0.09031158 acc: 0.9633827209472656\n",
      "epoch: 44 step: 14 loss: 0.10703087 acc: 0.9550933837890625\n",
      "epoch: 44 step: 15 loss: 0.10576013 acc: 0.9533500671386719\n",
      "epoch: 44 step: 16 loss: 0.11278474 acc: 0.9534378051757812\n",
      "epoch: 44 step: 17 loss: 0.10670655 acc: 0.954010009765625\n",
      "epoch: 44 step: 18 loss: 0.11559573 acc: 0.954437255859375\n",
      "epoch: 44 step: 19 loss: 0.10690051 acc: 0.9564743041992188\n",
      "epoch: 44 step: 20 loss: 0.10405898 acc: 0.9568443298339844\n",
      "epoch: 44 step: 21 loss: 0.10839394 acc: 0.955535888671875\n",
      "epoch: 44 step: 22 loss: 0.091622666 acc: 0.9633331298828125\n",
      "epoch: 44 step: 23 loss: 0.10202682 acc: 0.9617881774902344\n",
      "epoch: 44 step: 24 loss: 0.104879126 acc: 0.9564781188964844\n",
      "epoch: 44 step: 25 loss: 0.09780957 acc: 0.9559783935546875\n",
      "epoch: 44 step: 26 loss: 0.092713594 acc: 0.9615478515625\n",
      "epoch: 44 step: 27 loss: 0.09182761 acc: 0.9568748474121094\n",
      "epoch: 44 step: 28 loss: 0.11901953 acc: 0.9535713195800781\n",
      "epoch: 44 step: 29 loss: 0.085241236 acc: 0.9612388610839844\n",
      "epoch: 44 step: 30 loss: 0.1111266 acc: 0.9567413330078125\n",
      "epoch: 44 step: 31 loss: 0.09286461 acc: 0.961578369140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44 step: 32 loss: 0.08449362 acc: 0.9645576477050781\n",
      "epoch: 44 step: 33 loss: 0.099406384 acc: 0.9584159851074219\n",
      "epoch: 44 step: 34 loss: 0.09830796 acc: 0.9603462219238281\n",
      "epoch: 44 step: 35 loss: 0.09431691 acc: 0.9596443176269531\n",
      "epoch: 44 step: 36 loss: 0.0860267 acc: 0.9672012329101562\n",
      "epoch: 44 step: 37 loss: 0.090924524 acc: 0.9607734680175781\n",
      "epoch: 44 step: 38 loss: 0.11803219 acc: 0.9517707824707031\n",
      "epoch: 44 step: 39 loss: 0.11595011 acc: 0.9557609558105469\n",
      "epoch: 44 step: 40 loss: 0.10407762 acc: 0.9551963806152344\n",
      "epoch: 44 step: 41 loss: 0.105637856 acc: 0.9569091796875\n",
      "epoch: 44 step: 42 loss: 0.08406586 acc: 0.9605216979980469\n",
      "epoch: 44 step: 43 loss: 0.11608949 acc: 0.9519386291503906\n",
      "epoch: 44 step: 44 loss: 0.11273035 acc: 0.951629638671875\n",
      "epoch: 44 step: 45 loss: 0.112674095 acc: 0.9518013000488281\n",
      "epoch: 44 step: 46 loss: 0.09982312 acc: 0.9598312377929688\n",
      "epoch: 44 step: 47 loss: 0.09204028 acc: 0.965240478515625\n",
      "epoch: 44 step: 48 loss: 0.10858161 acc: 0.9567375183105469\n",
      "epoch: 44 step: 49 loss: 0.12388758 acc: 0.9554939270019531\n",
      "epoch: 44 step: 50 loss: 0.09557724 acc: 0.9621658325195312\n",
      "epoch: 44 step: 51 loss: 0.114971 acc: 0.9500007629394531\n",
      "epoch: 44 step: 52 loss: 0.1107105 acc: 0.9590225219726562\n",
      "epoch: 44 step: 53 loss: 0.108106315 acc: 0.954010009765625\n",
      "epoch: 44 step: 54 loss: 0.10449794 acc: 0.9542007446289062\n",
      "epoch: 44 step: 55 loss: 0.13234322 acc: 0.9558753967285156\n",
      "epoch: 44 step: 56 loss: 0.106321126 acc: 0.9575462341308594\n",
      "epoch: 44 step: 57 loss: 0.1010468 acc: 0.9619636535644531\n",
      "epoch: 44 step: 58 loss: 0.09763287 acc: 0.9596672058105469\n",
      "epoch: 44 step: 59 loss: 0.102019764 acc: 0.9648208618164062\n",
      "epoch: 44 step: 60 loss: 0.11237656 acc: 0.9617919921875\n",
      "epoch: 44 step: 61 loss: 0.1261606 acc: 0.9636611938476562\n",
      "epoch: 44 step: 62 loss: 0.10671748 acc: 0.9627532958984375\n",
      "epoch: 44 step: 63 loss: 0.13155004 acc: 0.9514083862304688\n",
      "epoch: 44 step: 64 loss: 0.13456467 acc: 0.9542427062988281\n",
      "epoch: 44 step: 65 loss: 0.101614974 acc: 0.9601478576660156\n",
      "epoch: 44 step: 66 loss: 0.10410481 acc: 0.9544525146484375\n",
      "epoch: 44 step: 67 loss: 0.10812201 acc: 0.9534835815429688\n",
      "epoch: 44 step: 68 loss: 0.10545343 acc: 0.9564704895019531\n",
      "epoch: 44 step: 69 loss: 0.13920942 acc: 0.9482269287109375\n",
      "epoch: 44 step: 70 loss: 0.10664538 acc: 0.9605445861816406\n",
      "epoch: 44 step: 71 loss: 0.111954994 acc: 0.9492149353027344\n",
      "epoch: 44 step: 72 loss: 0.09408597 acc: 0.9607772827148438\n",
      "epoch: 44 step: 73 loss: 0.0988418 acc: 0.9588775634765625\n",
      "epoch: 44 step: 74 loss: 0.09631836 acc: 0.9612159729003906\n",
      "epoch: 44 step: 75 loss: 0.10272784 acc: 0.9673118591308594\n",
      "epoch: 44 step: 76 loss: 0.087831914 acc: 0.969451904296875\n",
      "epoch: 44 step: 77 loss: 0.08549764 acc: 0.9716529846191406\n",
      "epoch: 44 step: 78 loss: 0.10087448 acc: 0.9613761901855469\n",
      "epoch: 44 step: 79 loss: 0.09690761 acc: 0.9653167724609375\n",
      "epoch: 44 step: 80 loss: 0.1078299 acc: 0.961395263671875\n",
      "epoch: 44 step: 81 loss: 0.08723625 acc: 0.9637413024902344\n",
      "epoch: 44 step: 82 loss: 0.09574199 acc: 0.9617919921875\n",
      "epoch: 44 step: 83 loss: 0.11549396 acc: 0.9579696655273438\n",
      "epoch: 44 step: 84 loss: 0.09707239 acc: 0.9591064453125\n",
      "epoch: 44 step: 85 loss: 0.110110454 acc: 0.9544029235839844\n",
      "epoch: 44 step: 86 loss: 0.119672276 acc: 0.9555549621582031\n",
      "epoch: 44 step: 87 loss: 0.09487816 acc: 0.9593505859375\n",
      "epoch: 44 step: 88 loss: 0.105547264 acc: 0.959808349609375\n",
      "epoch: 44 step: 89 loss: 0.08924911 acc: 0.9661407470703125\n",
      "epoch: 44 step: 90 loss: 0.08612246 acc: 0.9635009765625\n",
      "epoch: 44 step: 91 loss: 0.10510068 acc: 0.9571151733398438\n",
      "epoch: 44 step: 92 loss: 0.10250972 acc: 0.9590835571289062\n",
      "epoch: 44 step: 93 loss: 0.09999573 acc: 0.9583396911621094\n",
      "epoch: 44 step: 94 loss: 0.102074176 acc: 0.9666824340820312\n",
      "epoch: 44 step: 95 loss: 0.14242436 acc: 0.9545516967773438\n",
      "epoch: 44 step: 96 loss: 0.11043728 acc: 0.9613761901855469\n",
      "epoch: 44 step: 97 loss: 0.10059006 acc: 0.9599647521972656\n",
      "epoch: 44 step: 98 loss: 0.09599787 acc: 0.9648323059082031\n",
      "epoch: 44 step: 99 loss: 0.101960994 acc: 0.9611091613769531\n",
      "epoch: 44 step: 100 loss: 0.170324 acc: 0.9435539245605469\n",
      "epoch: 44 step: 101 loss: 0.100784756 acc: 0.9496650695800781\n",
      "epoch: 44 step: 102 loss: 0.099424124 acc: 0.9516983032226562\n",
      "epoch: 44 step: 103 loss: 0.10870527 acc: 0.9493217468261719\n",
      "epoch: 44 step: 104 loss: 0.1286004 acc: 0.9459075927734375\n",
      "epoch: 44 step: 105 loss: 0.12064817 acc: 0.9522018432617188\n",
      "epoch: 44 step: 106 loss: 0.10288769 acc: 0.9585990905761719\n",
      "epoch: 44 step: 107 loss: 0.10056826 acc: 0.9631881713867188\n",
      "epoch: 44 step: 108 loss: 0.11236533 acc: 0.9571533203125\n",
      "epoch: 44 step: 109 loss: 0.09002751 acc: 0.9615020751953125\n",
      "epoch: 44 step: 110 loss: 0.08583105 acc: 0.964111328125\n",
      "epoch: 44 step: 111 loss: 0.10181463 acc: 0.9611129760742188\n",
      "epoch: 44 step: 112 loss: 0.10963243 acc: 0.9590988159179688\n",
      "epoch: 44 step: 113 loss: 0.100474834 acc: 0.9613990783691406\n",
      "epoch: 44 step: 114 loss: 0.11725182 acc: 0.9544105529785156\n",
      "epoch: 44 step: 115 loss: 0.13425222 acc: 0.9524955749511719\n",
      "epoch: 44 step: 116 loss: 0.088485196 acc: 0.9646110534667969\n",
      "epoch: 44 step: 117 loss: 0.098333865 acc: 0.9601173400878906\n",
      "epoch: 44 step: 118 loss: 0.11823411 acc: 0.9557571411132812\n",
      "epoch: 44 step: 119 loss: 0.11888365 acc: 0.9540252685546875\n",
      "epoch: 44 step: 120 loss: 0.11407924 acc: 0.9513740539550781\n",
      "epoch: 44 step: 121 loss: 0.12167133 acc: 0.9481277465820312\n",
      "epoch: 44 step: 122 loss: 0.09433878 acc: 0.9563560485839844\n",
      "epoch: 44 step: 123 loss: 0.104203045 acc: 0.9488182067871094\n",
      "epoch: 44 step: 124 loss: 0.123505004 acc: 0.9494716099330357\n",
      "epoch: 44 validation_loss: 0.113 validation_dice: 0.822473603246913\n",
      "epoch: 44 test_dataset dice: 0.7184555979843266\n",
      "time cost 0.5393561085065206 min\n",
      "dice_best: 0.8274300650157543\n",
      "******************************** epoch  44  is finished. *********************************\n",
      "epoch: 45 step: 1 loss: 0.09627693 acc: 0.9576530456542969\n",
      "epoch: 45 step: 2 loss: 0.096111745 acc: 0.9581184387207031\n",
      "epoch: 45 step: 3 loss: 0.11255877 acc: 0.958465576171875\n",
      "epoch: 45 step: 4 loss: 0.10118066 acc: 0.9673118591308594\n",
      "epoch: 45 step: 5 loss: 0.10546922 acc: 0.9646148681640625\n",
      "epoch: 45 step: 6 loss: 0.13523903 acc: 0.9616661071777344\n",
      "epoch: 45 step: 7 loss: 0.10972953 acc: 0.9584312438964844\n",
      "epoch: 45 step: 8 loss: 0.09664608 acc: 0.9648094177246094\n",
      "epoch: 45 step: 9 loss: 0.107781276 acc: 0.9561080932617188\n",
      "epoch: 45 step: 10 loss: 0.12277523 acc: 0.9513893127441406\n",
      "epoch: 45 step: 11 loss: 0.13080414 acc: 0.9546051025390625\n",
      "epoch: 45 step: 12 loss: 0.10167037 acc: 0.95166015625\n",
      "epoch: 45 step: 13 loss: 0.11056743 acc: 0.9463157653808594\n",
      "epoch: 45 step: 14 loss: 0.11120319 acc: 0.9580497741699219\n",
      "epoch: 45 step: 15 loss: 0.102466874 acc: 0.9526863098144531\n",
      "epoch: 45 step: 16 loss: 0.1428624 acc: 0.94390869140625\n",
      "epoch: 45 step: 17 loss: 0.10834688 acc: 0.9500808715820312\n",
      "epoch: 45 step: 18 loss: 0.11953154 acc: 0.94866943359375\n",
      "epoch: 45 step: 19 loss: 0.111918315 acc: 0.9562568664550781\n",
      "epoch: 45 step: 20 loss: 0.09832682 acc: 0.9553947448730469\n",
      "epoch: 45 step: 21 loss: 0.1097272 acc: 0.9555397033691406\n",
      "epoch: 45 step: 22 loss: 0.09517356 acc: 0.9614830017089844\n",
      "epoch: 45 step: 23 loss: 0.1446222 acc: 0.9561195373535156\n",
      "epoch: 45 step: 24 loss: 0.0988968 acc: 0.9606246948242188\n",
      "epoch: 45 step: 25 loss: 0.0984985 acc: 0.9596176147460938\n",
      "epoch: 45 step: 26 loss: 0.092944495 acc: 0.9555702209472656\n",
      "epoch: 45 step: 27 loss: 0.101189256 acc: 0.9581871032714844\n",
      "epoch: 45 step: 28 loss: 0.11280579 acc: 0.9499473571777344\n",
      "epoch: 45 step: 29 loss: 0.093762495 acc: 0.9605789184570312\n",
      "epoch: 45 step: 30 loss: 0.10860326 acc: 0.9594306945800781\n",
      "epoch: 45 step: 31 loss: 0.10204885 acc: 0.9593620300292969\n",
      "epoch: 45 step: 32 loss: 0.12827873 acc: 0.9567489624023438\n",
      "epoch: 45 step: 33 loss: 0.09678064 acc: 0.9593009948730469\n",
      "epoch: 45 step: 34 loss: 0.09843206 acc: 0.9562416076660156\n",
      "epoch: 45 step: 35 loss: 0.095671535 acc: 0.9631767272949219\n",
      "epoch: 45 step: 36 loss: 0.097418875 acc: 0.9579849243164062\n",
      "epoch: 45 step: 37 loss: 0.09303404 acc: 0.9606475830078125\n",
      "epoch: 45 step: 38 loss: 0.096633755 acc: 0.9656219482421875\n",
      "epoch: 45 step: 39 loss: 0.09875902 acc: 0.9589881896972656\n",
      "epoch: 45 step: 40 loss: 0.11735379 acc: 0.9538955688476562\n",
      "epoch: 45 step: 41 loss: 0.089381665 acc: 0.9681549072265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45 step: 42 loss: 0.097491965 acc: 0.9654273986816406\n",
      "epoch: 45 step: 43 loss: 0.100420125 acc: 0.9570236206054688\n",
      "epoch: 45 step: 44 loss: 0.097291626 acc: 0.9643707275390625\n",
      "epoch: 45 step: 45 loss: 0.08965044 acc: 0.9636917114257812\n",
      "epoch: 45 step: 46 loss: 0.1404504 acc: 0.9493293762207031\n",
      "epoch: 45 step: 47 loss: 0.10886166 acc: 0.9576644897460938\n",
      "epoch: 45 step: 48 loss: 0.10061032 acc: 0.9515457153320312\n",
      "epoch: 45 step: 49 loss: 0.10777804 acc: 0.9594383239746094\n",
      "epoch: 45 step: 50 loss: 0.11043381 acc: 0.9598655700683594\n",
      "epoch: 45 step: 51 loss: 0.1082542 acc: 0.955780029296875\n",
      "epoch: 45 step: 52 loss: 0.10168449 acc: 0.9547157287597656\n",
      "epoch: 45 step: 53 loss: 0.10115078 acc: 0.9551239013671875\n",
      "epoch: 45 step: 54 loss: 0.111157976 acc: 0.9553871154785156\n",
      "epoch: 45 step: 55 loss: 0.10171517 acc: 0.9589157104492188\n",
      "epoch: 45 step: 56 loss: 0.09929577 acc: 0.9621849060058594\n",
      "epoch: 45 step: 57 loss: 0.12213174 acc: 0.9494094848632812\n",
      "epoch: 45 step: 58 loss: 0.12604155 acc: 0.9494094848632812\n",
      "epoch: 45 step: 59 loss: 0.10870275 acc: 0.9552001953125\n",
      "epoch: 45 step: 60 loss: 0.12981015 acc: 0.954681396484375\n",
      "epoch: 45 step: 61 loss: 0.10389683 acc: 0.9509010314941406\n",
      "epoch: 45 step: 62 loss: 0.113303 acc: 0.9576797485351562\n",
      "epoch: 45 step: 63 loss: 0.09160428 acc: 0.9554023742675781\n",
      "epoch: 45 step: 64 loss: 0.11068414 acc: 0.9583396911621094\n",
      "epoch: 45 step: 65 loss: 0.09588783 acc: 0.958770751953125\n",
      "epoch: 45 step: 66 loss: 0.08023993 acc: 0.9658241271972656\n",
      "epoch: 45 step: 67 loss: 0.1154131 acc: 0.9593048095703125\n",
      "epoch: 45 step: 68 loss: 0.10858635 acc: 0.9548835754394531\n",
      "epoch: 45 step: 69 loss: 0.09433022 acc: 0.9617919921875\n",
      "epoch: 45 step: 70 loss: 0.086590245 acc: 0.9659576416015625\n",
      "epoch: 45 step: 71 loss: 0.09243636 acc: 0.9575424194335938\n",
      "epoch: 45 step: 72 loss: 0.10296377 acc: 0.9594993591308594\n",
      "epoch: 45 step: 73 loss: 0.11385967 acc: 0.9608650207519531\n",
      "epoch: 45 step: 74 loss: 0.09794115 acc: 0.9605255126953125\n",
      "epoch: 45 step: 75 loss: 0.099733084 acc: 0.9616661071777344\n",
      "epoch: 45 step: 76 loss: 0.10585575 acc: 0.9513130187988281\n",
      "epoch: 45 step: 77 loss: 0.10767445 acc: 0.9593048095703125\n",
      "epoch: 45 step: 78 loss: 0.08692401 acc: 0.9643821716308594\n",
      "epoch: 45 step: 79 loss: 0.09317983 acc: 0.964324951171875\n",
      "epoch: 45 step: 80 loss: 0.11373054 acc: 0.95623779296875\n",
      "epoch: 45 step: 81 loss: 0.109697334 acc: 0.9576187133789062\n",
      "epoch: 45 step: 82 loss: 0.08684018 acc: 0.9634628295898438\n",
      "epoch: 45 step: 83 loss: 0.09646838 acc: 0.9548912048339844\n",
      "epoch: 45 step: 84 loss: 0.09560153 acc: 0.9579086303710938\n",
      "epoch: 45 step: 85 loss: 0.099791 acc: 0.9560356140136719\n",
      "epoch: 45 step: 86 loss: 0.10632255 acc: 0.9549751281738281\n",
      "epoch: 45 step: 87 loss: 0.09611434 acc: 0.9593162536621094\n",
      "epoch: 45 step: 88 loss: 0.092310324 acc: 0.9614791870117188\n",
      "epoch: 45 step: 89 loss: 0.10292494 acc: 0.9600410461425781\n",
      "epoch: 45 step: 90 loss: 0.101914614 acc: 0.9580039978027344\n",
      "epoch: 45 step: 91 loss: 0.09174966 acc: 0.9623756408691406\n",
      "epoch: 45 step: 92 loss: 0.08719727 acc: 0.9648551940917969\n",
      "epoch: 45 step: 93 loss: 0.09522679 acc: 0.9592475891113281\n",
      "epoch: 45 step: 94 loss: 0.09430226 acc: 0.9613304138183594\n",
      "epoch: 45 step: 95 loss: 0.09987076 acc: 0.9627952575683594\n",
      "epoch: 45 step: 96 loss: 0.10262198 acc: 0.9565391540527344\n",
      "epoch: 45 step: 97 loss: 0.11448538 acc: 0.9599266052246094\n",
      "epoch: 45 step: 98 loss: 0.08895573 acc: 0.9601898193359375\n",
      "epoch: 45 step: 99 loss: 0.082973495 acc: 0.9631500244140625\n",
      "epoch: 45 step: 100 loss: 0.1084652 acc: 0.953399658203125\n",
      "epoch: 45 step: 101 loss: 0.09060437 acc: 0.9612312316894531\n",
      "epoch: 45 step: 102 loss: 0.09309914 acc: 0.9625625610351562\n",
      "epoch: 45 step: 103 loss: 0.092777 acc: 0.9632644653320312\n",
      "epoch: 45 step: 104 loss: 0.09112064 acc: 0.9640426635742188\n",
      "epoch: 45 step: 105 loss: 0.11516468 acc: 0.949371337890625\n",
      "epoch: 45 step: 106 loss: 0.11031783 acc: 0.9610671997070312\n",
      "epoch: 45 step: 107 loss: 0.087896824 acc: 0.9621467590332031\n",
      "epoch: 45 step: 108 loss: 0.086109646 acc: 0.9576454162597656\n",
      "epoch: 45 step: 109 loss: 0.10026654 acc: 0.9617462158203125\n",
      "epoch: 45 step: 110 loss: 0.085914254 acc: 0.9638481140136719\n",
      "epoch: 45 step: 111 loss: 0.08239582 acc: 0.9625167846679688\n",
      "epoch: 45 step: 112 loss: 0.101079226 acc: 0.9591636657714844\n",
      "epoch: 45 step: 113 loss: 0.10511138 acc: 0.9608306884765625\n",
      "epoch: 45 step: 114 loss: 0.09770446 acc: 0.9617691040039062\n",
      "epoch: 45 step: 115 loss: 0.12065078 acc: 0.9585304260253906\n",
      "epoch: 45 step: 116 loss: 0.124764316 acc: 0.9537582397460938\n",
      "epoch: 45 step: 117 loss: 0.112390645 acc: 0.9504127502441406\n",
      "epoch: 45 step: 118 loss: 0.09541752 acc: 0.9541511535644531\n",
      "epoch: 45 step: 119 loss: 0.10331705 acc: 0.9504737854003906\n",
      "epoch: 45 step: 120 loss: 0.107742354 acc: 0.9537544250488281\n",
      "epoch: 45 step: 121 loss: 0.10159461 acc: 0.9586105346679688\n",
      "epoch: 45 step: 122 loss: 0.09897967 acc: 0.9590721130371094\n",
      "epoch: 45 step: 123 loss: 0.119169086 acc: 0.9654579162597656\n",
      "epoch: 45 step: 124 loss: 0.13964593 acc: 0.9489222935267857\n",
      "epoch: 45 validation_loss: 0.114 validation_dice: 0.8320022461836629\n",
      "epoch: 45 test_dataset dice: 0.7453072600399286\n",
      "time cost 0.539295768737793 min\n",
      "dice_best: 0.8320022461836629\n",
      "******************************** epoch  45  is finished. *********************************\n",
      "epoch: 46 step: 1 loss: 0.10151001 acc: 0.9639091491699219\n",
      "epoch: 46 step: 2 loss: 0.09903835 acc: 0.9666404724121094\n",
      "epoch: 46 step: 3 loss: 0.10989434 acc: 0.9598236083984375\n",
      "epoch: 46 step: 4 loss: 0.103392504 acc: 0.9546546936035156\n",
      "epoch: 46 step: 5 loss: 0.10740247 acc: 0.9577140808105469\n",
      "epoch: 46 step: 6 loss: 0.114323914 acc: 0.9559593200683594\n",
      "epoch: 46 step: 7 loss: 0.10175543 acc: 0.954376220703125\n",
      "epoch: 46 step: 8 loss: 0.10261988 acc: 0.9523963928222656\n",
      "epoch: 46 step: 9 loss: 0.09462539 acc: 0.9581947326660156\n",
      "epoch: 46 step: 10 loss: 0.098282516 acc: 0.96075439453125\n",
      "epoch: 46 step: 11 loss: 0.09759267 acc: 0.9596405029296875\n",
      "epoch: 46 step: 12 loss: 0.08923565 acc: 0.9610443115234375\n",
      "epoch: 46 step: 13 loss: 0.10891985 acc: 0.9508514404296875\n",
      "epoch: 46 step: 14 loss: 0.101993844 acc: 0.9622001647949219\n",
      "epoch: 46 step: 15 loss: 0.1027802 acc: 0.9584846496582031\n",
      "epoch: 46 step: 16 loss: 0.089667164 acc: 0.9625320434570312\n",
      "epoch: 46 step: 17 loss: 0.09005374 acc: 0.9650497436523438\n",
      "epoch: 46 step: 18 loss: 0.1008319 acc: 0.9587173461914062\n",
      "epoch: 46 step: 19 loss: 0.09350694 acc: 0.9566497802734375\n",
      "epoch: 46 step: 20 loss: 0.11540934 acc: 0.9541091918945312\n",
      "epoch: 46 step: 21 loss: 0.09573772 acc: 0.9615135192871094\n",
      "epoch: 46 step: 22 loss: 0.10731451 acc: 0.962188720703125\n",
      "epoch: 46 step: 23 loss: 0.109946355 acc: 0.9500389099121094\n",
      "epoch: 46 step: 24 loss: 0.086008824 acc: 0.9580345153808594\n",
      "epoch: 46 step: 25 loss: 0.078163445 acc: 0.9655723571777344\n",
      "epoch: 46 step: 26 loss: 0.094788074 acc: 0.9594955444335938\n",
      "epoch: 46 step: 27 loss: 0.08514039 acc: 0.9617233276367188\n",
      "epoch: 46 step: 28 loss: 0.10509781 acc: 0.9585800170898438\n",
      "epoch: 46 step: 29 loss: 0.11556035 acc: 0.9556388854980469\n",
      "epoch: 46 step: 30 loss: 0.0975093 acc: 0.964202880859375\n",
      "epoch: 46 step: 31 loss: 0.101612926 acc: 0.9568977355957031\n",
      "epoch: 46 step: 32 loss: 0.09983459 acc: 0.9640312194824219\n",
      "epoch: 46 step: 33 loss: 0.097172745 acc: 0.9682807922363281\n",
      "epoch: 46 step: 34 loss: 0.1070371 acc: 0.9626884460449219\n",
      "epoch: 46 step: 35 loss: 0.08658337 acc: 0.9663429260253906\n",
      "epoch: 46 step: 36 loss: 0.12278254 acc: 0.9614295959472656\n",
      "epoch: 46 step: 37 loss: 0.10032377 acc: 0.9517745971679688\n",
      "epoch: 46 step: 38 loss: 0.09323534 acc: 0.9538955688476562\n",
      "epoch: 46 step: 39 loss: 0.09893876 acc: 0.954498291015625\n",
      "epoch: 46 step: 40 loss: 0.09383764 acc: 0.9619026184082031\n",
      "epoch: 46 step: 41 loss: 0.114564545 acc: 0.95050048828125\n",
      "epoch: 46 step: 42 loss: 0.10739899 acc: 0.9631423950195312\n",
      "epoch: 46 step: 43 loss: 0.10555386 acc: 0.9599266052246094\n",
      "epoch: 46 step: 44 loss: 0.1073781 acc: 0.9550743103027344\n",
      "epoch: 46 step: 45 loss: 0.08959994 acc: 0.9625053405761719\n",
      "epoch: 46 step: 46 loss: 0.10677856 acc: 0.9638633728027344\n",
      "epoch: 46 step: 47 loss: 0.07583756 acc: 0.9687576293945312\n",
      "epoch: 46 step: 48 loss: 0.10246349 acc: 0.9568901062011719\n",
      "epoch: 46 step: 49 loss: 0.09626825 acc: 0.9626731872558594\n",
      "epoch: 46 step: 50 loss: 0.10367538 acc: 0.9554328918457031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 step: 51 loss: 0.09072856 acc: 0.9579391479492188\n",
      "epoch: 46 step: 52 loss: 0.10403093 acc: 0.9551620483398438\n",
      "epoch: 46 step: 53 loss: 0.10150679 acc: 0.95269775390625\n",
      "epoch: 46 step: 54 loss: 0.100677855 acc: 0.9512214660644531\n",
      "epoch: 46 step: 55 loss: 0.10170603 acc: 0.9562492370605469\n",
      "epoch: 46 step: 56 loss: 0.098194934 acc: 0.9539566040039062\n",
      "epoch: 46 step: 57 loss: 0.092316635 acc: 0.9647903442382812\n",
      "epoch: 46 step: 58 loss: 0.09278357 acc: 0.9641609191894531\n",
      "epoch: 46 step: 59 loss: 0.101680845 acc: 0.96319580078125\n",
      "epoch: 46 step: 60 loss: 0.11137485 acc: 0.9648094177246094\n",
      "epoch: 46 step: 61 loss: 0.10563349 acc: 0.9606704711914062\n",
      "epoch: 46 step: 62 loss: 0.09853622 acc: 0.960845947265625\n",
      "epoch: 46 step: 63 loss: 0.105170615 acc: 0.9623298645019531\n",
      "epoch: 46 step: 64 loss: 0.08881536 acc: 0.9665298461914062\n",
      "epoch: 46 step: 65 loss: 0.097674206 acc: 0.9596786499023438\n",
      "epoch: 46 step: 66 loss: 0.10232971 acc: 0.9648628234863281\n",
      "epoch: 46 step: 67 loss: 0.13592067 acc: 0.9552001953125\n",
      "epoch: 46 step: 68 loss: 0.11397644 acc: 0.954132080078125\n",
      "epoch: 46 step: 69 loss: 0.118978046 acc: 0.9567527770996094\n",
      "epoch: 46 step: 70 loss: 0.0977405 acc: 0.9631271362304688\n",
      "epoch: 46 step: 71 loss: 0.107275695 acc: 0.9481964111328125\n",
      "epoch: 46 step: 72 loss: 0.09971593 acc: 0.9491462707519531\n",
      "epoch: 46 step: 73 loss: 0.10601164 acc: 0.9541854858398438\n",
      "epoch: 46 step: 74 loss: 0.09514172 acc: 0.9542045593261719\n",
      "epoch: 46 step: 75 loss: 0.113516405 acc: 0.9506568908691406\n",
      "epoch: 46 step: 76 loss: 0.14295685 acc: 0.94720458984375\n",
      "epoch: 46 step: 77 loss: 0.11107611 acc: 0.9586601257324219\n",
      "epoch: 46 step: 78 loss: 0.11027039 acc: 0.964202880859375\n",
      "epoch: 46 step: 79 loss: 0.12974401 acc: 0.9627304077148438\n",
      "epoch: 46 step: 80 loss: 0.12470294 acc: 0.9661369323730469\n",
      "epoch: 46 step: 81 loss: 0.10905887 acc: 0.9583244323730469\n",
      "epoch: 46 step: 82 loss: 0.10667498 acc: 0.962890625\n",
      "epoch: 46 step: 83 loss: 0.10838525 acc: 0.9607810974121094\n",
      "epoch: 46 step: 84 loss: 0.110496834 acc: 0.9571342468261719\n",
      "epoch: 46 step: 85 loss: 0.14311142 acc: 0.9461479187011719\n",
      "epoch: 46 step: 86 loss: 0.12158675 acc: 0.9599838256835938\n",
      "epoch: 46 step: 87 loss: 0.106865205 acc: 0.9544868469238281\n",
      "epoch: 46 step: 88 loss: 0.11791948 acc: 0.9494171142578125\n",
      "epoch: 46 step: 89 loss: 0.099732526 acc: 0.9612846374511719\n",
      "epoch: 46 step: 90 loss: 0.1263785 acc: 0.9500350952148438\n",
      "epoch: 46 step: 91 loss: 0.13074169 acc: 0.9482460021972656\n",
      "epoch: 46 step: 92 loss: 0.12024334 acc: 0.9562911987304688\n",
      "epoch: 46 step: 93 loss: 0.12895748 acc: 0.9511871337890625\n",
      "epoch: 46 step: 94 loss: 0.09839181 acc: 0.9589805603027344\n",
      "epoch: 46 step: 95 loss: 0.109947376 acc: 0.9598960876464844\n",
      "epoch: 46 step: 96 loss: 0.10619937 acc: 0.9603538513183594\n",
      "epoch: 46 step: 97 loss: 0.116110496 acc: 0.9566917419433594\n",
      "epoch: 46 step: 98 loss: 0.115282305 acc: 0.9546585083007812\n",
      "epoch: 46 step: 99 loss: 0.14965023 acc: 0.9390144348144531\n",
      "epoch: 46 step: 100 loss: 0.1251901 acc: 0.9511833190917969\n",
      "epoch: 46 step: 101 loss: 0.11077669 acc: 0.9582061767578125\n",
      "epoch: 46 step: 102 loss: 0.11232807 acc: 0.9655265808105469\n",
      "epoch: 46 step: 103 loss: 0.12646848 acc: 0.9586448669433594\n",
      "epoch: 46 step: 104 loss: 0.11048154 acc: 0.9647102355957031\n",
      "epoch: 46 step: 105 loss: 0.11487343 acc: 0.96026611328125\n",
      "epoch: 46 step: 106 loss: 0.12996368 acc: 0.9583473205566406\n",
      "epoch: 46 step: 107 loss: 0.1335412 acc: 0.9581718444824219\n",
      "epoch: 46 step: 108 loss: 0.12483713 acc: 0.9538917541503906\n",
      "epoch: 46 step: 109 loss: 0.1504875 acc: 0.961029052734375\n",
      "epoch: 46 step: 110 loss: 0.083281904 acc: 0.9683303833007812\n",
      "epoch: 46 step: 111 loss: 0.14093147 acc: 0.9449272155761719\n",
      "epoch: 46 step: 112 loss: 0.13778497 acc: 0.9446640014648438\n",
      "epoch: 46 step: 113 loss: 0.1046457 acc: 0.9571113586425781\n",
      "epoch: 46 step: 114 loss: 0.13979276 acc: 0.9432907104492188\n",
      "epoch: 46 step: 115 loss: 0.13710293 acc: 0.9420509338378906\n",
      "epoch: 46 step: 116 loss: 0.101238556 acc: 0.9579353332519531\n",
      "epoch: 46 step: 117 loss: 0.13370535 acc: 0.9607582092285156\n",
      "epoch: 46 step: 118 loss: 0.15406087 acc: 0.9433670043945312\n",
      "epoch: 46 step: 119 loss: 0.13886382 acc: 0.9596595764160156\n",
      "epoch: 46 step: 120 loss: 0.13070689 acc: 0.9477119445800781\n",
      "epoch: 46 step: 121 loss: 0.12167772 acc: 0.9545135498046875\n",
      "epoch: 46 step: 122 loss: 0.1332246 acc: 0.9579200744628906\n",
      "epoch: 46 step: 123 loss: 0.121707655 acc: 0.9550247192382812\n",
      "epoch: 46 step: 124 loss: 0.13567005 acc: 0.9555228097098214\n",
      "epoch: 46 validation_loss: 0.13 validation_dice: 0.7953166271139593\n",
      "epoch: 46 test_dataset dice: 0.7102342988117865\n",
      "time cost 0.5403393626213073 min\n",
      "dice_best: 0.8320022461836629\n",
      "******************************** epoch  46  is finished. *********************************\n",
      "epoch: 47 step: 1 loss: 0.15134896 acc: 0.9475440979003906\n",
      "epoch: 47 step: 2 loss: 0.13935846 acc: 0.9622802734375\n",
      "epoch: 47 step: 3 loss: 0.12249061 acc: 0.9533233642578125\n",
      "epoch: 47 step: 4 loss: 0.14183329 acc: 0.9597091674804688\n",
      "epoch: 47 step: 5 loss: 0.20966806 acc: 0.9438362121582031\n",
      "epoch: 47 step: 6 loss: 0.14277688 acc: 0.9550628662109375\n",
      "epoch: 47 step: 7 loss: 0.18132193 acc: 0.9510078430175781\n",
      "epoch: 47 step: 8 loss: 0.3832572 acc: 0.9351577758789062\n",
      "epoch: 47 step: 9 loss: 0.1290623 acc: 0.9547080993652344\n",
      "epoch: 47 step: 10 loss: 0.19823197 acc: 0.9501571655273438\n",
      "epoch: 47 step: 11 loss: 0.2866064 acc: 0.9454345703125\n",
      "epoch: 47 step: 12 loss: 0.18830042 acc: 0.9383583068847656\n",
      "epoch: 47 step: 13 loss: 0.21865888 acc: 0.93084716796875\n",
      "epoch: 47 step: 14 loss: 0.19784051 acc: 0.9298171997070312\n",
      "epoch: 47 step: 15 loss: 0.18508807 acc: 0.9490852355957031\n",
      "epoch: 47 step: 16 loss: 0.19299097 acc: 0.9467201232910156\n",
      "epoch: 47 step: 17 loss: 0.21177746 acc: 0.9459190368652344\n",
      "epoch: 47 step: 18 loss: 0.18326135 acc: 0.9544410705566406\n",
      "epoch: 47 step: 19 loss: 0.23004241 acc: 0.9439773559570312\n",
      "epoch: 47 step: 20 loss: 0.20838065 acc: 0.9452095031738281\n",
      "epoch: 47 step: 21 loss: 0.20411351 acc: 0.9480819702148438\n",
      "epoch: 47 step: 22 loss: 0.2589112 acc: 0.9448890686035156\n",
      "epoch: 47 step: 23 loss: 0.15623993 acc: 0.9476699829101562\n",
      "epoch: 47 step: 24 loss: 0.24570796 acc: 0.9327926635742188\n",
      "epoch: 47 step: 25 loss: 0.20826416 acc: 0.9351348876953125\n",
      "epoch: 47 step: 26 loss: 0.21895298 acc: 0.9338798522949219\n",
      "epoch: 47 step: 27 loss: 0.1713341 acc: 0.9454231262207031\n",
      "epoch: 47 step: 28 loss: 0.20794101 acc: 0.9357757568359375\n",
      "epoch: 47 step: 29 loss: 0.21739492 acc: 0.9360771179199219\n",
      "epoch: 47 step: 30 loss: 0.21201313 acc: 0.9406585693359375\n",
      "epoch: 47 step: 31 loss: 0.21337304 acc: 0.9444580078125\n",
      "epoch: 47 step: 32 loss: 0.16594595 acc: 0.946746826171875\n",
      "epoch: 47 step: 33 loss: 0.18120338 acc: 0.9434356689453125\n",
      "epoch: 47 step: 34 loss: 0.18511188 acc: 0.9367828369140625\n",
      "epoch: 47 step: 35 loss: 0.17677246 acc: 0.9458847045898438\n",
      "epoch: 47 step: 36 loss: 0.16443689 acc: 0.9539260864257812\n",
      "epoch: 47 step: 37 loss: 0.16406983 acc: 0.9527931213378906\n",
      "epoch: 47 step: 38 loss: 0.19474322 acc: 0.9469261169433594\n",
      "epoch: 47 step: 39 loss: 0.12211457 acc: 0.9650306701660156\n",
      "epoch: 47 step: 40 loss: 0.15527609 acc: 0.9546585083007812\n",
      "epoch: 47 step: 41 loss: 0.15656237 acc: 0.9534835815429688\n",
      "epoch: 47 step: 42 loss: 0.19184901 acc: 0.9363327026367188\n",
      "epoch: 47 step: 43 loss: 0.14592816 acc: 0.9507789611816406\n",
      "epoch: 47 step: 44 loss: 0.1752419 acc: 0.9416046142578125\n",
      "epoch: 47 step: 45 loss: 0.16899124 acc: 0.9534645080566406\n",
      "epoch: 47 step: 46 loss: 0.15051185 acc: 0.9470672607421875\n",
      "epoch: 47 step: 47 loss: 0.21867096 acc: 0.92901611328125\n",
      "epoch: 47 step: 48 loss: 0.13354452 acc: 0.9464645385742188\n",
      "epoch: 47 step: 49 loss: 0.15194643 acc: 0.9511566162109375\n",
      "epoch: 47 step: 50 loss: 0.16697328 acc: 0.9485626220703125\n",
      "epoch: 47 step: 51 loss: 0.17540407 acc: 0.9453697204589844\n",
      "epoch: 47 step: 52 loss: 0.14066426 acc: 0.9517288208007812\n",
      "epoch: 47 step: 53 loss: 0.18724759 acc: 0.9480133056640625\n",
      "epoch: 47 step: 54 loss: 0.15000872 acc: 0.9527626037597656\n",
      "epoch: 47 step: 55 loss: 0.14549315 acc: 0.9437522888183594\n",
      "epoch: 47 step: 56 loss: 0.1660344 acc: 0.9502792358398438\n",
      "epoch: 47 step: 57 loss: 0.13435581 acc: 0.9539222717285156\n",
      "epoch: 47 step: 58 loss: 0.13420124 acc: 0.9492073059082031\n",
      "epoch: 47 step: 59 loss: 0.13892 acc: 0.9573211669921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47 step: 60 loss: 0.20333079 acc: 0.9453315734863281\n",
      "epoch: 47 step: 61 loss: 0.1262961 acc: 0.9547386169433594\n",
      "epoch: 47 step: 62 loss: 0.13076232 acc: 0.9507522583007812\n",
      "epoch: 47 step: 63 loss: 0.112987794 acc: 0.9534912109375\n",
      "epoch: 47 step: 64 loss: 0.11699209 acc: 0.9542694091796875\n",
      "epoch: 47 step: 65 loss: 0.18309988 acc: 0.9493446350097656\n",
      "epoch: 47 step: 66 loss: 0.15368654 acc: 0.9472274780273438\n",
      "epoch: 47 step: 67 loss: 0.15254986 acc: 0.9429359436035156\n",
      "epoch: 47 step: 68 loss: 0.12221053 acc: 0.9588356018066406\n",
      "epoch: 47 step: 69 loss: 0.21086751 acc: 0.9437789916992188\n",
      "epoch: 47 step: 70 loss: 0.1160357 acc: 0.9595146179199219\n",
      "epoch: 47 step: 71 loss: 0.1286311 acc: 0.9556694030761719\n",
      "epoch: 47 step: 72 loss: 0.13368085 acc: 0.9576988220214844\n",
      "epoch: 47 step: 73 loss: 0.16795473 acc: 0.9468307495117188\n",
      "epoch: 47 step: 74 loss: 0.13633507 acc: 0.9563980102539062\n",
      "epoch: 47 step: 75 loss: 0.20247146 acc: 0.9314994812011719\n",
      "epoch: 47 step: 76 loss: 0.13135271 acc: 0.9529953002929688\n",
      "epoch: 47 step: 77 loss: 0.1185789 acc: 0.9584770202636719\n",
      "epoch: 47 step: 78 loss: 0.1313836 acc: 0.9635658264160156\n",
      "epoch: 47 step: 79 loss: 0.13475156 acc: 0.9606819152832031\n",
      "epoch: 47 step: 80 loss: 0.16040449 acc: 0.94677734375\n",
      "epoch: 47 step: 81 loss: 0.15663926 acc: 0.9541053771972656\n",
      "epoch: 47 step: 82 loss: 0.1343085 acc: 0.9534835815429688\n",
      "epoch: 47 step: 83 loss: 0.13945541 acc: 0.9513931274414062\n",
      "epoch: 47 step: 84 loss: 0.13111207 acc: 0.958038330078125\n",
      "epoch: 47 step: 85 loss: 0.14561994 acc: 0.9526329040527344\n",
      "epoch: 47 step: 86 loss: 0.14424762 acc: 0.9512557983398438\n",
      "epoch: 47 step: 87 loss: 0.122944824 acc: 0.9436988830566406\n",
      "epoch: 47 step: 88 loss: 0.11590359 acc: 0.9591827392578125\n",
      "epoch: 47 step: 89 loss: 0.12682231 acc: 0.9503631591796875\n",
      "epoch: 47 step: 90 loss: 0.12908097 acc: 0.9463844299316406\n",
      "epoch: 47 step: 91 loss: 0.1133714 acc: 0.9576988220214844\n",
      "epoch: 47 step: 92 loss: 0.14342825 acc: 0.9479484558105469\n",
      "epoch: 47 step: 93 loss: 0.10277747 acc: 0.9588775634765625\n",
      "epoch: 47 step: 94 loss: 0.12638663 acc: 0.9535675048828125\n",
      "epoch: 47 step: 95 loss: 0.13794476 acc: 0.957611083984375\n",
      "epoch: 47 step: 96 loss: 0.107404985 acc: 0.959075927734375\n",
      "epoch: 47 step: 97 loss: 0.16648965 acc: 0.9517059326171875\n",
      "epoch: 47 step: 98 loss: 0.13403563 acc: 0.9579658508300781\n",
      "epoch: 47 step: 99 loss: 0.13204764 acc: 0.9555244445800781\n",
      "epoch: 47 step: 100 loss: 0.10530834 acc: 0.9615020751953125\n",
      "epoch: 47 step: 101 loss: 0.14834298 acc: 0.9545059204101562\n",
      "epoch: 47 step: 102 loss: 0.10787232 acc: 0.9610519409179688\n",
      "epoch: 47 step: 103 loss: 0.10395526 acc: 0.9673728942871094\n",
      "epoch: 47 step: 104 loss: 0.15552588 acc: 0.9427604675292969\n",
      "epoch: 47 step: 105 loss: 0.09692341 acc: 0.9627532958984375\n",
      "epoch: 47 step: 106 loss: 0.09884864 acc: 0.9600982666015625\n",
      "epoch: 47 step: 107 loss: 0.09580417 acc: 0.9641189575195312\n",
      "epoch: 47 step: 108 loss: 0.10591784 acc: 0.9628219604492188\n",
      "epoch: 47 step: 109 loss: 0.12733032 acc: 0.9539146423339844\n",
      "epoch: 47 step: 110 loss: 0.109824724 acc: 0.9590835571289062\n",
      "epoch: 47 step: 111 loss: 0.11539147 acc: 0.9588470458984375\n",
      "epoch: 47 step: 112 loss: 0.08732812 acc: 0.9631423950195312\n",
      "epoch: 47 step: 113 loss: 0.11766191 acc: 0.9529457092285156\n",
      "epoch: 47 step: 114 loss: 0.113220885 acc: 0.9522552490234375\n",
      "epoch: 47 step: 115 loss: 0.123429015 acc: 0.9554862976074219\n",
      "epoch: 47 step: 116 loss: 0.116337076 acc: 0.9594154357910156\n",
      "epoch: 47 step: 117 loss: 0.12664196 acc: 0.9532318115234375\n",
      "epoch: 47 step: 118 loss: 0.124089494 acc: 0.9544754028320312\n",
      "epoch: 47 step: 119 loss: 0.10805529 acc: 0.9577980041503906\n",
      "epoch: 47 step: 120 loss: 0.12305819 acc: 0.9507484436035156\n",
      "epoch: 47 step: 121 loss: 0.111142665 acc: 0.955078125\n",
      "epoch: 47 step: 122 loss: 0.10329703 acc: 0.9598960876464844\n",
      "epoch: 47 step: 123 loss: 0.12878549 acc: 0.9520187377929688\n",
      "epoch: 47 step: 124 loss: 0.11030586 acc: 0.95465087890625\n",
      "epoch: 47 validation_loss: 0.114 validation_dice: 0.8211720601282048\n",
      "epoch: 47 test_dataset dice: 0.7273615352746424\n",
      "time cost 0.5269642869631449 min\n",
      "dice_best: 0.8320022461836629\n",
      "******************************** epoch  47  is finished. *********************************\n",
      "epoch: 48 step: 1 loss: 0.11494391 acc: 0.9580764770507812\n",
      "epoch: 48 step: 2 loss: 0.13233268 acc: 0.9567947387695312\n",
      "epoch: 48 step: 3 loss: 0.09877499 acc: 0.9563407897949219\n",
      "epoch: 48 step: 4 loss: 0.1275684 acc: 0.9525604248046875\n",
      "epoch: 48 step: 5 loss: 0.11341742 acc: 0.9542388916015625\n",
      "epoch: 48 step: 6 loss: 0.11158313 acc: 0.9610214233398438\n",
      "epoch: 48 step: 7 loss: 0.085375726 acc: 0.9629173278808594\n",
      "epoch: 48 step: 8 loss: 0.113383174 acc: 0.9554443359375\n",
      "epoch: 48 step: 9 loss: 0.0891972 acc: 0.9613838195800781\n",
      "epoch: 48 step: 10 loss: 0.11420587 acc: 0.9507255554199219\n",
      "epoch: 48 step: 11 loss: 0.12008489 acc: 0.9550857543945312\n",
      "epoch: 48 step: 12 loss: 0.11620112 acc: 0.961517333984375\n",
      "epoch: 48 step: 13 loss: 0.08140802 acc: 0.9698905944824219\n",
      "epoch: 48 step: 14 loss: 0.13029498 acc: 0.9574127197265625\n",
      "epoch: 48 step: 15 loss: 0.112808555 acc: 0.9611930847167969\n",
      "epoch: 48 step: 16 loss: 0.11564324 acc: 0.9588394165039062\n",
      "epoch: 48 step: 17 loss: 0.09704559 acc: 0.9641189575195312\n",
      "epoch: 48 step: 18 loss: 0.09774011 acc: 0.9612159729003906\n",
      "epoch: 48 step: 19 loss: 0.1425694 acc: 0.9494552612304688\n",
      "epoch: 48 step: 20 loss: 0.11742352 acc: 0.9511985778808594\n",
      "epoch: 48 step: 21 loss: 0.11584377 acc: 0.9545936584472656\n",
      "epoch: 48 step: 22 loss: 0.10994134 acc: 0.9597015380859375\n",
      "epoch: 48 step: 23 loss: 0.09964551 acc: 0.9549789428710938\n",
      "epoch: 48 step: 24 loss: 0.09200803 acc: 0.9612464904785156\n",
      "epoch: 48 step: 25 loss: 0.12207801 acc: 0.9568672180175781\n",
      "epoch: 48 step: 26 loss: 0.116162255 acc: 0.9593696594238281\n",
      "epoch: 48 step: 27 loss: 0.11464108 acc: 0.9568023681640625\n",
      "epoch: 48 step: 28 loss: 0.12913208 acc: 0.9532127380371094\n",
      "epoch: 48 step: 29 loss: 0.10249883 acc: 0.9637298583984375\n",
      "epoch: 48 step: 30 loss: 0.098524414 acc: 0.9587821960449219\n",
      "epoch: 48 step: 31 loss: 0.09940777 acc: 0.9632453918457031\n",
      "epoch: 48 step: 32 loss: 0.12620142 acc: 0.9574623107910156\n",
      "epoch: 48 step: 33 loss: 0.11652924 acc: 0.9501609802246094\n",
      "epoch: 48 step: 34 loss: 0.11596168 acc: 0.9605560302734375\n",
      "epoch: 48 step: 35 loss: 0.10466834 acc: 0.9579277038574219\n",
      "epoch: 48 step: 36 loss: 0.10152597 acc: 0.9587593078613281\n",
      "epoch: 48 step: 37 loss: 0.110025175 acc: 0.9599456787109375\n",
      "epoch: 48 step: 38 loss: 0.12463404 acc: 0.9576301574707031\n",
      "epoch: 48 step: 39 loss: 0.10231341 acc: 0.9578628540039062\n",
      "epoch: 48 step: 40 loss: 0.09949212 acc: 0.9580459594726562\n",
      "epoch: 48 step: 41 loss: 0.101150826 acc: 0.9625396728515625\n",
      "epoch: 48 step: 42 loss: 0.12735987 acc: 0.9586029052734375\n",
      "epoch: 48 step: 43 loss: 0.098938316 acc: 0.9620323181152344\n",
      "epoch: 48 step: 44 loss: 0.111305416 acc: 0.9536819458007812\n",
      "epoch: 48 step: 45 loss: 0.101236895 acc: 0.9582176208496094\n",
      "epoch: 48 step: 46 loss: 0.09797609 acc: 0.9658432006835938\n",
      "epoch: 48 step: 47 loss: 0.102638505 acc: 0.9611053466796875\n",
      "epoch: 48 step: 48 loss: 0.15846127 acc: 0.95574951171875\n",
      "epoch: 48 step: 49 loss: 0.08565223 acc: 0.9626731872558594\n",
      "epoch: 48 step: 50 loss: 0.13445581 acc: 0.9532661437988281\n",
      "epoch: 48 step: 51 loss: 0.09635774 acc: 0.9708480834960938\n",
      "epoch: 48 step: 52 loss: 0.09092503 acc: 0.9698066711425781\n",
      "epoch: 48 step: 53 loss: 0.10840025 acc: 0.9573097229003906\n",
      "epoch: 48 step: 54 loss: 0.11718371 acc: 0.9617195129394531\n",
      "epoch: 48 step: 55 loss: 0.08997885 acc: 0.9637641906738281\n",
      "epoch: 48 step: 56 loss: 0.103977494 acc: 0.9586067199707031\n",
      "epoch: 48 step: 57 loss: 0.10095826 acc: 0.9566192626953125\n",
      "epoch: 48 step: 58 loss: 0.101538315 acc: 0.9553947448730469\n",
      "epoch: 48 step: 59 loss: 0.10114914 acc: 0.9617576599121094\n",
      "epoch: 48 step: 60 loss: 0.09747711 acc: 0.9527053833007812\n",
      "epoch: 48 step: 61 loss: 0.13533694 acc: 0.9493179321289062\n",
      "epoch: 48 step: 62 loss: 0.097779125 acc: 0.95733642578125\n",
      "epoch: 48 step: 63 loss: 0.11796846 acc: 0.9480934143066406\n",
      "epoch: 48 step: 64 loss: 0.09241742 acc: 0.9581336975097656\n",
      "epoch: 48 step: 65 loss: 0.12151176 acc: 0.9617042541503906\n",
      "epoch: 48 step: 66 loss: 0.13908947 acc: 0.96099853515625\n",
      "epoch: 48 step: 67 loss: 0.099281296 acc: 0.9619140625\n",
      "epoch: 48 step: 68 loss: 0.104493335 acc: 0.9656906127929688\n",
      "epoch: 48 step: 69 loss: 0.11879972 acc: 0.9663581848144531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48 step: 70 loss: 0.1315179 acc: 0.9623374938964844\n",
      "epoch: 48 step: 71 loss: 0.13149223 acc: 0.956146240234375\n",
      "epoch: 48 step: 72 loss: 0.11233508 acc: 0.9566421508789062\n",
      "epoch: 48 step: 73 loss: 0.120031975 acc: 0.954986572265625\n",
      "epoch: 48 step: 74 loss: 0.1048449 acc: 0.953338623046875\n",
      "epoch: 48 step: 75 loss: 0.09983987 acc: 0.9486770629882812\n",
      "epoch: 48 step: 76 loss: 0.10278962 acc: 0.943603515625\n",
      "epoch: 48 step: 77 loss: 0.12051496 acc: 0.9494094848632812\n",
      "epoch: 48 step: 78 loss: 0.11512582 acc: 0.9432640075683594\n",
      "epoch: 48 step: 79 loss: 0.13283722 acc: 0.9523277282714844\n",
      "epoch: 48 step: 80 loss: 0.118951015 acc: 0.958404541015625\n",
      "epoch: 48 step: 81 loss: 0.11620992 acc: 0.9600486755371094\n",
      "epoch: 48 step: 82 loss: 0.11459345 acc: 0.9563446044921875\n",
      "epoch: 48 step: 83 loss: 0.12265691 acc: 0.9538002014160156\n",
      "epoch: 48 step: 84 loss: 0.106954664 acc: 0.9639434814453125\n",
      "epoch: 48 step: 85 loss: 0.104458734 acc: 0.9672470092773438\n",
      "epoch: 48 step: 86 loss: 0.12758346 acc: 0.956939697265625\n",
      "epoch: 48 step: 87 loss: 0.11744821 acc: 0.9580001831054688\n",
      "epoch: 48 step: 88 loss: 0.14108631 acc: 0.9535598754882812\n",
      "epoch: 48 step: 89 loss: 0.119687185 acc: 0.9636688232421875\n",
      "epoch: 48 step: 90 loss: 0.11226789 acc: 0.9598159790039062\n",
      "epoch: 48 step: 91 loss: 0.11177763 acc: 0.9630279541015625\n",
      "epoch: 48 step: 92 loss: 0.13298163 acc: 0.9497718811035156\n",
      "epoch: 48 step: 93 loss: 0.11274214 acc: 0.9582633972167969\n",
      "epoch: 48 step: 94 loss: 0.10273219 acc: 0.9578971862792969\n",
      "epoch: 48 step: 95 loss: 0.11737434 acc: 0.9585685729980469\n",
      "epoch: 48 step: 96 loss: 0.12820567 acc: 0.9566650390625\n",
      "epoch: 48 step: 97 loss: 0.115819834 acc: 0.9553260803222656\n",
      "epoch: 48 step: 98 loss: 0.13345541 acc: 0.9453887939453125\n",
      "epoch: 48 step: 99 loss: 0.10215551 acc: 0.9539947509765625\n",
      "epoch: 48 step: 100 loss: 0.10612265 acc: 0.9594879150390625\n",
      "epoch: 48 step: 101 loss: 0.12080804 acc: 0.9557075500488281\n",
      "epoch: 48 step: 102 loss: 0.08968561 acc: 0.9605064392089844\n",
      "epoch: 48 step: 103 loss: 0.100750916 acc: 0.9615249633789062\n",
      "epoch: 48 step: 104 loss: 0.13773578 acc: 0.9504852294921875\n",
      "epoch: 48 step: 105 loss: 0.11159315 acc: 0.958038330078125\n",
      "epoch: 48 step: 106 loss: 0.10253602 acc: 0.9573440551757812\n",
      "epoch: 48 step: 107 loss: 0.0919505 acc: 0.9663314819335938\n",
      "epoch: 48 step: 108 loss: 0.1247431 acc: 0.9562530517578125\n",
      "epoch: 48 step: 109 loss: 0.09938088 acc: 0.9617576599121094\n",
      "epoch: 48 step: 110 loss: 0.098193586 acc: 0.9585113525390625\n",
      "epoch: 48 step: 111 loss: 0.09698748 acc: 0.9611015319824219\n",
      "epoch: 48 step: 112 loss: 0.11168353 acc: 0.9553070068359375\n",
      "epoch: 48 step: 113 loss: 0.12903291 acc: 0.9512596130371094\n",
      "epoch: 48 step: 114 loss: 0.10606335 acc: 0.9532318115234375\n",
      "epoch: 48 step: 115 loss: 0.12644355 acc: 0.9569473266601562\n",
      "epoch: 48 step: 116 loss: 0.08596879 acc: 0.9629020690917969\n",
      "epoch: 48 step: 117 loss: 0.10159933 acc: 0.9628639221191406\n",
      "epoch: 48 step: 118 loss: 0.10144508 acc: 0.9610748291015625\n",
      "epoch: 48 step: 119 loss: 0.092424184 acc: 0.9594154357910156\n",
      "epoch: 48 step: 120 loss: 0.106916055 acc: 0.9568634033203125\n",
      "epoch: 48 step: 121 loss: 0.12605485 acc: 0.9491043090820312\n",
      "epoch: 48 step: 122 loss: 0.101504534 acc: 0.9580230712890625\n",
      "epoch: 48 step: 123 loss: 0.09758701 acc: 0.9586296081542969\n",
      "epoch: 48 step: 124 loss: 0.12424292 acc: 0.95037841796875\n",
      "epoch: 48 validation_loss: 0.109 validation_dice: 0.8334964963220508\n",
      "epoch: 48 test_dataset dice: 0.7498894713326304\n",
      "time cost 0.5236006299654643 min\n",
      "dice_best: 0.8334964963220508\n",
      "******************************** epoch  48  is finished. *********************************\n",
      "epoch: 49 step: 1 loss: 0.10167595 acc: 0.9601325988769531\n",
      "epoch: 49 step: 2 loss: 0.09964635 acc: 0.952484130859375\n",
      "epoch: 49 step: 3 loss: 0.11667631 acc: 0.9508552551269531\n",
      "epoch: 49 step: 4 loss: 0.10473025 acc: 0.9529075622558594\n",
      "epoch: 49 step: 5 loss: 0.1029224 acc: 0.9595985412597656\n",
      "epoch: 49 step: 6 loss: 0.1062153 acc: 0.9544868469238281\n",
      "epoch: 49 step: 7 loss: 0.10181826 acc: 0.955047607421875\n",
      "epoch: 49 step: 8 loss: 0.098685905 acc: 0.9657249450683594\n",
      "epoch: 49 step: 9 loss: 0.10945276 acc: 0.9531097412109375\n",
      "epoch: 49 step: 10 loss: 0.12531352 acc: 0.9548072814941406\n",
      "epoch: 49 step: 11 loss: 0.12189023 acc: 0.9633750915527344\n",
      "epoch: 49 step: 12 loss: 0.10311701 acc: 0.9621353149414062\n",
      "epoch: 49 step: 13 loss: 0.10592321 acc: 0.9602699279785156\n",
      "epoch: 49 step: 14 loss: 0.10040621 acc: 0.9592323303222656\n",
      "epoch: 49 step: 15 loss: 0.1115069 acc: 0.9596900939941406\n",
      "epoch: 49 step: 16 loss: 0.09810966 acc: 0.9547195434570312\n",
      "epoch: 49 step: 17 loss: 0.09711684 acc: 0.9603004455566406\n",
      "epoch: 49 step: 18 loss: 0.12083582 acc: 0.9498481750488281\n",
      "epoch: 49 step: 19 loss: 0.109728076 acc: 0.9543113708496094\n",
      "epoch: 49 step: 20 loss: 0.10273476 acc: 0.9538612365722656\n",
      "epoch: 49 step: 21 loss: 0.08788939 acc: 0.9588584899902344\n",
      "epoch: 49 step: 22 loss: 0.12291092 acc: 0.9471321105957031\n",
      "epoch: 49 step: 23 loss: 0.11026698 acc: 0.9543495178222656\n",
      "epoch: 49 step: 24 loss: 0.10218202 acc: 0.9571762084960938\n",
      "epoch: 49 step: 25 loss: 0.094055094 acc: 0.9626045227050781\n",
      "epoch: 49 step: 26 loss: 0.09403753 acc: 0.9595527648925781\n",
      "epoch: 49 step: 27 loss: 0.087439306 acc: 0.9607429504394531\n",
      "epoch: 49 step: 28 loss: 0.09068654 acc: 0.9653205871582031\n",
      "epoch: 49 step: 29 loss: 0.11785805 acc: 0.9476280212402344\n",
      "epoch: 49 step: 30 loss: 0.09073738 acc: 0.9595718383789062\n",
      "epoch: 49 step: 31 loss: 0.09742842 acc: 0.9603614807128906\n",
      "epoch: 49 step: 32 loss: 0.10699999 acc: 0.954833984375\n",
      "epoch: 49 step: 33 loss: 0.09624468 acc: 0.9668922424316406\n",
      "epoch: 49 step: 34 loss: 0.0892736 acc: 0.9578857421875\n",
      "epoch: 49 step: 35 loss: 0.097167775 acc: 0.9626655578613281\n",
      "epoch: 49 step: 36 loss: 0.09376443 acc: 0.9654998779296875\n",
      "epoch: 49 step: 37 loss: 0.1057151 acc: 0.960174560546875\n",
      "epoch: 49 step: 38 loss: 0.07895072 acc: 0.9676132202148438\n",
      "epoch: 49 step: 39 loss: 0.102292866 acc: 0.960418701171875\n",
      "epoch: 49 step: 40 loss: 0.09232395 acc: 0.9625053405761719\n",
      "epoch: 49 step: 41 loss: 0.11138395 acc: 0.9552993774414062\n",
      "epoch: 49 step: 42 loss: 0.108009286 acc: 0.9496803283691406\n",
      "epoch: 49 step: 43 loss: 0.09005808 acc: 0.9602737426757812\n",
      "epoch: 49 step: 44 loss: 0.10081434 acc: 0.9570083618164062\n",
      "epoch: 49 step: 45 loss: 0.1005789 acc: 0.9531059265136719\n",
      "epoch: 49 step: 46 loss: 0.08372871 acc: 0.9675369262695312\n",
      "epoch: 49 step: 47 loss: 0.09625619 acc: 0.9665412902832031\n",
      "epoch: 49 step: 48 loss: 0.10350565 acc: 0.9636688232421875\n",
      "epoch: 49 step: 49 loss: 0.1004396 acc: 0.9571189880371094\n",
      "epoch: 49 step: 50 loss: 0.11555558 acc: 0.9527549743652344\n",
      "epoch: 49 step: 51 loss: 0.08148862 acc: 0.9658699035644531\n",
      "epoch: 49 step: 52 loss: 0.090991884 acc: 0.9692916870117188\n",
      "epoch: 49 step: 53 loss: 0.09621238 acc: 0.9640617370605469\n",
      "epoch: 49 step: 54 loss: 0.07715162 acc: 0.96624755859375\n",
      "epoch: 49 step: 55 loss: 0.09795536 acc: 0.96807861328125\n",
      "epoch: 49 step: 56 loss: 0.08627187 acc: 0.9625892639160156\n",
      "epoch: 49 step: 57 loss: 0.096788436 acc: 0.9600753784179688\n",
      "epoch: 49 step: 58 loss: 0.09034764 acc: 0.9588508605957031\n",
      "epoch: 49 step: 59 loss: 0.11175053 acc: 0.953521728515625\n",
      "epoch: 49 step: 60 loss: 0.10999188 acc: 0.9495010375976562\n",
      "epoch: 49 step: 61 loss: 0.1168198 acc: 0.9491233825683594\n",
      "epoch: 49 step: 62 loss: 0.10040641 acc: 0.9653205871582031\n",
      "epoch: 49 step: 63 loss: 0.08089481 acc: 0.9627647399902344\n",
      "epoch: 49 step: 64 loss: 0.096222475 acc: 0.958251953125\n",
      "epoch: 49 step: 65 loss: 0.09117719 acc: 0.9630889892578125\n",
      "epoch: 49 step: 66 loss: 0.09642031 acc: 0.9575614929199219\n",
      "epoch: 49 step: 67 loss: 0.10516173 acc: 0.965576171875\n",
      "epoch: 49 step: 68 loss: 0.097533956 acc: 0.9615020751953125\n",
      "epoch: 49 step: 69 loss: 0.08752973 acc: 0.9577865600585938\n",
      "epoch: 49 step: 70 loss: 0.098348424 acc: 0.963653564453125\n",
      "epoch: 49 step: 71 loss: 0.10525376 acc: 0.9574661254882812\n",
      "epoch: 49 step: 72 loss: 0.116970144 acc: 0.9567031860351562\n",
      "epoch: 49 step: 73 loss: 0.09407828 acc: 0.9632644653320312\n",
      "epoch: 49 step: 74 loss: 0.13834086 acc: 0.9542350769042969\n",
      "epoch: 49 step: 75 loss: 0.07890453 acc: 0.9687728881835938\n",
      "epoch: 49 step: 76 loss: 0.11388207 acc: 0.9507217407226562\n",
      "epoch: 49 step: 77 loss: 0.108033665 acc: 0.9626808166503906\n",
      "epoch: 49 step: 78 loss: 0.08690491 acc: 0.9622917175292969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 step: 79 loss: 0.10746753 acc: 0.9537086486816406\n",
      "epoch: 49 step: 80 loss: 0.078149274 acc: 0.9652748107910156\n",
      "epoch: 49 step: 81 loss: 0.104973555 acc: 0.9567489624023438\n",
      "epoch: 49 step: 82 loss: 0.11027329 acc: 0.95556640625\n",
      "epoch: 49 step: 83 loss: 0.10409284 acc: 0.9658355712890625\n",
      "epoch: 49 step: 84 loss: 0.086333334 acc: 0.9702415466308594\n",
      "epoch: 49 step: 85 loss: 0.096542284 acc: 0.9549636840820312\n",
      "epoch: 49 step: 86 loss: 0.09709664 acc: 0.9567832946777344\n",
      "epoch: 49 step: 87 loss: 0.09520114 acc: 0.9521369934082031\n",
      "epoch: 49 step: 88 loss: 0.1119769 acc: 0.9538841247558594\n",
      "epoch: 49 step: 89 loss: 0.085444435 acc: 0.9639205932617188\n",
      "epoch: 49 step: 90 loss: 0.094617695 acc: 0.9688873291015625\n",
      "epoch: 49 step: 91 loss: 0.09196178 acc: 0.9615974426269531\n",
      "epoch: 49 step: 92 loss: 0.114087686 acc: 0.9546470642089844\n",
      "epoch: 49 step: 93 loss: 0.11556317 acc: 0.9523468017578125\n",
      "epoch: 49 step: 94 loss: 0.09117207 acc: 0.9589157104492188\n",
      "epoch: 49 step: 95 loss: 0.101323225 acc: 0.9614639282226562\n",
      "epoch: 49 step: 96 loss: 0.09880519 acc: 0.9642143249511719\n",
      "epoch: 49 step: 97 loss: 0.08820228 acc: 0.9626502990722656\n",
      "epoch: 49 step: 98 loss: 0.09448069 acc: 0.9583053588867188\n",
      "epoch: 49 step: 99 loss: 0.10103357 acc: 0.969390869140625\n",
      "epoch: 49 step: 100 loss: 0.1076311 acc: 0.9607429504394531\n",
      "epoch: 49 step: 101 loss: 0.10215672 acc: 0.9670295715332031\n",
      "epoch: 49 step: 102 loss: 0.08728856 acc: 0.9657211303710938\n",
      "epoch: 49 step: 103 loss: 0.08906449 acc: 0.9686927795410156\n",
      "epoch: 49 step: 104 loss: 0.103461236 acc: 0.9611740112304688\n",
      "epoch: 49 step: 105 loss: 0.094640106 acc: 0.9619522094726562\n",
      "epoch: 49 step: 106 loss: 0.095577575 acc: 0.959014892578125\n",
      "epoch: 49 step: 107 loss: 0.15509625 acc: 0.9536476135253906\n",
      "epoch: 49 step: 108 loss: 0.116404 acc: 0.9544601440429688\n",
      "epoch: 49 step: 109 loss: 0.10093069 acc: 0.9544525146484375\n",
      "epoch: 49 step: 110 loss: 0.10801564 acc: 0.9567642211914062\n",
      "epoch: 49 step: 111 loss: 0.10978371 acc: 0.9448051452636719\n",
      "epoch: 49 step: 112 loss: 0.08501745 acc: 0.961212158203125\n",
      "epoch: 49 step: 113 loss: 0.11283714 acc: 0.9550323486328125\n",
      "epoch: 49 step: 114 loss: 0.08637948 acc: 0.9620132446289062\n",
      "epoch: 49 step: 115 loss: 0.1128982 acc: 0.9500312805175781\n",
      "epoch: 49 step: 116 loss: 0.10526595 acc: 0.95599365234375\n",
      "epoch: 49 step: 117 loss: 0.10313948 acc: 0.9607658386230469\n",
      "epoch: 49 step: 118 loss: 0.10665727 acc: 0.9586257934570312\n",
      "epoch: 49 step: 119 loss: 0.10686393 acc: 0.9609413146972656\n",
      "epoch: 49 step: 120 loss: 0.08778559 acc: 0.9664573669433594\n",
      "epoch: 49 step: 121 loss: 0.10213903 acc: 0.9559402465820312\n",
      "epoch: 49 step: 122 loss: 0.0853912 acc: 0.9688034057617188\n",
      "epoch: 49 step: 123 loss: 0.08684123 acc: 0.9678306579589844\n",
      "epoch: 49 step: 124 loss: 0.099457845 acc: 0.9561331612723214\n",
      "epoch: 49 validation_loss: 0.101 validation_dice: 0.8309136994315257\n",
      "epoch: 49 test_dataset dice: 0.7384906584791939\n",
      "time cost 0.5338458697001139 min\n",
      "dice_best: 0.8334964963220508\n",
      "******************************** epoch  49  is finished. *********************************\n",
      "epoch: 50 step: 1 loss: 0.094442 acc: 0.9540214538574219\n",
      "epoch: 50 step: 2 loss: 0.09315059 acc: 0.9647483825683594\n",
      "epoch: 50 step: 3 loss: 0.10466133 acc: 0.9584999084472656\n",
      "epoch: 50 step: 4 loss: 0.1097798 acc: 0.9490470886230469\n",
      "epoch: 50 step: 5 loss: 0.10739255 acc: 0.9608955383300781\n",
      "epoch: 50 step: 6 loss: 0.09740625 acc: 0.9644050598144531\n",
      "epoch: 50 step: 7 loss: 0.08320586 acc: 0.962646484375\n",
      "epoch: 50 step: 8 loss: 0.09848653 acc: 0.9641304016113281\n",
      "epoch: 50 step: 9 loss: 0.082461156 acc: 0.9585189819335938\n",
      "epoch: 50 step: 10 loss: 0.07989174 acc: 0.9631690979003906\n",
      "epoch: 50 step: 11 loss: 0.09323901 acc: 0.9622802734375\n",
      "epoch: 50 step: 12 loss: 0.092657864 acc: 0.9633827209472656\n",
      "epoch: 50 step: 13 loss: 0.10025485 acc: 0.9579353332519531\n",
      "epoch: 50 step: 14 loss: 0.0901975 acc: 0.9647483825683594\n",
      "epoch: 50 step: 15 loss: 0.10287457 acc: 0.9612770080566406\n",
      "epoch: 50 step: 16 loss: 0.08507839 acc: 0.9601936340332031\n",
      "epoch: 50 step: 17 loss: 0.090533234 acc: 0.9612770080566406\n",
      "epoch: 50 step: 18 loss: 0.07529447 acc: 0.9669876098632812\n",
      "epoch: 50 step: 19 loss: 0.107907765 acc: 0.9578475952148438\n",
      "epoch: 50 step: 20 loss: 0.10748834 acc: 0.9632186889648438\n",
      "epoch: 50 step: 21 loss: 0.09319559 acc: 0.9607505798339844\n",
      "epoch: 50 step: 22 loss: 0.09315556 acc: 0.9610557556152344\n",
      "epoch: 50 step: 23 loss: 0.08501873 acc: 0.9685859680175781\n",
      "epoch: 50 step: 24 loss: 0.09201546 acc: 0.9637908935546875\n",
      "epoch: 50 step: 25 loss: 0.09164994 acc: 0.9647903442382812\n",
      "epoch: 50 step: 26 loss: 0.09639156 acc: 0.9641799926757812\n",
      "epoch: 50 step: 27 loss: 0.1041185 acc: 0.9585418701171875\n",
      "epoch: 50 step: 28 loss: 0.11295707 acc: 0.9537200927734375\n",
      "epoch: 50 step: 29 loss: 0.0885489 acc: 0.96429443359375\n",
      "epoch: 50 step: 30 loss: 0.09179406 acc: 0.9513816833496094\n",
      "epoch: 50 step: 31 loss: 0.10188053 acc: 0.9527931213378906\n",
      "epoch: 50 step: 32 loss: 0.08921373 acc: 0.95574951171875\n",
      "epoch: 50 step: 33 loss: 0.09421332 acc: 0.9608078002929688\n",
      "epoch: 50 step: 34 loss: 0.09886323 acc: 0.9684982299804688\n",
      "epoch: 50 step: 35 loss: 0.1040298 acc: 0.9589271545410156\n",
      "epoch: 50 step: 36 loss: 0.11922282 acc: 0.9650726318359375\n",
      "epoch: 50 step: 37 loss: 0.09810872 acc: 0.9581069946289062\n",
      "epoch: 50 step: 38 loss: 0.12694198 acc: 0.9599494934082031\n",
      "epoch: 50 step: 39 loss: 0.105092876 acc: 0.9676666259765625\n",
      "epoch: 50 step: 40 loss: 0.10210788 acc: 0.9586830139160156\n",
      "epoch: 50 step: 41 loss: 0.098653235 acc: 0.9604721069335938\n",
      "epoch: 50 step: 42 loss: 0.10085848 acc: 0.9580841064453125\n",
      "epoch: 50 step: 43 loss: 0.101145834 acc: 0.9678230285644531\n",
      "epoch: 50 step: 44 loss: 0.10572868 acc: 0.9606704711914062\n",
      "epoch: 50 step: 45 loss: 0.114219375 acc: 0.952301025390625\n",
      "epoch: 50 step: 46 loss: 0.12911169 acc: 0.9399795532226562\n",
      "epoch: 50 step: 47 loss: 0.115557045 acc: 0.9515304565429688\n",
      "epoch: 50 step: 48 loss: 0.13040078 acc: 0.9502410888671875\n",
      "epoch: 50 step: 49 loss: 0.13336053 acc: 0.9579391479492188\n",
      "epoch: 50 step: 50 loss: 0.09571657 acc: 0.96099853515625\n",
      "epoch: 50 step: 51 loss: 0.107984684 acc: 0.9587631225585938\n",
      "epoch: 50 step: 52 loss: 0.09705189 acc: 0.9617958068847656\n",
      "epoch: 50 step: 53 loss: 0.10932365 acc: 0.9576339721679688\n",
      "epoch: 50 step: 54 loss: 0.13932483 acc: 0.9521903991699219\n",
      "epoch: 50 step: 55 loss: 0.1039073 acc: 0.9529533386230469\n",
      "epoch: 50 step: 56 loss: 0.10933177 acc: 0.9529647827148438\n",
      "epoch: 50 step: 57 loss: 0.101870425 acc: 0.9576225280761719\n",
      "epoch: 50 step: 58 loss: 0.10619504 acc: 0.958526611328125\n",
      "epoch: 50 step: 59 loss: 0.12342025 acc: 0.9511909484863281\n",
      "epoch: 50 step: 60 loss: 0.13554901 acc: 0.9533843994140625\n",
      "epoch: 50 step: 61 loss: 0.124520585 acc: 0.9582023620605469\n",
      "epoch: 50 step: 62 loss: 0.094357915 acc: 0.9637413024902344\n",
      "epoch: 50 step: 63 loss: 0.11286263 acc: 0.9596977233886719\n",
      "epoch: 50 step: 64 loss: 0.119397104 acc: 0.9573783874511719\n",
      "epoch: 50 step: 65 loss: 0.102039516 acc: 0.9534263610839844\n",
      "epoch: 50 step: 66 loss: 0.10559217 acc: 0.957366943359375\n",
      "epoch: 50 step: 67 loss: 0.1232081 acc: 0.9583892822265625\n",
      "epoch: 50 step: 68 loss: 0.11178322 acc: 0.9502029418945312\n",
      "epoch: 50 step: 69 loss: 0.10290038 acc: 0.9528694152832031\n",
      "epoch: 50 step: 70 loss: 0.10890183 acc: 0.9558868408203125\n",
      "epoch: 50 step: 71 loss: 0.10932718 acc: 0.9534149169921875\n",
      "epoch: 50 step: 72 loss: 0.10613493 acc: 0.9525566101074219\n",
      "epoch: 50 step: 73 loss: 0.11008196 acc: 0.9519577026367188\n",
      "epoch: 50 step: 74 loss: 0.11836272 acc: 0.956451416015625\n",
      "epoch: 50 step: 75 loss: 0.091790736 acc: 0.9632606506347656\n",
      "epoch: 50 step: 76 loss: 0.09458027 acc: 0.9658622741699219\n",
      "epoch: 50 step: 77 loss: 0.10362883 acc: 0.9642181396484375\n",
      "epoch: 50 step: 78 loss: 0.094557226 acc: 0.9629058837890625\n",
      "epoch: 50 step: 79 loss: 0.12659234 acc: 0.9600296020507812\n",
      "epoch: 50 step: 80 loss: 0.11273376 acc: 0.9645271301269531\n",
      "epoch: 50 step: 81 loss: 0.10133872 acc: 0.9634895324707031\n",
      "epoch: 50 step: 82 loss: 0.12088336 acc: 0.9566612243652344\n",
      "epoch: 50 step: 83 loss: 0.098017566 acc: 0.9631729125976562\n",
      "epoch: 50 step: 84 loss: 0.10185954 acc: 0.9613189697265625\n",
      "epoch: 50 step: 85 loss: 0.10768365 acc: 0.95550537109375\n",
      "epoch: 50 step: 86 loss: 0.11023606 acc: 0.9448051452636719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 step: 87 loss: 0.11462405 acc: 0.9573593139648438\n",
      "epoch: 50 step: 88 loss: 0.09027311 acc: 0.9616050720214844\n",
      "epoch: 50 step: 89 loss: 0.09409994 acc: 0.9607620239257812\n",
      "epoch: 50 step: 90 loss: 0.10993656 acc: 0.9600028991699219\n",
      "epoch: 50 step: 91 loss: 0.14765099 acc: 0.9497299194335938\n",
      "epoch: 50 step: 92 loss: 0.13030124 acc: 0.95703125\n",
      "epoch: 50 step: 93 loss: 0.09405507 acc: 0.9615936279296875\n",
      "epoch: 50 step: 94 loss: 0.09827677 acc: 0.9640007019042969\n",
      "epoch: 50 step: 95 loss: 0.10640991 acc: 0.9546318054199219\n",
      "epoch: 50 step: 96 loss: 0.120076284 acc: 0.9575843811035156\n",
      "epoch: 50 step: 97 loss: 0.095158644 acc: 0.9612655639648438\n",
      "epoch: 50 step: 98 loss: 0.11538171 acc: 0.96453857421875\n",
      "epoch: 50 step: 99 loss: 0.095602654 acc: 0.9657325744628906\n",
      "epoch: 50 step: 100 loss: 0.13073206 acc: 0.9540519714355469\n",
      "epoch: 50 step: 101 loss: 0.13500349 acc: 0.956756591796875\n",
      "epoch: 50 step: 102 loss: 0.12089375 acc: 0.9490013122558594\n",
      "epoch: 50 step: 103 loss: 0.09506929 acc: 0.9594688415527344\n",
      "epoch: 50 step: 104 loss: 0.11633013 acc: 0.9482383728027344\n",
      "epoch: 50 step: 105 loss: 0.12082867 acc: 0.94244384765625\n",
      "epoch: 50 step: 106 loss: 0.11022679 acc: 0.9510459899902344\n",
      "epoch: 50 step: 107 loss: 0.109171584 acc: 0.9569511413574219\n",
      "epoch: 50 step: 108 loss: 0.11064752 acc: 0.9482650756835938\n",
      "epoch: 50 step: 109 loss: 0.11857411 acc: 0.9506187438964844\n",
      "epoch: 50 step: 110 loss: 0.12228553 acc: 0.9551773071289062\n",
      "epoch: 50 step: 111 loss: 0.11030958 acc: 0.9608039855957031\n",
      "epoch: 50 step: 112 loss: 0.096618526 acc: 0.9656524658203125\n",
      "epoch: 50 step: 113 loss: 0.14593017 acc: 0.9587287902832031\n",
      "epoch: 50 step: 114 loss: 0.12156365 acc: 0.9570960998535156\n",
      "epoch: 50 step: 115 loss: 0.09499026 acc: 0.9700164794921875\n",
      "epoch: 50 step: 116 loss: 0.10672766 acc: 0.9658470153808594\n",
      "epoch: 50 step: 117 loss: 0.13899486 acc: 0.9553108215332031\n",
      "epoch: 50 step: 118 loss: 0.112388164 acc: 0.9603347778320312\n",
      "epoch: 50 step: 119 loss: 0.116313584 acc: 0.9593658447265625\n",
      "epoch: 50 step: 120 loss: 0.124758594 acc: 0.9454154968261719\n",
      "epoch: 50 step: 121 loss: 0.13647509 acc: 0.9417037963867188\n",
      "epoch: 50 step: 122 loss: 0.14452715 acc: 0.9530982971191406\n",
      "epoch: 50 step: 123 loss: 0.093672134 acc: 0.9624977111816406\n",
      "epoch: 50 step: 124 loss: 0.13774778 acc: 0.9483380998883929\n",
      "epoch: 50 validation_loss: 0.106 validation_dice: 0.8176929588383953\n",
      "epoch: 50 test_dataset dice: 0.740538326544138\n",
      "time cost 0.5367177645365397 min\n",
      "dice_best: 0.8334964963220508\n",
      "******************************** epoch  50  is finished. *********************************\n",
      "epoch: 51 step: 1 loss: 0.11198423 acc: 0.9460792541503906\n",
      "epoch: 51 step: 2 loss: 0.13647838 acc: 0.9515533447265625\n",
      "epoch: 51 step: 3 loss: 0.12083931 acc: 0.9536972045898438\n",
      "epoch: 51 step: 4 loss: 0.11948459 acc: 0.9573745727539062\n",
      "epoch: 51 step: 5 loss: 0.09802406 acc: 0.9590377807617188\n",
      "epoch: 51 step: 6 loss: 0.10186725 acc: 0.963623046875\n",
      "epoch: 51 step: 7 loss: 0.14243741 acc: 0.9551162719726562\n",
      "epoch: 51 step: 8 loss: 0.1155632 acc: 0.9564208984375\n",
      "epoch: 51 step: 9 loss: 0.10236877 acc: 0.9570960998535156\n",
      "epoch: 51 step: 10 loss: 0.11257339 acc: 0.9578704833984375\n",
      "epoch: 51 step: 11 loss: 0.117071584 acc: 0.9548988342285156\n",
      "epoch: 51 step: 12 loss: 0.0937891 acc: 0.9667434692382812\n",
      "epoch: 51 step: 13 loss: 0.092458755 acc: 0.9681320190429688\n",
      "epoch: 51 step: 14 loss: 0.08488831 acc: 0.96453857421875\n",
      "epoch: 51 step: 15 loss: 0.14468667 acc: 0.9496345520019531\n",
      "epoch: 51 step: 16 loss: 0.10199008 acc: 0.9541664123535156\n",
      "epoch: 51 step: 17 loss: 0.083081104 acc: 0.9606704711914062\n",
      "epoch: 51 step: 18 loss: 0.1045907 acc: 0.9635124206542969\n",
      "epoch: 51 step: 19 loss: 0.15052018 acc: 0.9515571594238281\n",
      "epoch: 51 step: 20 loss: 0.08174757 acc: 0.9675407409667969\n",
      "epoch: 51 step: 21 loss: 0.13007797 acc: 0.962615966796875\n",
      "epoch: 51 step: 22 loss: 0.108961284 acc: 0.9618377685546875\n",
      "epoch: 51 step: 23 loss: 0.11224117 acc: 0.9649085998535156\n",
      "epoch: 51 step: 24 loss: 0.11258946 acc: 0.9600601196289062\n",
      "epoch: 51 step: 25 loss: 0.10532947 acc: 0.967071533203125\n",
      "epoch: 51 step: 26 loss: 0.11060708 acc: 0.9589042663574219\n",
      "epoch: 51 step: 27 loss: 0.11787929 acc: 0.9493637084960938\n",
      "epoch: 51 step: 28 loss: 0.094088964 acc: 0.955352783203125\n",
      "epoch: 51 step: 29 loss: 0.08687232 acc: 0.963592529296875\n",
      "epoch: 51 step: 30 loss: 0.09413227 acc: 0.9536590576171875\n",
      "epoch: 51 step: 31 loss: 0.104439236 acc: 0.9543571472167969\n",
      "epoch: 51 step: 32 loss: 0.09971643 acc: 0.9571914672851562\n",
      "epoch: 51 step: 33 loss: 0.0926575 acc: 0.9590644836425781\n",
      "epoch: 51 step: 34 loss: 0.09566985 acc: 0.9569282531738281\n",
      "epoch: 51 step: 35 loss: 0.09911121 acc: 0.9651565551757812\n",
      "epoch: 51 step: 36 loss: 0.08861404 acc: 0.9663276672363281\n",
      "epoch: 51 step: 37 loss: 0.11207635 acc: 0.9547958374023438\n",
      "epoch: 51 step: 38 loss: 0.09766362 acc: 0.9638442993164062\n",
      "epoch: 51 step: 39 loss: 0.10115462 acc: 0.9654045104980469\n",
      "epoch: 51 step: 40 loss: 0.09620241 acc: 0.9663810729980469\n",
      "epoch: 51 step: 41 loss: 0.10849615 acc: 0.9561080932617188\n",
      "epoch: 51 step: 42 loss: 0.09094681 acc: 0.9625167846679688\n",
      "epoch: 51 step: 43 loss: 0.12066042 acc: 0.9559402465820312\n",
      "epoch: 51 step: 44 loss: 0.11800678 acc: 0.9576225280761719\n",
      "epoch: 51 step: 45 loss: 0.093941085 acc: 0.96435546875\n",
      "epoch: 51 step: 46 loss: 0.116649725 acc: 0.963775634765625\n",
      "epoch: 51 step: 47 loss: 0.097317055 acc: 0.9561882019042969\n",
      "epoch: 51 step: 48 loss: 0.095554106 acc: 0.9538230895996094\n",
      "epoch: 51 step: 49 loss: 0.08942228 acc: 0.9578094482421875\n",
      "epoch: 51 step: 50 loss: 0.089300364 acc: 0.9660415649414062\n",
      "epoch: 51 step: 51 loss: 0.09704048 acc: 0.9561195373535156\n",
      "epoch: 51 step: 52 loss: 0.10797062 acc: 0.9562606811523438\n",
      "epoch: 51 step: 53 loss: 0.098198995 acc: 0.9588470458984375\n",
      "epoch: 51 step: 54 loss: 0.09820236 acc: 0.9582405090332031\n",
      "epoch: 51 step: 55 loss: 0.08826999 acc: 0.9722671508789062\n",
      "epoch: 51 step: 56 loss: 0.08726511 acc: 0.9643936157226562\n",
      "epoch: 51 step: 57 loss: 0.10347566 acc: 0.9670486450195312\n",
      "epoch: 51 step: 58 loss: 0.10137096 acc: 0.9607315063476562\n",
      "epoch: 51 step: 59 loss: 0.12109232 acc: 0.9643630981445312\n",
      "epoch: 51 step: 60 loss: 0.096185036 acc: 0.9642753601074219\n",
      "epoch: 51 step: 61 loss: 0.099277824 acc: 0.9604377746582031\n",
      "epoch: 51 step: 62 loss: 0.084354624 acc: 0.963714599609375\n",
      "epoch: 51 step: 63 loss: 0.10148434 acc: 0.9593391418457031\n",
      "epoch: 51 step: 64 loss: 0.08813661 acc: 0.9639701843261719\n",
      "epoch: 51 step: 65 loss: 0.08908506 acc: 0.9629478454589844\n",
      "epoch: 51 step: 66 loss: 0.08718999 acc: 0.9571151733398438\n",
      "epoch: 51 step: 67 loss: 0.08175137 acc: 0.9594001770019531\n",
      "epoch: 51 step: 68 loss: 0.09652677 acc: 0.9556884765625\n",
      "epoch: 51 step: 69 loss: 0.09789567 acc: 0.9588394165039062\n",
      "epoch: 51 step: 70 loss: 0.1157128 acc: 0.9559440612792969\n",
      "epoch: 51 step: 71 loss: 0.07922224 acc: 0.9620628356933594\n",
      "epoch: 51 step: 72 loss: 0.109246686 acc: 0.9567184448242188\n",
      "epoch: 51 step: 73 loss: 0.10262996 acc: 0.9517250061035156\n",
      "epoch: 51 step: 74 loss: 0.09534612 acc: 0.9595260620117188\n",
      "epoch: 51 step: 75 loss: 0.096456915 acc: 0.9600181579589844\n",
      "epoch: 51 step: 76 loss: 0.095388435 acc: 0.9625129699707031\n",
      "epoch: 51 step: 77 loss: 0.10455276 acc: 0.9514007568359375\n",
      "epoch: 51 step: 78 loss: 0.112893514 acc: 0.9493446350097656\n",
      "epoch: 51 step: 79 loss: 0.10172731 acc: 0.9585723876953125\n",
      "epoch: 51 step: 80 loss: 0.10093306 acc: 0.9509391784667969\n",
      "epoch: 51 step: 81 loss: 0.10087781 acc: 0.9630355834960938\n",
      "epoch: 51 step: 82 loss: 0.10731426 acc: 0.9543418884277344\n",
      "epoch: 51 step: 83 loss: 0.10162964 acc: 0.9525299072265625\n",
      "epoch: 51 step: 84 loss: 0.08438709 acc: 0.9667816162109375\n",
      "epoch: 51 step: 85 loss: 0.09413704 acc: 0.9567756652832031\n",
      "epoch: 51 step: 86 loss: 0.09982039 acc: 0.959869384765625\n",
      "epoch: 51 step: 87 loss: 0.09036021 acc: 0.9580802917480469\n",
      "epoch: 51 step: 88 loss: 0.105665304 acc: 0.9542007446289062\n",
      "epoch: 51 step: 89 loss: 0.08826942 acc: 0.9619216918945312\n",
      "epoch: 51 step: 90 loss: 0.097429425 acc: 0.9605484008789062\n",
      "epoch: 51 step: 91 loss: 0.10439706 acc: 0.9605293273925781\n",
      "epoch: 51 step: 92 loss: 0.094093025 acc: 0.9640388488769531\n",
      "epoch: 51 step: 93 loss: 0.09272656 acc: 0.959991455078125\n",
      "epoch: 51 step: 94 loss: 0.08769713 acc: 0.9651107788085938\n",
      "epoch: 51 step: 95 loss: 0.093680784 acc: 0.9599647521972656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 51 step: 96 loss: 0.09854065 acc: 0.9583244323730469\n",
      "epoch: 51 step: 97 loss: 0.08696638 acc: 0.9644279479980469\n",
      "epoch: 51 step: 98 loss: 0.10356964 acc: 0.9548416137695312\n",
      "epoch: 51 step: 99 loss: 0.093093485 acc: 0.9658889770507812\n",
      "epoch: 51 step: 100 loss: 0.096136495 acc: 0.9635162353515625\n",
      "epoch: 51 step: 101 loss: 0.08690556 acc: 0.9605979919433594\n",
      "epoch: 51 step: 102 loss: 0.11596953 acc: 0.9568519592285156\n",
      "epoch: 51 step: 103 loss: 0.10053567 acc: 0.9643783569335938\n",
      "epoch: 51 step: 104 loss: 0.083801575 acc: 0.9576988220214844\n",
      "epoch: 51 step: 105 loss: 0.0832804 acc: 0.9623680114746094\n",
      "epoch: 51 step: 106 loss: 0.07683711 acc: 0.9690666198730469\n",
      "epoch: 51 step: 107 loss: 0.102856964 acc: 0.9498138427734375\n",
      "epoch: 51 step: 108 loss: 0.10486308 acc: 0.9632148742675781\n",
      "epoch: 51 step: 109 loss: 0.09590989 acc: 0.9578475952148438\n",
      "epoch: 51 step: 110 loss: 0.12389735 acc: 0.9528160095214844\n",
      "epoch: 51 step: 111 loss: 0.08940068 acc: 0.96356201171875\n",
      "epoch: 51 step: 112 loss: 0.08578868 acc: 0.9700241088867188\n",
      "epoch: 51 step: 113 loss: 0.088625655 acc: 0.9621772766113281\n",
      "epoch: 51 step: 114 loss: 0.10169929 acc: 0.9587936401367188\n",
      "epoch: 51 step: 115 loss: 0.07636572 acc: 0.9663658142089844\n",
      "epoch: 51 step: 116 loss: 0.11717463 acc: 0.9501533508300781\n",
      "epoch: 51 step: 117 loss: 0.1062017 acc: 0.9541206359863281\n",
      "epoch: 51 step: 118 loss: 0.09742945 acc: 0.9609909057617188\n",
      "epoch: 51 step: 119 loss: 0.075873286 acc: 0.9666709899902344\n",
      "epoch: 51 step: 120 loss: 0.110716976 acc: 0.9690742492675781\n",
      "epoch: 51 step: 121 loss: 0.10056721 acc: 0.9623069763183594\n",
      "epoch: 51 step: 122 loss: 0.098834455 acc: 0.9541969299316406\n",
      "epoch: 51 step: 123 loss: 0.111957744 acc: 0.9601898193359375\n",
      "epoch: 51 step: 124 loss: 0.07570131 acc: 0.9698050362723214\n",
      "epoch: 51 validation_loss: 0.105 validation_dice: 0.8151607680681648\n",
      "epoch: 51 test_dataset dice: 0.7463074299690529\n",
      "time cost 0.5367948969205221 min\n",
      "dice_best: 0.8334964963220508\n",
      "******************************** epoch  51  is finished. *********************************\n",
      "epoch: 52 step: 1 loss: 0.07919388 acc: 0.9716567993164062\n",
      "epoch: 52 step: 2 loss: 0.10664551 acc: 0.95697021484375\n",
      "epoch: 52 step: 3 loss: 0.10297832 acc: 0.9524574279785156\n",
      "epoch: 52 step: 4 loss: 0.09248702 acc: 0.962005615234375\n",
      "epoch: 52 step: 5 loss: 0.12489172 acc: 0.958160400390625\n",
      "epoch: 52 step: 6 loss: 0.09617606 acc: 0.9600028991699219\n",
      "epoch: 52 step: 7 loss: 0.11698484 acc: 0.9528579711914062\n",
      "epoch: 52 step: 8 loss: 0.0953279 acc: 0.9557418823242188\n",
      "epoch: 52 step: 9 loss: 0.11681664 acc: 0.9595909118652344\n",
      "epoch: 52 step: 10 loss: 0.10001714 acc: 0.9544601440429688\n",
      "epoch: 52 step: 11 loss: 0.1033133 acc: 0.9552536010742188\n",
      "epoch: 52 step: 12 loss: 0.0965166 acc: 0.9586944580078125\n",
      "epoch: 52 step: 13 loss: 0.08733186 acc: 0.9591827392578125\n",
      "epoch: 52 step: 14 loss: 0.0940776 acc: 0.96197509765625\n",
      "epoch: 52 step: 15 loss: 0.087481916 acc: 0.9596061706542969\n",
      "epoch: 52 step: 16 loss: 0.08581634 acc: 0.956390380859375\n",
      "epoch: 52 step: 17 loss: 0.08676848 acc: 0.9684906005859375\n",
      "epoch: 52 step: 18 loss: 0.10360579 acc: 0.9577484130859375\n",
      "epoch: 52 step: 19 loss: 0.08659979 acc: 0.9613876342773438\n",
      "epoch: 52 step: 20 loss: 0.10623024 acc: 0.9611396789550781\n",
      "epoch: 52 step: 21 loss: 0.08896655 acc: 0.9591293334960938\n",
      "epoch: 52 step: 22 loss: 0.11575212 acc: 0.9519920349121094\n",
      "epoch: 52 step: 23 loss: 0.090555 acc: 0.9590682983398438\n",
      "epoch: 52 step: 24 loss: 0.09295158 acc: 0.9588508605957031\n",
      "epoch: 52 step: 25 loss: 0.11662408 acc: 0.9517402648925781\n",
      "epoch: 52 step: 26 loss: 0.097185545 acc: 0.9462928771972656\n",
      "epoch: 52 step: 27 loss: 0.09159465 acc: 0.9513206481933594\n",
      "epoch: 52 step: 28 loss: 0.09312607 acc: 0.9542274475097656\n",
      "epoch: 52 step: 29 loss: 0.08325283 acc: 0.9542312622070312\n",
      "epoch: 52 step: 30 loss: 0.09417537 acc: 0.9533882141113281\n",
      "epoch: 52 step: 31 loss: 0.09832817 acc: 0.9512519836425781\n",
      "epoch: 52 step: 32 loss: 0.09199823 acc: 0.9655685424804688\n",
      "epoch: 52 step: 33 loss: 0.08398626 acc: 0.9672012329101562\n",
      "epoch: 52 step: 34 loss: 0.104106314 acc: 0.9582099914550781\n",
      "epoch: 52 step: 35 loss: 0.09991056 acc: 0.9596405029296875\n",
      "epoch: 52 step: 36 loss: 0.10290878 acc: 0.955230712890625\n",
      "epoch: 52 step: 37 loss: 0.0920528 acc: 0.9551544189453125\n",
      "epoch: 52 step: 38 loss: 0.09577792 acc: 0.9575042724609375\n",
      "epoch: 52 step: 39 loss: 0.091801494 acc: 0.9569206237792969\n",
      "epoch: 52 step: 40 loss: 0.09224706 acc: 0.959808349609375\n",
      "epoch: 52 step: 41 loss: 0.08621959 acc: 0.961090087890625\n",
      "epoch: 52 step: 42 loss: 0.08261027 acc: 0.9632301330566406\n",
      "epoch: 52 step: 43 loss: 0.12554695 acc: 0.9571647644042969\n",
      "epoch: 52 step: 44 loss: 0.09273284 acc: 0.9622077941894531\n",
      "epoch: 52 step: 45 loss: 0.08439605 acc: 0.9627685546875\n",
      "epoch: 52 step: 46 loss: 0.085219115 acc: 0.9554786682128906\n",
      "epoch: 52 step: 47 loss: 0.09429627 acc: 0.9590950012207031\n",
      "epoch: 52 step: 48 loss: 0.06886798 acc: 0.9719047546386719\n",
      "epoch: 52 step: 49 loss: 0.0841817 acc: 0.9661026000976562\n",
      "epoch: 52 step: 50 loss: 0.09767729 acc: 0.9612655639648438\n",
      "epoch: 52 step: 51 loss: 0.11537186 acc: 0.9542922973632812\n",
      "epoch: 52 step: 52 loss: 0.08895713 acc: 0.9711647033691406\n",
      "epoch: 52 step: 53 loss: 0.09431216 acc: 0.9629402160644531\n",
      "epoch: 52 step: 54 loss: 0.09404307 acc: 0.9532279968261719\n",
      "epoch: 52 step: 55 loss: 0.09649894 acc: 0.9584121704101562\n",
      "epoch: 52 step: 56 loss: 0.09133357 acc: 0.9568443298339844\n",
      "epoch: 52 step: 57 loss: 0.09270503 acc: 0.9599876403808594\n",
      "epoch: 52 step: 58 loss: 0.083704986 acc: 0.9589462280273438\n",
      "epoch: 52 step: 59 loss: 0.104028985 acc: 0.9465217590332031\n",
      "epoch: 52 step: 60 loss: 0.10458327 acc: 0.9569892883300781\n",
      "epoch: 52 step: 61 loss: 0.08381942 acc: 0.9663009643554688\n",
      "epoch: 52 step: 62 loss: 0.1013391 acc: 0.9628105163574219\n",
      "epoch: 52 step: 63 loss: 0.10283501 acc: 0.9517669677734375\n",
      "epoch: 52 step: 64 loss: 0.08379124 acc: 0.9608993530273438\n",
      "epoch: 52 step: 65 loss: 0.084703125 acc: 0.9663276672363281\n",
      "epoch: 52 step: 66 loss: 0.08744529 acc: 0.9625968933105469\n",
      "epoch: 52 step: 67 loss: 0.09565603 acc: 0.9612274169921875\n",
      "epoch: 52 step: 68 loss: 0.095484845 acc: 0.9623184204101562\n",
      "epoch: 52 step: 69 loss: 0.08588884 acc: 0.9664230346679688\n",
      "epoch: 52 step: 70 loss: 0.09732042 acc: 0.9633216857910156\n",
      "epoch: 52 step: 71 loss: 0.0994169 acc: 0.960906982421875\n",
      "epoch: 52 step: 72 loss: 0.10208613 acc: 0.9545173645019531\n",
      "epoch: 52 step: 73 loss: 0.08139809 acc: 0.9627304077148438\n",
      "epoch: 52 step: 74 loss: 0.1004893 acc: 0.9589920043945312\n",
      "epoch: 52 step: 75 loss: 0.083082765 acc: 0.9616584777832031\n",
      "epoch: 52 step: 76 loss: 0.089168265 acc: 0.9599266052246094\n",
      "epoch: 52 step: 77 loss: 0.10863106 acc: 0.9577407836914062\n",
      "epoch: 52 step: 78 loss: 0.09429937 acc: 0.9634323120117188\n",
      "epoch: 52 step: 79 loss: 0.07981876 acc: 0.9663772583007812\n",
      "epoch: 52 step: 80 loss: 0.10711385 acc: 0.9615554809570312\n",
      "epoch: 52 step: 81 loss: 0.098052375 acc: 0.9567222595214844\n",
      "epoch: 52 step: 82 loss: 0.11587701 acc: 0.9652366638183594\n",
      "epoch: 52 step: 83 loss: 0.09762416 acc: 0.9636688232421875\n",
      "epoch: 52 step: 84 loss: 0.08943068 acc: 0.9674568176269531\n",
      "epoch: 52 step: 85 loss: 0.1078526 acc: 0.9590682983398438\n",
      "epoch: 52 step: 86 loss: 0.10259637 acc: 0.9622764587402344\n",
      "epoch: 52 step: 87 loss: 0.10548917 acc: 0.9619369506835938\n",
      "epoch: 52 step: 88 loss: 0.09486409 acc: 0.9575538635253906\n",
      "epoch: 52 step: 89 loss: 0.09534074 acc: 0.9596138000488281\n",
      "epoch: 52 step: 90 loss: 0.1053607 acc: 0.953857421875\n",
      "epoch: 52 step: 91 loss: 0.09407462 acc: 0.9542694091796875\n",
      "epoch: 52 step: 92 loss: 0.10765269 acc: 0.9541664123535156\n",
      "epoch: 52 step: 93 loss: 0.07183871 acc: 0.969940185546875\n",
      "epoch: 52 step: 94 loss: 0.10907208 acc: 0.9630470275878906\n",
      "epoch: 52 step: 95 loss: 0.13134722 acc: 0.9634895324707031\n",
      "epoch: 52 step: 96 loss: 0.10759122 acc: 0.9592208862304688\n",
      "epoch: 52 step: 97 loss: 0.14522167 acc: 0.9630699157714844\n",
      "epoch: 52 step: 98 loss: 0.110633224 acc: 0.9566993713378906\n",
      "epoch: 52 step: 99 loss: 0.12252327 acc: 0.9515380859375\n",
      "epoch: 52 step: 100 loss: 0.104398854 acc: 0.9559059143066406\n",
      "epoch: 52 step: 101 loss: 0.13142383 acc: 0.95489501953125\n",
      "epoch: 52 step: 102 loss: 0.11836379 acc: 0.9459953308105469\n",
      "epoch: 52 step: 103 loss: 0.11109383 acc: 0.9572944641113281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 52 step: 104 loss: 0.12127856 acc: 0.9521827697753906\n",
      "epoch: 52 step: 105 loss: 0.095021605 acc: 0.9650535583496094\n",
      "epoch: 52 step: 106 loss: 0.11789489 acc: 0.9656715393066406\n",
      "epoch: 52 step: 107 loss: 0.10093982 acc: 0.9650306701660156\n",
      "epoch: 52 step: 108 loss: 0.10778201 acc: 0.9594306945800781\n",
      "epoch: 52 step: 109 loss: 0.111671925 acc: 0.9671669006347656\n",
      "epoch: 52 step: 110 loss: 0.10687054 acc: 0.9672355651855469\n",
      "epoch: 52 step: 111 loss: 0.11744876 acc: 0.9597396850585938\n",
      "epoch: 52 step: 112 loss: 0.08742185 acc: 0.9609870910644531\n",
      "epoch: 52 step: 113 loss: 0.108464785 acc: 0.9633827209472656\n",
      "epoch: 52 step: 114 loss: 0.11239538 acc: 0.9606475830078125\n",
      "epoch: 52 step: 115 loss: 0.11765053 acc: 0.9623451232910156\n",
      "epoch: 52 step: 116 loss: 0.11177974 acc: 0.9529571533203125\n",
      "epoch: 52 step: 117 loss: 0.115952305 acc: 0.9511337280273438\n",
      "epoch: 52 step: 118 loss: 0.09718698 acc: 0.9536247253417969\n",
      "epoch: 52 step: 119 loss: 0.09210458 acc: 0.9690742492675781\n",
      "epoch: 52 step: 120 loss: 0.082253024 acc: 0.9597434997558594\n",
      "epoch: 52 step: 121 loss: 0.0993904 acc: 0.9514236450195312\n",
      "epoch: 52 step: 122 loss: 0.10435221 acc: 0.9631080627441406\n",
      "epoch: 52 step: 123 loss: 0.09679678 acc: 0.9632759094238281\n",
      "epoch: 52 step: 124 loss: 0.09565415 acc: 0.9621233258928571\n",
      "epoch: 52 validation_loss: 0.111 validation_dice: 0.8293935490294391\n",
      "epoch: 52 test_dataset dice: 0.7205616551020987\n",
      "time cost 0.5375646750132242 min\n",
      "dice_best: 0.8334964963220508\n",
      "******************************** epoch  52  is finished. *********************************\n",
      "epoch: 53 step: 1 loss: 0.08457654 acc: 0.9620285034179688\n",
      "epoch: 53 step: 2 loss: 0.105777815 acc: 0.95880126953125\n",
      "epoch: 53 step: 3 loss: 0.1134464 acc: 0.9583549499511719\n",
      "epoch: 53 step: 4 loss: 0.13284749 acc: 0.9551010131835938\n",
      "epoch: 53 step: 5 loss: 0.09215274 acc: 0.9680900573730469\n",
      "epoch: 53 step: 6 loss: 0.11056739 acc: 0.9569129943847656\n",
      "epoch: 53 step: 7 loss: 0.09783505 acc: 0.9663200378417969\n",
      "epoch: 53 step: 8 loss: 0.09458259 acc: 0.9550514221191406\n",
      "epoch: 53 step: 9 loss: 0.09329547 acc: 0.9586524963378906\n",
      "epoch: 53 step: 10 loss: 0.09638143 acc: 0.953460693359375\n",
      "epoch: 53 step: 11 loss: 0.09558833 acc: 0.9568634033203125\n",
      "epoch: 53 step: 12 loss: 0.10474174 acc: 0.9658317565917969\n",
      "epoch: 53 step: 13 loss: 0.09091217 acc: 0.9653396606445312\n",
      "epoch: 53 step: 14 loss: 0.09177727 acc: 0.9646263122558594\n",
      "epoch: 53 step: 15 loss: 0.082812436 acc: 0.9706306457519531\n",
      "epoch: 53 step: 16 loss: 0.08232348 acc: 0.967529296875\n",
      "epoch: 53 step: 17 loss: 0.10328206 acc: 0.9607124328613281\n",
      "epoch: 53 step: 18 loss: 0.108505815 acc: 0.9512100219726562\n",
      "epoch: 53 step: 19 loss: 0.10137198 acc: 0.9598159790039062\n",
      "epoch: 53 step: 20 loss: 0.11733708 acc: 0.9554557800292969\n",
      "epoch: 53 step: 21 loss: 0.10448418 acc: 0.9522933959960938\n",
      "epoch: 53 step: 22 loss: 0.1118204 acc: 0.9501876831054688\n",
      "epoch: 53 step: 23 loss: 0.106246054 acc: 0.9553680419921875\n",
      "epoch: 53 step: 24 loss: 0.10098176 acc: 0.9503440856933594\n",
      "epoch: 53 step: 25 loss: 0.09693579 acc: 0.962615966796875\n",
      "epoch: 53 step: 26 loss: 0.09220305 acc: 0.9573745727539062\n",
      "epoch: 53 step: 27 loss: 0.13637416 acc: 0.9645042419433594\n",
      "epoch: 53 step: 28 loss: 0.07355933 acc: 0.9682960510253906\n",
      "epoch: 53 step: 29 loss: 0.109415434 acc: 0.9621315002441406\n",
      "epoch: 53 step: 30 loss: 0.10681525 acc: 0.9553985595703125\n",
      "epoch: 53 step: 31 loss: 0.107644536 acc: 0.9585113525390625\n",
      "epoch: 53 step: 32 loss: 0.10650545 acc: 0.96026611328125\n",
      "epoch: 53 step: 33 loss: 0.090221904 acc: 0.96282958984375\n",
      "epoch: 53 step: 34 loss: 0.1126042 acc: 0.9491233825683594\n",
      "epoch: 53 step: 35 loss: 0.09055172 acc: 0.9612541198730469\n",
      "epoch: 53 step: 36 loss: 0.085943 acc: 0.9671478271484375\n",
      "epoch: 53 step: 37 loss: 0.10134069 acc: 0.9562454223632812\n",
      "epoch: 53 step: 38 loss: 0.10416016 acc: 0.9590873718261719\n",
      "epoch: 53 step: 39 loss: 0.11167651 acc: 0.9511222839355469\n",
      "epoch: 53 step: 40 loss: 0.102436386 acc: 0.9574203491210938\n",
      "epoch: 53 step: 41 loss: 0.08623254 acc: 0.9634780883789062\n",
      "epoch: 53 step: 42 loss: 0.0990493 acc: 0.960723876953125\n",
      "epoch: 53 step: 43 loss: 0.11121832 acc: 0.9551200866699219\n",
      "epoch: 53 step: 44 loss: 0.10373001 acc: 0.9542655944824219\n",
      "epoch: 53 step: 45 loss: 0.113796465 acc: 0.9575881958007812\n",
      "epoch: 53 step: 46 loss: 0.08620791 acc: 0.9609947204589844\n",
      "epoch: 53 step: 47 loss: 0.09025821 acc: 0.9602546691894531\n",
      "epoch: 53 step: 48 loss: 0.09528626 acc: 0.9597587585449219\n",
      "epoch: 53 step: 49 loss: 0.07978495 acc: 0.9628028869628906\n",
      "epoch: 53 step: 50 loss: 0.11841584 acc: 0.9520835876464844\n",
      "epoch: 53 step: 51 loss: 0.1048335 acc: 0.9581642150878906\n",
      "epoch: 53 step: 52 loss: 0.09882314 acc: 0.9646835327148438\n",
      "epoch: 53 step: 53 loss: 0.099806 acc: 0.9607505798339844\n",
      "epoch: 53 step: 54 loss: 0.09375641 acc: 0.9585342407226562\n",
      "epoch: 53 step: 55 loss: 0.08067488 acc: 0.9710884094238281\n",
      "epoch: 53 step: 56 loss: 0.1072103 acc: 0.9623146057128906\n",
      "epoch: 53 step: 57 loss: 0.09151978 acc: 0.9633064270019531\n",
      "epoch: 53 step: 58 loss: 0.096691675 acc: 0.9609870910644531\n",
      "epoch: 53 step: 59 loss: 0.11476271 acc: 0.9581718444824219\n",
      "epoch: 53 step: 60 loss: 0.082379915 acc: 0.9590873718261719\n",
      "epoch: 53 step: 61 loss: 0.09294526 acc: 0.96124267578125\n",
      "epoch: 53 step: 62 loss: 0.10974425 acc: 0.9579849243164062\n",
      "epoch: 53 step: 63 loss: 0.102643944 acc: 0.9511451721191406\n",
      "epoch: 53 step: 64 loss: 0.08225573 acc: 0.9671630859375\n",
      "epoch: 53 step: 65 loss: 0.08461229 acc: 0.9614105224609375\n",
      "epoch: 53 step: 66 loss: 0.12687801 acc: 0.9584426879882812\n",
      "epoch: 53 step: 67 loss: 0.09648732 acc: 0.9660377502441406\n",
      "epoch: 53 step: 68 loss: 0.08614529 acc: 0.9609146118164062\n",
      "epoch: 53 step: 69 loss: 0.09554773 acc: 0.9600715637207031\n",
      "epoch: 53 step: 70 loss: 0.102725014 acc: 0.9658317565917969\n",
      "epoch: 53 step: 71 loss: 0.13571335 acc: 0.9539680480957031\n",
      "epoch: 53 step: 72 loss: 0.086802356 acc: 0.9601860046386719\n",
      "epoch: 53 step: 73 loss: 0.09280896 acc: 0.9631729125976562\n",
      "epoch: 53 step: 74 loss: 0.08790092 acc: 0.958648681640625\n",
      "epoch: 53 step: 75 loss: 0.09589399 acc: 0.9586219787597656\n",
      "epoch: 53 step: 76 loss: 0.094276235 acc: 0.9593429565429688\n",
      "epoch: 53 step: 77 loss: 0.10697264 acc: 0.9621925354003906\n",
      "epoch: 53 step: 78 loss: 0.10893415 acc: 0.9505386352539062\n",
      "epoch: 53 step: 79 loss: 0.113946535 acc: 0.9516067504882812\n",
      "epoch: 53 step: 80 loss: 0.110116936 acc: 0.9543380737304688\n",
      "epoch: 53 step: 81 loss: 0.09907931 acc: 0.9528312683105469\n",
      "epoch: 53 step: 82 loss: 0.08868183 acc: 0.9576835632324219\n",
      "epoch: 53 step: 83 loss: 0.1069453 acc: 0.954864501953125\n",
      "epoch: 53 step: 84 loss: 0.08482667 acc: 0.9700889587402344\n",
      "epoch: 53 step: 85 loss: 0.08393636 acc: 0.9597930908203125\n",
      "epoch: 53 step: 86 loss: 0.093001686 acc: 0.96282958984375\n",
      "epoch: 53 step: 87 loss: 0.096496195 acc: 0.9625930786132812\n",
      "epoch: 53 step: 88 loss: 0.108532384 acc: 0.9552726745605469\n",
      "epoch: 53 step: 89 loss: 0.0966031 acc: 0.958892822265625\n",
      "epoch: 53 step: 90 loss: 0.08216145 acc: 0.9714584350585938\n",
      "epoch: 53 step: 91 loss: 0.10709941 acc: 0.9624290466308594\n",
      "epoch: 53 step: 92 loss: 0.08767048 acc: 0.9549522399902344\n",
      "epoch: 53 step: 93 loss: 0.10369451 acc: 0.9565963745117188\n",
      "epoch: 53 step: 94 loss: 0.09839549 acc: 0.9597358703613281\n",
      "epoch: 53 step: 95 loss: 0.091966055 acc: 0.9597091674804688\n",
      "epoch: 53 step: 96 loss: 0.10697391 acc: 0.9557380676269531\n",
      "epoch: 53 step: 97 loss: 0.09266333 acc: 0.9589996337890625\n",
      "epoch: 53 step: 98 loss: 0.11430174 acc: 0.9459800720214844\n",
      "epoch: 53 step: 99 loss: 0.09151003 acc: 0.9645156860351562\n",
      "epoch: 53 step: 100 loss: 0.08484738 acc: 0.970367431640625\n",
      "epoch: 53 step: 101 loss: 0.09995067 acc: 0.9624290466308594\n",
      "epoch: 53 step: 102 loss: 0.07592394 acc: 0.9646186828613281\n",
      "epoch: 53 step: 103 loss: 0.08925503 acc: 0.9639205932617188\n",
      "epoch: 53 step: 104 loss: 0.117866576 acc: 0.9514999389648438\n",
      "epoch: 53 step: 105 loss: 0.09420263 acc: 0.9655914306640625\n",
      "epoch: 53 step: 106 loss: 0.091058336 acc: 0.9621315002441406\n",
      "epoch: 53 step: 107 loss: 0.097647674 acc: 0.9597930908203125\n",
      "epoch: 53 step: 108 loss: 0.09698769 acc: 0.9579925537109375\n",
      "epoch: 53 step: 109 loss: 0.08640372 acc: 0.9663810729980469\n",
      "epoch: 53 step: 110 loss: 0.10219269 acc: 0.9633560180664062\n",
      "epoch: 53 step: 111 loss: 0.1184569 acc: 0.9526710510253906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 53 step: 112 loss: 0.10319584 acc: 0.9532356262207031\n",
      "epoch: 53 step: 113 loss: 0.08681191 acc: 0.95849609375\n",
      "epoch: 53 step: 114 loss: 0.100675486 acc: 0.9595184326171875\n",
      "epoch: 53 step: 115 loss: 0.09300476 acc: 0.9574356079101562\n",
      "epoch: 53 step: 116 loss: 0.10906247 acc: 0.9530830383300781\n",
      "epoch: 53 step: 117 loss: 0.13068217 acc: 0.9580039978027344\n",
      "epoch: 53 step: 118 loss: 0.099628754 acc: 0.9648170471191406\n",
      "epoch: 53 step: 119 loss: 0.10187586 acc: 0.9597358703613281\n",
      "epoch: 53 step: 120 loss: 0.08643653 acc: 0.9590225219726562\n",
      "epoch: 53 step: 121 loss: 0.08361824 acc: 0.9663848876953125\n",
      "epoch: 53 step: 122 loss: 0.10662724 acc: 0.9625511169433594\n",
      "epoch: 53 step: 123 loss: 0.109092996 acc: 0.9589042663574219\n",
      "epoch: 53 step: 124 loss: 0.1778529 acc: 0.9354596819196429\n",
      "epoch: 53 validation_loss: 0.107 validation_dice: 0.8379796544308311\n",
      "epoch: 53 test_dataset dice: 0.7089101226931372\n",
      "time cost 0.5353275060653686 min\n",
      "dice_best: 0.8379796544308311\n",
      "******************************** epoch  53  is finished. *********************************\n",
      "epoch: 54 step: 1 loss: 0.0957268 acc: 0.9568023681640625\n",
      "epoch: 54 step: 2 loss: 0.087770715 acc: 0.9484405517578125\n",
      "epoch: 54 step: 3 loss: 0.10860281 acc: 0.9492721557617188\n",
      "epoch: 54 step: 4 loss: 0.08656454 acc: 0.9584541320800781\n",
      "epoch: 54 step: 5 loss: 0.104382895 acc: 0.9530830383300781\n",
      "epoch: 54 step: 6 loss: 0.091422334 acc: 0.9575119018554688\n",
      "epoch: 54 step: 7 loss: 0.11548962 acc: 0.9541893005371094\n",
      "epoch: 54 step: 8 loss: 0.07207074 acc: 0.969573974609375\n",
      "epoch: 54 step: 9 loss: 0.0831453 acc: 0.9647750854492188\n",
      "epoch: 54 step: 10 loss: 0.09506449 acc: 0.9590682983398438\n",
      "epoch: 54 step: 11 loss: 0.09723616 acc: 0.9575653076171875\n",
      "epoch: 54 step: 12 loss: 0.11774186 acc: 0.9588127136230469\n",
      "epoch: 54 step: 13 loss: 0.12270083 acc: 0.9547386169433594\n",
      "epoch: 54 step: 14 loss: 0.083372906 acc: 0.9628944396972656\n",
      "epoch: 54 step: 15 loss: 0.10063642 acc: 0.9538459777832031\n",
      "epoch: 54 step: 16 loss: 0.09863186 acc: 0.9611892700195312\n",
      "epoch: 54 step: 17 loss: 0.07628259 acc: 0.9618721008300781\n",
      "epoch: 54 step: 18 loss: 0.091935545 acc: 0.9566688537597656\n",
      "epoch: 54 step: 19 loss: 0.118472904 acc: 0.9488945007324219\n",
      "epoch: 54 step: 20 loss: 0.09979973 acc: 0.9410896301269531\n",
      "epoch: 54 step: 21 loss: 0.08572529 acc: 0.9584922790527344\n",
      "epoch: 54 step: 22 loss: 0.08898573 acc: 0.9607048034667969\n",
      "epoch: 54 step: 23 loss: 0.08999725 acc: 0.9620323181152344\n",
      "epoch: 54 step: 24 loss: 0.10210218 acc: 0.9541206359863281\n",
      "epoch: 54 step: 25 loss: 0.09451074 acc: 0.9567832946777344\n",
      "epoch: 54 step: 26 loss: 0.0937787 acc: 0.9535789489746094\n",
      "epoch: 54 step: 27 loss: 0.08544174 acc: 0.9609642028808594\n",
      "epoch: 54 step: 28 loss: 0.08483251 acc: 0.9666481018066406\n",
      "epoch: 54 step: 29 loss: 0.10030773 acc: 0.9612960815429688\n",
      "epoch: 54 step: 30 loss: 0.09609551 acc: 0.9565620422363281\n",
      "epoch: 54 step: 31 loss: 0.09393632 acc: 0.9647293090820312\n",
      "epoch: 54 step: 32 loss: 0.1059805 acc: 0.9578437805175781\n",
      "epoch: 54 step: 33 loss: 0.12105481 acc: 0.9487190246582031\n",
      "epoch: 54 step: 34 loss: 0.09076447 acc: 0.9644050598144531\n",
      "epoch: 54 step: 35 loss: 0.094708726 acc: 0.9684982299804688\n",
      "epoch: 54 step: 36 loss: 0.115321666 acc: 0.9633903503417969\n",
      "epoch: 54 step: 37 loss: 0.08343152 acc: 0.9668960571289062\n",
      "epoch: 54 step: 38 loss: 0.0907869 acc: 0.9665679931640625\n",
      "epoch: 54 step: 39 loss: 0.08879833 acc: 0.9592361450195312\n",
      "epoch: 54 step: 40 loss: 0.11875759 acc: 0.9589004516601562\n",
      "epoch: 54 step: 41 loss: 0.113300376 acc: 0.9604568481445312\n",
      "epoch: 54 step: 42 loss: 0.08802795 acc: 0.9592781066894531\n",
      "epoch: 54 step: 43 loss: 0.08357509 acc: 0.964599609375\n",
      "epoch: 54 step: 44 loss: 0.11830587 acc: 0.95587158203125\n",
      "epoch: 54 step: 45 loss: 0.10117638 acc: 0.959564208984375\n",
      "epoch: 54 step: 46 loss: 0.09834891 acc: 0.9586334228515625\n",
      "epoch: 54 step: 47 loss: 0.09713426 acc: 0.9650382995605469\n",
      "epoch: 54 step: 48 loss: 0.13888995 acc: 0.9603614807128906\n",
      "epoch: 54 step: 49 loss: 0.106703945 acc: 0.955230712890625\n",
      "epoch: 54 step: 50 loss: 0.099722475 acc: 0.9664230346679688\n",
      "epoch: 54 step: 51 loss: 0.09505037 acc: 0.9652099609375\n",
      "epoch: 54 step: 52 loss: 0.09209178 acc: 0.961883544921875\n",
      "epoch: 54 step: 53 loss: 0.08682591 acc: 0.9636383056640625\n",
      "epoch: 54 step: 54 loss: 0.10924191 acc: 0.9571304321289062\n",
      "epoch: 54 step: 55 loss: 0.12287237 acc: 0.9514617919921875\n",
      "epoch: 54 step: 56 loss: 0.10261254 acc: 0.9561271667480469\n",
      "epoch: 54 step: 57 loss: 0.10630182 acc: 0.9564399719238281\n",
      "epoch: 54 step: 58 loss: 0.096397296 acc: 0.9553642272949219\n",
      "epoch: 54 step: 59 loss: 0.103736445 acc: 0.9547653198242188\n",
      "epoch: 54 step: 60 loss: 0.0995063 acc: 0.9561309814453125\n",
      "epoch: 54 step: 61 loss: 0.0999504 acc: 0.9588813781738281\n",
      "epoch: 54 step: 62 loss: 0.09679365 acc: 0.9649009704589844\n",
      "epoch: 54 step: 63 loss: 0.11515712 acc: 0.9628791809082031\n",
      "epoch: 54 step: 64 loss: 0.08984681 acc: 0.9528770446777344\n",
      "epoch: 54 step: 65 loss: 0.10093472 acc: 0.9672737121582031\n",
      "epoch: 54 step: 66 loss: 0.09395025 acc: 0.9697189331054688\n",
      "epoch: 54 step: 67 loss: 0.11104728 acc: 0.9672126770019531\n",
      "epoch: 54 step: 68 loss: 0.1068024 acc: 0.9566421508789062\n",
      "epoch: 54 step: 69 loss: 0.11234211 acc: 0.9654731750488281\n",
      "epoch: 54 step: 70 loss: 0.09995262 acc: 0.9601631164550781\n",
      "epoch: 54 step: 71 loss: 0.077744 acc: 0.9677352905273438\n",
      "epoch: 54 step: 72 loss: 0.12487705 acc: 0.9538040161132812\n",
      "epoch: 54 step: 73 loss: 0.11244663 acc: 0.9625892639160156\n",
      "epoch: 54 step: 74 loss: 0.10545092 acc: 0.9559783935546875\n",
      "epoch: 54 step: 75 loss: 0.08458414 acc: 0.9573631286621094\n",
      "epoch: 54 step: 76 loss: 0.11386894 acc: 0.951324462890625\n",
      "epoch: 54 step: 77 loss: 0.09571952 acc: 0.9583702087402344\n",
      "epoch: 54 step: 78 loss: 0.10944297 acc: 0.9525260925292969\n",
      "epoch: 54 step: 79 loss: 0.10256726 acc: 0.9574050903320312\n",
      "epoch: 54 step: 80 loss: 0.09851078 acc: 0.9590415954589844\n",
      "epoch: 54 step: 81 loss: 0.10044236 acc: 0.9679946899414062\n",
      "epoch: 54 step: 82 loss: 0.08121612 acc: 0.9668006896972656\n",
      "epoch: 54 step: 83 loss: 0.10160738 acc: 0.9604110717773438\n",
      "epoch: 54 step: 84 loss: 0.09552766 acc: 0.9587364196777344\n",
      "epoch: 54 step: 85 loss: 0.091200136 acc: 0.9641532897949219\n",
      "epoch: 54 step: 86 loss: 0.101141036 acc: 0.9616279602050781\n",
      "epoch: 54 step: 87 loss: 0.12332407 acc: 0.95263671875\n",
      "epoch: 54 step: 88 loss: 0.09737734 acc: 0.9616928100585938\n",
      "epoch: 54 step: 89 loss: 0.0920693 acc: 0.9581375122070312\n",
      "epoch: 54 step: 90 loss: 0.08664274 acc: 0.9627342224121094\n",
      "epoch: 54 step: 91 loss: 0.096330315 acc: 0.9594497680664062\n",
      "epoch: 54 step: 92 loss: 0.09625487 acc: 0.9646530151367188\n",
      "epoch: 54 step: 93 loss: 0.08040669 acc: 0.9648971557617188\n",
      "epoch: 54 step: 94 loss: 0.11797575 acc: 0.9528465270996094\n",
      "epoch: 54 step: 95 loss: 0.093820326 acc: 0.9623451232910156\n",
      "epoch: 54 step: 96 loss: 0.097193435 acc: 0.9629898071289062\n",
      "epoch: 54 step: 97 loss: 0.09895731 acc: 0.96160888671875\n",
      "epoch: 54 step: 98 loss: 0.085010916 acc: 0.9645195007324219\n",
      "epoch: 54 step: 99 loss: 0.1069029 acc: 0.9533348083496094\n",
      "epoch: 54 step: 100 loss: 0.10787954 acc: 0.9554328918457031\n",
      "epoch: 54 step: 101 loss: 0.11143433 acc: 0.9541206359863281\n",
      "epoch: 54 step: 102 loss: 0.07829192 acc: 0.9662132263183594\n",
      "epoch: 54 step: 103 loss: 0.09644824 acc: 0.9581184387207031\n",
      "epoch: 54 step: 104 loss: 0.08433717 acc: 0.9704437255859375\n",
      "epoch: 54 step: 105 loss: 0.08560037 acc: 0.9666976928710938\n",
      "epoch: 54 step: 106 loss: 0.104415916 acc: 0.9623908996582031\n",
      "epoch: 54 step: 107 loss: 0.13251941 acc: 0.9568367004394531\n",
      "epoch: 54 step: 108 loss: 0.100265086 acc: 0.9678268432617188\n",
      "epoch: 54 step: 109 loss: 0.082362436 acc: 0.965850830078125\n",
      "epoch: 54 step: 110 loss: 0.12814641 acc: 0.9626007080078125\n",
      "epoch: 54 step: 111 loss: 0.0981099 acc: 0.9546966552734375\n",
      "epoch: 54 step: 112 loss: 0.10337152 acc: 0.9676513671875\n",
      "epoch: 54 step: 113 loss: 0.08980138 acc: 0.9670448303222656\n",
      "epoch: 54 step: 114 loss: 0.0882458 acc: 0.96685791015625\n",
      "epoch: 54 step: 115 loss: 0.116602115 acc: 0.9623832702636719\n",
      "epoch: 54 step: 116 loss: 0.10891876 acc: 0.9480438232421875\n",
      "epoch: 54 step: 117 loss: 0.108660676 acc: 0.9493637084960938\n",
      "epoch: 54 step: 118 loss: 0.11941436 acc: 0.9619903564453125\n",
      "epoch: 54 step: 119 loss: 0.1145773 acc: 0.9561843872070312\n",
      "epoch: 54 step: 120 loss: 0.11240099 acc: 0.9609947204589844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 54 step: 121 loss: 0.116753235 acc: 0.9542083740234375\n",
      "epoch: 54 step: 122 loss: 0.1341445 acc: 0.9485130310058594\n",
      "epoch: 54 step: 123 loss: 0.14972807 acc: 0.9618949890136719\n",
      "epoch: 54 step: 124 loss: 0.10342266 acc: 0.9485037667410714\n",
      "epoch: 54 validation_loss: 0.123 validation_dice: 0.8021030591607571\n",
      "epoch: 54 test_dataset dice: 0.7279332821380099\n",
      "time cost 0.535645051797231 min\n",
      "dice_best: 0.8379796544308311\n",
      "******************************** epoch  54  is finished. *********************************\n",
      "epoch: 55 step: 1 loss: 0.103980705 acc: 0.9607620239257812\n",
      "epoch: 55 step: 2 loss: 0.13750996 acc: 0.9453201293945312\n",
      "epoch: 55 step: 3 loss: 0.1559171 acc: 0.9489974975585938\n",
      "epoch: 55 step: 4 loss: 0.12145975 acc: 0.9480133056640625\n",
      "epoch: 55 step: 5 loss: 0.119983576 acc: 0.9577789306640625\n",
      "epoch: 55 step: 6 loss: 0.120931536 acc: 0.9537124633789062\n",
      "epoch: 55 step: 7 loss: 0.12216968 acc: 0.94384765625\n",
      "epoch: 55 step: 8 loss: 0.117856614 acc: 0.9544906616210938\n",
      "epoch: 55 step: 9 loss: 0.14229459 acc: 0.9515647888183594\n",
      "epoch: 55 step: 10 loss: 0.12218658 acc: 0.9633064270019531\n",
      "epoch: 55 step: 11 loss: 0.0925229 acc: 0.9602851867675781\n",
      "epoch: 55 step: 12 loss: 0.10749303 acc: 0.9593772888183594\n",
      "epoch: 55 step: 13 loss: 0.10991715 acc: 0.9625320434570312\n",
      "epoch: 55 step: 14 loss: 0.09415303 acc: 0.9624099731445312\n",
      "epoch: 55 step: 15 loss: 0.11554957 acc: 0.9692192077636719\n",
      "epoch: 55 step: 16 loss: 0.102856934 acc: 0.9599571228027344\n",
      "epoch: 55 step: 17 loss: 0.09311073 acc: 0.9652633666992188\n",
      "epoch: 55 step: 18 loss: 0.11288934 acc: 0.9622039794921875\n",
      "epoch: 55 step: 19 loss: 0.107814044 acc: 0.9576339721679688\n",
      "epoch: 55 step: 20 loss: 0.09604387 acc: 0.9641914367675781\n",
      "epoch: 55 step: 21 loss: 0.10988468 acc: 0.9572639465332031\n",
      "epoch: 55 step: 22 loss: 0.112475544 acc: 0.9603195190429688\n",
      "epoch: 55 step: 23 loss: 0.106777295 acc: 0.9542655944824219\n",
      "epoch: 55 step: 24 loss: 0.10326169 acc: 0.9536056518554688\n",
      "epoch: 55 step: 25 loss: 0.10312209 acc: 0.9560813903808594\n",
      "epoch: 55 step: 26 loss: 0.11709853 acc: 0.9533500671386719\n",
      "epoch: 55 step: 27 loss: 0.12193951 acc: 0.9529685974121094\n",
      "epoch: 55 step: 28 loss: 0.08982593 acc: 0.9647331237792969\n",
      "epoch: 55 step: 29 loss: 0.10434837 acc: 0.9610404968261719\n",
      "epoch: 55 step: 30 loss: 0.09713814 acc: 0.9591789245605469\n",
      "epoch: 55 step: 31 loss: 0.09470739 acc: 0.9575881958007812\n",
      "epoch: 55 step: 32 loss: 0.09864766 acc: 0.9579315185546875\n",
      "epoch: 55 step: 33 loss: 0.1123508 acc: 0.9608535766601562\n",
      "epoch: 55 step: 34 loss: 0.12277879 acc: 0.9666099548339844\n",
      "epoch: 55 step: 35 loss: 0.08751905 acc: 0.9666633605957031\n",
      "epoch: 55 step: 36 loss: 0.1043034 acc: 0.9577674865722656\n",
      "epoch: 55 step: 37 loss: 0.081583954 acc: 0.9661712646484375\n",
      "epoch: 55 step: 38 loss: 0.104663625 acc: 0.9606475830078125\n",
      "epoch: 55 step: 39 loss: 0.09955986 acc: 0.9537506103515625\n",
      "epoch: 55 step: 40 loss: 0.10519313 acc: 0.9590339660644531\n",
      "epoch: 55 step: 41 loss: 0.11351554 acc: 0.9537582397460938\n",
      "epoch: 55 step: 42 loss: 0.093131006 acc: 0.964263916015625\n",
      "epoch: 55 step: 43 loss: 0.1003558 acc: 0.9605445861816406\n",
      "epoch: 55 step: 44 loss: 0.09008128 acc: 0.9642868041992188\n",
      "epoch: 55 step: 45 loss: 0.114841074 acc: 0.94805908203125\n",
      "epoch: 55 step: 46 loss: 0.104403645 acc: 0.9672508239746094\n",
      "epoch: 55 step: 47 loss: 0.106520824 acc: 0.9594955444335938\n",
      "epoch: 55 step: 48 loss: 0.10279028 acc: 0.9570999145507812\n",
      "epoch: 55 step: 49 loss: 0.08372309 acc: 0.9660797119140625\n",
      "epoch: 55 step: 50 loss: 0.10007627 acc: 0.9618263244628906\n",
      "epoch: 55 step: 51 loss: 0.1105564 acc: 0.9535331726074219\n",
      "epoch: 55 step: 52 loss: 0.09579597 acc: 0.9605026245117188\n",
      "epoch: 55 step: 53 loss: 0.09345186 acc: 0.9605522155761719\n",
      "epoch: 55 step: 54 loss: 0.119085036 acc: 0.9574470520019531\n",
      "epoch: 55 step: 55 loss: 0.10408613 acc: 0.9607467651367188\n",
      "epoch: 55 step: 56 loss: 0.08373147 acc: 0.9644241333007812\n",
      "epoch: 55 step: 57 loss: 0.10442935 acc: 0.9581451416015625\n",
      "epoch: 55 step: 58 loss: 0.085788846 acc: 0.9619903564453125\n",
      "epoch: 55 step: 59 loss: 0.103468806 acc: 0.9589881896972656\n",
      "epoch: 55 step: 60 loss: 0.08258622 acc: 0.9632682800292969\n",
      "epoch: 55 step: 61 loss: 0.11518335 acc: 0.9527397155761719\n",
      "epoch: 55 step: 62 loss: 0.075450726 acc: 0.9709358215332031\n",
      "epoch: 55 step: 63 loss: 0.10649936 acc: 0.9607963562011719\n",
      "epoch: 55 step: 64 loss: 0.11626359 acc: 0.9586982727050781\n",
      "epoch: 55 step: 65 loss: 0.0931752 acc: 0.9603767395019531\n",
      "epoch: 55 step: 66 loss: 0.08155825 acc: 0.9670257568359375\n",
      "epoch: 55 step: 67 loss: 0.10618699 acc: 0.9557609558105469\n",
      "epoch: 55 step: 68 loss: 0.123916835 acc: 0.9494094848632812\n",
      "epoch: 55 step: 69 loss: 0.08591644 acc: 0.9573440551757812\n",
      "epoch: 55 step: 70 loss: 0.11967505 acc: 0.9525413513183594\n",
      "epoch: 55 step: 71 loss: 0.08462932 acc: 0.9613494873046875\n",
      "epoch: 55 step: 72 loss: 0.11160496 acc: 0.9592056274414062\n",
      "epoch: 55 step: 73 loss: 0.08868058 acc: 0.9640007019042969\n",
      "epoch: 55 step: 74 loss: 0.10438823 acc: 0.9550819396972656\n",
      "epoch: 55 step: 75 loss: 0.09600668 acc: 0.9592475891113281\n",
      "epoch: 55 step: 76 loss: 0.10576445 acc: 0.9604377746582031\n",
      "epoch: 55 step: 77 loss: 0.10498482 acc: 0.9649276733398438\n",
      "epoch: 55 step: 78 loss: 0.08402967 acc: 0.9700241088867188\n",
      "epoch: 55 step: 79 loss: 0.10848318 acc: 0.9564743041992188\n",
      "epoch: 55 step: 80 loss: 0.08958056 acc: 0.9588203430175781\n",
      "epoch: 55 step: 81 loss: 0.09480331 acc: 0.9638710021972656\n",
      "epoch: 55 step: 82 loss: 0.10731044 acc: 0.9604682922363281\n",
      "epoch: 55 step: 83 loss: 0.082942255 acc: 0.9626312255859375\n",
      "epoch: 55 step: 84 loss: 0.0939167 acc: 0.9612541198730469\n",
      "epoch: 55 step: 85 loss: 0.097843125 acc: 0.9604759216308594\n",
      "epoch: 55 step: 86 loss: 0.106628485 acc: 0.963165283203125\n",
      "epoch: 55 step: 87 loss: 0.097414926 acc: 0.9670753479003906\n",
      "epoch: 55 step: 88 loss: 0.10493567 acc: 0.9623222351074219\n",
      "epoch: 55 step: 89 loss: 0.08106522 acc: 0.9705467224121094\n",
      "epoch: 55 step: 90 loss: 0.111392386 acc: 0.9591064453125\n",
      "epoch: 55 step: 91 loss: 0.12514342 acc: 0.9515800476074219\n",
      "epoch: 55 step: 92 loss: 0.15509482 acc: 0.95098876953125\n",
      "epoch: 55 step: 93 loss: 0.1161683 acc: 0.9543838500976562\n",
      "epoch: 55 step: 94 loss: 0.083240874 acc: 0.9605941772460938\n",
      "epoch: 55 step: 95 loss: 0.0848443 acc: 0.9587173461914062\n",
      "epoch: 55 step: 96 loss: 0.08947299 acc: 0.96466064453125\n",
      "epoch: 55 step: 97 loss: 0.10979546 acc: 0.9584579467773438\n",
      "epoch: 55 step: 98 loss: 0.09205701 acc: 0.9648323059082031\n",
      "epoch: 55 step: 99 loss: 0.09331967 acc: 0.96197509765625\n",
      "epoch: 55 step: 100 loss: 0.09512168 acc: 0.959259033203125\n",
      "epoch: 55 step: 101 loss: 0.08556777 acc: 0.963592529296875\n",
      "epoch: 55 step: 102 loss: 0.09961253 acc: 0.9649658203125\n",
      "epoch: 55 step: 103 loss: 0.11663114 acc: 0.965606689453125\n",
      "epoch: 55 step: 104 loss: 0.09337597 acc: 0.9612846374511719\n",
      "epoch: 55 step: 105 loss: 0.08385562 acc: 0.9688301086425781\n",
      "epoch: 55 step: 106 loss: 0.1245103 acc: 0.9537162780761719\n",
      "epoch: 55 step: 107 loss: 0.111612715 acc: 0.9568214416503906\n",
      "epoch: 55 step: 108 loss: 0.09269941 acc: 0.9595489501953125\n",
      "epoch: 55 step: 109 loss: 0.11186585 acc: 0.9553375244140625\n",
      "epoch: 55 step: 110 loss: 0.11843486 acc: 0.9541397094726562\n",
      "epoch: 55 step: 111 loss: 0.0914571 acc: 0.9582290649414062\n",
      "epoch: 55 step: 112 loss: 0.0937005 acc: 0.9598731994628906\n",
      "epoch: 55 step: 113 loss: 0.12763368 acc: 0.9495277404785156\n",
      "epoch: 55 step: 114 loss: 0.1117409 acc: 0.9619903564453125\n",
      "epoch: 55 step: 115 loss: 0.103167914 acc: 0.9603843688964844\n",
      "epoch: 55 step: 116 loss: 0.08713393 acc: 0.962493896484375\n",
      "epoch: 55 step: 117 loss: 0.083527155 acc: 0.9600257873535156\n",
      "epoch: 55 step: 118 loss: 0.095264144 acc: 0.9627532958984375\n",
      "epoch: 55 step: 119 loss: 0.09715705 acc: 0.9631538391113281\n",
      "epoch: 55 step: 120 loss: 0.12146847 acc: 0.9634895324707031\n",
      "epoch: 55 step: 121 loss: 0.08970386 acc: 0.9683341979980469\n",
      "epoch: 55 step: 122 loss: 0.10958003 acc: 0.9602622985839844\n",
      "epoch: 55 step: 123 loss: 0.09943908 acc: 0.9620094299316406\n",
      "epoch: 55 step: 124 loss: 0.1275196 acc: 0.9537615094866071\n",
      "epoch: 55 validation_loss: 0.106 validation_dice: 0.8235740624111705\n",
      "epoch: 55 test_dataset dice: 0.7509239943498646\n",
      "time cost 0.5361865123112997 min\n",
      "dice_best: 0.8379796544308311\n",
      "******************************** epoch  55  is finished. *********************************\n",
      "epoch: 56 step: 1 loss: 0.08988324 acc: 0.9637413024902344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 56 step: 2 loss: 0.093914516 acc: 0.9619293212890625\n",
      "epoch: 56 step: 3 loss: 0.10263407 acc: 0.9582023620605469\n",
      "epoch: 56 step: 4 loss: 0.09569773 acc: 0.9617843627929688\n",
      "epoch: 56 step: 5 loss: 0.10609495 acc: 0.9621086120605469\n",
      "epoch: 56 step: 6 loss: 0.10418168 acc: 0.9580802917480469\n",
      "epoch: 56 step: 7 loss: 0.101434134 acc: 0.9548072814941406\n",
      "epoch: 56 step: 8 loss: 0.07431949 acc: 0.9717597961425781\n",
      "epoch: 56 step: 9 loss: 0.10434565 acc: 0.9642562866210938\n",
      "epoch: 56 step: 10 loss: 0.10621453 acc: 0.9636573791503906\n",
      "epoch: 56 step: 11 loss: 0.11677693 acc: 0.9583969116210938\n",
      "epoch: 56 step: 12 loss: 0.118827626 acc: 0.9543037414550781\n",
      "epoch: 56 step: 13 loss: 0.10018399 acc: 0.9567527770996094\n",
      "epoch: 56 step: 14 loss: 0.10127259 acc: 0.9566001892089844\n",
      "epoch: 56 step: 15 loss: 0.0798473 acc: 0.9647331237792969\n",
      "epoch: 56 step: 16 loss: 0.11191664 acc: 0.9554367065429688\n",
      "epoch: 56 step: 17 loss: 0.1152456 acc: 0.9564056396484375\n",
      "epoch: 56 step: 18 loss: 0.08020719 acc: 0.964813232421875\n",
      "epoch: 56 step: 19 loss: 0.08225325 acc: 0.9649276733398438\n",
      "epoch: 56 step: 20 loss: 0.104773805 acc: 0.958831787109375\n",
      "epoch: 56 step: 21 loss: 0.08222048 acc: 0.96649169921875\n",
      "epoch: 56 step: 22 loss: 0.0994279 acc: 0.9591789245605469\n",
      "epoch: 56 step: 23 loss: 0.10358107 acc: 0.9566001892089844\n",
      "epoch: 56 step: 24 loss: 0.10113747 acc: 0.9617233276367188\n",
      "epoch: 56 step: 25 loss: 0.11256513 acc: 0.9569854736328125\n",
      "epoch: 56 step: 26 loss: 0.109598264 acc: 0.9549446105957031\n",
      "epoch: 56 step: 27 loss: 0.09460386 acc: 0.9613037109375\n",
      "epoch: 56 step: 28 loss: 0.09823056 acc: 0.9631195068359375\n",
      "epoch: 56 step: 29 loss: 0.096276656 acc: 0.9592170715332031\n",
      "epoch: 56 step: 30 loss: 0.07852685 acc: 0.9690322875976562\n",
      "epoch: 56 step: 31 loss: 0.10345352 acc: 0.9553756713867188\n",
      "epoch: 56 step: 32 loss: 0.12580511 acc: 0.9580497741699219\n",
      "epoch: 56 step: 33 loss: 0.1056325 acc: 0.9559974670410156\n",
      "epoch: 56 step: 34 loss: 0.10880332 acc: 0.9537391662597656\n",
      "epoch: 56 step: 35 loss: 0.13025348 acc: 0.9600944519042969\n",
      "epoch: 56 step: 36 loss: 0.07786119 acc: 0.9646835327148438\n",
      "epoch: 56 step: 37 loss: 0.10241296 acc: 0.9576034545898438\n",
      "epoch: 56 step: 38 loss: 0.10375014 acc: 0.9615974426269531\n",
      "epoch: 56 step: 39 loss: 0.091625996 acc: 0.9569625854492188\n",
      "epoch: 56 step: 40 loss: 0.08648577 acc: 0.9590034484863281\n",
      "epoch: 56 step: 41 loss: 0.087642424 acc: 0.9654006958007812\n",
      "epoch: 56 step: 42 loss: 0.08779442 acc: 0.9722557067871094\n",
      "epoch: 56 step: 43 loss: 0.10547202 acc: 0.9618568420410156\n",
      "epoch: 56 step: 44 loss: 0.09797797 acc: 0.9612693786621094\n",
      "epoch: 56 step: 45 loss: 0.103674866 acc: 0.9644508361816406\n",
      "epoch: 56 step: 46 loss: 0.087824255 acc: 0.9582710266113281\n",
      "epoch: 56 step: 47 loss: 0.10955773 acc: 0.9593849182128906\n",
      "epoch: 56 step: 48 loss: 0.121319324 acc: 0.9538497924804688\n",
      "epoch: 56 step: 49 loss: 0.09074013 acc: 0.9624595642089844\n",
      "epoch: 56 step: 50 loss: 0.10348423 acc: 0.9591712951660156\n",
      "epoch: 56 step: 51 loss: 0.097479574 acc: 0.954498291015625\n",
      "epoch: 56 step: 52 loss: 0.09222665 acc: 0.9626007080078125\n",
      "epoch: 56 step: 53 loss: 0.08555599 acc: 0.9663238525390625\n",
      "epoch: 56 step: 54 loss: 0.092344515 acc: 0.9562263488769531\n",
      "epoch: 56 step: 55 loss: 0.09703345 acc: 0.9598922729492188\n",
      "epoch: 56 step: 56 loss: 0.13094805 acc: 0.9521446228027344\n",
      "epoch: 56 step: 57 loss: 0.1058759 acc: 0.9573478698730469\n",
      "epoch: 56 step: 58 loss: 0.091598846 acc: 0.9599266052246094\n",
      "epoch: 56 step: 59 loss: 0.11016791 acc: 0.9536399841308594\n",
      "epoch: 56 step: 60 loss: 0.07986878 acc: 0.9710121154785156\n",
      "epoch: 56 step: 61 loss: 0.09424304 acc: 0.9626197814941406\n",
      "epoch: 56 step: 62 loss: 0.09570052 acc: 0.9615669250488281\n",
      "epoch: 56 step: 63 loss: 0.07997631 acc: 0.968658447265625\n",
      "epoch: 56 step: 64 loss: 0.13964361 acc: 0.9478263854980469\n",
      "epoch: 56 step: 65 loss: 0.09732714 acc: 0.9621810913085938\n",
      "epoch: 56 step: 66 loss: 0.09282702 acc: 0.9664802551269531\n",
      "epoch: 56 step: 67 loss: 0.091758475 acc: 0.9596672058105469\n",
      "epoch: 56 step: 68 loss: 0.08741395 acc: 0.9601898193359375\n",
      "epoch: 56 step: 69 loss: 0.087449454 acc: 0.9643516540527344\n",
      "epoch: 56 step: 70 loss: 0.089997604 acc: 0.9646682739257812\n",
      "epoch: 56 step: 71 loss: 0.08876405 acc: 0.9620437622070312\n",
      "epoch: 56 step: 72 loss: 0.083348215 acc: 0.9657783508300781\n",
      "epoch: 56 step: 73 loss: 0.08593045 acc: 0.9638595581054688\n",
      "epoch: 56 step: 74 loss: 0.11693066 acc: 0.9556808471679688\n",
      "epoch: 56 step: 75 loss: 0.08882195 acc: 0.96673583984375\n",
      "epoch: 56 step: 76 loss: 0.08215979 acc: 0.9615058898925781\n",
      "epoch: 56 step: 77 loss: 0.11036803 acc: 0.9559059143066406\n",
      "epoch: 56 step: 78 loss: 0.0781634 acc: 0.9680404663085938\n",
      "epoch: 56 step: 79 loss: 0.10441007 acc: 0.9563102722167969\n",
      "epoch: 56 step: 80 loss: 0.0867842 acc: 0.9616813659667969\n",
      "epoch: 56 step: 81 loss: 0.07929102 acc: 0.9654769897460938\n",
      "epoch: 56 step: 82 loss: 0.10081834 acc: 0.9584465026855469\n",
      "epoch: 56 step: 83 loss: 0.10525345 acc: 0.9510841369628906\n",
      "epoch: 56 step: 84 loss: 0.07879097 acc: 0.960968017578125\n",
      "epoch: 56 step: 85 loss: 0.09476698 acc: 0.9566154479980469\n",
      "epoch: 56 step: 86 loss: 0.09309679 acc: 0.9599380493164062\n",
      "epoch: 56 step: 87 loss: 0.07401442 acc: 0.96929931640625\n",
      "epoch: 56 step: 88 loss: 0.08380059 acc: 0.9681282043457031\n",
      "epoch: 56 step: 89 loss: 0.08907752 acc: 0.9604454040527344\n",
      "epoch: 56 step: 90 loss: 0.10526819 acc: 0.9601669311523438\n",
      "epoch: 56 step: 91 loss: 0.08769345 acc: 0.9661865234375\n",
      "epoch: 56 step: 92 loss: 0.087609135 acc: 0.9609832763671875\n",
      "epoch: 56 step: 93 loss: 0.08723844 acc: 0.9548797607421875\n",
      "epoch: 56 step: 94 loss: 0.099517055 acc: 0.9587249755859375\n",
      "epoch: 56 step: 95 loss: 0.09727547 acc: 0.9591522216796875\n",
      "epoch: 56 step: 96 loss: 0.09394872 acc: 0.9524726867675781\n",
      "epoch: 56 step: 97 loss: 0.08767075 acc: 0.9551048278808594\n",
      "epoch: 56 step: 98 loss: 0.089953706 acc: 0.9604263305664062\n",
      "epoch: 56 step: 99 loss: 0.081659205 acc: 0.9660758972167969\n",
      "epoch: 56 step: 100 loss: 0.094342984 acc: 0.9564132690429688\n",
      "epoch: 56 step: 101 loss: 0.07986942 acc: 0.9600372314453125\n",
      "epoch: 56 step: 102 loss: 0.08461659 acc: 0.9601058959960938\n",
      "epoch: 56 step: 103 loss: 0.086976565 acc: 0.9681358337402344\n",
      "epoch: 56 step: 104 loss: 0.08279691 acc: 0.9670677185058594\n",
      "epoch: 56 step: 105 loss: 0.09258797 acc: 0.9636688232421875\n",
      "epoch: 56 step: 106 loss: 0.0813139 acc: 0.9668502807617188\n",
      "epoch: 56 step: 107 loss: 0.085766844 acc: 0.9633979797363281\n",
      "epoch: 56 step: 108 loss: 0.09968528 acc: 0.9545974731445312\n",
      "epoch: 56 step: 109 loss: 0.08799525 acc: 0.9646110534667969\n",
      "epoch: 56 step: 110 loss: 0.06658226 acc: 0.9716644287109375\n",
      "epoch: 56 step: 111 loss: 0.09613894 acc: 0.9570846557617188\n",
      "epoch: 56 step: 112 loss: 0.08391245 acc: 0.9640922546386719\n",
      "epoch: 56 step: 113 loss: 0.084305435 acc: 0.9659347534179688\n",
      "epoch: 56 step: 114 loss: 0.10073905 acc: 0.963714599609375\n",
      "epoch: 56 step: 115 loss: 0.10597373 acc: 0.9511146545410156\n",
      "epoch: 56 step: 116 loss: 0.09981433 acc: 0.9551010131835938\n",
      "epoch: 56 step: 117 loss: 0.08842251 acc: 0.9666023254394531\n",
      "epoch: 56 step: 118 loss: 0.09160603 acc: 0.9587440490722656\n",
      "epoch: 56 step: 119 loss: 0.087918624 acc: 0.9602928161621094\n",
      "epoch: 56 step: 120 loss: 0.08267286 acc: 0.9710693359375\n",
      "epoch: 56 step: 121 loss: 0.086939275 acc: 0.9637718200683594\n",
      "epoch: 56 step: 122 loss: 0.084689654 acc: 0.9718437194824219\n",
      "epoch: 56 step: 123 loss: 0.1118126 acc: 0.9639701843261719\n",
      "epoch: 56 step: 124 loss: 0.07555739 acc: 0.9702584402901786\n",
      "epoch: 56 validation_loss: 0.1 validation_dice: 0.8429257274299863\n",
      "epoch: 56 test_dataset dice: 0.7381270207112594\n",
      "time cost 0.5349968314170838 min\n",
      "dice_best: 0.8429257274299863\n",
      "******************************** epoch  56  is finished. *********************************\n",
      "epoch: 57 step: 1 loss: 0.096650526 acc: 0.958740234375\n",
      "epoch: 57 step: 2 loss: 0.12443662 acc: 0.9561843872070312\n",
      "epoch: 57 step: 3 loss: 0.0853264 acc: 0.9661064147949219\n",
      "epoch: 57 step: 4 loss: 0.08431793 acc: 0.9635505676269531\n",
      "epoch: 57 step: 5 loss: 0.08979216 acc: 0.9622344970703125\n",
      "epoch: 57 step: 6 loss: 0.081933215 acc: 0.9618492126464844\n",
      "epoch: 57 step: 7 loss: 0.08613746 acc: 0.9581108093261719\n",
      "epoch: 57 step: 8 loss: 0.087569036 acc: 0.9606246948242188\n",
      "epoch: 57 step: 9 loss: 0.100263685 acc: 0.9580001831054688\n",
      "epoch: 57 step: 10 loss: 0.083977036 acc: 0.9584884643554688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 57 step: 11 loss: 0.107252344 acc: 0.9610176086425781\n",
      "epoch: 57 step: 12 loss: 0.09751424 acc: 0.9553947448730469\n",
      "epoch: 57 step: 13 loss: 0.082637 acc: 0.96044921875\n",
      "epoch: 57 step: 14 loss: 0.08601066 acc: 0.9610252380371094\n",
      "epoch: 57 step: 15 loss: 0.08958251 acc: 0.9601783752441406\n",
      "epoch: 57 step: 16 loss: 0.0877252 acc: 0.9615325927734375\n",
      "epoch: 57 step: 17 loss: 0.09943279 acc: 0.971435546875\n",
      "epoch: 57 step: 18 loss: 0.07530985 acc: 0.9708099365234375\n",
      "epoch: 57 step: 19 loss: 0.10609971 acc: 0.9571952819824219\n",
      "epoch: 57 step: 20 loss: 0.10756975 acc: 0.958099365234375\n",
      "epoch: 57 step: 21 loss: 0.102290004 acc: 0.9519119262695312\n",
      "epoch: 57 step: 22 loss: 0.09181152 acc: 0.9664115905761719\n",
      "epoch: 57 step: 23 loss: 0.08984098 acc: 0.9560089111328125\n",
      "epoch: 57 step: 24 loss: 0.09624096 acc: 0.9558792114257812\n",
      "epoch: 57 step: 25 loss: 0.096784785 acc: 0.9572372436523438\n",
      "epoch: 57 step: 26 loss: 0.10744759 acc: 0.9590415954589844\n",
      "epoch: 57 step: 27 loss: 0.094920084 acc: 0.9549713134765625\n",
      "epoch: 57 step: 28 loss: 0.076861 acc: 0.9679222106933594\n",
      "epoch: 57 step: 29 loss: 0.09044785 acc: 0.9605522155761719\n",
      "epoch: 57 step: 30 loss: 0.09460726 acc: 0.9630813598632812\n",
      "epoch: 57 step: 31 loss: 0.09075485 acc: 0.9646263122558594\n",
      "epoch: 57 step: 32 loss: 0.095697105 acc: 0.9665374755859375\n",
      "epoch: 57 step: 33 loss: 0.07061654 acc: 0.9716644287109375\n",
      "epoch: 57 step: 34 loss: 0.10578939 acc: 0.9634323120117188\n",
      "epoch: 57 step: 35 loss: 0.08899048 acc: 0.9636993408203125\n",
      "epoch: 57 step: 36 loss: 0.0860118 acc: 0.9695701599121094\n",
      "epoch: 57 step: 37 loss: 0.08740909 acc: 0.96258544921875\n",
      "epoch: 57 step: 38 loss: 0.08800828 acc: 0.9594192504882812\n",
      "epoch: 57 step: 39 loss: 0.10013006 acc: 0.9526138305664062\n",
      "epoch: 57 step: 40 loss: 0.08364482 acc: 0.9561119079589844\n",
      "epoch: 57 step: 41 loss: 0.11604449 acc: 0.9528770446777344\n",
      "epoch: 57 step: 42 loss: 0.10769133 acc: 0.9512367248535156\n",
      "epoch: 57 step: 43 loss: 0.11104337 acc: 0.9559059143066406\n",
      "epoch: 57 step: 44 loss: 0.106870145 acc: 0.952301025390625\n",
      "epoch: 57 step: 45 loss: 0.08517175 acc: 0.9600372314453125\n",
      "epoch: 57 step: 46 loss: 0.08554873 acc: 0.9662590026855469\n",
      "epoch: 57 step: 47 loss: 0.088474356 acc: 0.9533843994140625\n",
      "epoch: 57 step: 48 loss: 0.107760556 acc: 0.9568061828613281\n",
      "epoch: 57 step: 49 loss: 0.082726896 acc: 0.9649085998535156\n",
      "epoch: 57 step: 50 loss: 0.09654582 acc: 0.9596786499023438\n",
      "epoch: 57 step: 51 loss: 0.10238243 acc: 0.9608840942382812\n",
      "epoch: 57 step: 52 loss: 0.10056279 acc: 0.9621505737304688\n",
      "epoch: 57 step: 53 loss: 0.07725898 acc: 0.9649543762207031\n",
      "epoch: 57 step: 54 loss: 0.09364015 acc: 0.9628524780273438\n",
      "epoch: 57 step: 55 loss: 0.108294055 acc: 0.9512939453125\n",
      "epoch: 57 step: 56 loss: 0.11781054 acc: 0.9564666748046875\n",
      "epoch: 57 step: 57 loss: 0.09291241 acc: 0.9546051025390625\n",
      "epoch: 57 step: 58 loss: 0.09268395 acc: 0.9580192565917969\n",
      "epoch: 57 step: 59 loss: 0.09299925 acc: 0.9599533081054688\n",
      "epoch: 57 step: 60 loss: 0.09005039 acc: 0.955810546875\n",
      "epoch: 57 step: 61 loss: 0.076380305 acc: 0.9626045227050781\n",
      "epoch: 57 step: 62 loss: 0.096554056 acc: 0.9550361633300781\n",
      "epoch: 57 step: 63 loss: 0.08505434 acc: 0.9612197875976562\n",
      "epoch: 57 step: 64 loss: 0.0776277 acc: 0.9717903137207031\n",
      "epoch: 57 step: 65 loss: 0.0830823 acc: 0.9724960327148438\n",
      "epoch: 57 step: 66 loss: 0.0987891 acc: 0.9610176086425781\n",
      "epoch: 57 step: 67 loss: 0.08447495 acc: 0.963653564453125\n",
      "epoch: 57 step: 68 loss: 0.1132237 acc: 0.9571113586425781\n",
      "epoch: 57 step: 69 loss: 0.09608481 acc: 0.9607505798339844\n",
      "epoch: 57 step: 70 loss: 0.09915647 acc: 0.9631614685058594\n",
      "epoch: 57 step: 71 loss: 0.08349715 acc: 0.9677009582519531\n",
      "epoch: 57 step: 72 loss: 0.09507413 acc: 0.9550895690917969\n",
      "epoch: 57 step: 73 loss: 0.09823059 acc: 0.9550628662109375\n",
      "epoch: 57 step: 74 loss: 0.08726243 acc: 0.9581642150878906\n",
      "epoch: 57 step: 75 loss: 0.09131842 acc: 0.9645957946777344\n",
      "epoch: 57 step: 76 loss: 0.102666266 acc: 0.9613380432128906\n",
      "epoch: 57 step: 77 loss: 0.08178451 acc: 0.96209716796875\n",
      "epoch: 57 step: 78 loss: 0.09050763 acc: 0.95843505859375\n",
      "epoch: 57 step: 79 loss: 0.06858189 acc: 0.9702644348144531\n",
      "epoch: 57 step: 80 loss: 0.08660888 acc: 0.961578369140625\n",
      "epoch: 57 step: 81 loss: 0.07704403 acc: 0.9661026000976562\n",
      "epoch: 57 step: 82 loss: 0.09689849 acc: 0.9612655639648438\n",
      "epoch: 57 step: 83 loss: 0.09079489 acc: 0.9651527404785156\n",
      "epoch: 57 step: 84 loss: 0.076063305 acc: 0.9645652770996094\n",
      "epoch: 57 step: 85 loss: 0.08276182 acc: 0.9639968872070312\n",
      "epoch: 57 step: 86 loss: 0.07743793 acc: 0.9638671875\n",
      "epoch: 57 step: 87 loss: 0.07438127 acc: 0.96502685546875\n",
      "epoch: 57 step: 88 loss: 0.07790001 acc: 0.9608230590820312\n",
      "epoch: 57 step: 89 loss: 0.10239 acc: 0.9551887512207031\n",
      "epoch: 57 step: 90 loss: 0.08226998 acc: 0.9631462097167969\n",
      "epoch: 57 step: 91 loss: 0.10790835 acc: 0.9476127624511719\n",
      "epoch: 57 step: 92 loss: 0.09358479 acc: 0.9623260498046875\n",
      "epoch: 57 step: 93 loss: 0.08391872 acc: 0.961944580078125\n",
      "epoch: 57 step: 94 loss: 0.10227838 acc: 0.9593467712402344\n",
      "epoch: 57 step: 95 loss: 0.07668479 acc: 0.9713020324707031\n",
      "epoch: 57 step: 96 loss: 0.08573908 acc: 0.9667205810546875\n",
      "epoch: 57 step: 97 loss: 0.0870845 acc: 0.9652442932128906\n",
      "epoch: 57 step: 98 loss: 0.087589726 acc: 0.9639816284179688\n",
      "epoch: 57 step: 99 loss: 0.08579834 acc: 0.9640998840332031\n",
      "epoch: 57 step: 100 loss: 0.0916308 acc: 0.9655494689941406\n",
      "epoch: 57 step: 101 loss: 0.08144537 acc: 0.9667434692382812\n",
      "epoch: 57 step: 102 loss: 0.10188216 acc: 0.9579238891601562\n",
      "epoch: 57 step: 103 loss: 0.09017734 acc: 0.964111328125\n",
      "epoch: 57 step: 104 loss: 0.091614045 acc: 0.9568367004394531\n",
      "epoch: 57 step: 105 loss: 0.10826645 acc: 0.9606170654296875\n",
      "epoch: 57 step: 106 loss: 0.068585984 acc: 0.9689559936523438\n",
      "epoch: 57 step: 107 loss: 0.08457615 acc: 0.9627113342285156\n",
      "epoch: 57 step: 108 loss: 0.06985992 acc: 0.9677238464355469\n",
      "epoch: 57 step: 109 loss: 0.08574875 acc: 0.9666328430175781\n",
      "epoch: 57 step: 110 loss: 0.10789765 acc: 0.9551773071289062\n",
      "epoch: 57 step: 111 loss: 0.09117586 acc: 0.9576835632324219\n",
      "epoch: 57 step: 112 loss: 0.09669976 acc: 0.9564323425292969\n",
      "epoch: 57 step: 113 loss: 0.10074975 acc: 0.9664268493652344\n",
      "epoch: 57 step: 114 loss: 0.08499743 acc: 0.9589614868164062\n",
      "epoch: 57 step: 115 loss: 0.10369427 acc: 0.9539947509765625\n",
      "epoch: 57 step: 116 loss: 0.0773006 acc: 0.9642295837402344\n",
      "epoch: 57 step: 117 loss: 0.066433474 acc: 0.9688949584960938\n",
      "epoch: 57 step: 118 loss: 0.09095979 acc: 0.9617424011230469\n",
      "epoch: 57 step: 119 loss: 0.089629464 acc: 0.9603347778320312\n",
      "epoch: 57 step: 120 loss: 0.08563821 acc: 0.9669952392578125\n",
      "epoch: 57 step: 121 loss: 0.0912027 acc: 0.9614486694335938\n",
      "epoch: 57 step: 122 loss: 0.086026974 acc: 0.963348388671875\n",
      "epoch: 57 step: 123 loss: 0.09143364 acc: 0.960296630859375\n",
      "epoch: 57 step: 124 loss: 0.09489206 acc: 0.9645560128348214\n",
      "epoch: 57 validation_loss: 0.094 validation_dice: 0.835844342539793\n",
      "epoch: 57 test_dataset dice: 0.7576908747711196\n",
      "time cost 0.5361424565315247 min\n",
      "dice_best: 0.8429257274299863\n",
      "******************************** epoch  57  is finished. *********************************\n",
      "epoch: 58 step: 1 loss: 0.09288064 acc: 0.9550971984863281\n",
      "epoch: 58 step: 2 loss: 0.07522634 acc: 0.9636268615722656\n",
      "epoch: 58 step: 3 loss: 0.11563242 acc: 0.9518203735351562\n",
      "epoch: 58 step: 4 loss: 0.08531078 acc: 0.9648780822753906\n",
      "epoch: 58 step: 5 loss: 0.099028386 acc: 0.9574623107910156\n",
      "epoch: 58 step: 6 loss: 0.08295208 acc: 0.9668655395507812\n",
      "epoch: 58 step: 7 loss: 0.104965284 acc: 0.9538650512695312\n",
      "epoch: 58 step: 8 loss: 0.08857029 acc: 0.9634361267089844\n",
      "epoch: 58 step: 9 loss: 0.08643265 acc: 0.95867919921875\n",
      "epoch: 58 step: 10 loss: 0.080522545 acc: 0.9678497314453125\n",
      "epoch: 58 step: 11 loss: 0.0755258 acc: 0.965545654296875\n",
      "epoch: 58 step: 12 loss: 0.10302953 acc: 0.9597892761230469\n",
      "epoch: 58 step: 13 loss: 0.08099238 acc: 0.9648590087890625\n",
      "epoch: 58 step: 14 loss: 0.09323595 acc: 0.9610862731933594\n",
      "epoch: 58 step: 15 loss: 0.09601286 acc: 0.9601631164550781\n",
      "epoch: 58 step: 16 loss: 0.09330247 acc: 0.9579315185546875\n",
      "epoch: 58 step: 17 loss: 0.07268807 acc: 0.9632835388183594\n",
      "epoch: 58 step: 18 loss: 0.111393176 acc: 0.958282470703125\n",
      "epoch: 58 step: 19 loss: 0.0786254 acc: 0.9633941650390625\n",
      "epoch: 58 step: 20 loss: 0.10311774 acc: 0.9588775634765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 58 step: 21 loss: 0.08765967 acc: 0.9616317749023438\n",
      "epoch: 58 step: 22 loss: 0.089355126 acc: 0.9634666442871094\n",
      "epoch: 58 step: 23 loss: 0.09298906 acc: 0.9658203125\n",
      "epoch: 58 step: 24 loss: 0.08047301 acc: 0.9653854370117188\n",
      "epoch: 58 step: 25 loss: 0.10644374 acc: 0.9649162292480469\n",
      "epoch: 58 step: 26 loss: 0.088567056 acc: 0.9656639099121094\n",
      "epoch: 58 step: 27 loss: 0.10433608 acc: 0.9613685607910156\n",
      "epoch: 58 step: 28 loss: 0.088028945 acc: 0.9574241638183594\n",
      "epoch: 58 step: 29 loss: 0.105517745 acc: 0.9511375427246094\n",
      "epoch: 58 step: 30 loss: 0.0820691 acc: 0.9592170715332031\n",
      "epoch: 58 step: 31 loss: 0.08635816 acc: 0.9610748291015625\n",
      "epoch: 58 step: 32 loss: 0.09318342 acc: 0.9583702087402344\n",
      "epoch: 58 step: 33 loss: 0.11603362 acc: 0.9472236633300781\n",
      "epoch: 58 step: 34 loss: 0.10600546 acc: 0.9606132507324219\n",
      "epoch: 58 step: 35 loss: 0.07340543 acc: 0.9652595520019531\n",
      "epoch: 58 step: 36 loss: 0.09953022 acc: 0.9652748107910156\n",
      "epoch: 58 step: 37 loss: 0.09095377 acc: 0.9628067016601562\n",
      "epoch: 58 step: 38 loss: 0.111010514 acc: 0.9557876586914062\n",
      "epoch: 58 step: 39 loss: 0.08586877 acc: 0.958282470703125\n",
      "epoch: 58 step: 40 loss: 0.114563726 acc: 0.9609527587890625\n",
      "epoch: 58 step: 41 loss: 0.09206462 acc: 0.9611663818359375\n",
      "epoch: 58 step: 42 loss: 0.085112646 acc: 0.9598922729492188\n",
      "epoch: 58 step: 43 loss: 0.10060408 acc: 0.9615402221679688\n",
      "epoch: 58 step: 44 loss: 0.105887175 acc: 0.963470458984375\n",
      "epoch: 58 step: 45 loss: 0.093424425 acc: 0.9696311950683594\n",
      "epoch: 58 step: 46 loss: 0.08482435 acc: 0.9612159729003906\n",
      "epoch: 58 step: 47 loss: 0.08821173 acc: 0.9687576293945312\n",
      "epoch: 58 step: 48 loss: 0.08468366 acc: 0.9583740234375\n",
      "epoch: 58 step: 49 loss: 0.08987456 acc: 0.9573554992675781\n",
      "epoch: 58 step: 50 loss: 0.095191464 acc: 0.9659767150878906\n",
      "epoch: 58 step: 51 loss: 0.08612257 acc: 0.9621315002441406\n",
      "epoch: 58 step: 52 loss: 0.09666504 acc: 0.9645271301269531\n",
      "epoch: 58 step: 53 loss: 0.20799525 acc: 0.9585609436035156\n",
      "epoch: 58 step: 54 loss: 0.10066633 acc: 0.9661750793457031\n",
      "epoch: 58 step: 55 loss: 0.18609007 acc: 0.9579887390136719\n",
      "epoch: 58 step: 56 loss: 0.14423107 acc: 0.9576034545898438\n",
      "epoch: 58 step: 57 loss: 0.13156274 acc: 0.9514427185058594\n",
      "epoch: 58 step: 58 loss: 0.15218137 acc: 0.9473762512207031\n",
      "epoch: 58 step: 59 loss: 0.12568991 acc: 0.9567604064941406\n",
      "epoch: 58 step: 60 loss: 0.16816698 acc: 0.9508399963378906\n",
      "epoch: 58 step: 61 loss: 0.15750517 acc: 0.9517707824707031\n",
      "epoch: 58 step: 62 loss: 0.117619984 acc: 0.9597816467285156\n",
      "epoch: 58 step: 63 loss: 0.16449705 acc: 0.9467010498046875\n",
      "epoch: 58 step: 64 loss: 0.22166146 acc: 0.9479179382324219\n",
      "epoch: 58 step: 65 loss: 0.18003887 acc: 0.9484100341796875\n",
      "epoch: 58 step: 66 loss: 0.146585 acc: 0.9481964111328125\n",
      "epoch: 58 step: 67 loss: 0.14539735 acc: 0.9396553039550781\n",
      "epoch: 58 step: 68 loss: 0.17049213 acc: 0.9454689025878906\n",
      "epoch: 58 step: 69 loss: 0.14555375 acc: 0.9562530517578125\n",
      "epoch: 58 step: 70 loss: 0.13055173 acc: 0.9598350524902344\n",
      "epoch: 58 step: 71 loss: 0.14448823 acc: 0.9557266235351562\n",
      "epoch: 58 step: 72 loss: 0.2276549 acc: 0.9540863037109375\n",
      "epoch: 58 step: 73 loss: 0.16743858 acc: 0.9561576843261719\n",
      "epoch: 58 step: 74 loss: 0.15755917 acc: 0.9517822265625\n",
      "epoch: 58 step: 75 loss: 0.18664624 acc: 0.9483909606933594\n",
      "epoch: 58 step: 76 loss: 0.14114706 acc: 0.9438705444335938\n",
      "epoch: 58 step: 77 loss: 0.15315437 acc: 0.9374542236328125\n",
      "epoch: 58 step: 78 loss: 0.15994577 acc: 0.9393348693847656\n",
      "epoch: 58 step: 79 loss: 0.15820168 acc: 0.9530754089355469\n",
      "epoch: 58 step: 80 loss: 0.17552724 acc: 0.9428825378417969\n",
      "epoch: 58 step: 81 loss: 0.16015999 acc: 0.9484024047851562\n",
      "epoch: 58 step: 82 loss: 0.19220544 acc: 0.9486618041992188\n",
      "epoch: 58 step: 83 loss: 0.16475289 acc: 0.9527359008789062\n",
      "epoch: 58 step: 84 loss: 0.13856588 acc: 0.9523353576660156\n",
      "epoch: 58 step: 85 loss: 0.14828289 acc: 0.9475936889648438\n",
      "epoch: 58 step: 86 loss: 0.16786973 acc: 0.9490432739257812\n",
      "epoch: 58 step: 87 loss: 0.15207876 acc: 0.9549102783203125\n",
      "epoch: 58 step: 88 loss: 0.19214922 acc: 0.942840576171875\n",
      "epoch: 58 step: 89 loss: 0.14460112 acc: 0.943572998046875\n",
      "epoch: 58 step: 90 loss: 0.14914308 acc: 0.9507789611816406\n",
      "epoch: 58 step: 91 loss: 0.12926812 acc: 0.9527320861816406\n",
      "epoch: 58 step: 92 loss: 0.17228076 acc: 0.9398574829101562\n",
      "epoch: 58 step: 93 loss: 0.185503 acc: 0.9467391967773438\n",
      "epoch: 58 step: 94 loss: 0.15300836 acc: 0.9524993896484375\n",
      "epoch: 58 step: 95 loss: 0.15434933 acc: 0.9569015502929688\n",
      "epoch: 58 step: 96 loss: 0.17592162 acc: 0.9524345397949219\n",
      "epoch: 58 step: 97 loss: 0.1372876 acc: 0.9494590759277344\n",
      "epoch: 58 step: 98 loss: 0.16983636 acc: 0.9484024047851562\n",
      "epoch: 58 step: 99 loss: 0.15255897 acc: 0.9588699340820312\n",
      "epoch: 58 step: 100 loss: 0.19286828 acc: 0.9462051391601562\n",
      "epoch: 58 step: 101 loss: 0.115949154 acc: 0.9537773132324219\n",
      "epoch: 58 step: 102 loss: 0.12755555 acc: 0.9645729064941406\n",
      "epoch: 58 step: 103 loss: 0.22615819 acc: 0.9512710571289062\n",
      "epoch: 58 step: 104 loss: 0.18881682 acc: 0.9526634216308594\n",
      "epoch: 58 step: 105 loss: 0.17170902 acc: 0.9528579711914062\n",
      "epoch: 58 step: 106 loss: 0.15036775 acc: 0.9481658935546875\n",
      "epoch: 58 step: 107 loss: 0.1589072 acc: 0.9513626098632812\n",
      "epoch: 58 step: 108 loss: 0.20629261 acc: 0.9364395141601562\n",
      "epoch: 58 step: 109 loss: 0.16138758 acc: 0.9386940002441406\n",
      "epoch: 58 step: 110 loss: 0.14586504 acc: 0.9458045959472656\n",
      "epoch: 58 step: 111 loss: 0.1415377 acc: 0.9444084167480469\n",
      "epoch: 58 step: 112 loss: 0.15235883 acc: 0.9382095336914062\n",
      "epoch: 58 step: 113 loss: 0.16480547 acc: 0.934173583984375\n",
      "epoch: 58 step: 114 loss: 0.12444254 acc: 0.954559326171875\n",
      "epoch: 58 step: 115 loss: 0.13919142 acc: 0.9493370056152344\n",
      "epoch: 58 step: 116 loss: 0.19380663 acc: 0.9496650695800781\n",
      "epoch: 58 step: 117 loss: 0.16208103 acc: 0.9530029296875\n",
      "epoch: 58 step: 118 loss: 0.12682177 acc: 0.9547348022460938\n",
      "epoch: 58 step: 119 loss: 0.14219517 acc: 0.9592056274414062\n",
      "epoch: 58 step: 120 loss: 0.14707056 acc: 0.9516029357910156\n",
      "epoch: 58 step: 121 loss: 0.15362702 acc: 0.9469451904296875\n",
      "epoch: 58 step: 122 loss: 0.1177461 acc: 0.9508743286132812\n",
      "epoch: 58 step: 123 loss: 0.108650014 acc: 0.9578475952148438\n",
      "epoch: 58 step: 124 loss: 0.12403287 acc: 0.9522007533482143\n",
      "epoch: 58 validation_loss: 0.133 validation_dice: 0.7933753235859804\n",
      "epoch: 58 test_dataset dice: 0.7404958044053269\n",
      "time cost 0.536342199643453 min\n",
      "dice_best: 0.8429257274299863\n",
      "******************************** epoch  58  is finished. *********************************\n",
      "epoch: 59 step: 1 loss: 0.1593223 acc: 0.9496002197265625\n",
      "epoch: 59 step: 2 loss: 0.116045766 acc: 0.9567489624023438\n",
      "epoch: 59 step: 3 loss: 0.100075915 acc: 0.9578056335449219\n",
      "epoch: 59 step: 4 loss: 0.15177067 acc: 0.9491653442382812\n",
      "epoch: 59 step: 5 loss: 0.124919504 acc: 0.9610061645507812\n",
      "epoch: 59 step: 6 loss: 0.18301398 acc: 0.95245361328125\n",
      "epoch: 59 step: 7 loss: 0.12522307 acc: 0.9563369750976562\n",
      "epoch: 59 step: 8 loss: 0.120147385 acc: 0.9553794860839844\n",
      "epoch: 59 step: 9 loss: 0.102027155 acc: 0.9617462158203125\n",
      "epoch: 59 step: 10 loss: 0.118679956 acc: 0.9503440856933594\n",
      "epoch: 59 step: 11 loss: 0.12273726 acc: 0.9628410339355469\n",
      "epoch: 59 step: 12 loss: 0.1095269 acc: 0.9574432373046875\n",
      "epoch: 59 step: 13 loss: 0.14700517 acc: 0.9449958801269531\n",
      "epoch: 59 step: 14 loss: 0.13507766 acc: 0.9486961364746094\n",
      "epoch: 59 step: 15 loss: 0.096739516 acc: 0.9639739990234375\n",
      "epoch: 59 step: 16 loss: 0.110913485 acc: 0.9590835571289062\n",
      "epoch: 59 step: 17 loss: 0.12726268 acc: 0.9564437866210938\n",
      "epoch: 59 step: 18 loss: 0.11001445 acc: 0.9576416015625\n",
      "epoch: 59 step: 19 loss: 0.115637116 acc: 0.9517021179199219\n",
      "epoch: 59 step: 20 loss: 0.11093851 acc: 0.963287353515625\n",
      "epoch: 59 step: 21 loss: 0.1053398 acc: 0.9593734741210938\n",
      "epoch: 59 step: 22 loss: 0.1056867 acc: 0.9622764587402344\n",
      "epoch: 59 step: 23 loss: 0.103998974 acc: 0.9624214172363281\n",
      "epoch: 59 step: 24 loss: 0.094536774 acc: 0.9584197998046875\n",
      "epoch: 59 step: 25 loss: 0.10773072 acc: 0.9601478576660156\n",
      "epoch: 59 step: 26 loss: 0.100692175 acc: 0.9567832946777344\n",
      "epoch: 59 step: 27 loss: 0.08490955 acc: 0.9664344787597656\n",
      "epoch: 59 step: 28 loss: 0.09273504 acc: 0.959564208984375\n",
      "epoch: 59 step: 29 loss: 0.10801994 acc: 0.9529609680175781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 59 step: 30 loss: 0.10880317 acc: 0.9557762145996094\n",
      "epoch: 59 step: 31 loss: 0.094548956 acc: 0.9622344970703125\n",
      "epoch: 59 step: 32 loss: 0.11334236 acc: 0.9574661254882812\n",
      "epoch: 59 step: 33 loss: 0.122641206 acc: 0.9577484130859375\n",
      "epoch: 59 step: 34 loss: 0.10601497 acc: 0.9607124328613281\n",
      "epoch: 59 step: 35 loss: 0.10887091 acc: 0.9589691162109375\n",
      "epoch: 59 step: 36 loss: 0.10793596 acc: 0.9606399536132812\n",
      "epoch: 59 step: 37 loss: 0.09782429 acc: 0.9636764526367188\n",
      "epoch: 59 step: 38 loss: 0.09642775 acc: 0.9579391479492188\n",
      "epoch: 59 step: 39 loss: 0.108892 acc: 0.9589614868164062\n",
      "epoch: 59 step: 40 loss: 0.098208845 acc: 0.9603118896484375\n",
      "epoch: 59 step: 41 loss: 0.08697211 acc: 0.9666976928710938\n",
      "epoch: 59 step: 42 loss: 0.10332927 acc: 0.9607467651367188\n",
      "epoch: 59 step: 43 loss: 0.09461987 acc: 0.9679222106933594\n",
      "epoch: 59 step: 44 loss: 0.1150515 acc: 0.9595527648925781\n",
      "epoch: 59 step: 45 loss: 0.11970898 acc: 0.9570846557617188\n",
      "epoch: 59 step: 46 loss: 0.0875446 acc: 0.9625015258789062\n",
      "epoch: 59 step: 47 loss: 0.13141333 acc: 0.949615478515625\n",
      "epoch: 59 step: 48 loss: 0.08502596 acc: 0.9641380310058594\n",
      "epoch: 59 step: 49 loss: 0.14860535 acc: 0.9513740539550781\n",
      "epoch: 59 step: 50 loss: 0.09909387 acc: 0.9622993469238281\n",
      "epoch: 59 step: 51 loss: 0.09740453 acc: 0.9618186950683594\n",
      "epoch: 59 step: 52 loss: 0.13983962 acc: 0.9476966857910156\n",
      "epoch: 59 step: 53 loss: 0.109359 acc: 0.9615898132324219\n",
      "epoch: 59 step: 54 loss: 0.11879286 acc: 0.9516410827636719\n",
      "epoch: 59 step: 55 loss: 0.13912036 acc: 0.9480171203613281\n",
      "epoch: 59 step: 56 loss: 0.09542141 acc: 0.9605445861816406\n",
      "epoch: 59 step: 57 loss: 0.12246609 acc: 0.9509963989257812\n",
      "epoch: 59 step: 58 loss: 0.09275864 acc: 0.9653167724609375\n",
      "epoch: 59 step: 59 loss: 0.12650749 acc: 0.9522476196289062\n",
      "epoch: 59 step: 60 loss: 0.09877845 acc: 0.95916748046875\n",
      "epoch: 59 step: 61 loss: 0.13601243 acc: 0.956695556640625\n",
      "epoch: 59 step: 62 loss: 0.119140685 acc: 0.9592132568359375\n",
      "epoch: 59 step: 63 loss: 0.10291098 acc: 0.9568252563476562\n",
      "epoch: 59 step: 64 loss: 0.09960351 acc: 0.963470458984375\n",
      "epoch: 59 step: 65 loss: 0.13121365 acc: 0.951751708984375\n",
      "epoch: 59 step: 66 loss: 0.11389513 acc: 0.9583511352539062\n",
      "epoch: 59 step: 67 loss: 0.08957917 acc: 0.9687576293945312\n",
      "epoch: 59 step: 68 loss: 0.1103546 acc: 0.9517631530761719\n",
      "epoch: 59 step: 69 loss: 0.11837742 acc: 0.9535560607910156\n",
      "epoch: 59 step: 70 loss: 0.10666546 acc: 0.9525032043457031\n",
      "epoch: 59 step: 71 loss: 0.09042669 acc: 0.9634895324707031\n",
      "epoch: 59 step: 72 loss: 0.12393463 acc: 0.946563720703125\n",
      "epoch: 59 step: 73 loss: 0.08872984 acc: 0.9648551940917969\n",
      "epoch: 59 step: 74 loss: 0.1074011 acc: 0.9595222473144531\n",
      "epoch: 59 step: 75 loss: 0.10494154 acc: 0.958709716796875\n",
      "epoch: 59 step: 76 loss: 0.10594183 acc: 0.9670639038085938\n",
      "epoch: 59 step: 77 loss: 0.08878077 acc: 0.9633445739746094\n",
      "epoch: 59 step: 78 loss: 0.099175565 acc: 0.96221923828125\n",
      "epoch: 59 step: 79 loss: 0.11599165 acc: 0.9533309936523438\n",
      "epoch: 59 step: 80 loss: 0.10830617 acc: 0.9530868530273438\n",
      "epoch: 59 step: 81 loss: 0.080476314 acc: 0.9619903564453125\n",
      "epoch: 59 step: 82 loss: 0.13448873 acc: 0.9544143676757812\n",
      "epoch: 59 step: 83 loss: 0.097417496 acc: 0.9582672119140625\n",
      "epoch: 59 step: 84 loss: 0.08588451 acc: 0.9626197814941406\n",
      "epoch: 59 step: 85 loss: 0.10425735 acc: 0.9645423889160156\n",
      "epoch: 59 step: 86 loss: 0.09792826 acc: 0.9647712707519531\n",
      "epoch: 59 step: 87 loss: 0.09561993 acc: 0.9704780578613281\n",
      "epoch: 59 step: 88 loss: 0.0880396 acc: 0.9727287292480469\n",
      "epoch: 59 step: 89 loss: 0.07547317 acc: 0.9689521789550781\n",
      "epoch: 59 step: 90 loss: 0.10352204 acc: 0.9686088562011719\n",
      "epoch: 59 step: 91 loss: 0.098014124 acc: 0.9643821716308594\n",
      "epoch: 59 step: 92 loss: 0.11160066 acc: 0.9572372436523438\n",
      "epoch: 59 step: 93 loss: 0.09522115 acc: 0.9587860107421875\n",
      "epoch: 59 step: 94 loss: 0.09076884 acc: 0.9555130004882812\n",
      "epoch: 59 step: 95 loss: 0.08886467 acc: 0.9623069763183594\n",
      "epoch: 59 step: 96 loss: 0.097135 acc: 0.9591636657714844\n",
      "epoch: 59 step: 97 loss: 0.104174815 acc: 0.9523162841796875\n",
      "epoch: 59 step: 98 loss: 0.12243372 acc: 0.9507331848144531\n",
      "epoch: 59 step: 99 loss: 0.10282368 acc: 0.9617271423339844\n",
      "epoch: 59 step: 100 loss: 0.117262475 acc: 0.9545745849609375\n",
      "epoch: 59 step: 101 loss: 0.108064115 acc: 0.9577522277832031\n",
      "epoch: 59 step: 102 loss: 0.100204915 acc: 0.9542655944824219\n",
      "epoch: 59 step: 103 loss: 0.11368913 acc: 0.9495201110839844\n",
      "epoch: 59 step: 104 loss: 0.09705466 acc: 0.9599723815917969\n",
      "epoch: 59 step: 105 loss: 0.09773525 acc: 0.9570274353027344\n",
      "epoch: 59 step: 106 loss: 0.084514745 acc: 0.96270751953125\n",
      "epoch: 59 step: 107 loss: 0.092686445 acc: 0.9636878967285156\n",
      "epoch: 59 step: 108 loss: 0.093308374 acc: 0.9579849243164062\n",
      "epoch: 59 step: 109 loss: 0.1060763 acc: 0.9563102722167969\n",
      "epoch: 59 step: 110 loss: 0.08819999 acc: 0.9665145874023438\n",
      "epoch: 59 step: 111 loss: 0.11829388 acc: 0.963623046875\n",
      "epoch: 59 step: 112 loss: 0.08849413 acc: 0.9665298461914062\n",
      "epoch: 59 step: 113 loss: 0.099675395 acc: 0.9617729187011719\n",
      "epoch: 59 step: 114 loss: 0.13916227 acc: 0.9621505737304688\n",
      "epoch: 59 step: 115 loss: 0.102919556 acc: 0.9606361389160156\n",
      "epoch: 59 step: 116 loss: 0.09812604 acc: 0.9597396850585938\n",
      "epoch: 59 step: 117 loss: 0.09293703 acc: 0.9622116088867188\n",
      "epoch: 59 step: 118 loss: 0.10404838 acc: 0.9534568786621094\n",
      "epoch: 59 step: 119 loss: 0.11471964 acc: 0.9481124877929688\n",
      "epoch: 59 step: 120 loss: 0.10554692 acc: 0.9544525146484375\n",
      "epoch: 59 step: 121 loss: 0.13028015 acc: 0.9419517517089844\n",
      "epoch: 59 step: 122 loss: 0.09256525 acc: 0.9510993957519531\n",
      "epoch: 59 step: 123 loss: 0.10793427 acc: 0.9494361877441406\n",
      "epoch: 59 step: 124 loss: 0.09279276 acc: 0.9694126674107143\n",
      "epoch: 59 validation_loss: 0.101 validation_dice: 0.8255347795896122\n",
      "epoch: 59 test_dataset dice: 0.736078279142952\n",
      "time cost 0.5374417980511983 min\n",
      "dice_best: 0.8429257274299863\n",
      "******************************** epoch  59  is finished. *********************************\n",
      "epoch: 60 step: 1 loss: 0.10157618 acc: 0.9519805908203125\n",
      "epoch: 60 step: 2 loss: 0.09767826 acc: 0.9571495056152344\n",
      "epoch: 60 step: 3 loss: 0.1042213 acc: 0.9465370178222656\n",
      "epoch: 60 step: 4 loss: 0.11765566 acc: 0.9571952819824219\n",
      "epoch: 60 step: 5 loss: 0.09581452 acc: 0.9623374938964844\n",
      "epoch: 60 step: 6 loss: 0.104196005 acc: 0.9608573913574219\n",
      "epoch: 60 step: 7 loss: 0.09521205 acc: 0.9591407775878906\n",
      "epoch: 60 step: 8 loss: 0.08696448 acc: 0.9627685546875\n",
      "epoch: 60 step: 9 loss: 0.07891387 acc: 0.9635581970214844\n",
      "epoch: 60 step: 10 loss: 0.10355032 acc: 0.9587936401367188\n",
      "epoch: 60 step: 11 loss: 0.08076412 acc: 0.9609031677246094\n",
      "epoch: 60 step: 12 loss: 0.09329267 acc: 0.95501708984375\n",
      "epoch: 60 step: 13 loss: 0.10435841 acc: 0.9569168090820312\n",
      "epoch: 60 step: 14 loss: 0.09242999 acc: 0.9592094421386719\n",
      "epoch: 60 step: 15 loss: 0.08680392 acc: 0.9602317810058594\n",
      "epoch: 60 step: 16 loss: 0.075048685 acc: 0.9651947021484375\n",
      "epoch: 60 step: 17 loss: 0.09537053 acc: 0.9591026306152344\n",
      "epoch: 60 step: 18 loss: 0.09213994 acc: 0.9579391479492188\n",
      "epoch: 60 step: 19 loss: 0.0961165 acc: 0.9639778137207031\n",
      "epoch: 60 step: 20 loss: 0.08448077 acc: 0.9626541137695312\n",
      "epoch: 60 step: 21 loss: 0.08612374 acc: 0.9633941650390625\n",
      "epoch: 60 step: 22 loss: 0.093198106 acc: 0.9658584594726562\n",
      "epoch: 60 step: 23 loss: 0.090611175 acc: 0.9641189575195312\n",
      "epoch: 60 step: 24 loss: 0.09445931 acc: 0.9605140686035156\n",
      "epoch: 60 step: 25 loss: 0.07610162 acc: 0.966278076171875\n",
      "epoch: 60 step: 26 loss: 0.09592634 acc: 0.9626922607421875\n",
      "epoch: 60 step: 27 loss: 0.11358693 acc: 0.9552383422851562\n",
      "epoch: 60 step: 28 loss: 0.08078534 acc: 0.96270751953125\n",
      "epoch: 60 step: 29 loss: 0.083053164 acc: 0.9672088623046875\n",
      "epoch: 60 step: 30 loss: 0.09665426 acc: 0.9585304260253906\n",
      "epoch: 60 step: 31 loss: 0.087639414 acc: 0.9641036987304688\n",
      "epoch: 60 step: 32 loss: 0.09155808 acc: 0.9600334167480469\n",
      "epoch: 60 step: 33 loss: 0.096327 acc: 0.9614715576171875\n",
      "epoch: 60 step: 34 loss: 0.09184186 acc: 0.9597740173339844\n",
      "epoch: 60 step: 35 loss: 0.103965886 acc: 0.95556640625\n",
      "epoch: 60 step: 36 loss: 0.09039461 acc: 0.9661407470703125\n",
      "epoch: 60 step: 37 loss: 0.07618954 acc: 0.9681167602539062\n",
      "epoch: 60 step: 38 loss: 0.08743981 acc: 0.9599456787109375\n",
      "epoch: 60 step: 39 loss: 0.09644349 acc: 0.9632949829101562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 60 step: 40 loss: 0.0789676 acc: 0.9662055969238281\n",
      "epoch: 60 step: 41 loss: 0.092933275 acc: 0.9643745422363281\n",
      "epoch: 60 step: 42 loss: 0.08036414 acc: 0.96112060546875\n",
      "epoch: 60 step: 43 loss: 0.09924637 acc: 0.9630279541015625\n",
      "epoch: 60 step: 44 loss: 0.08842769 acc: 0.9618759155273438\n",
      "epoch: 60 step: 45 loss: 0.1297809 acc: 0.9518585205078125\n",
      "epoch: 60 step: 46 loss: 0.11066119 acc: 0.9586257934570312\n",
      "epoch: 60 step: 47 loss: 0.1022322 acc: 0.9584693908691406\n",
      "epoch: 60 step: 48 loss: 0.090564236 acc: 0.96112060546875\n",
      "epoch: 60 step: 49 loss: 0.08642973 acc: 0.9586143493652344\n",
      "epoch: 60 step: 50 loss: 0.093080364 acc: 0.959808349609375\n",
      "epoch: 60 step: 51 loss: 0.0722564 acc: 0.9637527465820312\n",
      "epoch: 60 step: 52 loss: 0.08541035 acc: 0.9629020690917969\n",
      "epoch: 60 step: 53 loss: 0.104763344 acc: 0.9533271789550781\n",
      "epoch: 60 step: 54 loss: 0.09365061 acc: 0.9655685424804688\n",
      "epoch: 60 step: 55 loss: 0.08498059 acc: 0.9592857360839844\n",
      "epoch: 60 step: 56 loss: 0.08967255 acc: 0.9699058532714844\n",
      "epoch: 60 step: 57 loss: 0.08199743 acc: 0.9660186767578125\n",
      "epoch: 60 step: 58 loss: 0.08764286 acc: 0.9670791625976562\n",
      "epoch: 60 step: 59 loss: 0.10045464 acc: 0.9572944641113281\n",
      "epoch: 60 step: 60 loss: 0.10350264 acc: 0.9557151794433594\n",
      "epoch: 60 step: 61 loss: 0.08457533 acc: 0.9596633911132812\n",
      "epoch: 60 step: 62 loss: 0.0886023 acc: 0.966339111328125\n",
      "epoch: 60 step: 63 loss: 0.10779243 acc: 0.9527168273925781\n",
      "epoch: 60 step: 64 loss: 0.091560006 acc: 0.9589195251464844\n",
      "epoch: 60 step: 65 loss: 0.09958027 acc: 0.9580154418945312\n",
      "epoch: 60 step: 66 loss: 0.09455649 acc: 0.9607658386230469\n",
      "epoch: 60 step: 67 loss: 0.08779898 acc: 0.9569168090820312\n",
      "epoch: 60 step: 68 loss: 0.09095466 acc: 0.9582366943359375\n",
      "epoch: 60 step: 69 loss: 0.09493022 acc: 0.96002197265625\n",
      "epoch: 60 step: 70 loss: 0.094709724 acc: 0.9566917419433594\n",
      "epoch: 60 step: 71 loss: 0.0813933 acc: 0.9700469970703125\n",
      "epoch: 60 step: 72 loss: 0.0892245 acc: 0.9658699035644531\n",
      "epoch: 60 step: 73 loss: 0.086164035 acc: 0.9707260131835938\n",
      "epoch: 60 step: 74 loss: 0.09361699 acc: 0.9617958068847656\n",
      "epoch: 60 step: 75 loss: 0.08332889 acc: 0.9624290466308594\n",
      "epoch: 60 step: 76 loss: 0.07534144 acc: 0.9663963317871094\n",
      "epoch: 60 step: 77 loss: 0.09202225 acc: 0.9640045166015625\n",
      "epoch: 60 step: 78 loss: 0.0961902 acc: 0.9594001770019531\n",
      "epoch: 60 step: 79 loss: 0.072650835 acc: 0.96270751953125\n",
      "epoch: 60 step: 80 loss: 0.08538696 acc: 0.9577102661132812\n",
      "epoch: 60 step: 81 loss: 0.09522968 acc: 0.9533424377441406\n",
      "epoch: 60 step: 82 loss: 0.104725696 acc: 0.9587287902832031\n",
      "epoch: 60 step: 83 loss: 0.079652354 acc: 0.9681129455566406\n",
      "epoch: 60 step: 84 loss: 0.09811083 acc: 0.9582481384277344\n",
      "epoch: 60 step: 85 loss: 0.08939866 acc: 0.9630279541015625\n",
      "epoch: 60 step: 86 loss: 0.087690294 acc: 0.9625778198242188\n",
      "epoch: 60 step: 87 loss: 0.097806096 acc: 0.9598197937011719\n",
      "epoch: 60 step: 88 loss: 0.08005516 acc: 0.9673919677734375\n",
      "epoch: 60 step: 89 loss: 0.13178918 acc: 0.9560203552246094\n",
      "epoch: 60 step: 90 loss: 0.08947658 acc: 0.9628829956054688\n",
      "epoch: 60 step: 91 loss: 0.09228456 acc: 0.9650344848632812\n",
      "epoch: 60 step: 92 loss: 0.09211339 acc: 0.9614524841308594\n",
      "epoch: 60 step: 93 loss: 0.10096064 acc: 0.9573516845703125\n",
      "epoch: 60 step: 94 loss: 0.114578135 acc: 0.9569168090820312\n",
      "epoch: 60 step: 95 loss: 0.10807583 acc: 0.9539299011230469\n",
      "epoch: 60 step: 96 loss: 0.089954354 acc: 0.9624366760253906\n",
      "epoch: 60 step: 97 loss: 0.0940534 acc: 0.9586715698242188\n",
      "epoch: 60 step: 98 loss: 0.08926726 acc: 0.9643898010253906\n",
      "epoch: 60 step: 99 loss: 0.11247918 acc: 0.9564132690429688\n",
      "epoch: 60 step: 100 loss: 0.09930617 acc: 0.9626388549804688\n",
      "epoch: 60 step: 101 loss: 0.11356967 acc: 0.9571609497070312\n",
      "epoch: 60 step: 102 loss: 0.10304453 acc: 0.9582290649414062\n",
      "epoch: 60 step: 103 loss: 0.08646156 acc: 0.9603118896484375\n",
      "epoch: 60 step: 104 loss: 0.10601297 acc: 0.9518623352050781\n",
      "epoch: 60 step: 105 loss: 0.13194847 acc: 0.9434967041015625\n",
      "epoch: 60 step: 106 loss: 0.07886571 acc: 0.9655189514160156\n",
      "epoch: 60 step: 107 loss: 0.11231686 acc: 0.9535713195800781\n",
      "epoch: 60 step: 108 loss: 0.10096494 acc: 0.9565238952636719\n",
      "epoch: 60 step: 109 loss: 0.09168761 acc: 0.9585914611816406\n",
      "epoch: 60 step: 110 loss: 0.069431014 acc: 0.970428466796875\n",
      "epoch: 60 step: 111 loss: 0.11387164 acc: 0.9522361755371094\n",
      "epoch: 60 step: 112 loss: 0.10306215 acc: 0.9597396850585938\n",
      "epoch: 60 step: 113 loss: 0.098486654 acc: 0.9616737365722656\n",
      "epoch: 60 step: 114 loss: 0.09205224 acc: 0.9655265808105469\n",
      "epoch: 60 step: 115 loss: 0.08353135 acc: 0.9653663635253906\n",
      "epoch: 60 step: 116 loss: 0.10931187 acc: 0.9604682922363281\n",
      "epoch: 60 step: 117 loss: 0.08843208 acc: 0.9645195007324219\n",
      "epoch: 60 step: 118 loss: 0.10345534 acc: 0.9559173583984375\n",
      "epoch: 60 step: 119 loss: 0.10375632 acc: 0.9540786743164062\n",
      "epoch: 60 step: 120 loss: 0.07903685 acc: 0.9693794250488281\n",
      "epoch: 60 step: 121 loss: 0.10100743 acc: 0.96142578125\n",
      "epoch: 60 step: 122 loss: 0.093465105 acc: 0.9647712707519531\n",
      "epoch: 60 step: 123 loss: 0.097124614 acc: 0.9645881652832031\n",
      "epoch: 60 step: 124 loss: 0.097577706 acc: 0.9602137974330357\n",
      "epoch: 60 validation_loss: 0.11 validation_dice: 0.8234437124672129\n",
      "epoch: 60 test_dataset dice: 0.7399135864318134\n",
      "time cost 0.5362163027127583 min\n",
      "dice_best: 0.8429257274299863\n",
      "******************************** epoch  60  is finished. *********************************\n",
      "epoch: 61 step: 1 loss: 0.10037408 acc: 0.95452880859375\n",
      "epoch: 61 step: 2 loss: 0.10540254 acc: 0.9621849060058594\n",
      "epoch: 61 step: 3 loss: 0.08053209 acc: 0.9675979614257812\n",
      "epoch: 61 step: 4 loss: 0.08135169 acc: 0.9659233093261719\n",
      "epoch: 61 step: 5 loss: 0.09105285 acc: 0.954498291015625\n",
      "epoch: 61 step: 6 loss: 0.0803767 acc: 0.9570693969726562\n",
      "epoch: 61 step: 7 loss: 0.096558176 acc: 0.9576263427734375\n",
      "epoch: 61 step: 8 loss: 0.08489502 acc: 0.9718170166015625\n",
      "epoch: 61 step: 9 loss: 0.10717278 acc: 0.960296630859375\n",
      "epoch: 61 step: 10 loss: 0.110841885 acc: 0.9659652709960938\n",
      "epoch: 61 step: 11 loss: 0.09216019 acc: 0.9631538391113281\n",
      "epoch: 61 step: 12 loss: 0.095724344 acc: 0.95849609375\n",
      "epoch: 61 step: 13 loss: 0.092211924 acc: 0.9590072631835938\n",
      "epoch: 61 step: 14 loss: 0.10504087 acc: 0.9547004699707031\n",
      "epoch: 61 step: 15 loss: 0.11316506 acc: 0.9587898254394531\n",
      "epoch: 61 step: 16 loss: 0.06920023 acc: 0.9710502624511719\n",
      "epoch: 61 step: 17 loss: 0.090591095 acc: 0.9629135131835938\n",
      "epoch: 61 step: 18 loss: 0.09752178 acc: 0.9614982604980469\n",
      "epoch: 61 step: 19 loss: 0.084042504 acc: 0.9598617553710938\n",
      "epoch: 61 step: 20 loss: 0.08118592 acc: 0.9654808044433594\n",
      "epoch: 61 step: 21 loss: 0.12258678 acc: 0.9558982849121094\n",
      "epoch: 61 step: 22 loss: 0.09218526 acc: 0.9586410522460938\n",
      "epoch: 61 step: 23 loss: 0.080212146 acc: 0.9666557312011719\n",
      "epoch: 61 step: 24 loss: 0.09304096 acc: 0.95684814453125\n",
      "epoch: 61 step: 25 loss: 0.06931315 acc: 0.9639167785644531\n",
      "epoch: 61 step: 26 loss: 0.10358643 acc: 0.9553642272949219\n",
      "epoch: 61 step: 27 loss: 0.078042425 acc: 0.96649169921875\n",
      "epoch: 61 step: 28 loss: 0.079775974 acc: 0.9654998779296875\n",
      "epoch: 61 step: 29 loss: 0.10485305 acc: 0.9547119140625\n",
      "epoch: 61 step: 30 loss: 0.09174686 acc: 0.9561424255371094\n",
      "epoch: 61 step: 31 loss: 0.10523094 acc: 0.9583358764648438\n",
      "epoch: 61 step: 32 loss: 0.105721354 acc: 0.9590377807617188\n",
      "epoch: 61 step: 33 loss: 0.092047036 acc: 0.9604568481445312\n",
      "epoch: 61 step: 34 loss: 0.091282606 acc: 0.9665489196777344\n",
      "epoch: 61 step: 35 loss: 0.08561892 acc: 0.9636344909667969\n",
      "epoch: 61 step: 36 loss: 0.091259114 acc: 0.9614067077636719\n",
      "epoch: 61 step: 37 loss: 0.0802212 acc: 0.9601860046386719\n",
      "epoch: 61 step: 38 loss: 0.101149835 acc: 0.9561271667480469\n",
      "epoch: 61 step: 39 loss: 0.086354405 acc: 0.9645805358886719\n",
      "epoch: 61 step: 40 loss: 0.09031935 acc: 0.9643745422363281\n",
      "epoch: 61 step: 41 loss: 0.0955289 acc: 0.962493896484375\n",
      "epoch: 61 step: 42 loss: 0.11250393 acc: 0.9536323547363281\n",
      "epoch: 61 step: 43 loss: 0.0936009 acc: 0.9678726196289062\n",
      "epoch: 61 step: 44 loss: 0.09894261 acc: 0.9557266235351562\n",
      "epoch: 61 step: 45 loss: 0.092845924 acc: 0.9590339660644531\n",
      "epoch: 61 step: 46 loss: 0.083078355 acc: 0.96343994140625\n",
      "epoch: 61 step: 47 loss: 0.1033929 acc: 0.9589042663574219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 61 step: 48 loss: 0.088443 acc: 0.9620437622070312\n",
      "epoch: 61 step: 49 loss: 0.105845995 acc: 0.9587974548339844\n",
      "epoch: 61 step: 50 loss: 0.087769285 acc: 0.9626922607421875\n",
      "epoch: 61 step: 51 loss: 0.090323456 acc: 0.9608345031738281\n",
      "epoch: 61 step: 52 loss: 0.094468236 acc: 0.9544448852539062\n",
      "epoch: 61 step: 53 loss: 0.07492331 acc: 0.9689369201660156\n",
      "epoch: 61 step: 54 loss: 0.07635078 acc: 0.9699859619140625\n",
      "epoch: 61 step: 55 loss: 0.09106155 acc: 0.9623947143554688\n",
      "epoch: 61 step: 56 loss: 0.08613615 acc: 0.9665870666503906\n",
      "epoch: 61 step: 57 loss: 0.098216414 acc: 0.9555206298828125\n",
      "epoch: 61 step: 58 loss: 0.07802555 acc: 0.9678421020507812\n",
      "epoch: 61 step: 59 loss: 0.085236736 acc: 0.961181640625\n",
      "epoch: 61 step: 60 loss: 0.09322556 acc: 0.960968017578125\n",
      "epoch: 61 step: 61 loss: 0.07790567 acc: 0.9644241333007812\n",
      "epoch: 61 step: 62 loss: 0.07518261 acc: 0.9676284790039062\n",
      "epoch: 61 step: 63 loss: 0.09037282 acc: 0.9668998718261719\n",
      "epoch: 61 step: 64 loss: 0.08083609 acc: 0.9616928100585938\n",
      "epoch: 61 step: 65 loss: 0.08989273 acc: 0.9571762084960938\n",
      "epoch: 61 step: 66 loss: 0.082285404 acc: 0.9660568237304688\n",
      "epoch: 61 step: 67 loss: 0.08124469 acc: 0.9606475830078125\n",
      "epoch: 61 step: 68 loss: 0.07692421 acc: 0.9728584289550781\n",
      "epoch: 61 step: 69 loss: 0.07552508 acc: 0.9658966064453125\n",
      "epoch: 61 step: 70 loss: 0.07608463 acc: 0.9695243835449219\n",
      "epoch: 61 step: 71 loss: 0.106697224 acc: 0.9581413269042969\n",
      "epoch: 61 step: 72 loss: 0.0893064 acc: 0.9606781005859375\n",
      "epoch: 61 step: 73 loss: 0.079595834 acc: 0.9654808044433594\n",
      "epoch: 61 step: 74 loss: 0.10024731 acc: 0.9628486633300781\n",
      "epoch: 61 step: 75 loss: 0.087931745 acc: 0.9666099548339844\n",
      "epoch: 61 step: 76 loss: 0.07669816 acc: 0.9667167663574219\n",
      "epoch: 61 step: 77 loss: 0.1037097 acc: 0.9595832824707031\n",
      "epoch: 61 step: 78 loss: 0.09000144 acc: 0.9629669189453125\n",
      "epoch: 61 step: 79 loss: 0.09169611 acc: 0.9630699157714844\n",
      "epoch: 61 step: 80 loss: 0.088209 acc: 0.9626846313476562\n",
      "epoch: 61 step: 81 loss: 0.1041187 acc: 0.9548454284667969\n",
      "epoch: 61 step: 82 loss: 0.09592677 acc: 0.9523582458496094\n",
      "epoch: 61 step: 83 loss: 0.09699368 acc: 0.9597320556640625\n",
      "epoch: 61 step: 84 loss: 0.110132806 acc: 0.9528846740722656\n",
      "epoch: 61 step: 85 loss: 0.0800922 acc: 0.9683647155761719\n",
      "epoch: 61 step: 86 loss: 0.085189 acc: 0.9681396484375\n",
      "epoch: 61 step: 87 loss: 0.106183924 acc: 0.9641914367675781\n",
      "epoch: 61 step: 88 loss: 0.09831843 acc: 0.9569549560546875\n",
      "epoch: 61 step: 89 loss: 0.095706105 acc: 0.9642486572265625\n",
      "epoch: 61 step: 90 loss: 0.11118826 acc: 0.9637870788574219\n",
      "epoch: 61 step: 91 loss: 0.0882317 acc: 0.9609642028808594\n",
      "epoch: 61 step: 92 loss: 0.084673986 acc: 0.9595527648925781\n",
      "epoch: 61 step: 93 loss: 0.08819209 acc: 0.9556999206542969\n",
      "epoch: 61 step: 94 loss: 0.10749141 acc: 0.9463882446289062\n",
      "epoch: 61 step: 95 loss: 0.086532116 acc: 0.9583511352539062\n",
      "epoch: 61 step: 96 loss: 0.07847116 acc: 0.9591331481933594\n",
      "epoch: 61 step: 97 loss: 0.097881556 acc: 0.956146240234375\n",
      "epoch: 61 step: 98 loss: 0.104116485 acc: 0.9591789245605469\n",
      "epoch: 61 step: 99 loss: 0.096044764 acc: 0.9619903564453125\n",
      "epoch: 61 step: 100 loss: 0.07701196 acc: 0.9708290100097656\n",
      "epoch: 61 step: 101 loss: 0.09341834 acc: 0.9683265686035156\n",
      "epoch: 61 step: 102 loss: 0.08190902 acc: 0.9644813537597656\n",
      "epoch: 61 step: 103 loss: 0.09014224 acc: 0.9699249267578125\n",
      "epoch: 61 step: 104 loss: 0.09777281 acc: 0.9639320373535156\n",
      "epoch: 61 step: 105 loss: 0.0946268 acc: 0.9580726623535156\n",
      "epoch: 61 step: 106 loss: 0.11347069 acc: 0.9639854431152344\n",
      "epoch: 61 step: 107 loss: 0.09191451 acc: 0.96124267578125\n",
      "epoch: 61 step: 108 loss: 0.09787348 acc: 0.9632110595703125\n",
      "epoch: 61 step: 109 loss: 0.088141076 acc: 0.9568367004394531\n",
      "epoch: 61 step: 110 loss: 0.083523035 acc: 0.9663276672363281\n",
      "epoch: 61 step: 111 loss: 0.09271971 acc: 0.9575347900390625\n",
      "epoch: 61 step: 112 loss: 0.1075004 acc: 0.9530220031738281\n",
      "epoch: 61 step: 113 loss: 0.09288711 acc: 0.9630851745605469\n",
      "epoch: 61 step: 114 loss: 0.0847318 acc: 0.959228515625\n",
      "epoch: 61 step: 115 loss: 0.09663061 acc: 0.9573936462402344\n",
      "epoch: 61 step: 116 loss: 0.07724216 acc: 0.9656562805175781\n",
      "epoch: 61 step: 117 loss: 0.08146068 acc: 0.9670753479003906\n",
      "epoch: 61 step: 118 loss: 0.09608808 acc: 0.9602203369140625\n",
      "epoch: 61 step: 119 loss: 0.087950274 acc: 0.960418701171875\n",
      "epoch: 61 step: 120 loss: 0.085324176 acc: 0.9620895385742188\n",
      "epoch: 61 step: 121 loss: 0.09221998 acc: 0.958648681640625\n",
      "epoch: 61 step: 122 loss: 0.10513023 acc: 0.9620170593261719\n",
      "epoch: 61 step: 123 loss: 0.107650466 acc: 0.957427978515625\n",
      "epoch: 61 step: 124 loss: 0.13209392 acc: 0.9494454520089286\n",
      "epoch: 61 validation_loss: 0.096 validation_dice: 0.8481205045118958\n",
      "epoch: 61 test_dataset dice: 0.728078865478914\n",
      "time cost 0.5361556927363078 min\n",
      "dice_best: 0.8481205045118958\n",
      "******************************** epoch  61  is finished. *********************************\n",
      "epoch: 62 step: 1 loss: 0.08678101 acc: 0.9621543884277344\n",
      "epoch: 62 step: 2 loss: 0.11326035 acc: 0.9502944946289062\n",
      "epoch: 62 step: 3 loss: 0.083077274 acc: 0.966217041015625\n",
      "epoch: 62 step: 4 loss: 0.08154865 acc: 0.9652328491210938\n",
      "epoch: 62 step: 5 loss: 0.102027796 acc: 0.95306396484375\n",
      "epoch: 62 step: 6 loss: 0.08491555 acc: 0.960906982421875\n",
      "epoch: 62 step: 7 loss: 0.094594635 acc: 0.9607200622558594\n",
      "epoch: 62 step: 8 loss: 0.093393475 acc: 0.9592437744140625\n",
      "epoch: 62 step: 9 loss: 0.08067138 acc: 0.965118408203125\n",
      "epoch: 62 step: 10 loss: 0.08592575 acc: 0.9612655639648438\n",
      "epoch: 62 step: 11 loss: 0.10645688 acc: 0.9619674682617188\n",
      "epoch: 62 step: 12 loss: 0.09187777 acc: 0.9545669555664062\n",
      "epoch: 62 step: 13 loss: 0.07750442 acc: 0.962615966796875\n",
      "epoch: 62 step: 14 loss: 0.08426661 acc: 0.9587364196777344\n",
      "epoch: 62 step: 15 loss: 0.07872063 acc: 0.9653778076171875\n",
      "epoch: 62 step: 16 loss: 0.07949845 acc: 0.9656333923339844\n",
      "epoch: 62 step: 17 loss: 0.09283747 acc: 0.9567375183105469\n",
      "epoch: 62 step: 18 loss: 0.08393672 acc: 0.9593124389648438\n",
      "epoch: 62 step: 19 loss: 0.08266533 acc: 0.9714317321777344\n",
      "epoch: 62 step: 20 loss: 0.11476945 acc: 0.9610557556152344\n",
      "epoch: 62 step: 21 loss: 0.09748596 acc: 0.9618873596191406\n",
      "epoch: 62 step: 22 loss: 0.10011227 acc: 0.9623298645019531\n",
      "epoch: 62 step: 23 loss: 0.08875326 acc: 0.9599647521972656\n",
      "epoch: 62 step: 24 loss: 0.09797225 acc: 0.95867919921875\n",
      "epoch: 62 step: 25 loss: 0.086739674 acc: 0.972991943359375\n",
      "epoch: 62 step: 26 loss: 0.08190295 acc: 0.9646644592285156\n",
      "epoch: 62 step: 27 loss: 0.093258806 acc: 0.9622764587402344\n",
      "epoch: 62 step: 28 loss: 0.09787143 acc: 0.9561653137207031\n",
      "epoch: 62 step: 29 loss: 0.0890882 acc: 0.9623908996582031\n",
      "epoch: 62 step: 30 loss: 0.10782625 acc: 0.9564743041992188\n",
      "epoch: 62 step: 31 loss: 0.088410296 acc: 0.9670448303222656\n",
      "epoch: 62 step: 32 loss: 0.102953084 acc: 0.9619369506835938\n",
      "epoch: 62 step: 33 loss: 0.104958124 acc: 0.9548492431640625\n",
      "epoch: 62 step: 34 loss: 0.09577184 acc: 0.9597587585449219\n",
      "epoch: 62 step: 35 loss: 0.083404765 acc: 0.9589004516601562\n",
      "epoch: 62 step: 36 loss: 0.08497458 acc: 0.958282470703125\n",
      "epoch: 62 step: 37 loss: 0.09972484 acc: 0.9629096984863281\n",
      "epoch: 62 step: 38 loss: 0.08772282 acc: 0.9613151550292969\n",
      "epoch: 62 step: 39 loss: 0.08886087 acc: 0.9582099914550781\n",
      "epoch: 62 step: 40 loss: 0.08327067 acc: 0.9638519287109375\n",
      "epoch: 62 step: 41 loss: 0.09378666 acc: 0.9665107727050781\n",
      "epoch: 62 step: 42 loss: 0.07850859 acc: 0.9679145812988281\n",
      "epoch: 62 step: 43 loss: 0.07334353 acc: 0.9683265686035156\n",
      "epoch: 62 step: 44 loss: 0.08701848 acc: 0.9583663940429688\n",
      "epoch: 62 step: 45 loss: 0.08958138 acc: 0.9613265991210938\n",
      "epoch: 62 step: 46 loss: 0.09564756 acc: 0.9633903503417969\n",
      "epoch: 62 step: 47 loss: 0.10318113 acc: 0.9594993591308594\n",
      "epoch: 62 step: 48 loss: 0.081493005 acc: 0.9638442993164062\n",
      "epoch: 62 step: 49 loss: 0.10082774 acc: 0.9543914794921875\n",
      "epoch: 62 step: 50 loss: 0.076930486 acc: 0.9727096557617188\n",
      "epoch: 62 step: 51 loss: 0.091176145 acc: 0.9659080505371094\n",
      "epoch: 62 step: 52 loss: 0.08132295 acc: 0.9650230407714844\n",
      "epoch: 62 step: 53 loss: 0.07179677 acc: 0.9683036804199219\n",
      "epoch: 62 step: 54 loss: 0.08165765 acc: 0.9679985046386719\n",
      "epoch: 62 step: 55 loss: 0.09641047 acc: 0.9634284973144531\n",
      "epoch: 62 step: 56 loss: 0.07906264 acc: 0.9674568176269531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 62 step: 57 loss: 0.07572344 acc: 0.9630203247070312\n",
      "epoch: 62 step: 58 loss: 0.10462995 acc: 0.9548683166503906\n",
      "epoch: 62 step: 59 loss: 0.09591624 acc: 0.9533500671386719\n",
      "epoch: 62 step: 60 loss: 0.08013704 acc: 0.9597053527832031\n",
      "epoch: 62 step: 61 loss: 0.077046156 acc: 0.9639625549316406\n",
      "epoch: 62 step: 62 loss: 0.094758116 acc: 0.961395263671875\n",
      "epoch: 62 step: 63 loss: 0.11363257 acc: 0.9603347778320312\n",
      "epoch: 62 step: 64 loss: 0.088692784 acc: 0.9600563049316406\n",
      "epoch: 62 step: 65 loss: 0.06891295 acc: 0.9700279235839844\n",
      "epoch: 62 step: 66 loss: 0.08505633 acc: 0.9639396667480469\n",
      "epoch: 62 step: 67 loss: 0.08088197 acc: 0.9637794494628906\n",
      "epoch: 62 step: 68 loss: 0.08044544 acc: 0.9656333923339844\n",
      "epoch: 62 step: 69 loss: 0.10280889 acc: 0.9640579223632812\n",
      "epoch: 62 step: 70 loss: 0.087887526 acc: 0.9667625427246094\n",
      "epoch: 62 step: 71 loss: 0.1600447 acc: 0.9574661254882812\n",
      "epoch: 62 step: 72 loss: 0.09361442 acc: 0.9649696350097656\n",
      "epoch: 62 step: 73 loss: 0.1076248 acc: 0.9588203430175781\n",
      "epoch: 62 step: 74 loss: 0.0927303 acc: 0.9609565734863281\n",
      "epoch: 62 step: 75 loss: 0.12134134 acc: 0.9587287902832031\n",
      "epoch: 62 step: 76 loss: 0.10620548 acc: 0.9484710693359375\n",
      "epoch: 62 step: 77 loss: 0.10222898 acc: 0.9556388854980469\n",
      "epoch: 62 step: 78 loss: 0.09114126 acc: 0.9630699157714844\n",
      "epoch: 62 step: 79 loss: 0.0894011 acc: 0.9577713012695312\n",
      "epoch: 62 step: 80 loss: 0.11538684 acc: 0.9608001708984375\n",
      "epoch: 62 step: 81 loss: 0.0840589 acc: 0.9625778198242188\n",
      "epoch: 62 step: 82 loss: 0.09962655 acc: 0.9600906372070312\n",
      "epoch: 62 step: 83 loss: 0.08866756 acc: 0.9642753601074219\n",
      "epoch: 62 step: 84 loss: 0.11380692 acc: 0.9668922424316406\n",
      "epoch: 62 step: 85 loss: 0.09963431 acc: 0.9633331298828125\n",
      "epoch: 62 step: 86 loss: 0.08588745 acc: 0.9654579162597656\n",
      "epoch: 62 step: 87 loss: 0.10205793 acc: 0.9614601135253906\n",
      "epoch: 62 step: 88 loss: 0.11262221 acc: 0.9629020690917969\n",
      "epoch: 62 step: 89 loss: 0.09644219 acc: 0.9632911682128906\n",
      "epoch: 62 step: 90 loss: 0.08725384 acc: 0.9623069763183594\n",
      "epoch: 62 step: 91 loss: 0.09919974 acc: 0.968994140625\n",
      "epoch: 62 step: 92 loss: 0.101640366 acc: 0.9597969055175781\n",
      "epoch: 62 step: 93 loss: 0.086424105 acc: 0.9618911743164062\n",
      "epoch: 62 step: 94 loss: 0.08966354 acc: 0.9704360961914062\n",
      "epoch: 62 step: 95 loss: 0.08508701 acc: 0.9677925109863281\n",
      "epoch: 62 step: 96 loss: 0.079196274 acc: 0.964508056640625\n",
      "epoch: 62 step: 97 loss: 0.110190116 acc: 0.9605598449707031\n",
      "epoch: 62 step: 98 loss: 0.109348685 acc: 0.9580879211425781\n",
      "epoch: 62 step: 99 loss: 0.102724575 acc: 0.9588813781738281\n",
      "epoch: 62 step: 100 loss: 0.09509423 acc: 0.9594192504882812\n",
      "epoch: 62 step: 101 loss: 0.094826005 acc: 0.9656906127929688\n",
      "epoch: 62 step: 102 loss: 0.10404417 acc: 0.9620552062988281\n",
      "epoch: 62 step: 103 loss: 0.090966485 acc: 0.9627685546875\n",
      "epoch: 62 step: 104 loss: 0.097838655 acc: 0.9643211364746094\n",
      "epoch: 62 step: 105 loss: 0.08581772 acc: 0.9607124328613281\n",
      "epoch: 62 step: 106 loss: 0.08745791 acc: 0.9595146179199219\n",
      "epoch: 62 step: 107 loss: 0.09868461 acc: 0.95318603515625\n",
      "epoch: 62 step: 108 loss: 0.11036794 acc: 0.9552040100097656\n",
      "epoch: 62 step: 109 loss: 0.094484285 acc: 0.9583168029785156\n",
      "epoch: 62 step: 110 loss: 0.0834338 acc: 0.9609222412109375\n",
      "epoch: 62 step: 111 loss: 0.101077214 acc: 0.9674301147460938\n",
      "epoch: 62 step: 112 loss: 0.087491296 acc: 0.9648551940917969\n",
      "epoch: 62 step: 113 loss: 0.10736454 acc: 0.9630661010742188\n",
      "epoch: 62 step: 114 loss: 0.10700547 acc: 0.9590034484863281\n",
      "epoch: 62 step: 115 loss: 0.09807016 acc: 0.9583244323730469\n",
      "epoch: 62 step: 116 loss: 0.09977141 acc: 0.9571304321289062\n",
      "epoch: 62 step: 117 loss: 0.09360064 acc: 0.9558677673339844\n",
      "epoch: 62 step: 118 loss: 0.083654724 acc: 0.9611320495605469\n",
      "epoch: 62 step: 119 loss: 0.096601285 acc: 0.9567298889160156\n",
      "epoch: 62 step: 120 loss: 0.103094384 acc: 0.9561576843261719\n",
      "epoch: 62 step: 121 loss: 0.07819616 acc: 0.9700965881347656\n",
      "epoch: 62 step: 122 loss: 0.09090096 acc: 0.9660224914550781\n",
      "epoch: 62 step: 123 loss: 0.10112626 acc: 0.9689865112304688\n",
      "epoch: 62 step: 124 loss: 0.11479017 acc: 0.9621669224330357\n",
      "epoch: 62 validation_loss: 0.096 validation_dice: 0.8403095014203142\n",
      "epoch: 62 test_dataset dice: 0.7479323106929772\n",
      "time cost 0.5366422772407532 min\n",
      "dice_best: 0.8481205045118958\n",
      "******************************** epoch  62  is finished. *********************************\n",
      "epoch: 63 step: 1 loss: 0.099766634 acc: 0.9600372314453125\n",
      "epoch: 63 step: 2 loss: 0.09187247 acc: 0.9582786560058594\n",
      "epoch: 63 step: 3 loss: 0.10076955 acc: 0.9548606872558594\n",
      "epoch: 63 step: 4 loss: 0.08896832 acc: 0.9653778076171875\n",
      "epoch: 63 step: 5 loss: 0.09562562 acc: 0.9553031921386719\n",
      "epoch: 63 step: 6 loss: 0.1072968 acc: 0.9517898559570312\n",
      "epoch: 63 step: 7 loss: 0.08691704 acc: 0.9590568542480469\n",
      "epoch: 63 step: 8 loss: 0.096983045 acc: 0.9585952758789062\n",
      "epoch: 63 step: 9 loss: 0.098885916 acc: 0.9609451293945312\n",
      "epoch: 63 step: 10 loss: 0.09998011 acc: 0.9574317932128906\n",
      "epoch: 63 step: 11 loss: 0.09752371 acc: 0.9622573852539062\n",
      "epoch: 63 step: 12 loss: 0.08714957 acc: 0.9650001525878906\n",
      "epoch: 63 step: 13 loss: 0.091988154 acc: 0.9632377624511719\n",
      "epoch: 63 step: 14 loss: 0.0885696 acc: 0.9715042114257812\n",
      "epoch: 63 step: 15 loss: 0.10014642 acc: 0.9628677368164062\n",
      "epoch: 63 step: 16 loss: 0.09571213 acc: 0.9561080932617188\n",
      "epoch: 63 step: 17 loss: 0.08337528 acc: 0.964691162109375\n",
      "epoch: 63 step: 18 loss: 0.08644921 acc: 0.9623298645019531\n",
      "epoch: 63 step: 19 loss: 0.090367265 acc: 0.9581184387207031\n",
      "epoch: 63 step: 20 loss: 0.08842064 acc: 0.9715576171875\n",
      "epoch: 63 step: 21 loss: 0.09368556 acc: 0.9652786254882812\n",
      "epoch: 63 step: 22 loss: 0.09620161 acc: 0.9600143432617188\n",
      "epoch: 63 step: 23 loss: 0.107541725 acc: 0.94970703125\n",
      "epoch: 63 step: 24 loss: 0.088214554 acc: 0.9586830139160156\n",
      "epoch: 63 step: 25 loss: 0.091883905 acc: 0.9584541320800781\n",
      "epoch: 63 step: 26 loss: 0.10052885 acc: 0.9552383422851562\n",
      "epoch: 63 step: 27 loss: 0.10351246 acc: 0.9489860534667969\n",
      "epoch: 63 step: 28 loss: 0.10355164 acc: 0.9661903381347656\n",
      "epoch: 63 step: 29 loss: 0.08748963 acc: 0.9577484130859375\n",
      "epoch: 63 step: 30 loss: 0.11586339 acc: 0.9555397033691406\n",
      "epoch: 63 step: 31 loss: 0.08294417 acc: 0.9706764221191406\n",
      "epoch: 63 step: 32 loss: 0.095066756 acc: 0.9593963623046875\n",
      "epoch: 63 step: 33 loss: 0.11091405 acc: 0.956878662109375\n",
      "epoch: 63 step: 34 loss: 0.10862241 acc: 0.9590339660644531\n",
      "epoch: 63 step: 35 loss: 0.089440666 acc: 0.9630203247070312\n",
      "epoch: 63 step: 36 loss: 0.10209407 acc: 0.9608879089355469\n",
      "epoch: 63 step: 37 loss: 0.08841359 acc: 0.9646263122558594\n",
      "epoch: 63 step: 38 loss: 0.10250077 acc: 0.9576644897460938\n",
      "epoch: 63 step: 39 loss: 0.08562511 acc: 0.9521064758300781\n",
      "epoch: 63 step: 40 loss: 0.101484396 acc: 0.9592781066894531\n",
      "epoch: 63 step: 41 loss: 0.078162275 acc: 0.9679946899414062\n",
      "epoch: 63 step: 42 loss: 0.08424737 acc: 0.9655647277832031\n",
      "epoch: 63 step: 43 loss: 0.12901902 acc: 0.9507217407226562\n",
      "epoch: 63 step: 44 loss: 0.091060475 acc: 0.9620437622070312\n",
      "epoch: 63 step: 45 loss: 0.09724777 acc: 0.9620246887207031\n",
      "epoch: 63 step: 46 loss: 0.09409221 acc: 0.9649734497070312\n",
      "epoch: 63 step: 47 loss: 0.08707836 acc: 0.9607810974121094\n",
      "epoch: 63 step: 48 loss: 0.105986506 acc: 0.9547920227050781\n",
      "epoch: 63 step: 49 loss: 0.07346396 acc: 0.9684181213378906\n",
      "epoch: 63 step: 50 loss: 0.07730875 acc: 0.9706573486328125\n",
      "epoch: 63 step: 51 loss: 0.1033459 acc: 0.9563522338867188\n",
      "epoch: 63 step: 52 loss: 0.09779698 acc: 0.9543952941894531\n",
      "epoch: 63 step: 53 loss: 0.08671503 acc: 0.9579048156738281\n",
      "epoch: 63 step: 54 loss: 0.09807387 acc: 0.9581871032714844\n",
      "epoch: 63 step: 55 loss: 0.080982566 acc: 0.9629669189453125\n",
      "epoch: 63 step: 56 loss: 0.092076115 acc: 0.9611091613769531\n",
      "epoch: 63 step: 57 loss: 0.09616553 acc: 0.95977783203125\n",
      "epoch: 63 step: 58 loss: 0.077022426 acc: 0.9658012390136719\n",
      "epoch: 63 step: 59 loss: 0.09105725 acc: 0.9622879028320312\n",
      "epoch: 63 step: 60 loss: 0.10631388 acc: 0.9566802978515625\n",
      "epoch: 63 step: 61 loss: 0.095794775 acc: 0.9610862731933594\n",
      "epoch: 63 step: 62 loss: 0.086517006 acc: 0.9609375\n",
      "epoch: 63 step: 63 loss: 0.082776815 acc: 0.9557991027832031\n",
      "epoch: 63 step: 64 loss: 0.0919211 acc: 0.9570045471191406\n",
      "epoch: 63 step: 65 loss: 0.122639716 acc: 0.9461517333984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 63 step: 66 loss: 0.08579429 acc: 0.9578590393066406\n",
      "epoch: 63 step: 67 loss: 0.0931501 acc: 0.9586639404296875\n",
      "epoch: 63 step: 68 loss: 0.08470486 acc: 0.9627418518066406\n",
      "epoch: 63 step: 69 loss: 0.08579996 acc: 0.9657440185546875\n",
      "epoch: 63 step: 70 loss: 0.08104365 acc: 0.9692268371582031\n",
      "epoch: 63 step: 71 loss: 0.07918378 acc: 0.968658447265625\n",
      "epoch: 63 step: 72 loss: 0.09916217 acc: 0.9608993530273438\n",
      "epoch: 63 step: 73 loss: 0.10146876 acc: 0.956512451171875\n",
      "epoch: 63 step: 74 loss: 0.08311267 acc: 0.9640350341796875\n",
      "epoch: 63 step: 75 loss: 0.09470222 acc: 0.958465576171875\n",
      "epoch: 63 step: 76 loss: 0.08427262 acc: 0.9717140197753906\n",
      "epoch: 63 step: 77 loss: 0.103202485 acc: 0.9582710266113281\n",
      "epoch: 63 step: 78 loss: 0.10187068 acc: 0.9528770446777344\n",
      "epoch: 63 step: 79 loss: 0.077864125 acc: 0.96282958984375\n",
      "epoch: 63 step: 80 loss: 0.094530836 acc: 0.9522514343261719\n",
      "epoch: 63 step: 81 loss: 0.08693579 acc: 0.9569854736328125\n",
      "epoch: 63 step: 82 loss: 0.08502006 acc: 0.9549293518066406\n",
      "epoch: 63 step: 83 loss: 0.08647817 acc: 0.9588432312011719\n",
      "epoch: 63 step: 84 loss: 0.086591974 acc: 0.9615058898925781\n",
      "epoch: 63 step: 85 loss: 0.07218753 acc: 0.968505859375\n",
      "epoch: 63 step: 86 loss: 0.08948997 acc: 0.9691390991210938\n",
      "epoch: 63 step: 87 loss: 0.08982076 acc: 0.9592094421386719\n",
      "epoch: 63 step: 88 loss: 0.082848914 acc: 0.9635848999023438\n",
      "epoch: 63 step: 89 loss: 0.09042708 acc: 0.9611434936523438\n",
      "epoch: 63 step: 90 loss: 0.08394741 acc: 0.9694976806640625\n",
      "epoch: 63 step: 91 loss: 0.08277493 acc: 0.9617538452148438\n",
      "epoch: 63 step: 92 loss: 0.09599656 acc: 0.9641532897949219\n",
      "epoch: 63 step: 93 loss: 0.07701483 acc: 0.9623374938964844\n",
      "epoch: 63 step: 94 loss: 0.08357972 acc: 0.9676017761230469\n",
      "epoch: 63 step: 95 loss: 0.076364666 acc: 0.9637069702148438\n",
      "epoch: 63 step: 96 loss: 0.0888285 acc: 0.9643783569335938\n",
      "epoch: 63 step: 97 loss: 0.089128844 acc: 0.9499893188476562\n",
      "epoch: 63 step: 98 loss: 0.08074611 acc: 0.9641952514648438\n",
      "epoch: 63 step: 99 loss: 0.07712253 acc: 0.96221923828125\n",
      "epoch: 63 step: 100 loss: 0.08900251 acc: 0.9566688537597656\n",
      "epoch: 63 step: 101 loss: 0.08195332 acc: 0.9613151550292969\n",
      "epoch: 63 step: 102 loss: 0.10476472 acc: 0.9586868286132812\n",
      "epoch: 63 step: 103 loss: 0.07258672 acc: 0.9720382690429688\n",
      "epoch: 63 step: 104 loss: 0.098801926 acc: 0.9629058837890625\n",
      "epoch: 63 step: 105 loss: 0.08699985 acc: 0.9614334106445312\n",
      "epoch: 63 step: 106 loss: 0.10101911 acc: 0.9626426696777344\n",
      "epoch: 63 step: 107 loss: 0.075472325 acc: 0.9685173034667969\n",
      "epoch: 63 step: 108 loss: 0.10179729 acc: 0.970428466796875\n",
      "epoch: 63 step: 109 loss: 0.08336403 acc: 0.9655532836914062\n",
      "epoch: 63 step: 110 loss: 0.097589456 acc: 0.9603614807128906\n",
      "epoch: 63 step: 111 loss: 0.1203242 acc: 0.9609222412109375\n",
      "epoch: 63 step: 112 loss: 0.095814675 acc: 0.9601631164550781\n",
      "epoch: 63 step: 113 loss: 0.08527199 acc: 0.9643630981445312\n",
      "epoch: 63 step: 114 loss: 0.08178161 acc: 0.9591903686523438\n",
      "epoch: 63 step: 115 loss: 0.09077347 acc: 0.9601402282714844\n",
      "epoch: 63 step: 116 loss: 0.09533236 acc: 0.9580841064453125\n",
      "epoch: 63 step: 117 loss: 0.09455233 acc: 0.9596099853515625\n",
      "epoch: 63 step: 118 loss: 0.08694656 acc: 0.9622840881347656\n",
      "epoch: 63 step: 119 loss: 0.104258314 acc: 0.9572410583496094\n",
      "epoch: 63 step: 120 loss: 0.090055555 acc: 0.9603424072265625\n",
      "epoch: 63 step: 121 loss: 0.09934037 acc: 0.9537620544433594\n",
      "epoch: 63 step: 122 loss: 0.079115376 acc: 0.9589385986328125\n",
      "epoch: 63 step: 123 loss: 0.09199573 acc: 0.958282470703125\n",
      "epoch: 63 step: 124 loss: 0.09319009 acc: 0.9660731724330357\n",
      "epoch: 63 validation_loss: 0.106 validation_dice: 0.8419700087381998\n",
      "epoch: 63 test_dataset dice: 0.7498416612050224\n",
      "time cost 0.5365903973579407 min\n",
      "dice_best: 0.8481205045118958\n",
      "******************************** epoch  63  is finished. *********************************\n",
      "epoch: 64 step: 1 loss: 0.098398186 acc: 0.9569244384765625\n",
      "epoch: 64 step: 2 loss: 0.10315948 acc: 0.9616928100585938\n",
      "epoch: 64 step: 3 loss: 0.08800994 acc: 0.9612312316894531\n",
      "epoch: 64 step: 4 loss: 0.08962322 acc: 0.969635009765625\n",
      "epoch: 64 step: 5 loss: 0.08287276 acc: 0.9660148620605469\n",
      "epoch: 64 step: 6 loss: 0.10766399 acc: 0.9614295959472656\n",
      "epoch: 64 step: 7 loss: 0.091172636 acc: 0.9695472717285156\n",
      "epoch: 64 step: 8 loss: 0.0963292 acc: 0.9618759155273438\n",
      "epoch: 64 step: 9 loss: 0.08907422 acc: 0.9616203308105469\n",
      "epoch: 64 step: 10 loss: 0.08372737 acc: 0.9640426635742188\n",
      "epoch: 64 step: 11 loss: 0.10625985 acc: 0.960968017578125\n",
      "epoch: 64 step: 12 loss: 0.0878604 acc: 0.9591827392578125\n",
      "epoch: 64 step: 13 loss: 0.092161335 acc: 0.9556465148925781\n",
      "epoch: 64 step: 14 loss: 0.08288493 acc: 0.961273193359375\n",
      "epoch: 64 step: 15 loss: 0.08479307 acc: 0.9584197998046875\n",
      "epoch: 64 step: 16 loss: 0.08948767 acc: 0.961151123046875\n",
      "epoch: 64 step: 17 loss: 0.09440514 acc: 0.9569129943847656\n",
      "epoch: 64 step: 18 loss: 0.08485295 acc: 0.9599761962890625\n",
      "epoch: 64 step: 19 loss: 0.11354536 acc: 0.9597129821777344\n",
      "epoch: 64 step: 20 loss: 0.077889495 acc: 0.9696769714355469\n",
      "epoch: 64 step: 21 loss: 0.116437554 acc: 0.9605484008789062\n",
      "epoch: 64 step: 22 loss: 0.09095792 acc: 0.9593391418457031\n",
      "epoch: 64 step: 23 loss: 0.1001794 acc: 0.9540519714355469\n",
      "epoch: 64 step: 24 loss: 0.07782603 acc: 0.9671516418457031\n",
      "epoch: 64 step: 25 loss: 0.080528356 acc: 0.9650535583496094\n",
      "epoch: 64 step: 26 loss: 0.085238755 acc: 0.9545326232910156\n",
      "epoch: 64 step: 27 loss: 0.089798756 acc: 0.9524002075195312\n",
      "epoch: 64 step: 28 loss: 0.08912427 acc: 0.9570121765136719\n",
      "epoch: 64 step: 29 loss: 0.078592524 acc: 0.9642219543457031\n",
      "epoch: 64 step: 30 loss: 0.0834564 acc: 0.9663619995117188\n",
      "epoch: 64 step: 31 loss: 0.08754502 acc: 0.9629669189453125\n",
      "epoch: 64 step: 32 loss: 0.08599608 acc: 0.9658355712890625\n",
      "epoch: 64 step: 33 loss: 0.07388348 acc: 0.9692039489746094\n",
      "epoch: 64 step: 34 loss: 0.079405636 acc: 0.9692001342773438\n",
      "epoch: 64 step: 35 loss: 0.087452635 acc: 0.9658546447753906\n",
      "epoch: 64 step: 36 loss: 0.091658786 acc: 0.9618759155273438\n",
      "epoch: 64 step: 37 loss: 0.09477999 acc: 0.9673576354980469\n",
      "epoch: 64 step: 38 loss: 0.09120556 acc: 0.9610939025878906\n",
      "epoch: 64 step: 39 loss: 0.0911595 acc: 0.9569053649902344\n",
      "epoch: 64 step: 40 loss: 0.09793099 acc: 0.95587158203125\n",
      "epoch: 64 step: 41 loss: 0.10314348 acc: 0.952606201171875\n",
      "epoch: 64 step: 42 loss: 0.09626929 acc: 0.9569282531738281\n",
      "epoch: 64 step: 43 loss: 0.08768651 acc: 0.9623641967773438\n",
      "epoch: 64 step: 44 loss: 0.09491869 acc: 0.9524955749511719\n",
      "epoch: 64 step: 45 loss: 0.09266716 acc: 0.9604415893554688\n",
      "epoch: 64 step: 46 loss: 0.09644925 acc: 0.9581794738769531\n",
      "epoch: 64 step: 47 loss: 0.08222111 acc: 0.9638175964355469\n",
      "epoch: 64 step: 48 loss: 0.07039617 acc: 0.9693527221679688\n",
      "epoch: 64 step: 49 loss: 0.1151554 acc: 0.9573783874511719\n",
      "epoch: 64 step: 50 loss: 0.103286326 acc: 0.9614753723144531\n",
      "epoch: 64 step: 51 loss: 0.09421908 acc: 0.969635009765625\n",
      "epoch: 64 step: 52 loss: 0.111633494 acc: 0.9574317932128906\n",
      "epoch: 64 step: 53 loss: 0.08314702 acc: 0.9656410217285156\n",
      "epoch: 64 step: 54 loss: 0.07738102 acc: 0.9633407592773438\n",
      "epoch: 64 step: 55 loss: 0.080744274 acc: 0.9614295959472656\n",
      "epoch: 64 step: 56 loss: 0.08956394 acc: 0.9559135437011719\n",
      "epoch: 64 step: 57 loss: 0.06611475 acc: 0.9701690673828125\n",
      "epoch: 64 step: 58 loss: 0.082089975 acc: 0.9639205932617188\n",
      "epoch: 64 step: 59 loss: 0.08996877 acc: 0.9508285522460938\n",
      "epoch: 64 step: 60 loss: 0.094746046 acc: 0.9524726867675781\n",
      "epoch: 64 step: 61 loss: 0.07347703 acc: 0.969818115234375\n",
      "epoch: 64 step: 62 loss: 0.119779035 acc: 0.9545745849609375\n",
      "epoch: 64 step: 63 loss: 0.077144526 acc: 0.9712448120117188\n",
      "epoch: 64 step: 64 loss: 0.11863392 acc: 0.9623527526855469\n",
      "epoch: 64 step: 65 loss: 0.088281035 acc: 0.9553337097167969\n",
      "epoch: 64 step: 66 loss: 0.07092714 acc: 0.9698638916015625\n",
      "epoch: 64 step: 67 loss: 0.091302484 acc: 0.95819091796875\n",
      "epoch: 64 step: 68 loss: 0.085745856 acc: 0.9636383056640625\n",
      "epoch: 64 step: 69 loss: 0.079354905 acc: 0.9635543823242188\n",
      "epoch: 64 step: 70 loss: 0.0838602 acc: 0.9641036987304688\n",
      "epoch: 64 step: 71 loss: 0.08002052 acc: 0.9647941589355469\n",
      "epoch: 64 step: 72 loss: 0.093110405 acc: 0.9586105346679688\n",
      "epoch: 64 step: 73 loss: 0.09289898 acc: 0.9620361328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 64 step: 74 loss: 0.08814242 acc: 0.9610748291015625\n",
      "epoch: 64 step: 75 loss: 0.10752706 acc: 0.9684677124023438\n",
      "epoch: 64 step: 76 loss: 0.082114995 acc: 0.958343505859375\n",
      "epoch: 64 step: 77 loss: 0.084456734 acc: 0.9703788757324219\n",
      "epoch: 64 step: 78 loss: 0.12005442 acc: 0.9577140808105469\n",
      "epoch: 64 step: 79 loss: 0.09273245 acc: 0.9692268371582031\n",
      "epoch: 64 step: 80 loss: 0.10527208 acc: 0.9544258117675781\n",
      "epoch: 64 step: 81 loss: 0.13653685 acc: 0.9646797180175781\n",
      "epoch: 64 step: 82 loss: 0.10869611 acc: 0.9638023376464844\n",
      "epoch: 64 step: 83 loss: 0.09532717 acc: 0.96453857421875\n",
      "epoch: 64 step: 84 loss: 0.091461524 acc: 0.9558258056640625\n",
      "epoch: 64 step: 85 loss: 0.116779976 acc: 0.9466629028320312\n",
      "epoch: 64 step: 86 loss: 0.10017176 acc: 0.9519309997558594\n",
      "epoch: 64 step: 87 loss: 0.09965203 acc: 0.9556007385253906\n",
      "epoch: 64 step: 88 loss: 0.09768838 acc: 0.9607086181640625\n",
      "epoch: 64 step: 89 loss: 0.11379797 acc: 0.952789306640625\n",
      "epoch: 64 step: 90 loss: 0.08538541 acc: 0.9652671813964844\n",
      "epoch: 64 step: 91 loss: 0.07905454 acc: 0.9706497192382812\n",
      "epoch: 64 step: 92 loss: 0.08216594 acc: 0.9703598022460938\n",
      "epoch: 64 step: 93 loss: 0.114772916 acc: 0.9635047912597656\n",
      "epoch: 64 step: 94 loss: 0.09332366 acc: 0.9703140258789062\n",
      "epoch: 64 step: 95 loss: 0.0941014 acc: 0.9668960571289062\n",
      "epoch: 64 step: 96 loss: 0.106299676 acc: 0.9667434692382812\n",
      "epoch: 64 step: 97 loss: 0.09434368 acc: 0.9643898010253906\n",
      "epoch: 64 step: 98 loss: 0.12643413 acc: 0.9620933532714844\n",
      "epoch: 64 step: 99 loss: 0.09932087 acc: 0.9553070068359375\n",
      "epoch: 64 step: 100 loss: 0.1302064 acc: 0.9549140930175781\n",
      "epoch: 64 step: 101 loss: 0.11048467 acc: 0.9534034729003906\n",
      "epoch: 64 step: 102 loss: 0.09499208 acc: 0.9576797485351562\n",
      "epoch: 64 step: 103 loss: 0.09698451 acc: 0.9557113647460938\n",
      "epoch: 64 step: 104 loss: 0.09770128 acc: 0.9615554809570312\n",
      "epoch: 64 step: 105 loss: 0.11062076 acc: 0.9654731750488281\n",
      "epoch: 64 step: 106 loss: 0.09770686 acc: 0.9575424194335938\n",
      "epoch: 64 step: 107 loss: 0.11859129 acc: 0.9650688171386719\n",
      "epoch: 64 step: 108 loss: 0.10404631 acc: 0.9648323059082031\n",
      "epoch: 64 step: 109 loss: 0.08506436 acc: 0.9647178649902344\n",
      "epoch: 64 step: 110 loss: 0.08004527 acc: 0.9638404846191406\n",
      "epoch: 64 step: 111 loss: 0.10157071 acc: 0.9672813415527344\n",
      "epoch: 64 step: 112 loss: 0.08500344 acc: 0.9648933410644531\n",
      "epoch: 64 step: 113 loss: 0.10014052 acc: 0.9602241516113281\n",
      "epoch: 64 step: 114 loss: 0.091650635 acc: 0.963592529296875\n",
      "epoch: 64 step: 115 loss: 0.09744534 acc: 0.9594039916992188\n",
      "epoch: 64 step: 116 loss: 0.10053812 acc: 0.9555778503417969\n",
      "epoch: 64 step: 117 loss: 0.106716596 acc: 0.9524993896484375\n",
      "epoch: 64 step: 118 loss: 0.08903569 acc: 0.9603500366210938\n",
      "epoch: 64 step: 119 loss: 0.115671635 acc: 0.9511222839355469\n",
      "epoch: 64 step: 120 loss: 0.11456429 acc: 0.9501762390136719\n",
      "epoch: 64 step: 121 loss: 0.11988253 acc: 0.9557609558105469\n",
      "epoch: 64 step: 122 loss: 0.10024147 acc: 0.9609451293945312\n",
      "epoch: 64 step: 123 loss: 0.08759097 acc: 0.9612922668457031\n",
      "epoch: 64 step: 124 loss: 0.11345968 acc: 0.9622017996651786\n",
      "epoch: 64 validation_loss: 0.1 validation_dice: 0.8278284932552684\n",
      "epoch: 64 test_dataset dice: 0.7360003158126537\n",
      "time cost 0.5363105654716491 min\n",
      "dice_best: 0.8481205045118958\n",
      "******************************** epoch  64  is finished. *********************************\n",
      "epoch: 65 step: 1 loss: 0.09239924 acc: 0.9534225463867188\n",
      "epoch: 65 step: 2 loss: 0.08414327 acc: 0.9563636779785156\n",
      "epoch: 65 step: 3 loss: 0.0945674 acc: 0.9662857055664062\n",
      "epoch: 65 step: 4 loss: 0.08954101 acc: 0.9631385803222656\n",
      "epoch: 65 step: 5 loss: 0.09231367 acc: 0.9617805480957031\n",
      "epoch: 65 step: 6 loss: 0.08487752 acc: 0.96441650390625\n",
      "epoch: 65 step: 7 loss: 0.09320428 acc: 0.9615554809570312\n",
      "epoch: 65 step: 8 loss: 0.09354823 acc: 0.9609375\n",
      "epoch: 65 step: 9 loss: 0.09374663 acc: 0.9599227905273438\n",
      "epoch: 65 step: 10 loss: 0.108114235 acc: 0.9602813720703125\n",
      "epoch: 65 step: 11 loss: 0.08751774 acc: 0.9643669128417969\n",
      "epoch: 65 step: 12 loss: 0.09004817 acc: 0.9627189636230469\n",
      "epoch: 65 step: 13 loss: 0.08919701 acc: 0.9649620056152344\n",
      "epoch: 65 step: 14 loss: 0.08563877 acc: 0.9587669372558594\n",
      "epoch: 65 step: 15 loss: 0.10359416 acc: 0.9589805603027344\n",
      "epoch: 65 step: 16 loss: 0.114746876 acc: 0.9569129943847656\n",
      "epoch: 65 step: 17 loss: 0.075099 acc: 0.9704399108886719\n",
      "epoch: 65 step: 18 loss: 0.08473936 acc: 0.9681739807128906\n",
      "epoch: 65 step: 19 loss: 0.081141315 acc: 0.9658470153808594\n",
      "epoch: 65 step: 20 loss: 0.09856446 acc: 0.9602775573730469\n",
      "epoch: 65 step: 21 loss: 0.08754316 acc: 0.9676551818847656\n",
      "epoch: 65 step: 22 loss: 0.09853498 acc: 0.9610328674316406\n",
      "epoch: 65 step: 23 loss: 0.08537722 acc: 0.96832275390625\n",
      "epoch: 65 step: 24 loss: 0.082639635 acc: 0.9727783203125\n",
      "epoch: 65 step: 25 loss: 0.0983969 acc: 0.96337890625\n",
      "epoch: 65 step: 26 loss: 0.09326567 acc: 0.9645614624023438\n",
      "epoch: 65 step: 27 loss: 0.083286084 acc: 0.9703712463378906\n",
      "epoch: 65 step: 28 loss: 0.110690914 acc: 0.9550704956054688\n",
      "epoch: 65 step: 29 loss: 0.09000044 acc: 0.9650154113769531\n",
      "epoch: 65 step: 30 loss: 0.10332663 acc: 0.9582138061523438\n",
      "epoch: 65 step: 31 loss: 0.102103546 acc: 0.9638404846191406\n",
      "epoch: 65 step: 32 loss: 0.09264681 acc: 0.9648551940917969\n",
      "epoch: 65 step: 33 loss: 0.12493816 acc: 0.957855224609375\n",
      "epoch: 65 step: 34 loss: 0.09831872 acc: 0.9587821960449219\n",
      "epoch: 65 step: 35 loss: 0.108793125 acc: 0.9600296020507812\n",
      "epoch: 65 step: 36 loss: 0.09167008 acc: 0.9623298645019531\n",
      "epoch: 65 step: 37 loss: 0.10460904 acc: 0.9563560485839844\n",
      "epoch: 65 step: 38 loss: 0.09481281 acc: 0.9569969177246094\n",
      "epoch: 65 step: 39 loss: 0.09460359 acc: 0.9578132629394531\n",
      "epoch: 65 step: 40 loss: 0.10108474 acc: 0.9576950073242188\n",
      "epoch: 65 step: 41 loss: 0.087404646 acc: 0.9620590209960938\n",
      "epoch: 65 step: 42 loss: 0.10048928 acc: 0.9667243957519531\n",
      "epoch: 65 step: 43 loss: 0.10109006 acc: 0.9556770324707031\n",
      "epoch: 65 step: 44 loss: 0.11228872 acc: 0.9600334167480469\n",
      "epoch: 65 step: 45 loss: 0.10574645 acc: 0.9609222412109375\n",
      "epoch: 65 step: 46 loss: 0.08336896 acc: 0.9641609191894531\n",
      "epoch: 65 step: 47 loss: 0.103801295 acc: 0.9573822021484375\n",
      "epoch: 65 step: 48 loss: 0.10736902 acc: 0.953216552734375\n",
      "epoch: 65 step: 49 loss: 0.08435462 acc: 0.9625587463378906\n",
      "epoch: 65 step: 50 loss: 0.106012285 acc: 0.9529266357421875\n",
      "epoch: 65 step: 51 loss: 0.10522037 acc: 0.9562950134277344\n",
      "epoch: 65 step: 52 loss: 0.10125393 acc: 0.9594879150390625\n",
      "epoch: 65 step: 53 loss: 0.08691645 acc: 0.9554672241210938\n",
      "epoch: 65 step: 54 loss: 0.112103164 acc: 0.9581680297851562\n",
      "epoch: 65 step: 55 loss: 0.105386585 acc: 0.9503593444824219\n",
      "epoch: 65 step: 56 loss: 0.10416201 acc: 0.9595756530761719\n",
      "epoch: 65 step: 57 loss: 0.098113686 acc: 0.9561347961425781\n",
      "epoch: 65 step: 58 loss: 0.09142792 acc: 0.9573478698730469\n",
      "epoch: 65 step: 59 loss: 0.08360423 acc: 0.9646377563476562\n",
      "epoch: 65 step: 60 loss: 0.083346404 acc: 0.9674148559570312\n",
      "epoch: 65 step: 61 loss: 0.10289938 acc: 0.9586257934570312\n",
      "epoch: 65 step: 62 loss: 0.089021765 acc: 0.9617080688476562\n",
      "epoch: 65 step: 63 loss: 0.08296179 acc: 0.9652938842773438\n",
      "epoch: 65 step: 64 loss: 0.08835583 acc: 0.9644088745117188\n",
      "epoch: 65 step: 65 loss: 0.08878736 acc: 0.9604644775390625\n",
      "epoch: 65 step: 66 loss: 0.08789301 acc: 0.9544448852539062\n",
      "epoch: 65 step: 67 loss: 0.09006368 acc: 0.956024169921875\n",
      "epoch: 65 step: 68 loss: 0.0866841 acc: 0.9587173461914062\n",
      "epoch: 65 step: 69 loss: 0.109634385 acc: 0.9487571716308594\n",
      "epoch: 65 step: 70 loss: 0.07608382 acc: 0.9636802673339844\n",
      "epoch: 65 step: 71 loss: 0.08699151 acc: 0.9589881896972656\n",
      "epoch: 65 step: 72 loss: 0.10389152 acc: 0.9582061767578125\n",
      "epoch: 65 step: 73 loss: 0.0940747 acc: 0.9544410705566406\n",
      "epoch: 65 step: 74 loss: 0.0993219 acc: 0.9649772644042969\n",
      "epoch: 65 step: 75 loss: 0.093599916 acc: 0.95892333984375\n",
      "epoch: 65 step: 76 loss: 0.07504235 acc: 0.9686660766601562\n",
      "epoch: 65 step: 77 loss: 0.07905796 acc: 0.9586372375488281\n",
      "epoch: 65 step: 78 loss: 0.08398874 acc: 0.96368408203125\n",
      "epoch: 65 step: 79 loss: 0.09287296 acc: 0.9591140747070312\n",
      "epoch: 65 step: 80 loss: 0.08807318 acc: 0.9609642028808594\n",
      "epoch: 65 step: 81 loss: 0.0838207 acc: 0.9552078247070312\n",
      "epoch: 65 step: 82 loss: 0.10015133 acc: 0.9599571228027344\n",
      "epoch: 65 step: 83 loss: 0.10003527 acc: 0.9529762268066406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 65 step: 84 loss: 0.075784355 acc: 0.9652633666992188\n",
      "epoch: 65 step: 85 loss: 0.07797911 acc: 0.9709587097167969\n",
      "epoch: 65 step: 86 loss: 0.08276943 acc: 0.9663009643554688\n",
      "epoch: 65 step: 87 loss: 0.080556065 acc: 0.9664268493652344\n",
      "epoch: 65 step: 88 loss: 0.079206176 acc: 0.9627151489257812\n",
      "epoch: 65 step: 89 loss: 0.08638136 acc: 0.9667243957519531\n",
      "epoch: 65 step: 90 loss: 0.10492021 acc: 0.9578895568847656\n",
      "epoch: 65 step: 91 loss: 0.10499756 acc: 0.9590835571289062\n",
      "epoch: 65 step: 92 loss: 0.09656042 acc: 0.9651756286621094\n",
      "epoch: 65 step: 93 loss: 0.0844854 acc: 0.9640083312988281\n",
      "epoch: 65 step: 94 loss: 0.10149589 acc: 0.9493331909179688\n",
      "epoch: 65 step: 95 loss: 0.08558427 acc: 0.9608726501464844\n",
      "epoch: 65 step: 96 loss: 0.12625425 acc: 0.9412422180175781\n",
      "epoch: 65 step: 97 loss: 0.10473089 acc: 0.9505805969238281\n",
      "epoch: 65 step: 98 loss: 0.11434836 acc: 0.9485702514648438\n",
      "epoch: 65 step: 99 loss: 0.101663284 acc: 0.955352783203125\n",
      "epoch: 65 step: 100 loss: 0.100653425 acc: 0.9566726684570312\n",
      "epoch: 65 step: 101 loss: 0.10824842 acc: 0.96881103515625\n",
      "epoch: 65 step: 102 loss: 0.12120496 acc: 0.9628677368164062\n",
      "epoch: 65 step: 103 loss: 0.10010706 acc: 0.9652748107910156\n",
      "epoch: 65 step: 104 loss: 0.094159566 acc: 0.9624099731445312\n",
      "epoch: 65 step: 105 loss: 0.13649671 acc: 0.95184326171875\n",
      "epoch: 65 step: 106 loss: 0.099391825 acc: 0.9567298889160156\n",
      "epoch: 65 step: 107 loss: 0.13517964 acc: 0.9518928527832031\n",
      "epoch: 65 step: 108 loss: 0.08512925 acc: 0.9672966003417969\n",
      "epoch: 65 step: 109 loss: 0.098470144 acc: 0.9575767517089844\n",
      "epoch: 65 step: 110 loss: 0.1328607 acc: 0.9515419006347656\n",
      "epoch: 65 step: 111 loss: 0.11687936 acc: 0.9543685913085938\n",
      "epoch: 65 step: 112 loss: 0.10814807 acc: 0.9527549743652344\n",
      "epoch: 65 step: 113 loss: 0.11556021 acc: 0.9617271423339844\n",
      "epoch: 65 step: 114 loss: 0.099866465 acc: 0.9554634094238281\n",
      "epoch: 65 step: 115 loss: 0.096487194 acc: 0.9594345092773438\n",
      "epoch: 65 step: 116 loss: 0.12415585 acc: 0.9560699462890625\n",
      "epoch: 65 step: 117 loss: 0.13279437 acc: 0.9656715393066406\n",
      "epoch: 65 step: 118 loss: 0.12956733 acc: 0.9644737243652344\n",
      "epoch: 65 step: 119 loss: 0.101043716 acc: 0.9573020935058594\n",
      "epoch: 65 step: 120 loss: 0.12550698 acc: 0.9513130187988281\n",
      "epoch: 65 step: 121 loss: 0.118768856 acc: 0.96295166015625\n",
      "epoch: 65 step: 122 loss: 0.104724854 acc: 0.9533462524414062\n",
      "epoch: 65 step: 123 loss: 0.11065574 acc: 0.95794677734375\n",
      "epoch: 65 step: 124 loss: 0.088935494 acc: 0.9600917271205357\n",
      "epoch: 65 validation_loss: 0.119 validation_dice: 0.7945380026303446\n",
      "epoch: 65 test_dataset dice: 0.7389101544047597\n",
      "time cost 0.5349594076474508 min\n",
      "dice_best: 0.8481205045118958\n",
      "******************************** epoch  65  is finished. *********************************\n",
      "epoch: 66 step: 1 loss: 0.10857978 acc: 0.9543342590332031\n",
      "epoch: 66 step: 2 loss: 0.10390801 acc: 0.9597206115722656\n",
      "epoch: 66 step: 3 loss: 0.1162152 acc: 0.9521102905273438\n",
      "epoch: 66 step: 4 loss: 0.10362014 acc: 0.9528923034667969\n",
      "epoch: 66 step: 5 loss: 0.11517917 acc: 0.9582862854003906\n",
      "epoch: 66 step: 6 loss: 0.09450443 acc: 0.9693832397460938\n",
      "epoch: 66 step: 7 loss: 0.096595965 acc: 0.9536514282226562\n",
      "epoch: 66 step: 8 loss: 0.10578107 acc: 0.95355224609375\n",
      "epoch: 66 step: 9 loss: 0.09123756 acc: 0.9557456970214844\n",
      "epoch: 66 step: 10 loss: 0.110243805 acc: 0.9571495056152344\n",
      "epoch: 66 step: 11 loss: 0.10170188 acc: 0.9599037170410156\n",
      "epoch: 66 step: 12 loss: 0.10165485 acc: 0.9662628173828125\n",
      "epoch: 66 step: 13 loss: 0.11660167 acc: 0.9538154602050781\n",
      "epoch: 66 step: 14 loss: 0.101126626 acc: 0.9597854614257812\n",
      "epoch: 66 step: 15 loss: 0.11083051 acc: 0.9590568542480469\n",
      "epoch: 66 step: 16 loss: 0.09294021 acc: 0.9595298767089844\n",
      "epoch: 66 step: 17 loss: 0.099640585 acc: 0.9582061767578125\n",
      "epoch: 66 step: 18 loss: 0.093352124 acc: 0.9599380493164062\n",
      "epoch: 66 step: 19 loss: 0.08681945 acc: 0.9588851928710938\n",
      "epoch: 66 step: 20 loss: 0.09207651 acc: 0.9633636474609375\n",
      "epoch: 66 step: 21 loss: 0.09167915 acc: 0.9631118774414062\n",
      "epoch: 66 step: 22 loss: 0.09161915 acc: 0.9637374877929688\n",
      "epoch: 66 step: 23 loss: 0.10141022 acc: 0.9635963439941406\n",
      "epoch: 66 step: 24 loss: 0.08986023 acc: 0.9643173217773438\n",
      "epoch: 66 step: 25 loss: 0.087212436 acc: 0.9636611938476562\n",
      "epoch: 66 step: 26 loss: 0.096532136 acc: 0.9567794799804688\n",
      "epoch: 66 step: 27 loss: 0.08378986 acc: 0.9589004516601562\n",
      "epoch: 66 step: 28 loss: 0.11478044 acc: 0.9511833190917969\n",
      "epoch: 66 step: 29 loss: 0.08802486 acc: 0.9586639404296875\n",
      "epoch: 66 step: 30 loss: 0.08497951 acc: 0.9598274230957031\n",
      "epoch: 66 step: 31 loss: 0.09749397 acc: 0.9598617553710938\n",
      "epoch: 66 step: 32 loss: 0.08664775 acc: 0.9605216979980469\n",
      "epoch: 66 step: 33 loss: 0.08547355 acc: 0.9669723510742188\n",
      "epoch: 66 step: 34 loss: 0.09261624 acc: 0.960906982421875\n",
      "epoch: 66 step: 35 loss: 0.09456309 acc: 0.9546051025390625\n",
      "epoch: 66 step: 36 loss: 0.10914767 acc: 0.9628868103027344\n",
      "epoch: 66 step: 37 loss: 0.11992174 acc: 0.958709716796875\n",
      "epoch: 66 step: 38 loss: 0.099001236 acc: 0.9604644775390625\n",
      "epoch: 66 step: 39 loss: 0.08954939 acc: 0.9616775512695312\n",
      "epoch: 66 step: 40 loss: 0.09330219 acc: 0.9490776062011719\n",
      "epoch: 66 step: 41 loss: 0.0878587 acc: 0.9540596008300781\n",
      "epoch: 66 step: 42 loss: 0.10579371 acc: 0.9495773315429688\n",
      "epoch: 66 step: 43 loss: 0.080378614 acc: 0.9664573669433594\n",
      "epoch: 66 step: 44 loss: 0.10689435 acc: 0.95013427734375\n",
      "epoch: 66 step: 45 loss: 0.095583245 acc: 0.9622039794921875\n",
      "epoch: 66 step: 46 loss: 0.1026931 acc: 0.9587059020996094\n",
      "epoch: 66 step: 47 loss: 0.10157056 acc: 0.960052490234375\n",
      "epoch: 66 step: 48 loss: 0.09223097 acc: 0.9594497680664062\n",
      "epoch: 66 step: 49 loss: 0.0838468 acc: 0.9611015319824219\n",
      "epoch: 66 step: 50 loss: 0.096589 acc: 0.9595108032226562\n",
      "epoch: 66 step: 51 loss: 0.11524909 acc: 0.9498558044433594\n",
      "epoch: 66 step: 52 loss: 0.10605966 acc: 0.9544601440429688\n",
      "epoch: 66 step: 53 loss: 0.0868255 acc: 0.9639244079589844\n",
      "epoch: 66 step: 54 loss: 0.09350345 acc: 0.9646873474121094\n",
      "epoch: 66 step: 55 loss: 0.1022965 acc: 0.9561042785644531\n",
      "epoch: 66 step: 56 loss: 0.09164431 acc: 0.9592437744140625\n",
      "epoch: 66 step: 57 loss: 0.091141194 acc: 0.9582939147949219\n",
      "epoch: 66 step: 58 loss: 0.10429058 acc: 0.9615859985351562\n",
      "epoch: 66 step: 59 loss: 0.08454904 acc: 0.9599800109863281\n",
      "epoch: 66 step: 60 loss: 0.086035505 acc: 0.9614105224609375\n",
      "epoch: 66 step: 61 loss: 0.094100825 acc: 0.9522361755371094\n",
      "epoch: 66 step: 62 loss: 0.08259335 acc: 0.9569473266601562\n",
      "epoch: 66 step: 63 loss: 0.09212587 acc: 0.9609870910644531\n",
      "epoch: 66 step: 64 loss: 0.084141314 acc: 0.9742965698242188\n",
      "epoch: 66 step: 65 loss: 0.09077663 acc: 0.9622879028320312\n",
      "epoch: 66 step: 66 loss: 0.087439165 acc: 0.9650955200195312\n",
      "epoch: 66 step: 67 loss: 0.093215175 acc: 0.9590225219726562\n",
      "epoch: 66 step: 68 loss: 0.09264864 acc: 0.9618873596191406\n",
      "epoch: 66 step: 69 loss: 0.08537903 acc: 0.956787109375\n",
      "epoch: 66 step: 70 loss: 0.08831695 acc: 0.9582176208496094\n",
      "epoch: 66 step: 71 loss: 0.082154736 acc: 0.9572410583496094\n",
      "epoch: 66 step: 72 loss: 0.08301735 acc: 0.9598007202148438\n",
      "epoch: 66 step: 73 loss: 0.098257795 acc: 0.9574317932128906\n",
      "epoch: 66 step: 74 loss: 0.08486838 acc: 0.9654006958007812\n",
      "epoch: 66 step: 75 loss: 0.08124571 acc: 0.9656524658203125\n",
      "epoch: 66 step: 76 loss: 0.084155194 acc: 0.9633331298828125\n",
      "epoch: 66 step: 77 loss: 0.08775751 acc: 0.9641036987304688\n",
      "epoch: 66 step: 78 loss: 0.09358288 acc: 0.9604148864746094\n",
      "epoch: 66 step: 79 loss: 0.09681018 acc: 0.9539070129394531\n",
      "epoch: 66 step: 80 loss: 0.082482584 acc: 0.96112060546875\n",
      "epoch: 66 step: 81 loss: 0.065466635 acc: 0.9623680114746094\n",
      "epoch: 66 step: 82 loss: 0.07162224 acc: 0.9656448364257812\n",
      "epoch: 66 step: 83 loss: 0.11806939 acc: 0.9512557983398438\n",
      "epoch: 66 step: 84 loss: 0.08638685 acc: 0.9587135314941406\n",
      "epoch: 66 step: 85 loss: 0.07401079 acc: 0.9630813598632812\n",
      "epoch: 66 step: 86 loss: 0.08790324 acc: 0.9629249572753906\n",
      "epoch: 66 step: 87 loss: 0.08267279 acc: 0.9581451416015625\n",
      "epoch: 66 step: 88 loss: 0.09188431 acc: 0.9673423767089844\n",
      "epoch: 66 step: 89 loss: 0.07698315 acc: 0.96685791015625\n",
      "epoch: 66 step: 90 loss: 0.078494556 acc: 0.97137451171875\n",
      "epoch: 66 step: 91 loss: 0.0855504 acc: 0.9630584716796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 66 step: 92 loss: 0.080950595 acc: 0.9687957763671875\n",
      "epoch: 66 step: 93 loss: 0.09269049 acc: 0.9660797119140625\n",
      "epoch: 66 step: 94 loss: 0.08212183 acc: 0.9656219482421875\n",
      "epoch: 66 step: 95 loss: 0.08831978 acc: 0.9626693725585938\n",
      "epoch: 66 step: 96 loss: 0.07805769 acc: 0.9679412841796875\n",
      "epoch: 66 step: 97 loss: 0.079413705 acc: 0.9580612182617188\n",
      "epoch: 66 step: 98 loss: 0.09449248 acc: 0.9591636657714844\n",
      "epoch: 66 step: 99 loss: 0.07417106 acc: 0.9624404907226562\n",
      "epoch: 66 step: 100 loss: 0.07608185 acc: 0.96551513671875\n",
      "epoch: 66 step: 101 loss: 0.096081264 acc: 0.9588508605957031\n",
      "epoch: 66 step: 102 loss: 0.08135534 acc: 0.9641227722167969\n",
      "epoch: 66 step: 103 loss: 0.076591045 acc: 0.9582939147949219\n",
      "epoch: 66 step: 104 loss: 0.06945898 acc: 0.9632186889648438\n",
      "epoch: 66 step: 105 loss: 0.07809081 acc: 0.9692268371582031\n",
      "epoch: 66 step: 106 loss: 0.0843818 acc: 0.9589042663574219\n",
      "epoch: 66 step: 107 loss: 0.10549798 acc: 0.9638099670410156\n",
      "epoch: 66 step: 108 loss: 0.08352751 acc: 0.9652519226074219\n",
      "epoch: 66 step: 109 loss: 0.08198031 acc: 0.9665679931640625\n",
      "epoch: 66 step: 110 loss: 0.070941955 acc: 0.968017578125\n",
      "epoch: 66 step: 111 loss: 0.07770474 acc: 0.9630088806152344\n",
      "epoch: 66 step: 112 loss: 0.076826684 acc: 0.9639778137207031\n",
      "epoch: 66 step: 113 loss: 0.08519987 acc: 0.9572181701660156\n",
      "epoch: 66 step: 114 loss: 0.08255288 acc: 0.96197509765625\n",
      "epoch: 66 step: 115 loss: 0.07811097 acc: 0.964813232421875\n",
      "epoch: 66 step: 116 loss: 0.08064127 acc: 0.9590721130371094\n",
      "epoch: 66 step: 117 loss: 0.0890045 acc: 0.9589805603027344\n",
      "epoch: 66 step: 118 loss: 0.08285408 acc: 0.9624557495117188\n",
      "epoch: 66 step: 119 loss: 0.0929772 acc: 0.9569740295410156\n",
      "epoch: 66 step: 120 loss: 0.08253472 acc: 0.9634628295898438\n",
      "epoch: 66 step: 121 loss: 0.08664418 acc: 0.9602165222167969\n",
      "epoch: 66 step: 122 loss: 0.09016169 acc: 0.9630622863769531\n",
      "epoch: 66 step: 123 loss: 0.08632626 acc: 0.9569091796875\n",
      "epoch: 66 step: 124 loss: 0.12414155 acc: 0.9542410714285714\n",
      "epoch: 66 validation_loss: 0.089 validation_dice: 0.8481610312377363\n",
      "epoch: 66 test_dataset dice: 0.7391495160503151\n",
      "time cost 0.5366019248962403 min\n",
      "dice_best: 0.8481610312377363\n",
      "******************************** epoch  66  is finished. *********************************\n",
      "epoch: 67 step: 1 loss: 0.082079105 acc: 0.9655609130859375\n",
      "epoch: 67 step: 2 loss: 0.08495238 acc: 0.9617156982421875\n",
      "epoch: 67 step: 3 loss: 0.079502836 acc: 0.962493896484375\n",
      "epoch: 67 step: 4 loss: 0.09254757 acc: 0.9666786193847656\n",
      "epoch: 67 step: 5 loss: 0.12117787 acc: 0.9561119079589844\n",
      "epoch: 67 step: 6 loss: 0.08917896 acc: 0.9581146240234375\n",
      "epoch: 67 step: 7 loss: 0.09144977 acc: 0.9621696472167969\n",
      "epoch: 67 step: 8 loss: 0.073444806 acc: 0.9652442932128906\n",
      "epoch: 67 step: 9 loss: 0.08522647 acc: 0.9655075073242188\n",
      "epoch: 67 step: 10 loss: 0.081971556 acc: 0.9669685363769531\n",
      "epoch: 67 step: 11 loss: 0.09438701 acc: 0.9628639221191406\n",
      "epoch: 67 step: 12 loss: 0.093382366 acc: 0.9633712768554688\n",
      "epoch: 67 step: 13 loss: 0.113314435 acc: 0.952667236328125\n",
      "epoch: 67 step: 14 loss: 0.101063766 acc: 0.9482917785644531\n",
      "epoch: 67 step: 15 loss: 0.08045941 acc: 0.9599685668945312\n",
      "epoch: 67 step: 16 loss: 0.075851515 acc: 0.9656600952148438\n",
      "epoch: 67 step: 17 loss: 0.0912408 acc: 0.9633903503417969\n",
      "epoch: 67 step: 18 loss: 0.079549946 acc: 0.9674758911132812\n",
      "epoch: 67 step: 19 loss: 0.0903444 acc: 0.9645843505859375\n",
      "epoch: 67 step: 20 loss: 0.0841142 acc: 0.9663238525390625\n",
      "epoch: 67 step: 21 loss: 0.113412954 acc: 0.9623794555664062\n",
      "epoch: 67 step: 22 loss: 0.097865045 acc: 0.9589920043945312\n",
      "epoch: 67 step: 23 loss: 0.09580817 acc: 0.9550895690917969\n",
      "epoch: 67 step: 24 loss: 0.10409074 acc: 0.9561958312988281\n",
      "epoch: 67 step: 25 loss: 0.09641424 acc: 0.9606208801269531\n",
      "epoch: 67 step: 26 loss: 0.07692827 acc: 0.9672279357910156\n",
      "epoch: 67 step: 27 loss: 0.096257135 acc: 0.9541664123535156\n",
      "epoch: 67 step: 28 loss: 0.09129077 acc: 0.9641799926757812\n",
      "epoch: 67 step: 29 loss: 0.0783233 acc: 0.9664421081542969\n",
      "epoch: 67 step: 30 loss: 0.08725851 acc: 0.9630050659179688\n",
      "epoch: 67 step: 31 loss: 0.07742313 acc: 0.9624557495117188\n",
      "epoch: 67 step: 32 loss: 0.092300765 acc: 0.9600982666015625\n",
      "epoch: 67 step: 33 loss: 0.075639874 acc: 0.9681243896484375\n",
      "epoch: 67 step: 34 loss: 0.09606517 acc: 0.9606857299804688\n",
      "epoch: 67 step: 35 loss: 0.07536213 acc: 0.9651451110839844\n",
      "epoch: 67 step: 36 loss: 0.096483655 acc: 0.9600563049316406\n",
      "epoch: 67 step: 37 loss: 0.088775374 acc: 0.9578208923339844\n",
      "epoch: 67 step: 38 loss: 0.08041299 acc: 0.9692344665527344\n",
      "epoch: 67 step: 39 loss: 0.084196776 acc: 0.9649009704589844\n",
      "epoch: 67 step: 40 loss: 0.07687496 acc: 0.9665985107421875\n",
      "epoch: 67 step: 41 loss: 0.09205706 acc: 0.9580802917480469\n",
      "epoch: 67 step: 42 loss: 0.0812312 acc: 0.9640769958496094\n",
      "epoch: 67 step: 43 loss: 0.09281668 acc: 0.9571495056152344\n",
      "epoch: 67 step: 44 loss: 0.09413526 acc: 0.9541015625\n",
      "epoch: 67 step: 45 loss: 0.0828997 acc: 0.9577522277832031\n",
      "epoch: 67 step: 46 loss: 0.08004201 acc: 0.9619865417480469\n",
      "epoch: 67 step: 47 loss: 0.09611145 acc: 0.9556427001953125\n",
      "epoch: 67 step: 48 loss: 0.095151454 acc: 0.9581108093261719\n",
      "epoch: 67 step: 49 loss: 0.077826984 acc: 0.9634208679199219\n",
      "epoch: 67 step: 50 loss: 0.08662121 acc: 0.9607467651367188\n",
      "epoch: 67 step: 51 loss: 0.08398198 acc: 0.9581146240234375\n",
      "epoch: 67 step: 52 loss: 0.084523894 acc: 0.9598884582519531\n",
      "epoch: 67 step: 53 loss: 0.07855561 acc: 0.9655952453613281\n",
      "epoch: 67 step: 54 loss: 0.08238931 acc: 0.9625778198242188\n",
      "epoch: 67 step: 55 loss: 0.06981622 acc: 0.9679107666015625\n",
      "epoch: 67 step: 56 loss: 0.08230892 acc: 0.9626731872558594\n",
      "epoch: 67 step: 57 loss: 0.07344797 acc: 0.9665946960449219\n",
      "epoch: 67 step: 58 loss: 0.09928043 acc: 0.9612579345703125\n",
      "epoch: 67 step: 59 loss: 0.08566189 acc: 0.961334228515625\n",
      "epoch: 67 step: 60 loss: 0.07249904 acc: 0.9652252197265625\n",
      "epoch: 67 step: 61 loss: 0.07779423 acc: 0.9712295532226562\n",
      "epoch: 67 step: 62 loss: 0.084379315 acc: 0.960784912109375\n",
      "epoch: 67 step: 63 loss: 0.07947939 acc: 0.9613494873046875\n",
      "epoch: 67 step: 64 loss: 0.083644435 acc: 0.9630775451660156\n",
      "epoch: 67 step: 65 loss: 0.09115972 acc: 0.9565620422363281\n",
      "epoch: 67 step: 66 loss: 0.07338326 acc: 0.96319580078125\n",
      "epoch: 67 step: 67 loss: 0.07281034 acc: 0.9641151428222656\n",
      "epoch: 67 step: 68 loss: 0.08574201 acc: 0.9672813415527344\n",
      "epoch: 67 step: 69 loss: 0.0751431 acc: 0.9662055969238281\n",
      "epoch: 67 step: 70 loss: 0.07999128 acc: 0.9655532836914062\n",
      "epoch: 67 step: 71 loss: 0.08424767 acc: 0.963104248046875\n",
      "epoch: 67 step: 72 loss: 0.084823705 acc: 0.9709281921386719\n",
      "epoch: 67 step: 73 loss: 0.07960942 acc: 0.9640731811523438\n",
      "epoch: 67 step: 74 loss: 0.10283267 acc: 0.9542427062988281\n",
      "epoch: 67 step: 75 loss: 0.08089932 acc: 0.9633941650390625\n",
      "epoch: 67 step: 76 loss: 0.0787961 acc: 0.9650726318359375\n",
      "epoch: 67 step: 77 loss: 0.085049026 acc: 0.961212158203125\n",
      "epoch: 67 step: 78 loss: 0.07996923 acc: 0.9610939025878906\n",
      "epoch: 67 step: 79 loss: 0.08897636 acc: 0.96295166015625\n",
      "epoch: 67 step: 80 loss: 0.09524543 acc: 0.9521064758300781\n",
      "epoch: 67 step: 81 loss: 0.08055913 acc: 0.959564208984375\n",
      "epoch: 67 step: 82 loss: 0.097598724 acc: 0.9571723937988281\n",
      "epoch: 67 step: 83 loss: 0.09173201 acc: 0.9632682800292969\n",
      "epoch: 67 step: 84 loss: 0.08183245 acc: 0.9688949584960938\n",
      "epoch: 67 step: 85 loss: 0.06929835 acc: 0.9694709777832031\n",
      "epoch: 67 step: 86 loss: 0.08873509 acc: 0.9655876159667969\n",
      "epoch: 67 step: 87 loss: 0.07728167 acc: 0.9727630615234375\n",
      "epoch: 67 step: 88 loss: 0.08994019 acc: 0.9562301635742188\n",
      "epoch: 67 step: 89 loss: 0.08893023 acc: 0.9631614685058594\n",
      "epoch: 67 step: 90 loss: 0.09337516 acc: 0.9591789245605469\n",
      "epoch: 67 step: 91 loss: 0.074463725 acc: 0.9628257751464844\n",
      "epoch: 67 step: 92 loss: 0.07754168 acc: 0.964752197265625\n",
      "epoch: 67 step: 93 loss: 0.09939076 acc: 0.9619903564453125\n",
      "epoch: 67 step: 94 loss: 0.0828507 acc: 0.9614677429199219\n",
      "epoch: 67 step: 95 loss: 0.08717577 acc: 0.9605674743652344\n",
      "epoch: 67 step: 96 loss: 0.075782605 acc: 0.9734382629394531\n",
      "epoch: 67 step: 97 loss: 0.09657764 acc: 0.9653663635253906\n",
      "epoch: 67 step: 98 loss: 0.07103127 acc: 0.9678878784179688\n",
      "epoch: 67 step: 99 loss: 0.07781316 acc: 0.9741630554199219\n",
      "epoch: 67 step: 100 loss: 0.10685582 acc: 0.96246337890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 67 step: 101 loss: 0.10907179 acc: 0.9618606567382812\n",
      "epoch: 67 step: 102 loss: 0.089350566 acc: 0.9521217346191406\n",
      "epoch: 67 step: 103 loss: 0.091318265 acc: 0.9624290466308594\n",
      "epoch: 67 step: 104 loss: 0.08914883 acc: 0.9619827270507812\n",
      "epoch: 67 step: 105 loss: 0.101564154 acc: 0.9559593200683594\n",
      "epoch: 67 step: 106 loss: 0.082784146 acc: 0.9581832885742188\n",
      "epoch: 67 step: 107 loss: 0.104917094 acc: 0.9631500244140625\n",
      "epoch: 67 step: 108 loss: 0.08837584 acc: 0.9583511352539062\n",
      "epoch: 67 step: 109 loss: 0.073014334 acc: 0.9654388427734375\n",
      "epoch: 67 step: 110 loss: 0.083150044 acc: 0.9589195251464844\n",
      "epoch: 67 step: 111 loss: 0.074402995 acc: 0.9644241333007812\n",
      "epoch: 67 step: 112 loss: 0.102298595 acc: 0.9616775512695312\n",
      "epoch: 67 step: 113 loss: 0.08069432 acc: 0.9601783752441406\n",
      "epoch: 67 step: 114 loss: 0.092505574 acc: 0.9631309509277344\n",
      "epoch: 67 step: 115 loss: 0.09910856 acc: 0.9549598693847656\n",
      "epoch: 67 step: 116 loss: 0.09087255 acc: 0.9664649963378906\n",
      "epoch: 67 step: 117 loss: 0.07573873 acc: 0.9693336486816406\n",
      "epoch: 67 step: 118 loss: 0.084633715 acc: 0.9605522155761719\n",
      "epoch: 67 step: 119 loss: 0.08253632 acc: 0.9653205871582031\n",
      "epoch: 67 step: 120 loss: 0.082988724 acc: 0.9627304077148438\n",
      "epoch: 67 step: 121 loss: 0.06687511 acc: 0.9700469970703125\n",
      "epoch: 67 step: 122 loss: 0.094813116 acc: 0.9582099914550781\n",
      "epoch: 67 step: 123 loss: 0.072444186 acc: 0.9720230102539062\n",
      "epoch: 67 step: 124 loss: 0.09869188 acc: 0.961181640625\n",
      "epoch: 67 validation_loss: 0.094 validation_dice: 0.8413915891446377\n",
      "epoch: 67 test_dataset dice: 0.7409166779341835\n",
      "time cost 0.5362337311108907 min\n",
      "dice_best: 0.8481610312377363\n",
      "******************************** epoch  67  is finished. *********************************\n",
      "epoch: 68 step: 1 loss: 0.07665421 acc: 0.9688720703125\n",
      "epoch: 68 step: 2 loss: 0.08164854 acc: 0.9657745361328125\n",
      "epoch: 68 step: 3 loss: 0.08514576 acc: 0.9645309448242188\n",
      "epoch: 68 step: 4 loss: 0.07775717 acc: 0.9626617431640625\n",
      "epoch: 68 step: 5 loss: 0.084386036 acc: 0.962677001953125\n",
      "epoch: 68 step: 6 loss: 0.10011645 acc: 0.9606285095214844\n",
      "epoch: 68 step: 7 loss: 0.077338174 acc: 0.9628562927246094\n",
      "epoch: 68 step: 8 loss: 0.08059021 acc: 0.9630279541015625\n",
      "epoch: 68 step: 9 loss: 0.081426956 acc: 0.9526634216308594\n",
      "epoch: 68 step: 10 loss: 0.08244401 acc: 0.9614105224609375\n",
      "epoch: 68 step: 11 loss: 0.08903158 acc: 0.9646034240722656\n",
      "epoch: 68 step: 12 loss: 0.07787427 acc: 0.9691963195800781\n",
      "epoch: 68 step: 13 loss: 0.07536562 acc: 0.9649200439453125\n",
      "epoch: 68 step: 14 loss: 0.09925132 acc: 0.9597625732421875\n",
      "epoch: 68 step: 15 loss: 0.08932343 acc: 0.9644927978515625\n",
      "epoch: 68 step: 16 loss: 0.07855946 acc: 0.9672317504882812\n",
      "epoch: 68 step: 17 loss: 0.08212729 acc: 0.9590225219726562\n",
      "epoch: 68 step: 18 loss: 0.09594007 acc: 0.9576759338378906\n",
      "epoch: 68 step: 19 loss: 0.079589225 acc: 0.9656410217285156\n",
      "epoch: 68 step: 20 loss: 0.08528313 acc: 0.9625968933105469\n",
      "epoch: 68 step: 21 loss: 0.08591073 acc: 0.9619178771972656\n",
      "epoch: 68 step: 22 loss: 0.07610663 acc: 0.9666786193847656\n",
      "epoch: 68 step: 23 loss: 0.080719866 acc: 0.9615020751953125\n",
      "epoch: 68 step: 24 loss: 0.088531435 acc: 0.9620094299316406\n",
      "epoch: 68 step: 25 loss: 0.0837673 acc: 0.9616775512695312\n",
      "epoch: 68 step: 26 loss: 0.09566837 acc: 0.9562911987304688\n",
      "epoch: 68 step: 27 loss: 0.078261994 acc: 0.9637336730957031\n",
      "epoch: 68 step: 28 loss: 0.097981 acc: 0.9598846435546875\n",
      "epoch: 68 step: 29 loss: 0.08056327 acc: 0.9663429260253906\n",
      "epoch: 68 step: 30 loss: 0.07663578 acc: 0.9614448547363281\n",
      "epoch: 68 step: 31 loss: 0.095607914 acc: 0.9600753784179688\n",
      "epoch: 68 step: 32 loss: 0.07833369 acc: 0.9586219787597656\n",
      "epoch: 68 step: 33 loss: 0.09096971 acc: 0.962310791015625\n",
      "epoch: 68 step: 34 loss: 0.07867956 acc: 0.9646453857421875\n",
      "epoch: 68 step: 35 loss: 0.09026321 acc: 0.9569778442382812\n",
      "epoch: 68 step: 36 loss: 0.080492504 acc: 0.9703025817871094\n",
      "epoch: 68 step: 37 loss: 0.09128861 acc: 0.9687538146972656\n",
      "epoch: 68 step: 38 loss: 0.095604815 acc: 0.9621429443359375\n",
      "epoch: 68 step: 39 loss: 0.07655221 acc: 0.9634208679199219\n",
      "epoch: 68 step: 40 loss: 0.08916684 acc: 0.9633216857910156\n",
      "epoch: 68 step: 41 loss: 0.0948126 acc: 0.9625778198242188\n",
      "epoch: 68 step: 42 loss: 0.096144475 acc: 0.9566688537597656\n",
      "epoch: 68 step: 43 loss: 0.07718239 acc: 0.9659996032714844\n",
      "epoch: 68 step: 44 loss: 0.0910908 acc: 0.9627761840820312\n",
      "epoch: 68 step: 45 loss: 0.077192724 acc: 0.9647483825683594\n",
      "epoch: 68 step: 46 loss: 0.08589048 acc: 0.9606590270996094\n",
      "epoch: 68 step: 47 loss: 0.08506721 acc: 0.9578475952148438\n",
      "epoch: 68 step: 48 loss: 0.08689325 acc: 0.9599266052246094\n",
      "epoch: 68 step: 49 loss: 0.081678145 acc: 0.9641952514648438\n",
      "epoch: 68 step: 50 loss: 0.10008729 acc: 0.9583625793457031\n",
      "epoch: 68 step: 51 loss: 0.08412582 acc: 0.9647483825683594\n",
      "epoch: 68 step: 52 loss: 0.07981831 acc: 0.963043212890625\n",
      "epoch: 68 step: 53 loss: 0.0711797 acc: 0.9701271057128906\n",
      "epoch: 68 step: 54 loss: 0.084680155 acc: 0.967559814453125\n",
      "epoch: 68 step: 55 loss: 0.077634536 acc: 0.960723876953125\n",
      "epoch: 68 step: 56 loss: 0.08338772 acc: 0.9617919921875\n",
      "epoch: 68 step: 57 loss: 0.078371115 acc: 0.96148681640625\n",
      "epoch: 68 step: 58 loss: 0.095296465 acc: 0.9598617553710938\n",
      "epoch: 68 step: 59 loss: 0.07355475 acc: 0.9682693481445312\n",
      "epoch: 68 step: 60 loss: 0.08334507 acc: 0.966400146484375\n",
      "epoch: 68 step: 61 loss: 0.08483592 acc: 0.9601860046386719\n",
      "epoch: 68 step: 62 loss: 0.09513854 acc: 0.9573211669921875\n",
      "epoch: 68 step: 63 loss: 0.08147328 acc: 0.9688491821289062\n",
      "epoch: 68 step: 64 loss: 0.075447276 acc: 0.9604682922363281\n",
      "epoch: 68 step: 65 loss: 0.08923649 acc: 0.9590682983398438\n",
      "epoch: 68 step: 66 loss: 0.08624375 acc: 0.960906982421875\n",
      "epoch: 68 step: 67 loss: 0.07882662 acc: 0.9632492065429688\n",
      "epoch: 68 step: 68 loss: 0.07918139 acc: 0.9597282409667969\n",
      "epoch: 68 step: 69 loss: 0.10384223 acc: 0.9569358825683594\n",
      "epoch: 68 step: 70 loss: 0.09196463 acc: 0.9568672180175781\n",
      "epoch: 68 step: 71 loss: 0.09135485 acc: 0.9654769897460938\n",
      "epoch: 68 step: 72 loss: 0.07432889 acc: 0.9655799865722656\n",
      "epoch: 68 step: 73 loss: 0.08434873 acc: 0.9682159423828125\n",
      "epoch: 68 step: 74 loss: 0.0867074 acc: 0.9589385986328125\n",
      "epoch: 68 step: 75 loss: 0.067154646 acc: 0.9702606201171875\n",
      "epoch: 68 step: 76 loss: 0.087166876 acc: 0.9661102294921875\n",
      "epoch: 68 step: 77 loss: 0.08523349 acc: 0.9664268493652344\n",
      "epoch: 68 step: 78 loss: 0.07798325 acc: 0.9669418334960938\n",
      "epoch: 68 step: 79 loss: 0.08978967 acc: 0.9659919738769531\n",
      "epoch: 68 step: 80 loss: 0.0845602 acc: 0.96405029296875\n",
      "epoch: 68 step: 81 loss: 0.08554435 acc: 0.964752197265625\n",
      "epoch: 68 step: 82 loss: 0.07756611 acc: 0.9648704528808594\n",
      "epoch: 68 step: 83 loss: 0.077802986 acc: 0.9672508239746094\n",
      "epoch: 68 step: 84 loss: 0.08813225 acc: 0.9600448608398438\n",
      "epoch: 68 step: 85 loss: 0.08536333 acc: 0.9644966125488281\n",
      "epoch: 68 step: 86 loss: 0.0833031 acc: 0.9676399230957031\n",
      "epoch: 68 step: 87 loss: 0.07359223 acc: 0.9660186767578125\n",
      "epoch: 68 step: 88 loss: 0.08450503 acc: 0.9621429443359375\n",
      "epoch: 68 step: 89 loss: 0.0822129 acc: 0.9682197570800781\n",
      "epoch: 68 step: 90 loss: 0.07479056 acc: 0.9670028686523438\n",
      "epoch: 68 step: 91 loss: 0.073214225 acc: 0.9674835205078125\n",
      "epoch: 68 step: 92 loss: 0.07744635 acc: 0.970855712890625\n",
      "epoch: 68 step: 93 loss: 0.10597185 acc: 0.9593162536621094\n",
      "epoch: 68 step: 94 loss: 0.09405048 acc: 0.9597549438476562\n",
      "epoch: 68 step: 95 loss: 0.076887146 acc: 0.9627037048339844\n",
      "epoch: 68 step: 96 loss: 0.08284484 acc: 0.9588394165039062\n",
      "epoch: 68 step: 97 loss: 0.08499356 acc: 0.9636993408203125\n",
      "epoch: 68 step: 98 loss: 0.08748299 acc: 0.9621810913085938\n",
      "epoch: 68 step: 99 loss: 0.100940086 acc: 0.9596786499023438\n",
      "epoch: 68 step: 100 loss: 0.069643095 acc: 0.96728515625\n",
      "epoch: 68 step: 101 loss: 0.07136082 acc: 0.9658737182617188\n",
      "epoch: 68 step: 102 loss: 0.09481419 acc: 0.9546737670898438\n",
      "epoch: 68 step: 103 loss: 0.095014066 acc: 0.9582366943359375\n",
      "epoch: 68 step: 104 loss: 0.084909685 acc: 0.9633369445800781\n",
      "epoch: 68 step: 105 loss: 0.084643334 acc: 0.9555282592773438\n",
      "epoch: 68 step: 106 loss: 0.08673617 acc: 0.9575424194335938\n",
      "epoch: 68 step: 107 loss: 0.06576912 acc: 0.9679527282714844\n",
      "epoch: 68 step: 108 loss: 0.09656662 acc: 0.9622383117675781\n",
      "epoch: 68 step: 109 loss: 0.08860923 acc: 0.9650077819824219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 68 step: 110 loss: 0.07881546 acc: 0.9684104919433594\n",
      "epoch: 68 step: 111 loss: 0.082419895 acc: 0.9623336791992188\n",
      "epoch: 68 step: 112 loss: 0.09538916 acc: 0.9596366882324219\n",
      "epoch: 68 step: 113 loss: 0.08158559 acc: 0.9631614685058594\n",
      "epoch: 68 step: 114 loss: 0.08226337 acc: 0.9667396545410156\n",
      "epoch: 68 step: 115 loss: 0.0805042 acc: 0.9592399597167969\n",
      "epoch: 68 step: 116 loss: 0.080665916 acc: 0.9623374938964844\n",
      "epoch: 68 step: 117 loss: 0.080508724 acc: 0.9628829956054688\n",
      "epoch: 68 step: 118 loss: 0.08153993 acc: 0.9582252502441406\n",
      "epoch: 68 step: 119 loss: 0.084702685 acc: 0.9593925476074219\n",
      "epoch: 68 step: 120 loss: 0.07488759 acc: 0.9642105102539062\n",
      "epoch: 68 step: 121 loss: 0.088200636 acc: 0.9607315063476562\n",
      "epoch: 68 step: 122 loss: 0.077307 acc: 0.9652900695800781\n",
      "epoch: 68 step: 123 loss: 0.08912017 acc: 0.9595069885253906\n",
      "epoch: 68 step: 124 loss: 0.14171262 acc: 0.951416015625\n",
      "epoch: 68 validation_loss: 0.09 validation_dice: 0.8551517496360885\n",
      "epoch: 68 test_dataset dice: 0.7378882979154594\n",
      "time cost 0.5360780080159505 min\n",
      "dice_best: 0.8551517496360885\n",
      "******************************** epoch  68  is finished. *********************************\n",
      "epoch: 69 step: 1 loss: 0.086795 acc: 0.9621620178222656\n",
      "epoch: 69 step: 2 loss: 0.08485567 acc: 0.968719482421875\n",
      "epoch: 69 step: 3 loss: 0.10094517 acc: 0.9651832580566406\n",
      "epoch: 69 step: 4 loss: 0.08193402 acc: 0.9639015197753906\n",
      "epoch: 69 step: 5 loss: 0.091351435 acc: 0.9575920104980469\n",
      "epoch: 69 step: 6 loss: 0.09289715 acc: 0.9608192443847656\n",
      "epoch: 69 step: 7 loss: 0.09419672 acc: 0.9534339904785156\n",
      "epoch: 69 step: 8 loss: 0.08693215 acc: 0.9549903869628906\n",
      "epoch: 69 step: 9 loss: 0.0988355 acc: 0.9529533386230469\n",
      "epoch: 69 step: 10 loss: 0.095203765 acc: 0.9582176208496094\n",
      "epoch: 69 step: 11 loss: 0.0913949 acc: 0.9688339233398438\n",
      "epoch: 69 step: 12 loss: 0.098358296 acc: 0.9612274169921875\n",
      "epoch: 69 step: 13 loss: 0.07704623 acc: 0.9692878723144531\n",
      "epoch: 69 step: 14 loss: 0.09409418 acc: 0.9604835510253906\n",
      "epoch: 69 step: 15 loss: 0.07291127 acc: 0.9639968872070312\n",
      "epoch: 69 step: 16 loss: 0.08150689 acc: 0.9658889770507812\n",
      "epoch: 69 step: 17 loss: 0.08692414 acc: 0.97259521484375\n",
      "epoch: 69 step: 18 loss: 0.096512064 acc: 0.9623832702636719\n",
      "epoch: 69 step: 19 loss: 0.08162546 acc: 0.968963623046875\n",
      "epoch: 69 step: 20 loss: 0.09416091 acc: 0.9615135192871094\n",
      "epoch: 69 step: 21 loss: 0.087473445 acc: 0.9539451599121094\n",
      "epoch: 69 step: 22 loss: 0.076430805 acc: 0.9719047546386719\n",
      "epoch: 69 step: 23 loss: 0.09969937 acc: 0.9540214538574219\n",
      "epoch: 69 step: 24 loss: 0.09635782 acc: 0.9532470703125\n",
      "epoch: 69 step: 25 loss: 0.076766096 acc: 0.9685020446777344\n",
      "epoch: 69 step: 26 loss: 0.08803342 acc: 0.9594802856445312\n",
      "epoch: 69 step: 27 loss: 0.08003233 acc: 0.9658355712890625\n",
      "epoch: 69 step: 28 loss: 0.09930651 acc: 0.9622077941894531\n",
      "epoch: 69 step: 29 loss: 0.09541708 acc: 0.9596443176269531\n",
      "epoch: 69 step: 30 loss: 0.06439862 acc: 0.9726371765136719\n",
      "epoch: 69 step: 31 loss: 0.09050981 acc: 0.960723876953125\n",
      "epoch: 69 step: 32 loss: 0.10752775 acc: 0.9570045471191406\n",
      "epoch: 69 step: 33 loss: 0.11657727 acc: 0.9554214477539062\n",
      "epoch: 69 step: 34 loss: 0.07851319 acc: 0.9647750854492188\n",
      "epoch: 69 step: 35 loss: 0.101306 acc: 0.9526329040527344\n",
      "epoch: 69 step: 36 loss: 0.09237993 acc: 0.95849609375\n",
      "epoch: 69 step: 37 loss: 0.08502724 acc: 0.9653549194335938\n",
      "epoch: 69 step: 38 loss: 0.084518105 acc: 0.9543495178222656\n",
      "epoch: 69 step: 39 loss: 0.09397818 acc: 0.9570159912109375\n",
      "epoch: 69 step: 40 loss: 0.086591735 acc: 0.9577522277832031\n",
      "epoch: 69 step: 41 loss: 0.08594718 acc: 0.9665412902832031\n",
      "epoch: 69 step: 42 loss: 0.07735856 acc: 0.9665641784667969\n",
      "epoch: 69 step: 43 loss: 0.077544525 acc: 0.9687423706054688\n",
      "epoch: 69 step: 44 loss: 0.1001541 acc: 0.96240234375\n",
      "epoch: 69 step: 45 loss: 0.09840261 acc: 0.9637336730957031\n",
      "epoch: 69 step: 46 loss: 0.08126841 acc: 0.959808349609375\n",
      "epoch: 69 step: 47 loss: 0.08697284 acc: 0.9670143127441406\n",
      "epoch: 69 step: 48 loss: 0.073015414 acc: 0.9651870727539062\n",
      "epoch: 69 step: 49 loss: 0.09009021 acc: 0.9573707580566406\n",
      "epoch: 69 step: 50 loss: 0.08438383 acc: 0.9563674926757812\n",
      "epoch: 69 step: 51 loss: 0.09613607 acc: 0.9562301635742188\n",
      "epoch: 69 step: 52 loss: 0.09358941 acc: 0.9637374877929688\n",
      "epoch: 69 step: 53 loss: 0.07566755 acc: 0.9655647277832031\n",
      "epoch: 69 step: 54 loss: 0.07788596 acc: 0.9698219299316406\n",
      "epoch: 69 step: 55 loss: 0.070283286 acc: 0.9713287353515625\n",
      "epoch: 69 step: 56 loss: 0.07081342 acc: 0.9683456420898438\n",
      "epoch: 69 step: 57 loss: 0.07477512 acc: 0.9657096862792969\n",
      "epoch: 69 step: 58 loss: 0.09173737 acc: 0.9681892395019531\n",
      "epoch: 69 step: 59 loss: 0.08969014 acc: 0.9660453796386719\n",
      "epoch: 69 step: 60 loss: 0.08330794 acc: 0.9705924987792969\n",
      "epoch: 69 step: 61 loss: 0.07533326 acc: 0.9706573486328125\n",
      "epoch: 69 step: 62 loss: 0.11529145 acc: 0.9619102478027344\n",
      "epoch: 69 step: 63 loss: 0.10793696 acc: 0.9576759338378906\n",
      "epoch: 69 step: 64 loss: 0.09274853 acc: 0.9605216979980469\n",
      "epoch: 69 step: 65 loss: 0.076065354 acc: 0.9673805236816406\n",
      "epoch: 69 step: 66 loss: 0.08935055 acc: 0.9659881591796875\n",
      "epoch: 69 step: 67 loss: 0.07019013 acc: 0.9689140319824219\n",
      "epoch: 69 step: 68 loss: 0.09080951 acc: 0.9618911743164062\n",
      "epoch: 69 step: 69 loss: 0.09336565 acc: 0.9549331665039062\n",
      "epoch: 69 step: 70 loss: 0.102674015 acc: 0.9583549499511719\n",
      "epoch: 69 step: 71 loss: 0.09427977 acc: 0.9547042846679688\n",
      "epoch: 69 step: 72 loss: 0.09732904 acc: 0.9531288146972656\n",
      "epoch: 69 step: 73 loss: 0.10503843 acc: 0.9510574340820312\n",
      "epoch: 69 step: 74 loss: 0.08365963 acc: 0.9616775512695312\n",
      "epoch: 69 step: 75 loss: 0.07929206 acc: 0.9659271240234375\n",
      "epoch: 69 step: 76 loss: 0.08650317 acc: 0.9584465026855469\n",
      "epoch: 69 step: 77 loss: 0.096333 acc: 0.9650344848632812\n",
      "epoch: 69 step: 78 loss: 0.07778797 acc: 0.9666328430175781\n",
      "epoch: 69 step: 79 loss: 0.09620627 acc: 0.9587020874023438\n",
      "epoch: 69 step: 80 loss: 0.087397605 acc: 0.962310791015625\n",
      "epoch: 69 step: 81 loss: 0.09600169 acc: 0.9621009826660156\n",
      "epoch: 69 step: 82 loss: 0.07711315 acc: 0.9673080444335938\n",
      "epoch: 69 step: 83 loss: 0.09618852 acc: 0.9568748474121094\n",
      "epoch: 69 step: 84 loss: 0.09029482 acc: 0.9614372253417969\n",
      "epoch: 69 step: 85 loss: 0.0962206 acc: 0.9580726623535156\n",
      "epoch: 69 step: 86 loss: 0.096956335 acc: 0.9615478515625\n",
      "epoch: 69 step: 87 loss: 0.104003444 acc: 0.9633026123046875\n",
      "epoch: 69 step: 88 loss: 0.094993494 acc: 0.9560546875\n",
      "epoch: 69 step: 89 loss: 0.07535703 acc: 0.9693336486816406\n",
      "epoch: 69 step: 90 loss: 0.10324235 acc: 0.9609413146972656\n",
      "epoch: 69 step: 91 loss: 0.09972093 acc: 0.9633445739746094\n",
      "epoch: 69 step: 92 loss: 0.08630474 acc: 0.9595756530761719\n",
      "epoch: 69 step: 93 loss: 0.08303834 acc: 0.9597129821777344\n",
      "epoch: 69 step: 94 loss: 0.11552913 acc: 0.9526939392089844\n",
      "epoch: 69 step: 95 loss: 0.082322165 acc: 0.9538650512695312\n",
      "epoch: 69 step: 96 loss: 0.08353351 acc: 0.9646148681640625\n",
      "epoch: 69 step: 97 loss: 0.09128948 acc: 0.9586143493652344\n",
      "epoch: 69 step: 98 loss: 0.08839061 acc: 0.9565505981445312\n",
      "epoch: 69 step: 99 loss: 0.08708085 acc: 0.9619407653808594\n",
      "epoch: 69 step: 100 loss: 0.09622041 acc: 0.9647941589355469\n",
      "epoch: 69 step: 101 loss: 0.08644086 acc: 0.9656028747558594\n",
      "epoch: 69 step: 102 loss: 0.07628977 acc: 0.9752006530761719\n",
      "epoch: 69 step: 103 loss: 0.093178384 acc: 0.9607124328613281\n",
      "epoch: 69 step: 104 loss: 0.09310977 acc: 0.9665870666503906\n",
      "epoch: 69 step: 105 loss: 0.09223511 acc: 0.9613990783691406\n",
      "epoch: 69 step: 106 loss: 0.076841936 acc: 0.96856689453125\n",
      "epoch: 69 step: 107 loss: 0.09576584 acc: 0.9600067138671875\n",
      "epoch: 69 step: 108 loss: 0.07719486 acc: 0.9663658142089844\n",
      "epoch: 69 step: 109 loss: 0.093265995 acc: 0.9591178894042969\n",
      "epoch: 69 step: 110 loss: 0.07643497 acc: 0.961212158203125\n",
      "epoch: 69 step: 111 loss: 0.07577013 acc: 0.9641304016113281\n",
      "epoch: 69 step: 112 loss: 0.10416027 acc: 0.95745849609375\n",
      "epoch: 69 step: 113 loss: 0.10523715 acc: 0.9611167907714844\n",
      "epoch: 69 step: 114 loss: 0.08532923 acc: 0.9592742919921875\n",
      "epoch: 69 step: 115 loss: 0.08393586 acc: 0.9670524597167969\n",
      "epoch: 69 step: 116 loss: 0.09332556 acc: 0.9622344970703125\n",
      "epoch: 69 step: 117 loss: 0.07863085 acc: 0.9609527587890625\n",
      "epoch: 69 step: 118 loss: 0.09710816 acc: 0.963531494140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 69 step: 119 loss: 0.08580855 acc: 0.9674453735351562\n",
      "epoch: 69 step: 120 loss: 0.07829639 acc: 0.9680557250976562\n",
      "epoch: 69 step: 121 loss: 0.10072566 acc: 0.9579238891601562\n",
      "epoch: 69 step: 122 loss: 0.084287815 acc: 0.9682655334472656\n",
      "epoch: 69 step: 123 loss: 0.079835504 acc: 0.9610252380371094\n",
      "epoch: 69 step: 124 loss: 0.087979354 acc: 0.9660993303571429\n",
      "epoch: 69 validation_loss: 0.092 validation_dice: 0.8523515520003465\n",
      "epoch: 69 test_dataset dice: 0.7261549879078882\n",
      "time cost 0.5367598215738932 min\n",
      "dice_best: 0.8551517496360885\n",
      "******************************** epoch  69  is finished. *********************************\n",
      "epoch: 70 step: 1 loss: 0.0763118 acc: 0.9655265808105469\n",
      "epoch: 70 step: 2 loss: 0.0974658 acc: 0.9609413146972656\n",
      "epoch: 70 step: 3 loss: 0.08039278 acc: 0.9646759033203125\n",
      "epoch: 70 step: 4 loss: 0.09280863 acc: 0.9608497619628906\n",
      "epoch: 70 step: 5 loss: 0.08011317 acc: 0.9638137817382812\n",
      "epoch: 70 step: 6 loss: 0.07859335 acc: 0.9668388366699219\n",
      "epoch: 70 step: 7 loss: 0.10331069 acc: 0.9571609497070312\n",
      "epoch: 70 step: 8 loss: 0.08043852 acc: 0.9610137939453125\n",
      "epoch: 70 step: 9 loss: 0.08446307 acc: 0.9676704406738281\n",
      "epoch: 70 step: 10 loss: 0.09454664 acc: 0.9537734985351562\n",
      "epoch: 70 step: 11 loss: 0.086733095 acc: 0.9594497680664062\n",
      "epoch: 70 step: 12 loss: 0.07902163 acc: 0.962921142578125\n",
      "epoch: 70 step: 13 loss: 0.08026725 acc: 0.9698257446289062\n",
      "epoch: 70 step: 14 loss: 0.08786984 acc: 0.9560699462890625\n",
      "epoch: 70 step: 15 loss: 0.059745185 acc: 0.9735069274902344\n",
      "epoch: 70 step: 16 loss: 0.0780594 acc: 0.9593315124511719\n",
      "epoch: 70 step: 17 loss: 0.08974266 acc: 0.9668426513671875\n",
      "epoch: 70 step: 18 loss: 0.08604489 acc: 0.9584159851074219\n",
      "epoch: 70 step: 19 loss: 0.07611346 acc: 0.9677619934082031\n",
      "epoch: 70 step: 20 loss: 0.07956534 acc: 0.9602584838867188\n",
      "epoch: 70 step: 21 loss: 0.09053959 acc: 0.9611930847167969\n",
      "epoch: 70 step: 22 loss: 0.08827056 acc: 0.9571075439453125\n",
      "epoch: 70 step: 23 loss: 0.078835845 acc: 0.9664230346679688\n",
      "epoch: 70 step: 24 loss: 0.1090163 acc: 0.9519538879394531\n",
      "epoch: 70 step: 25 loss: 0.08036455 acc: 0.9651527404785156\n",
      "epoch: 70 step: 26 loss: 0.074309625 acc: 0.9664115905761719\n",
      "epoch: 70 step: 27 loss: 0.0927434 acc: 0.9646835327148438\n",
      "epoch: 70 step: 28 loss: 0.09018366 acc: 0.9554672241210938\n",
      "epoch: 70 step: 29 loss: 0.080704756 acc: 0.9640998840332031\n",
      "epoch: 70 step: 30 loss: 0.08592863 acc: 0.9596138000488281\n",
      "epoch: 70 step: 31 loss: 0.09886472 acc: 0.9590263366699219\n",
      "epoch: 70 step: 32 loss: 0.07873612 acc: 0.9629898071289062\n",
      "epoch: 70 step: 33 loss: 0.08931351 acc: 0.9558219909667969\n",
      "epoch: 70 step: 34 loss: 0.06983198 acc: 0.9607505798339844\n",
      "epoch: 70 step: 35 loss: 0.09480171 acc: 0.9559860229492188\n",
      "epoch: 70 step: 36 loss: 0.08339683 acc: 0.9559822082519531\n",
      "epoch: 70 step: 37 loss: 0.07946871 acc: 0.9679298400878906\n",
      "epoch: 70 step: 38 loss: 0.0764585 acc: 0.9634323120117188\n",
      "epoch: 70 step: 39 loss: 0.09401903 acc: 0.9585380554199219\n",
      "epoch: 70 step: 40 loss: 0.09378624 acc: 0.95989990234375\n",
      "epoch: 70 step: 41 loss: 0.062494885 acc: 0.970794677734375\n",
      "epoch: 70 step: 42 loss: 0.08259177 acc: 0.9682121276855469\n",
      "epoch: 70 step: 43 loss: 0.07352952 acc: 0.9678840637207031\n",
      "epoch: 70 step: 44 loss: 0.07536609 acc: 0.9731254577636719\n",
      "epoch: 70 step: 45 loss: 0.09041686 acc: 0.9672279357910156\n",
      "epoch: 70 step: 46 loss: 0.0812729 acc: 0.9725914001464844\n",
      "epoch: 70 step: 47 loss: 0.08801546 acc: 0.9640579223632812\n",
      "epoch: 70 step: 48 loss: 0.08656603 acc: 0.9636001586914062\n",
      "epoch: 70 step: 49 loss: 0.06696531 acc: 0.9700431823730469\n",
      "epoch: 70 step: 50 loss: 0.072357245 acc: 0.9634246826171875\n",
      "epoch: 70 step: 51 loss: 0.08936798 acc: 0.9603042602539062\n",
      "epoch: 70 step: 52 loss: 0.09060794 acc: 0.9546890258789062\n",
      "epoch: 70 step: 53 loss: 0.10713576 acc: 0.955810546875\n",
      "epoch: 70 step: 54 loss: 0.10313319 acc: 0.9627532958984375\n",
      "epoch: 70 step: 55 loss: 0.085304186 acc: 0.9549484252929688\n",
      "epoch: 70 step: 56 loss: 0.08425238 acc: 0.9572906494140625\n",
      "epoch: 70 step: 57 loss: 0.08054451 acc: 0.9640922546386719\n",
      "epoch: 70 step: 58 loss: 0.083465114 acc: 0.9653472900390625\n",
      "epoch: 70 step: 59 loss: 0.07317727 acc: 0.9737091064453125\n",
      "epoch: 70 step: 60 loss: 0.10183888 acc: 0.9594993591308594\n",
      "epoch: 70 step: 61 loss: 0.09544958 acc: 0.9606666564941406\n",
      "epoch: 70 step: 62 loss: 0.07175309 acc: 0.9666671752929688\n",
      "epoch: 70 step: 63 loss: 0.084532365 acc: 0.9655799865722656\n",
      "epoch: 70 step: 64 loss: 0.08081955 acc: 0.9640159606933594\n",
      "epoch: 70 step: 65 loss: 0.08845188 acc: 0.9591064453125\n",
      "epoch: 70 step: 66 loss: 0.09666734 acc: 0.9649162292480469\n",
      "epoch: 70 step: 67 loss: 0.11385767 acc: 0.9534645080566406\n",
      "epoch: 70 step: 68 loss: 0.07311736 acc: 0.9709968566894531\n",
      "epoch: 70 step: 69 loss: 0.10311192 acc: 0.9516716003417969\n",
      "epoch: 70 step: 70 loss: 0.07649452 acc: 0.9650421142578125\n",
      "epoch: 70 step: 71 loss: 0.09916917 acc: 0.9591102600097656\n",
      "epoch: 70 step: 72 loss: 0.069324136 acc: 0.9707908630371094\n",
      "epoch: 70 step: 73 loss: 0.08470687 acc: 0.9553985595703125\n",
      "epoch: 70 step: 74 loss: 0.08120604 acc: 0.96185302734375\n",
      "epoch: 70 step: 75 loss: 0.079649046 acc: 0.9686431884765625\n",
      "epoch: 70 step: 76 loss: 0.088585734 acc: 0.959747314453125\n",
      "epoch: 70 step: 77 loss: 0.08434366 acc: 0.9636306762695312\n",
      "epoch: 70 step: 78 loss: 0.08412544 acc: 0.9697341918945312\n",
      "epoch: 70 step: 79 loss: 0.088878386 acc: 0.9718475341796875\n",
      "epoch: 70 step: 80 loss: 0.07397474 acc: 0.9698219299316406\n",
      "epoch: 70 step: 81 loss: 0.0906188 acc: 0.9684867858886719\n",
      "epoch: 70 step: 82 loss: 0.0744347 acc: 0.9719810485839844\n",
      "epoch: 70 step: 83 loss: 0.06727821 acc: 0.9706153869628906\n",
      "epoch: 70 step: 84 loss: 0.10634533 acc: 0.955963134765625\n",
      "epoch: 70 step: 85 loss: 0.09459151 acc: 0.958251953125\n",
      "epoch: 70 step: 86 loss: 0.096868105 acc: 0.9547767639160156\n",
      "epoch: 70 step: 87 loss: 0.08919444 acc: 0.9556541442871094\n",
      "epoch: 70 step: 88 loss: 0.0922098 acc: 0.9601020812988281\n",
      "epoch: 70 step: 89 loss: 0.09024207 acc: 0.9585494995117188\n",
      "epoch: 70 step: 90 loss: 0.08721846 acc: 0.959197998046875\n",
      "epoch: 70 step: 91 loss: 0.08940452 acc: 0.9560165405273438\n",
      "epoch: 70 step: 92 loss: 0.09223461 acc: 0.9623031616210938\n",
      "epoch: 70 step: 93 loss: 0.09640931 acc: 0.95758056640625\n",
      "epoch: 70 step: 94 loss: 0.08121584 acc: 0.9692115783691406\n",
      "epoch: 70 step: 95 loss: 0.102172665 acc: 0.9568328857421875\n",
      "epoch: 70 step: 96 loss: 0.10358884 acc: 0.9616241455078125\n",
      "epoch: 70 step: 97 loss: 0.068962716 acc: 0.966064453125\n",
      "epoch: 70 step: 98 loss: 0.082319684 acc: 0.9609642028808594\n",
      "epoch: 70 step: 99 loss: 0.081874855 acc: 0.9643325805664062\n",
      "epoch: 70 step: 100 loss: 0.087774724 acc: 0.9591865539550781\n",
      "epoch: 70 step: 101 loss: 0.08879015 acc: 0.9593391418457031\n",
      "epoch: 70 step: 102 loss: 0.08101236 acc: 0.9533653259277344\n",
      "epoch: 70 step: 103 loss: 0.07886621 acc: 0.9654083251953125\n",
      "epoch: 70 step: 104 loss: 0.06911063 acc: 0.972747802734375\n",
      "epoch: 70 step: 105 loss: 0.06763483 acc: 0.9724998474121094\n",
      "epoch: 70 step: 106 loss: 0.07348004 acc: 0.9716873168945312\n",
      "epoch: 70 step: 107 loss: 0.08792545 acc: 0.9591064453125\n",
      "epoch: 70 step: 108 loss: 0.075230114 acc: 0.9648895263671875\n",
      "epoch: 70 step: 109 loss: 0.08641689 acc: 0.9635047912597656\n",
      "epoch: 70 step: 110 loss: 0.103120014 acc: 0.9568977355957031\n",
      "epoch: 70 step: 111 loss: 0.074292436 acc: 0.9645881652832031\n",
      "epoch: 70 step: 112 loss: 0.09209525 acc: 0.9610176086425781\n",
      "epoch: 70 step: 113 loss: 0.07214983 acc: 0.9655723571777344\n",
      "epoch: 70 step: 114 loss: 0.08477961 acc: 0.9618148803710938\n",
      "epoch: 70 step: 115 loss: 0.081215344 acc: 0.9619827270507812\n",
      "epoch: 70 step: 116 loss: 0.073583275 acc: 0.9629440307617188\n",
      "epoch: 70 step: 117 loss: 0.070018575 acc: 0.9722442626953125\n",
      "epoch: 70 step: 118 loss: 0.081617884 acc: 0.9718589782714844\n",
      "epoch: 70 step: 119 loss: 0.09677664 acc: 0.9604034423828125\n",
      "epoch: 70 step: 120 loss: 0.08704906 acc: 0.9641990661621094\n",
      "epoch: 70 step: 121 loss: 0.0891372 acc: 0.9671630859375\n",
      "epoch: 70 step: 122 loss: 0.09528139 acc: 0.9579658508300781\n",
      "epoch: 70 step: 123 loss: 0.086322375 acc: 0.9590835571289062\n",
      "epoch: 70 step: 124 loss: 0.086122334 acc: 0.9624546595982143\n",
      "epoch: 70 validation_loss: 0.091 validation_dice: 0.8284151184029811\n",
      "epoch: 70 test_dataset dice: 0.7432138591335657\n",
      "time cost 0.538365372021993 min\n",
      "dice_best: 0.8551517496360885\n",
      "******************************** epoch  70  is finished. *********************************\n",
      "epoch: 71 step: 1 loss: 0.10118251 acc: 0.9597816467285156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 71 step: 2 loss: 0.07894286 acc: 0.9619102478027344\n",
      "epoch: 71 step: 3 loss: 0.0800079 acc: 0.9639205932617188\n",
      "epoch: 71 step: 4 loss: 0.08061083 acc: 0.9608688354492188\n",
      "epoch: 71 step: 5 loss: 0.07735455 acc: 0.9707489013671875\n",
      "epoch: 71 step: 6 loss: 0.08054733 acc: 0.9628448486328125\n",
      "epoch: 71 step: 7 loss: 0.094649814 acc: 0.9680366516113281\n",
      "epoch: 71 step: 8 loss: 0.081452146 acc: 0.9654273986816406\n",
      "epoch: 71 step: 9 loss: 0.08289035 acc: 0.9622001647949219\n",
      "epoch: 71 step: 10 loss: 0.07783034 acc: 0.9625358581542969\n",
      "epoch: 71 step: 11 loss: 0.11090339 acc: 0.9602546691894531\n",
      "epoch: 71 step: 12 loss: 0.085226 acc: 0.9664268493652344\n",
      "epoch: 71 step: 13 loss: 0.09098554 acc: 0.9580039978027344\n",
      "epoch: 71 step: 14 loss: 0.0864578 acc: 0.9640121459960938\n",
      "epoch: 71 step: 15 loss: 0.0754211 acc: 0.9649658203125\n",
      "epoch: 71 step: 16 loss: 0.0805482 acc: 0.9593772888183594\n",
      "epoch: 71 step: 17 loss: 0.07353356 acc: 0.9643402099609375\n",
      "epoch: 71 step: 18 loss: 0.06467989 acc: 0.9676246643066406\n",
      "epoch: 71 step: 19 loss: 0.08691089 acc: 0.9653205871582031\n",
      "epoch: 71 step: 20 loss: 0.07839116 acc: 0.9693260192871094\n",
      "epoch: 71 step: 21 loss: 0.08195868 acc: 0.9696693420410156\n",
      "epoch: 71 step: 22 loss: 0.08739365 acc: 0.9643669128417969\n",
      "epoch: 71 step: 23 loss: 0.07325862 acc: 0.9701194763183594\n",
      "epoch: 71 step: 24 loss: 0.09603365 acc: 0.9646072387695312\n",
      "epoch: 71 step: 25 loss: 0.085967354 acc: 0.9669570922851562\n",
      "epoch: 71 step: 26 loss: 0.084520206 acc: 0.9613380432128906\n",
      "epoch: 71 step: 27 loss: 0.07865416 acc: 0.9670219421386719\n",
      "epoch: 71 step: 28 loss: 0.086220704 acc: 0.9622611999511719\n",
      "epoch: 71 step: 29 loss: 0.08642949 acc: 0.9591865539550781\n",
      "epoch: 71 step: 30 loss: 0.0741494 acc: 0.9631576538085938\n",
      "epoch: 71 step: 31 loss: 0.091971636 acc: 0.9532203674316406\n",
      "epoch: 71 step: 32 loss: 0.06943669 acc: 0.9635353088378906\n",
      "epoch: 71 step: 33 loss: 0.08203841 acc: 0.9640541076660156\n",
      "epoch: 71 step: 34 loss: 0.07582878 acc: 0.9674606323242188\n",
      "epoch: 71 step: 35 loss: 0.08533555 acc: 0.9646949768066406\n",
      "epoch: 71 step: 36 loss: 0.106090836 acc: 0.9559478759765625\n",
      "epoch: 71 step: 37 loss: 0.069003016 acc: 0.9706993103027344\n",
      "epoch: 71 step: 38 loss: 0.07921347 acc: 0.9645881652832031\n",
      "epoch: 71 step: 39 loss: 0.06814076 acc: 0.9676437377929688\n",
      "epoch: 71 step: 40 loss: 0.08565775 acc: 0.9631614685058594\n",
      "epoch: 71 step: 41 loss: 0.08257614 acc: 0.9612007141113281\n",
      "epoch: 71 step: 42 loss: 0.079107314 acc: 0.961456298828125\n",
      "epoch: 71 step: 43 loss: 0.07549125 acc: 0.9623870849609375\n",
      "epoch: 71 step: 44 loss: 0.082045935 acc: 0.95843505859375\n",
      "epoch: 71 step: 45 loss: 0.070849225 acc: 0.9628448486328125\n",
      "epoch: 71 step: 46 loss: 0.07630956 acc: 0.9640960693359375\n",
      "epoch: 71 step: 47 loss: 0.096508175 acc: 0.9586143493652344\n",
      "epoch: 71 step: 48 loss: 0.07838309 acc: 0.9642486572265625\n",
      "epoch: 71 step: 49 loss: 0.0867203 acc: 0.9617996215820312\n",
      "epoch: 71 step: 50 loss: 0.09788362 acc: 0.9628181457519531\n",
      "epoch: 71 step: 51 loss: 0.079225205 acc: 0.9644050598144531\n",
      "epoch: 71 step: 52 loss: 0.08310677 acc: 0.9567375183105469\n",
      "epoch: 71 step: 53 loss: 0.08395366 acc: 0.9594764709472656\n",
      "epoch: 71 step: 54 loss: 0.07911763 acc: 0.966705322265625\n",
      "epoch: 71 step: 55 loss: 0.07732804 acc: 0.9641876220703125\n",
      "epoch: 71 step: 56 loss: 0.07188934 acc: 0.9690475463867188\n",
      "epoch: 71 step: 57 loss: 0.09713045 acc: 0.9591560363769531\n",
      "epoch: 71 step: 58 loss: 0.094798766 acc: 0.9575958251953125\n",
      "epoch: 71 step: 59 loss: 0.094359264 acc: 0.9578781127929688\n",
      "epoch: 71 step: 60 loss: 0.08596304 acc: 0.9643363952636719\n",
      "epoch: 71 step: 61 loss: 0.07421233 acc: 0.9661064147949219\n",
      "epoch: 71 step: 62 loss: 0.08117154 acc: 0.9642181396484375\n",
      "epoch: 71 step: 63 loss: 0.08109801 acc: 0.9626426696777344\n",
      "epoch: 71 step: 64 loss: 0.0793057 acc: 0.9621734619140625\n",
      "epoch: 71 step: 65 loss: 0.08269678 acc: 0.9645195007324219\n",
      "epoch: 71 step: 66 loss: 0.098211356 acc: 0.9620285034179688\n",
      "epoch: 71 step: 67 loss: 0.08462089 acc: 0.9555435180664062\n",
      "epoch: 71 step: 68 loss: 0.087432824 acc: 0.9597244262695312\n",
      "epoch: 71 step: 69 loss: 0.09569128 acc: 0.96240234375\n",
      "epoch: 71 step: 70 loss: 0.086113125 acc: 0.9575080871582031\n",
      "epoch: 71 step: 71 loss: 0.08925797 acc: 0.9522590637207031\n",
      "epoch: 71 step: 72 loss: 0.08795587 acc: 0.9596900939941406\n",
      "epoch: 71 step: 73 loss: 0.08968882 acc: 0.9538383483886719\n",
      "epoch: 71 step: 74 loss: 0.094398364 acc: 0.95556640625\n",
      "epoch: 71 step: 75 loss: 0.07520499 acc: 0.9712371826171875\n",
      "epoch: 71 step: 76 loss: 0.09108287 acc: 0.9614906311035156\n",
      "epoch: 71 step: 77 loss: 0.08431617 acc: 0.9661445617675781\n",
      "epoch: 71 step: 78 loss: 0.10458508 acc: 0.9606170654296875\n",
      "epoch: 71 step: 79 loss: 0.086543135 acc: 0.9674301147460938\n",
      "epoch: 71 step: 80 loss: 0.07013323 acc: 0.96734619140625\n",
      "epoch: 71 step: 81 loss: 0.07886076 acc: 0.9653778076171875\n",
      "epoch: 71 step: 82 loss: 0.08676055 acc: 0.9666748046875\n",
      "epoch: 71 step: 83 loss: 0.09456405 acc: 0.9627952575683594\n",
      "epoch: 71 step: 84 loss: 0.082231246 acc: 0.9653511047363281\n",
      "epoch: 71 step: 85 loss: 0.084925264 acc: 0.9575424194335938\n",
      "epoch: 71 step: 86 loss: 0.09697116 acc: 0.9543304443359375\n",
      "epoch: 71 step: 87 loss: 0.092215896 acc: 0.9658432006835938\n",
      "epoch: 71 step: 88 loss: 0.07897506 acc: 0.9669647216796875\n",
      "epoch: 71 step: 89 loss: 0.08082317 acc: 0.9653701782226562\n",
      "epoch: 71 step: 90 loss: 0.08401966 acc: 0.966522216796875\n",
      "epoch: 71 step: 91 loss: 0.08107049 acc: 0.9612045288085938\n",
      "epoch: 71 step: 92 loss: 0.0692533 acc: 0.9696159362792969\n",
      "epoch: 71 step: 93 loss: 0.06948745 acc: 0.9683685302734375\n",
      "epoch: 71 step: 94 loss: 0.074174024 acc: 0.9657630920410156\n",
      "epoch: 71 step: 95 loss: 0.079511136 acc: 0.9658203125\n",
      "epoch: 71 step: 96 loss: 0.08521855 acc: 0.9599380493164062\n",
      "epoch: 71 step: 97 loss: 0.0970693 acc: 0.9629096984863281\n",
      "epoch: 71 step: 98 loss: 0.07655431 acc: 0.9658317565917969\n",
      "epoch: 71 step: 99 loss: 0.10456728 acc: 0.9592361450195312\n",
      "epoch: 71 step: 100 loss: 0.084467396 acc: 0.9580802917480469\n",
      "epoch: 71 step: 101 loss: 0.090275824 acc: 0.9612770080566406\n",
      "epoch: 71 step: 102 loss: 0.080507495 acc: 0.9643898010253906\n",
      "epoch: 71 step: 103 loss: 0.09564553 acc: 0.9585418701171875\n",
      "epoch: 71 step: 104 loss: 0.08817823 acc: 0.9690284729003906\n",
      "epoch: 71 step: 105 loss: 0.08671433 acc: 0.9636764526367188\n",
      "epoch: 71 step: 106 loss: 0.0937726 acc: 0.9671173095703125\n",
      "epoch: 71 step: 107 loss: 0.10486065 acc: 0.9513816833496094\n",
      "epoch: 71 step: 108 loss: 0.076478876 acc: 0.9675750732421875\n",
      "epoch: 71 step: 109 loss: 0.0923308 acc: 0.96234130859375\n",
      "epoch: 71 step: 110 loss: 0.09083403 acc: 0.9606513977050781\n",
      "epoch: 71 step: 111 loss: 0.0897726 acc: 0.9677810668945312\n",
      "epoch: 71 step: 112 loss: 0.078018636 acc: 0.9659309387207031\n",
      "epoch: 71 step: 113 loss: 0.11510962 acc: 0.9554939270019531\n",
      "epoch: 71 step: 114 loss: 0.08901446 acc: 0.96295166015625\n",
      "epoch: 71 step: 115 loss: 0.087631956 acc: 0.9593582153320312\n",
      "epoch: 71 step: 116 loss: 0.08202828 acc: 0.9631118774414062\n",
      "epoch: 71 step: 117 loss: 0.08023858 acc: 0.9641609191894531\n",
      "epoch: 71 step: 118 loss: 0.0932068 acc: 0.9572067260742188\n",
      "epoch: 71 step: 119 loss: 0.08293264 acc: 0.96148681640625\n",
      "epoch: 71 step: 120 loss: 0.07997543 acc: 0.9663047790527344\n",
      "epoch: 71 step: 121 loss: 0.10511407 acc: 0.9530563354492188\n",
      "epoch: 71 step: 122 loss: 0.092475675 acc: 0.9601325988769531\n",
      "epoch: 71 step: 123 loss: 0.081538565 acc: 0.9676246643066406\n",
      "epoch: 71 step: 124 loss: 0.08056302 acc: 0.9639543805803571\n",
      "epoch: 71 validation_loss: 0.094 validation_dice: 0.8529219551384317\n",
      "epoch: 71 test_dataset dice: 0.7437507697576473\n",
      "time cost 0.5371615688006083 min\n",
      "dice_best: 0.8551517496360885\n",
      "******************************** epoch  71  is finished. *********************************\n",
      "epoch: 72 step: 1 loss: 0.07719993 acc: 0.9695701599121094\n",
      "epoch: 72 step: 2 loss: 0.07302122 acc: 0.9712104797363281\n",
      "epoch: 72 step: 3 loss: 0.083212726 acc: 0.9704360961914062\n",
      "epoch: 72 step: 4 loss: 0.08820849 acc: 0.9654579162597656\n",
      "epoch: 72 step: 5 loss: 0.09256855 acc: 0.963531494140625\n",
      "epoch: 72 step: 6 loss: 0.103517145 acc: 0.9638290405273438\n",
      "epoch: 72 step: 7 loss: 0.08394561 acc: 0.958984375\n",
      "epoch: 72 step: 8 loss: 0.090307616 acc: 0.953033447265625\n",
      "epoch: 72 step: 9 loss: 0.0867133 acc: 0.9595603942871094\n",
      "epoch: 72 step: 10 loss: 0.08055503 acc: 0.9624557495117188\n",
      "epoch: 72 step: 11 loss: 0.083574176 acc: 0.9621963500976562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 72 step: 12 loss: 0.09187533 acc: 0.9591865539550781\n",
      "epoch: 72 step: 13 loss: 0.088717066 acc: 0.9593086242675781\n",
      "epoch: 72 step: 14 loss: 0.109215915 acc: 0.9558601379394531\n",
      "epoch: 72 step: 15 loss: 0.09741312 acc: 0.9575462341308594\n",
      "epoch: 72 step: 16 loss: 0.08280743 acc: 0.9586448669433594\n",
      "epoch: 72 step: 17 loss: 0.07802549 acc: 0.9596023559570312\n",
      "epoch: 72 step: 18 loss: 0.07930088 acc: 0.9646377563476562\n",
      "epoch: 72 step: 19 loss: 0.07536526 acc: 0.9650192260742188\n",
      "epoch: 72 step: 20 loss: 0.08683633 acc: 0.9625930786132812\n",
      "epoch: 72 step: 21 loss: 0.08808625 acc: 0.9619827270507812\n",
      "epoch: 72 step: 22 loss: 0.08436202 acc: 0.9690170288085938\n",
      "epoch: 72 step: 23 loss: 0.077688225 acc: 0.9704017639160156\n",
      "epoch: 72 step: 24 loss: 0.07588786 acc: 0.9681510925292969\n",
      "epoch: 72 step: 25 loss: 0.10367788 acc: 0.9722366333007812\n",
      "epoch: 72 step: 26 loss: 0.110518254 acc: 0.9568710327148438\n",
      "epoch: 72 step: 27 loss: 0.09444177 acc: 0.9606208801269531\n",
      "epoch: 72 step: 28 loss: 0.07395711 acc: 0.9639320373535156\n",
      "epoch: 72 step: 29 loss: 0.08586215 acc: 0.9632759094238281\n",
      "epoch: 72 step: 30 loss: 0.087332584 acc: 0.9537086486816406\n",
      "epoch: 72 step: 31 loss: 0.081064284 acc: 0.9611778259277344\n",
      "epoch: 72 step: 32 loss: 0.09738935 acc: 0.9470558166503906\n",
      "epoch: 72 step: 33 loss: 0.08436516 acc: 0.9601478576660156\n",
      "epoch: 72 step: 34 loss: 0.0871047 acc: 0.9600791931152344\n",
      "epoch: 72 step: 35 loss: 0.07532472 acc: 0.9719314575195312\n",
      "epoch: 72 step: 36 loss: 0.093008734 acc: 0.9621238708496094\n",
      "epoch: 72 step: 37 loss: 0.089645304 acc: 0.9673233032226562\n",
      "epoch: 72 step: 38 loss: 0.087268196 acc: 0.9620704650878906\n",
      "epoch: 72 step: 39 loss: 0.08065521 acc: 0.9628410339355469\n",
      "epoch: 72 step: 40 loss: 0.07904909 acc: 0.9719352722167969\n",
      "epoch: 72 step: 41 loss: 0.076684766 acc: 0.9671707153320312\n",
      "epoch: 72 step: 42 loss: 0.07182239 acc: 0.967803955078125\n",
      "epoch: 72 step: 43 loss: 0.114895165 acc: 0.9556045532226562\n",
      "epoch: 72 step: 44 loss: 0.09628511 acc: 0.9519615173339844\n",
      "epoch: 72 step: 45 loss: 0.08675334 acc: 0.9592437744140625\n",
      "epoch: 72 step: 46 loss: 0.06468615 acc: 0.9724655151367188\n",
      "epoch: 72 step: 47 loss: 0.10290149 acc: 0.9585800170898438\n",
      "epoch: 72 step: 48 loss: 0.08046408 acc: 0.9626541137695312\n",
      "epoch: 72 step: 49 loss: 0.08260257 acc: 0.9631881713867188\n",
      "epoch: 72 step: 50 loss: 0.08923954 acc: 0.9638595581054688\n",
      "epoch: 72 step: 51 loss: 0.0837172 acc: 0.9710731506347656\n",
      "epoch: 72 step: 52 loss: 0.10001596 acc: 0.9574508666992188\n",
      "epoch: 72 step: 53 loss: 0.10768747 acc: 0.9654693603515625\n",
      "epoch: 72 step: 54 loss: 0.08261932 acc: 0.9609031677246094\n",
      "epoch: 72 step: 55 loss: 0.072468415 acc: 0.9650726318359375\n",
      "epoch: 72 step: 56 loss: 0.080453165 acc: 0.9606132507324219\n",
      "epoch: 72 step: 57 loss: 0.09163557 acc: 0.9685325622558594\n",
      "epoch: 72 step: 58 loss: 0.07910713 acc: 0.9647407531738281\n",
      "epoch: 72 step: 59 loss: 0.08720413 acc: 0.9625282287597656\n",
      "epoch: 72 step: 60 loss: 0.089063615 acc: 0.9665145874023438\n",
      "epoch: 72 step: 61 loss: 0.10690765 acc: 0.9543304443359375\n",
      "epoch: 72 step: 62 loss: 0.08767416 acc: 0.9642333984375\n",
      "epoch: 72 step: 63 loss: 0.07370188 acc: 0.9644050598144531\n",
      "epoch: 72 step: 64 loss: 0.08101943 acc: 0.9630203247070312\n",
      "epoch: 72 step: 65 loss: 0.08783511 acc: 0.9663352966308594\n",
      "epoch: 72 step: 66 loss: 0.08083768 acc: 0.9650115966796875\n",
      "epoch: 72 step: 67 loss: 0.08295911 acc: 0.9634933471679688\n",
      "epoch: 72 step: 68 loss: 0.094189055 acc: 0.9635124206542969\n",
      "epoch: 72 step: 69 loss: 0.07346119 acc: 0.9666786193847656\n",
      "epoch: 72 step: 70 loss: 0.08118452 acc: 0.9653205871582031\n",
      "epoch: 72 step: 71 loss: 0.052157924 acc: 0.9772682189941406\n",
      "epoch: 72 step: 72 loss: 0.085404135 acc: 0.9643287658691406\n",
      "epoch: 72 step: 73 loss: 0.080122136 acc: 0.9626312255859375\n",
      "epoch: 72 step: 74 loss: 0.07764517 acc: 0.9687995910644531\n",
      "epoch: 72 step: 75 loss: 0.10476682 acc: 0.9475517272949219\n",
      "epoch: 72 step: 76 loss: 0.09411631 acc: 0.9577445983886719\n",
      "epoch: 72 step: 77 loss: 0.107119724 acc: 0.9475059509277344\n",
      "epoch: 72 step: 78 loss: 0.08033396 acc: 0.9625015258789062\n",
      "epoch: 72 step: 79 loss: 0.07873084 acc: 0.9657325744628906\n",
      "epoch: 72 step: 80 loss: 0.08580925 acc: 0.9591751098632812\n",
      "epoch: 72 step: 81 loss: 0.0893433 acc: 0.9648094177246094\n",
      "epoch: 72 step: 82 loss: 0.07927745 acc: 0.9652099609375\n",
      "epoch: 72 step: 83 loss: 0.07341723 acc: 0.9686317443847656\n",
      "epoch: 72 step: 84 loss: 0.07860013 acc: 0.9671096801757812\n",
      "epoch: 72 step: 85 loss: 0.07002362 acc: 0.9734382629394531\n",
      "epoch: 72 step: 86 loss: 0.0760954 acc: 0.967376708984375\n",
      "epoch: 72 step: 87 loss: 0.08105378 acc: 0.9679107666015625\n",
      "epoch: 72 step: 88 loss: 0.07430864 acc: 0.9637489318847656\n",
      "epoch: 72 step: 89 loss: 0.109577864 acc: 0.9563140869140625\n",
      "epoch: 72 step: 90 loss: 0.08864292 acc: 0.9612846374511719\n",
      "epoch: 72 step: 91 loss: 0.0806259 acc: 0.9643783569335938\n",
      "epoch: 72 step: 92 loss: 0.084622465 acc: 0.9679985046386719\n",
      "epoch: 72 step: 93 loss: 0.081569314 acc: 0.9601821899414062\n",
      "epoch: 72 step: 94 loss: 0.08935939 acc: 0.9592018127441406\n",
      "epoch: 72 step: 95 loss: 0.07711238 acc: 0.9673347473144531\n",
      "epoch: 72 step: 96 loss: 0.09287991 acc: 0.9582328796386719\n",
      "epoch: 72 step: 97 loss: 0.076624736 acc: 0.9692153930664062\n",
      "epoch: 72 step: 98 loss: 0.08956941 acc: 0.9640274047851562\n",
      "epoch: 72 step: 99 loss: 0.0945128 acc: 0.9644889831542969\n",
      "epoch: 72 step: 100 loss: 0.087450616 acc: 0.9621505737304688\n",
      "epoch: 72 step: 101 loss: 0.08407323 acc: 0.9566230773925781\n",
      "epoch: 72 step: 102 loss: 0.07563988 acc: 0.9710426330566406\n",
      "epoch: 72 step: 103 loss: 0.087743744 acc: 0.9616813659667969\n",
      "epoch: 72 step: 104 loss: 0.07627304 acc: 0.9664726257324219\n",
      "epoch: 72 step: 105 loss: 0.08171951 acc: 0.9669685363769531\n",
      "epoch: 72 step: 106 loss: 0.07561001 acc: 0.964202880859375\n",
      "epoch: 72 step: 107 loss: 0.084751636 acc: 0.9676971435546875\n",
      "epoch: 72 step: 108 loss: 0.07923027 acc: 0.9665641784667969\n",
      "epoch: 72 step: 109 loss: 0.103244126 acc: 0.9576187133789062\n",
      "epoch: 72 step: 110 loss: 0.07930806 acc: 0.9633445739746094\n",
      "epoch: 72 step: 111 loss: 0.08289669 acc: 0.9605216979980469\n",
      "epoch: 72 step: 112 loss: 0.07793756 acc: 0.9614677429199219\n",
      "epoch: 72 step: 113 loss: 0.1052634 acc: 0.9517898559570312\n",
      "epoch: 72 step: 114 loss: 0.0948134 acc: 0.9553947448730469\n",
      "epoch: 72 step: 115 loss: 0.09712131 acc: 0.9557037353515625\n",
      "epoch: 72 step: 116 loss: 0.08066226 acc: 0.959808349609375\n",
      "epoch: 72 step: 117 loss: 0.08911729 acc: 0.9537200927734375\n",
      "epoch: 72 step: 118 loss: 0.08066809 acc: 0.9632606506347656\n",
      "epoch: 72 step: 119 loss: 0.103641056 acc: 0.9527549743652344\n",
      "epoch: 72 step: 120 loss: 0.08857859 acc: 0.9637718200683594\n",
      "epoch: 72 step: 121 loss: 0.085183725 acc: 0.9631004333496094\n",
      "epoch: 72 step: 122 loss: 0.06786389 acc: 0.9718704223632812\n",
      "epoch: 72 step: 123 loss: 0.08952975 acc: 0.9613265991210938\n",
      "epoch: 72 step: 124 loss: 0.1025774 acc: 0.9738682338169643\n",
      "epoch: 72 validation_loss: 0.088 validation_dice: 0.8409435382353344\n",
      "epoch: 72 test_dataset dice: 0.752859648552161\n",
      "time cost 0.5360352396965027 min\n",
      "dice_best: 0.8551517496360885\n",
      "******************************** epoch  72  is finished. *********************************\n",
      "epoch: 73 step: 1 loss: 0.09369023 acc: 0.9658164978027344\n",
      "epoch: 73 step: 2 loss: 0.07873529 acc: 0.9697418212890625\n",
      "epoch: 73 step: 3 loss: 0.082967855 acc: 0.9633445739746094\n",
      "epoch: 73 step: 4 loss: 0.08558401 acc: 0.9553871154785156\n",
      "epoch: 73 step: 5 loss: 0.078094125 acc: 0.9633064270019531\n",
      "epoch: 73 step: 6 loss: 0.09491813 acc: 0.9565620422363281\n",
      "epoch: 73 step: 7 loss: 0.088603795 acc: 0.9654541015625\n",
      "epoch: 73 step: 8 loss: 0.092671596 acc: 0.9600753784179688\n",
      "epoch: 73 step: 9 loss: 0.093765214 acc: 0.9588050842285156\n",
      "epoch: 73 step: 10 loss: 0.0737342 acc: 0.9631080627441406\n",
      "epoch: 73 step: 11 loss: 0.07992738 acc: 0.9630279541015625\n",
      "epoch: 73 step: 12 loss: 0.07425995 acc: 0.9686546325683594\n",
      "epoch: 73 step: 13 loss: 0.07489592 acc: 0.9641609191894531\n",
      "epoch: 73 step: 14 loss: 0.06955822 acc: 0.9692230224609375\n",
      "epoch: 73 step: 15 loss: 0.07590607 acc: 0.9657211303710938\n",
      "epoch: 73 step: 16 loss: 0.09337482 acc: 0.9660568237304688\n",
      "epoch: 73 step: 17 loss: 0.07533378 acc: 0.9618797302246094\n",
      "epoch: 73 step: 18 loss: 0.10042756 acc: 0.9587211608886719\n",
      "epoch: 73 step: 19 loss: 0.082914285 acc: 0.970550537109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 73 step: 20 loss: 0.076923355 acc: 0.9583969116210938\n",
      "epoch: 73 step: 21 loss: 0.07875494 acc: 0.9726409912109375\n",
      "epoch: 73 step: 22 loss: 0.07376989 acc: 0.9635200500488281\n",
      "epoch: 73 step: 23 loss: 0.08054414 acc: 0.9692039489746094\n",
      "epoch: 73 step: 24 loss: 0.07851974 acc: 0.9632911682128906\n",
      "epoch: 73 step: 25 loss: 0.07524618 acc: 0.9654426574707031\n",
      "epoch: 73 step: 26 loss: 0.0758418 acc: 0.9665870666503906\n",
      "epoch: 73 step: 27 loss: 0.09158166 acc: 0.9637451171875\n",
      "epoch: 73 step: 28 loss: 0.065221146 acc: 0.9738960266113281\n",
      "epoch: 73 step: 29 loss: 0.08693766 acc: 0.9669380187988281\n",
      "epoch: 73 step: 30 loss: 0.08022876 acc: 0.9603385925292969\n",
      "epoch: 73 step: 31 loss: 0.076910414 acc: 0.9705543518066406\n",
      "epoch: 73 step: 32 loss: 0.083250366 acc: 0.96173095703125\n",
      "epoch: 73 step: 33 loss: 0.07648199 acc: 0.9630966186523438\n",
      "epoch: 73 step: 34 loss: 0.10080462 acc: 0.9553413391113281\n",
      "epoch: 73 step: 35 loss: 0.092642546 acc: 0.9526138305664062\n",
      "epoch: 73 step: 36 loss: 0.093032 acc: 0.9507179260253906\n",
      "epoch: 73 step: 37 loss: 0.09051091 acc: 0.9548683166503906\n",
      "epoch: 73 step: 38 loss: 0.089202456 acc: 0.96539306640625\n",
      "epoch: 73 step: 39 loss: 0.1009988 acc: 0.9586601257324219\n",
      "epoch: 73 step: 40 loss: 0.09282966 acc: 0.9553337097167969\n",
      "epoch: 73 step: 41 loss: 0.08962517 acc: 0.9599533081054688\n",
      "epoch: 73 step: 42 loss: 0.08921113 acc: 0.9617424011230469\n",
      "epoch: 73 step: 43 loss: 0.09199591 acc: 0.9594612121582031\n",
      "epoch: 73 step: 44 loss: 0.09003804 acc: 0.9622726440429688\n",
      "epoch: 73 step: 45 loss: 0.094553255 acc: 0.96453857421875\n",
      "epoch: 73 step: 46 loss: 0.07431823 acc: 0.9646720886230469\n",
      "epoch: 73 step: 47 loss: 0.07501826 acc: 0.9637718200683594\n",
      "epoch: 73 step: 48 loss: 0.07862866 acc: 0.9639396667480469\n",
      "epoch: 73 step: 49 loss: 0.083348095 acc: 0.9657096862792969\n",
      "epoch: 73 step: 50 loss: 0.07821144 acc: 0.9630126953125\n",
      "epoch: 73 step: 51 loss: 0.094960414 acc: 0.9611053466796875\n",
      "epoch: 73 step: 52 loss: 0.07616087 acc: 0.9685134887695312\n",
      "epoch: 73 step: 53 loss: 0.08089174 acc: 0.9662971496582031\n",
      "epoch: 73 step: 54 loss: 0.07697282 acc: 0.9641799926757812\n",
      "epoch: 73 step: 55 loss: 0.09657816 acc: 0.9562263488769531\n",
      "epoch: 73 step: 56 loss: 0.08859729 acc: 0.9574241638183594\n",
      "epoch: 73 step: 57 loss: 0.084251136 acc: 0.9628181457519531\n",
      "epoch: 73 step: 58 loss: 0.092897706 acc: 0.96307373046875\n",
      "epoch: 73 step: 59 loss: 0.094761185 acc: 0.9546585083007812\n",
      "epoch: 73 step: 60 loss: 0.0805887 acc: 0.9598007202148438\n",
      "epoch: 73 step: 61 loss: 0.08147237 acc: 0.9651756286621094\n",
      "epoch: 73 step: 62 loss: 0.08681369 acc: 0.9667015075683594\n",
      "epoch: 73 step: 63 loss: 0.07119704 acc: 0.9660491943359375\n",
      "epoch: 73 step: 64 loss: 0.079886936 acc: 0.9608535766601562\n",
      "epoch: 73 step: 65 loss: 0.07186133 acc: 0.9680824279785156\n",
      "epoch: 73 step: 66 loss: 0.07743811 acc: 0.9598884582519531\n",
      "epoch: 73 step: 67 loss: 0.08677969 acc: 0.96185302734375\n",
      "epoch: 73 step: 68 loss: 0.0919731 acc: 0.959136962890625\n",
      "epoch: 73 step: 69 loss: 0.08561468 acc: 0.9651565551757812\n",
      "epoch: 73 step: 70 loss: 0.0964032 acc: 0.9543380737304688\n",
      "epoch: 73 step: 71 loss: 0.08292249 acc: 0.96142578125\n",
      "epoch: 73 step: 72 loss: 0.084616534 acc: 0.962249755859375\n",
      "epoch: 73 step: 73 loss: 0.081729814 acc: 0.970855712890625\n",
      "epoch: 73 step: 74 loss: 0.07691064 acc: 0.9700393676757812\n",
      "epoch: 73 step: 75 loss: 0.09216107 acc: 0.9631195068359375\n",
      "epoch: 73 step: 76 loss: 0.0756148 acc: 0.965362548828125\n",
      "epoch: 73 step: 77 loss: 0.093925394 acc: 0.9607276916503906\n",
      "epoch: 73 step: 78 loss: 0.07013679 acc: 0.9711456298828125\n",
      "epoch: 73 step: 79 loss: 0.083539076 acc: 0.9635009765625\n",
      "epoch: 73 step: 80 loss: 0.084789336 acc: 0.9616928100585938\n",
      "epoch: 73 step: 81 loss: 0.070982635 acc: 0.9692420959472656\n",
      "epoch: 73 step: 82 loss: 0.08196162 acc: 0.9568748474121094\n",
      "epoch: 73 step: 83 loss: 0.08731342 acc: 0.9597320556640625\n",
      "epoch: 73 step: 84 loss: 0.07991401 acc: 0.9647712707519531\n",
      "epoch: 73 step: 85 loss: 0.07840016 acc: 0.9670372009277344\n",
      "epoch: 73 step: 86 loss: 0.066054754 acc: 0.9772377014160156\n",
      "epoch: 73 step: 87 loss: 0.093407564 acc: 0.9618453979492188\n",
      "epoch: 73 step: 88 loss: 0.080312535 acc: 0.9625587463378906\n",
      "epoch: 73 step: 89 loss: 0.07458532 acc: 0.9679527282714844\n",
      "epoch: 73 step: 90 loss: 0.080997534 acc: 0.9701271057128906\n",
      "epoch: 73 step: 91 loss: 0.07348065 acc: 0.9659690856933594\n",
      "epoch: 73 step: 92 loss: 0.08340566 acc: 0.9677238464355469\n",
      "epoch: 73 step: 93 loss: 0.08358686 acc: 0.9634132385253906\n",
      "epoch: 73 step: 94 loss: 0.08136672 acc: 0.9583511352539062\n",
      "epoch: 73 step: 95 loss: 0.070377834 acc: 0.9642181396484375\n",
      "epoch: 73 step: 96 loss: 0.08153933 acc: 0.9566879272460938\n",
      "epoch: 73 step: 97 loss: 0.08318972 acc: 0.9611778259277344\n",
      "epoch: 73 step: 98 loss: 0.07344279 acc: 0.9665641784667969\n",
      "epoch: 73 step: 99 loss: 0.082063496 acc: 0.9632377624511719\n",
      "epoch: 73 step: 100 loss: 0.07996282 acc: 0.9569587707519531\n",
      "epoch: 73 step: 101 loss: 0.074181005 acc: 0.9702644348144531\n",
      "epoch: 73 step: 102 loss: 0.0730992 acc: 0.9688453674316406\n",
      "epoch: 73 step: 103 loss: 0.08393972 acc: 0.9675674438476562\n",
      "epoch: 73 step: 104 loss: 0.076994635 acc: 0.9633598327636719\n",
      "epoch: 73 step: 105 loss: 0.08545069 acc: 0.9681930541992188\n",
      "epoch: 73 step: 106 loss: 0.08138473 acc: 0.9626998901367188\n",
      "epoch: 73 step: 107 loss: 0.09744804 acc: 0.9585609436035156\n",
      "epoch: 73 step: 108 loss: 0.07520372 acc: 0.9635009765625\n",
      "epoch: 73 step: 109 loss: 0.07417724 acc: 0.9645156860351562\n",
      "epoch: 73 step: 110 loss: 0.06928472 acc: 0.9613075256347656\n",
      "epoch: 73 step: 111 loss: 0.082237 acc: 0.9646949768066406\n",
      "epoch: 73 step: 112 loss: 0.09637199 acc: 0.9648208618164062\n",
      "epoch: 73 step: 113 loss: 0.09724878 acc: 0.9581069946289062\n",
      "epoch: 73 step: 114 loss: 0.09173067 acc: 0.9639472961425781\n",
      "epoch: 73 step: 115 loss: 0.08151897 acc: 0.9564743041992188\n",
      "epoch: 73 step: 116 loss: 0.07552779 acc: 0.9697113037109375\n",
      "epoch: 73 step: 117 loss: 0.07774386 acc: 0.9640617370605469\n",
      "epoch: 73 step: 118 loss: 0.056571294 acc: 0.9774589538574219\n",
      "epoch: 73 step: 119 loss: 0.07839381 acc: 0.9670448303222656\n",
      "epoch: 73 step: 120 loss: 0.079108685 acc: 0.962005615234375\n",
      "epoch: 73 step: 121 loss: 0.08359076 acc: 0.96307373046875\n",
      "epoch: 73 step: 122 loss: 0.08032692 acc: 0.9615058898925781\n",
      "epoch: 73 step: 123 loss: 0.08730905 acc: 0.9576263427734375\n",
      "epoch: 73 step: 124 loss: 0.074937776 acc: 0.9592372349330357\n",
      "epoch: 73 validation_loss: 0.089 validation_dice: 0.8391077036610148\n",
      "epoch: 73 test_dataset dice: 0.7409769802746307\n",
      "time cost 0.5360893090565999 min\n",
      "dice_best: 0.8551517496360885\n",
      "******************************** epoch  73  is finished. *********************************\n",
      "epoch: 74 step: 1 loss: 0.08799202 acc: 0.9591217041015625\n",
      "epoch: 74 step: 2 loss: 0.088445716 acc: 0.9625205993652344\n",
      "epoch: 74 step: 3 loss: 0.06694243 acc: 0.9662284851074219\n",
      "epoch: 74 step: 4 loss: 0.08338716 acc: 0.9662437438964844\n",
      "epoch: 74 step: 5 loss: 0.066270396 acc: 0.9707145690917969\n",
      "epoch: 74 step: 6 loss: 0.07786675 acc: 0.9642372131347656\n",
      "epoch: 74 step: 7 loss: 0.08502825 acc: 0.967193603515625\n",
      "epoch: 74 step: 8 loss: 0.06937263 acc: 0.970123291015625\n",
      "epoch: 74 step: 9 loss: 0.084356636 acc: 0.9625167846679688\n",
      "epoch: 74 step: 10 loss: 0.082557626 acc: 0.9685630798339844\n",
      "epoch: 74 step: 11 loss: 0.072047986 acc: 0.9672164916992188\n",
      "epoch: 74 step: 12 loss: 0.09717183 acc: 0.9575576782226562\n",
      "epoch: 74 step: 13 loss: 0.09530081 acc: 0.9527473449707031\n",
      "epoch: 74 step: 14 loss: 0.095468536 acc: 0.955780029296875\n",
      "epoch: 74 step: 15 loss: 0.0769959 acc: 0.9596481323242188\n",
      "epoch: 74 step: 16 loss: 0.0819727 acc: 0.9658851623535156\n",
      "epoch: 74 step: 17 loss: 0.07485681 acc: 0.9606552124023438\n",
      "epoch: 74 step: 18 loss: 0.08874675 acc: 0.9550895690917969\n",
      "epoch: 74 step: 19 loss: 0.088911 acc: 0.9593925476074219\n",
      "epoch: 74 step: 20 loss: 0.06694587 acc: 0.9626197814941406\n",
      "epoch: 74 step: 21 loss: 0.08857521 acc: 0.9612159729003906\n",
      "epoch: 74 step: 22 loss: 0.07314109 acc: 0.9623832702636719\n",
      "epoch: 74 step: 23 loss: 0.08113665 acc: 0.9624786376953125\n",
      "epoch: 74 step: 24 loss: 0.065358594 acc: 0.9678382873535156\n",
      "epoch: 74 step: 25 loss: 0.06841624 acc: 0.9694252014160156\n",
      "epoch: 74 step: 26 loss: 0.08445117 acc: 0.9638175964355469\n",
      "epoch: 74 step: 27 loss: 0.08355511 acc: 0.961822509765625\n",
      "epoch: 74 step: 28 loss: 0.07664707 acc: 0.9673805236816406\n",
      "epoch: 74 step: 29 loss: 0.11850219 acc: 0.9554328918457031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 74 step: 30 loss: 0.09177344 acc: 0.9531631469726562\n",
      "epoch: 74 step: 31 loss: 0.08384557 acc: 0.9563026428222656\n",
      "epoch: 74 step: 32 loss: 0.07882787 acc: 0.9636497497558594\n",
      "epoch: 74 step: 33 loss: 0.08465316 acc: 0.9582290649414062\n",
      "epoch: 74 step: 34 loss: 0.074817844 acc: 0.9613571166992188\n",
      "epoch: 74 step: 35 loss: 0.097225696 acc: 0.9503822326660156\n",
      "epoch: 74 step: 36 loss: 0.08004946 acc: 0.96124267578125\n",
      "epoch: 74 step: 37 loss: 0.08601591 acc: 0.9592208862304688\n",
      "epoch: 74 step: 38 loss: 0.08225082 acc: 0.9636611938476562\n",
      "epoch: 74 step: 39 loss: 0.08082701 acc: 0.9643440246582031\n",
      "epoch: 74 step: 40 loss: 0.07229414 acc: 0.9682769775390625\n",
      "epoch: 74 step: 41 loss: 0.085087426 acc: 0.9651947021484375\n",
      "epoch: 74 step: 42 loss: 0.0878435 acc: 0.9598007202148438\n",
      "epoch: 74 step: 43 loss: 0.06588728 acc: 0.9713630676269531\n",
      "epoch: 74 step: 44 loss: 0.06847745 acc: 0.9720420837402344\n",
      "epoch: 74 step: 45 loss: 0.0653456 acc: 0.9757728576660156\n",
      "epoch: 74 step: 46 loss: 0.07870903 acc: 0.962493896484375\n",
      "epoch: 74 step: 47 loss: 0.07351758 acc: 0.96307373046875\n",
      "epoch: 74 step: 48 loss: 0.080031574 acc: 0.9616851806640625\n",
      "epoch: 74 step: 49 loss: 0.07836656 acc: 0.9607353210449219\n",
      "epoch: 74 step: 50 loss: 0.07200211 acc: 0.9649124145507812\n",
      "epoch: 74 step: 51 loss: 0.07241235 acc: 0.9675216674804688\n",
      "epoch: 74 step: 52 loss: 0.07039649 acc: 0.96551513671875\n",
      "epoch: 74 step: 53 loss: 0.08670589 acc: 0.9598159790039062\n",
      "epoch: 74 step: 54 loss: 0.08100649 acc: 0.958465576171875\n",
      "epoch: 74 step: 55 loss: 0.07644405 acc: 0.9624557495117188\n",
      "epoch: 74 step: 56 loss: 0.07163197 acc: 0.9699592590332031\n",
      "epoch: 74 step: 57 loss: 0.06478849 acc: 0.9696731567382812\n",
      "epoch: 74 step: 58 loss: 0.0676537 acc: 0.9703483581542969\n",
      "epoch: 74 step: 59 loss: 0.07081354 acc: 0.9690284729003906\n",
      "epoch: 74 step: 60 loss: 0.08007528 acc: 0.9662094116210938\n",
      "epoch: 74 step: 61 loss: 0.07687108 acc: 0.9649658203125\n",
      "epoch: 74 step: 62 loss: 0.074076734 acc: 0.969024658203125\n",
      "epoch: 74 step: 63 loss: 0.07517862 acc: 0.967559814453125\n",
      "epoch: 74 step: 64 loss: 0.08355529 acc: 0.9625129699707031\n",
      "epoch: 74 step: 65 loss: 0.08920424 acc: 0.9615859985351562\n",
      "epoch: 74 step: 66 loss: 0.067451425 acc: 0.9676971435546875\n",
      "epoch: 74 step: 67 loss: 0.07068056 acc: 0.9716835021972656\n",
      "epoch: 74 step: 68 loss: 0.08558286 acc: 0.9606056213378906\n",
      "epoch: 74 step: 69 loss: 0.0762083 acc: 0.9615325927734375\n",
      "epoch: 74 step: 70 loss: 0.087134905 acc: 0.9574432373046875\n",
      "epoch: 74 step: 71 loss: 0.07231011 acc: 0.9672698974609375\n",
      "epoch: 74 step: 72 loss: 0.08706249 acc: 0.95599365234375\n",
      "epoch: 74 step: 73 loss: 0.085932255 acc: 0.9629096984863281\n",
      "epoch: 74 step: 74 loss: 0.07376828 acc: 0.9662551879882812\n",
      "epoch: 74 step: 75 loss: 0.08720716 acc: 0.9604568481445312\n",
      "epoch: 74 step: 76 loss: 0.066903025 acc: 0.9689788818359375\n",
      "epoch: 74 step: 77 loss: 0.085205175 acc: 0.9593734741210938\n",
      "epoch: 74 step: 78 loss: 0.07715511 acc: 0.9670486450195312\n",
      "epoch: 74 step: 79 loss: 0.07427817 acc: 0.9664497375488281\n",
      "epoch: 74 step: 80 loss: 0.08619209 acc: 0.9611320495605469\n",
      "epoch: 74 step: 81 loss: 0.08352612 acc: 0.9600753784179688\n",
      "epoch: 74 step: 82 loss: 0.08686636 acc: 0.9655303955078125\n",
      "epoch: 74 step: 83 loss: 0.08078848 acc: 0.9603729248046875\n",
      "epoch: 74 step: 84 loss: 0.07388923 acc: 0.9682159423828125\n",
      "epoch: 74 step: 85 loss: 0.07040324 acc: 0.9653854370117188\n",
      "epoch: 74 step: 86 loss: 0.105064325 acc: 0.9492607116699219\n",
      "epoch: 74 step: 87 loss: 0.0715242 acc: 0.9608383178710938\n",
      "epoch: 74 step: 88 loss: 0.07237178 acc: 0.9639091491699219\n",
      "epoch: 74 step: 89 loss: 0.080224894 acc: 0.9661712646484375\n",
      "epoch: 74 step: 90 loss: 0.08083835 acc: 0.9604225158691406\n",
      "epoch: 74 step: 91 loss: 0.07331927 acc: 0.9723052978515625\n",
      "epoch: 74 step: 92 loss: 0.0929155 acc: 0.9658126831054688\n",
      "epoch: 74 step: 93 loss: 0.095613874 acc: 0.9577674865722656\n",
      "epoch: 74 step: 94 loss: 0.06457317 acc: 0.9761161804199219\n",
      "epoch: 74 step: 95 loss: 0.080803335 acc: 0.9567718505859375\n",
      "epoch: 74 step: 96 loss: 0.07946906 acc: 0.9644088745117188\n",
      "epoch: 74 step: 97 loss: 0.07973202 acc: 0.9697608947753906\n",
      "epoch: 74 step: 98 loss: 0.07899182 acc: 0.9553947448730469\n",
      "epoch: 74 step: 99 loss: 0.07577205 acc: 0.966705322265625\n",
      "epoch: 74 step: 100 loss: 0.078839496 acc: 0.9610404968261719\n",
      "epoch: 74 step: 101 loss: 0.08671829 acc: 0.962921142578125\n",
      "epoch: 74 step: 102 loss: 0.09803226 acc: 0.9564018249511719\n",
      "epoch: 74 step: 103 loss: 0.104115084 acc: 0.9518013000488281\n",
      "epoch: 74 step: 104 loss: 0.06516388 acc: 0.9711380004882812\n",
      "epoch: 74 step: 105 loss: 0.07502238 acc: 0.9698104858398438\n",
      "epoch: 74 step: 106 loss: 0.0915742 acc: 0.9598426818847656\n",
      "epoch: 74 step: 107 loss: 0.070279606 acc: 0.9731979370117188\n",
      "epoch: 74 step: 108 loss: 0.07497046 acc: 0.9742774963378906\n",
      "epoch: 74 step: 109 loss: 0.07703088 acc: 0.9725570678710938\n",
      "epoch: 74 step: 110 loss: 0.10709764 acc: 0.9670486450195312\n",
      "epoch: 74 step: 111 loss: 0.0815698 acc: 0.9651947021484375\n",
      "epoch: 74 step: 112 loss: 0.08182658 acc: 0.9595146179199219\n",
      "epoch: 74 step: 113 loss: 0.08497066 acc: 0.9600372314453125\n",
      "epoch: 74 step: 114 loss: 0.09171267 acc: 0.9555473327636719\n",
      "epoch: 74 step: 115 loss: 0.081482 acc: 0.9558792114257812\n",
      "epoch: 74 step: 116 loss: 0.07327581 acc: 0.9698486328125\n",
      "epoch: 74 step: 117 loss: 0.08023582 acc: 0.9650192260742188\n",
      "epoch: 74 step: 118 loss: 0.09248968 acc: 0.9646034240722656\n",
      "epoch: 74 step: 119 loss: 0.0748108 acc: 0.9686851501464844\n",
      "epoch: 74 step: 120 loss: 0.08160228 acc: 0.9665985107421875\n",
      "epoch: 74 step: 121 loss: 0.10343874 acc: 0.9632606506347656\n",
      "epoch: 74 step: 122 loss: 0.07651874 acc: 0.9767112731933594\n",
      "epoch: 74 step: 123 loss: 0.10378499 acc: 0.9562301635742188\n",
      "epoch: 74 step: 124 loss: 0.08707429 acc: 0.9576241629464286\n",
      "epoch: 74 validation_loss: 0.087 validation_dice: 0.8500159861854446\n",
      "epoch: 74 test_dataset dice: 0.7437908320869837\n",
      "time cost 0.5370363275210063 min\n",
      "dice_best: 0.8551517496360885\n",
      "******************************** epoch  74  is finished. *********************************\n",
      "epoch: 75 step: 1 loss: 0.07343612 acc: 0.9643745422363281\n",
      "epoch: 75 step: 2 loss: 0.08648198 acc: 0.9586181640625\n",
      "epoch: 75 step: 3 loss: 0.08653632 acc: 0.9579544067382812\n",
      "epoch: 75 step: 4 loss: 0.08511676 acc: 0.9583396911621094\n",
      "epoch: 75 step: 5 loss: 0.08555536 acc: 0.9589385986328125\n",
      "epoch: 75 step: 6 loss: 0.066185705 acc: 0.9661521911621094\n",
      "epoch: 75 step: 7 loss: 0.10733454 acc: 0.9498634338378906\n",
      "epoch: 75 step: 8 loss: 0.0743194 acc: 0.9646873474121094\n",
      "epoch: 75 step: 9 loss: 0.09345001 acc: 0.9649162292480469\n",
      "epoch: 75 step: 10 loss: 0.11275985 acc: 0.9577140808105469\n",
      "epoch: 75 step: 11 loss: 0.08279629 acc: 0.9662094116210938\n",
      "epoch: 75 step: 12 loss: 0.08809673 acc: 0.9626312255859375\n",
      "epoch: 75 step: 13 loss: 0.08416135 acc: 0.9682655334472656\n",
      "epoch: 75 step: 14 loss: 0.08298796 acc: 0.9607315063476562\n",
      "epoch: 75 step: 15 loss: 0.11050298 acc: 0.9640274047851562\n",
      "epoch: 75 step: 16 loss: 0.07801275 acc: 0.9615936279296875\n",
      "epoch: 75 step: 17 loss: 0.080559015 acc: 0.9628829956054688\n",
      "epoch: 75 step: 18 loss: 0.07845821 acc: 0.9662094116210938\n",
      "epoch: 75 step: 19 loss: 0.09399645 acc: 0.9571533203125\n",
      "epoch: 75 step: 20 loss: 0.0814184 acc: 0.9681816101074219\n",
      "epoch: 75 step: 21 loss: 0.08007337 acc: 0.9705162048339844\n",
      "epoch: 75 step: 22 loss: 0.08632337 acc: 0.9698944091796875\n",
      "epoch: 75 step: 23 loss: 0.07763615 acc: 0.9685935974121094\n",
      "epoch: 75 step: 24 loss: 0.07904561 acc: 0.9718055725097656\n",
      "epoch: 75 step: 25 loss: 0.10710475 acc: 0.9622764587402344\n",
      "epoch: 75 step: 26 loss: 0.088078365 acc: 0.9668769836425781\n",
      "epoch: 75 step: 27 loss: 0.092463635 acc: 0.9626312255859375\n",
      "epoch: 75 step: 28 loss: 0.082381636 acc: 0.9597740173339844\n",
      "epoch: 75 step: 29 loss: 0.114204675 acc: 0.946014404296875\n",
      "epoch: 75 step: 30 loss: 0.080144875 acc: 0.9550285339355469\n",
      "epoch: 75 step: 31 loss: 0.10598035 acc: 0.9513397216796875\n",
      "epoch: 75 step: 32 loss: 0.100054674 acc: 0.9556350708007812\n",
      "epoch: 75 step: 33 loss: 0.08740298 acc: 0.9621658325195312\n",
      "epoch: 75 step: 34 loss: 0.102859266 acc: 0.9665412902832031\n",
      "epoch: 75 step: 35 loss: 0.08106591 acc: 0.9661026000976562\n",
      "epoch: 75 step: 36 loss: 0.09466782 acc: 0.9619483947753906\n",
      "epoch: 75 step: 37 loss: 0.08777042 acc: 0.9674301147460938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 75 step: 38 loss: 0.091260575 acc: 0.9617652893066406\n",
      "epoch: 75 step: 39 loss: 0.094855845 acc: 0.9649238586425781\n",
      "epoch: 75 step: 40 loss: 0.0798313 acc: 0.9607505798339844\n",
      "epoch: 75 step: 41 loss: 0.07882876 acc: 0.9644737243652344\n",
      "epoch: 75 step: 42 loss: 0.09574153 acc: 0.9529151916503906\n",
      "epoch: 75 step: 43 loss: 0.0884566 acc: 0.9601020812988281\n",
      "epoch: 75 step: 44 loss: 0.091231376 acc: 0.9596443176269531\n",
      "epoch: 75 step: 45 loss: 0.070647396 acc: 0.9658317565917969\n",
      "epoch: 75 step: 46 loss: 0.08436018 acc: 0.9669876098632812\n",
      "epoch: 75 step: 47 loss: 0.08238084 acc: 0.9656791687011719\n",
      "epoch: 75 step: 48 loss: 0.0924863 acc: 0.9631614685058594\n",
      "epoch: 75 step: 49 loss: 0.0989216 acc: 0.9638748168945312\n",
      "epoch: 75 step: 50 loss: 0.07668949 acc: 0.9616165161132812\n",
      "epoch: 75 step: 51 loss: 0.0828971 acc: 0.9663314819335938\n",
      "epoch: 75 step: 52 loss: 0.108442195 acc: 0.9610710144042969\n",
      "epoch: 75 step: 53 loss: 0.090529494 acc: 0.9627532958984375\n",
      "epoch: 75 step: 54 loss: 0.08951423 acc: 0.9591751098632812\n",
      "epoch: 75 step: 55 loss: 0.0942568 acc: 0.9566497802734375\n",
      "epoch: 75 step: 56 loss: 0.09272848 acc: 0.9514579772949219\n",
      "epoch: 75 step: 57 loss: 0.0838749 acc: 0.9642601013183594\n",
      "epoch: 75 step: 58 loss: 0.07936254 acc: 0.9560165405273438\n",
      "epoch: 75 step: 59 loss: 0.07929591 acc: 0.9647750854492188\n",
      "epoch: 75 step: 60 loss: 0.09605501 acc: 0.9616470336914062\n",
      "epoch: 75 step: 61 loss: 0.079295285 acc: 0.958404541015625\n",
      "epoch: 75 step: 62 loss: 0.08922434 acc: 0.9581413269042969\n",
      "epoch: 75 step: 63 loss: 0.088264585 acc: 0.9558067321777344\n",
      "epoch: 75 step: 64 loss: 0.09788613 acc: 0.9592323303222656\n",
      "epoch: 75 step: 65 loss: 0.09072527 acc: 0.9535064697265625\n",
      "epoch: 75 step: 66 loss: 0.08734756 acc: 0.9626884460449219\n",
      "epoch: 75 step: 67 loss: 0.08220105 acc: 0.958343505859375\n",
      "epoch: 75 step: 68 loss: 0.07898173 acc: 0.9594993591308594\n",
      "epoch: 75 step: 69 loss: 0.083680466 acc: 0.9594345092773438\n",
      "epoch: 75 step: 70 loss: 0.08335256 acc: 0.9650802612304688\n",
      "epoch: 75 step: 71 loss: 0.06892031 acc: 0.9683570861816406\n",
      "epoch: 75 step: 72 loss: 0.09906667 acc: 0.9608802795410156\n",
      "epoch: 75 step: 73 loss: 0.06620173 acc: 0.9735603332519531\n",
      "epoch: 75 step: 74 loss: 0.06783209 acc: 0.9707298278808594\n",
      "epoch: 75 step: 75 loss: 0.08122372 acc: 0.9678001403808594\n",
      "epoch: 75 step: 76 loss: 0.07541529 acc: 0.9671478271484375\n",
      "epoch: 75 step: 77 loss: 0.06716568 acc: 0.9687919616699219\n",
      "epoch: 75 step: 78 loss: 0.07137002 acc: 0.9674606323242188\n",
      "epoch: 75 step: 79 loss: 0.071315706 acc: 0.9669570922851562\n",
      "epoch: 75 step: 80 loss: 0.08334496 acc: 0.96600341796875\n",
      "epoch: 75 step: 81 loss: 0.08337172 acc: 0.9625740051269531\n",
      "epoch: 75 step: 82 loss: 0.09830068 acc: 0.9628334045410156\n",
      "epoch: 75 step: 83 loss: 0.065364234 acc: 0.9743309020996094\n",
      "epoch: 75 step: 84 loss: 0.061339542 acc: 0.97186279296875\n",
      "epoch: 75 step: 85 loss: 0.07751747 acc: 0.9668655395507812\n",
      "epoch: 75 step: 86 loss: 0.0807599 acc: 0.9635772705078125\n",
      "epoch: 75 step: 87 loss: 0.07179955 acc: 0.9667510986328125\n",
      "epoch: 75 step: 88 loss: 0.07864245 acc: 0.9680099487304688\n",
      "epoch: 75 step: 89 loss: 0.089305654 acc: 0.9639778137207031\n",
      "epoch: 75 step: 90 loss: 0.07679217 acc: 0.9656105041503906\n",
      "epoch: 75 step: 91 loss: 0.07451322 acc: 0.9626350402832031\n",
      "epoch: 75 step: 92 loss: 0.095299155 acc: 0.9584808349609375\n",
      "epoch: 75 step: 93 loss: 0.07380288 acc: 0.9686355590820312\n",
      "epoch: 75 step: 94 loss: 0.07247555 acc: 0.9657821655273438\n",
      "epoch: 75 step: 95 loss: 0.08095116 acc: 0.9676742553710938\n",
      "epoch: 75 step: 96 loss: 0.08463377 acc: 0.9581069946289062\n",
      "epoch: 75 step: 97 loss: 0.099910125 acc: 0.9578323364257812\n",
      "epoch: 75 step: 98 loss: 0.08262324 acc: 0.9578704833984375\n",
      "epoch: 75 step: 99 loss: 0.08762082 acc: 0.9658775329589844\n",
      "epoch: 75 step: 100 loss: 0.08206753 acc: 0.9652252197265625\n",
      "epoch: 75 step: 101 loss: 0.079989225 acc: 0.9685516357421875\n",
      "epoch: 75 step: 102 loss: 0.06931459 acc: 0.9747390747070312\n",
      "epoch: 75 step: 103 loss: 0.07786196 acc: 0.973724365234375\n",
      "epoch: 75 step: 104 loss: 0.082437195 acc: 0.9638557434082031\n",
      "epoch: 75 step: 105 loss: 0.07683896 acc: 0.9658699035644531\n",
      "epoch: 75 step: 106 loss: 0.07760838 acc: 0.9633216857910156\n",
      "epoch: 75 step: 107 loss: 0.09686812 acc: 0.9528236389160156\n",
      "epoch: 75 step: 108 loss: 0.08326119 acc: 0.9584999084472656\n",
      "epoch: 75 step: 109 loss: 0.07216316 acc: 0.969696044921875\n",
      "epoch: 75 step: 110 loss: 0.07594894 acc: 0.96087646484375\n",
      "epoch: 75 step: 111 loss: 0.07349775 acc: 0.9625015258789062\n",
      "epoch: 75 step: 112 loss: 0.09068264 acc: 0.9661331176757812\n",
      "epoch: 75 step: 113 loss: 0.082647525 acc: 0.9632377624511719\n",
      "epoch: 75 step: 114 loss: 0.06790333 acc: 0.9726638793945312\n",
      "epoch: 75 step: 115 loss: 0.073874384 acc: 0.9712371826171875\n",
      "epoch: 75 step: 116 loss: 0.08646425 acc: 0.9663505554199219\n",
      "epoch: 75 step: 117 loss: 0.10868208 acc: 0.95831298828125\n",
      "epoch: 75 step: 118 loss: 0.069948204 acc: 0.9659652709960938\n",
      "epoch: 75 step: 119 loss: 0.087975346 acc: 0.9624252319335938\n",
      "epoch: 75 step: 120 loss: 0.080909364 acc: 0.9562950134277344\n",
      "epoch: 75 step: 121 loss: 0.10084035 acc: 0.9547882080078125\n",
      "epoch: 75 step: 122 loss: 0.08467046 acc: 0.9659500122070312\n",
      "epoch: 75 step: 123 loss: 0.065080136 acc: 0.9687881469726562\n",
      "epoch: 75 step: 124 loss: 0.074897304 acc: 0.9648001534598214\n",
      "epoch: 75 validation_loss: 0.089 validation_dice: 0.8461203769103962\n",
      "epoch: 75 test_dataset dice: 0.7456446446690975\n",
      "time cost 0.5360382437705994 min\n",
      "dice_best: 0.8551517496360885\n",
      "******************************** epoch  75  is finished. *********************************\n",
      "epoch: 76 step: 1 loss: 0.08666094 acc: 0.9557838439941406\n",
      "epoch: 76 step: 2 loss: 0.0849839 acc: 0.9650459289550781\n",
      "epoch: 76 step: 3 loss: 0.0781235 acc: 0.9695777893066406\n",
      "epoch: 76 step: 4 loss: 0.082005896 acc: 0.9684829711914062\n",
      "epoch: 76 step: 5 loss: 0.075875096 acc: 0.9738655090332031\n",
      "epoch: 76 step: 6 loss: 0.06526548 acc: 0.9758071899414062\n",
      "epoch: 76 step: 7 loss: 0.07764689 acc: 0.9718017578125\n",
      "epoch: 76 step: 8 loss: 0.086866535 acc: 0.9714012145996094\n",
      "epoch: 76 step: 9 loss: 0.0782099 acc: 0.97027587890625\n",
      "epoch: 76 step: 10 loss: 0.065509215 acc: 0.9672393798828125\n",
      "epoch: 76 step: 11 loss: 0.07320929 acc: 0.964874267578125\n",
      "epoch: 76 step: 12 loss: 0.084512204 acc: 0.9572105407714844\n",
      "epoch: 76 step: 13 loss: 0.10053202 acc: 0.9576492309570312\n",
      "epoch: 76 step: 14 loss: 0.07684618 acc: 0.9635963439941406\n",
      "epoch: 76 step: 15 loss: 0.07936987 acc: 0.9596595764160156\n",
      "epoch: 76 step: 16 loss: 0.08755426 acc: 0.9597015380859375\n",
      "epoch: 76 step: 17 loss: 0.07146787 acc: 0.9640045166015625\n",
      "epoch: 76 step: 18 loss: 0.0848032 acc: 0.9599685668945312\n",
      "epoch: 76 step: 19 loss: 0.07688027 acc: 0.9657096862792969\n",
      "epoch: 76 step: 20 loss: 0.073395714 acc: 0.9669532775878906\n",
      "epoch: 76 step: 21 loss: 0.0827761 acc: 0.9679794311523438\n",
      "epoch: 76 step: 22 loss: 0.08973571 acc: 0.9589157104492188\n",
      "epoch: 76 step: 23 loss: 0.0947087 acc: 0.9621772766113281\n",
      "epoch: 76 step: 24 loss: 0.09361877 acc: 0.9583206176757812\n",
      "epoch: 76 step: 25 loss: 0.07559332 acc: 0.9592475891113281\n",
      "epoch: 76 step: 26 loss: 0.06969029 acc: 0.9626045227050781\n",
      "epoch: 76 step: 27 loss: 0.07927329 acc: 0.9654121398925781\n",
      "epoch: 76 step: 28 loss: 0.063579306 acc: 0.9719047546386719\n",
      "epoch: 76 step: 29 loss: 0.071651526 acc: 0.9693107604980469\n",
      "epoch: 76 step: 30 loss: 0.0783056 acc: 0.9665145874023438\n",
      "epoch: 76 step: 31 loss: 0.08214402 acc: 0.9655227661132812\n",
      "epoch: 76 step: 32 loss: 0.08315529 acc: 0.9701118469238281\n",
      "epoch: 76 step: 33 loss: 0.07517074 acc: 0.9636955261230469\n",
      "epoch: 76 step: 34 loss: 0.08015598 acc: 0.9645156860351562\n",
      "epoch: 76 step: 35 loss: 0.08263575 acc: 0.964630126953125\n",
      "epoch: 76 step: 36 loss: 0.08534254 acc: 0.9598464965820312\n",
      "epoch: 76 step: 37 loss: 0.07745702 acc: 0.970611572265625\n",
      "epoch: 76 step: 38 loss: 0.076403596 acc: 0.9623870849609375\n",
      "epoch: 76 step: 39 loss: 0.09258196 acc: 0.960296630859375\n",
      "epoch: 76 step: 40 loss: 0.09380908 acc: 0.9512214660644531\n",
      "epoch: 76 step: 41 loss: 0.06901734 acc: 0.9657249450683594\n",
      "epoch: 76 step: 42 loss: 0.09265158 acc: 0.9616584777832031\n",
      "epoch: 76 step: 43 loss: 0.08780419 acc: 0.958953857421875\n",
      "epoch: 76 step: 44 loss: 0.08370189 acc: 0.96392822265625\n",
      "epoch: 76 step: 45 loss: 0.07742188 acc: 0.9603462219238281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 76 step: 46 loss: 0.07906186 acc: 0.966461181640625\n",
      "epoch: 76 step: 47 loss: 0.07579952 acc: 0.9610519409179688\n",
      "epoch: 76 step: 48 loss: 0.088897176 acc: 0.9616966247558594\n",
      "epoch: 76 step: 49 loss: 0.07509827 acc: 0.9650688171386719\n",
      "epoch: 76 step: 50 loss: 0.08781101 acc: 0.9576263427734375\n",
      "epoch: 76 step: 51 loss: 0.089614674 acc: 0.9623489379882812\n",
      "epoch: 76 step: 52 loss: 0.071915716 acc: 0.9628829956054688\n",
      "epoch: 76 step: 53 loss: 0.072495036 acc: 0.9663848876953125\n",
      "epoch: 76 step: 54 loss: 0.08722615 acc: 0.9556159973144531\n",
      "epoch: 76 step: 55 loss: 0.07829298 acc: 0.9607086181640625\n",
      "epoch: 76 step: 56 loss: 0.0920096 acc: 0.9605941772460938\n",
      "epoch: 76 step: 57 loss: 0.08827635 acc: 0.9609451293945312\n",
      "epoch: 76 step: 58 loss: 0.082233846 acc: 0.9611587524414062\n",
      "epoch: 76 step: 59 loss: 0.06742381 acc: 0.970367431640625\n",
      "epoch: 76 step: 60 loss: 0.08248185 acc: 0.9612083435058594\n",
      "epoch: 76 step: 61 loss: 0.06849415 acc: 0.9693183898925781\n",
      "epoch: 76 step: 62 loss: 0.07078328 acc: 0.9643974304199219\n",
      "epoch: 76 step: 63 loss: 0.073527865 acc: 0.9670257568359375\n",
      "epoch: 76 step: 64 loss: 0.083910145 acc: 0.9703712463378906\n",
      "epoch: 76 step: 65 loss: 0.080706954 acc: 0.961334228515625\n",
      "epoch: 76 step: 66 loss: 0.06481418 acc: 0.9755477905273438\n",
      "epoch: 76 step: 67 loss: 0.08940792 acc: 0.9628448486328125\n",
      "epoch: 76 step: 68 loss: 0.09133975 acc: 0.9635505676269531\n",
      "epoch: 76 step: 69 loss: 0.0726157 acc: 0.962799072265625\n",
      "epoch: 76 step: 70 loss: 0.08199305 acc: 0.9603042602539062\n",
      "epoch: 76 step: 71 loss: 0.07885179 acc: 0.9660606384277344\n",
      "epoch: 76 step: 72 loss: 0.07681931 acc: 0.9592399597167969\n",
      "epoch: 76 step: 73 loss: 0.082606666 acc: 0.9637641906738281\n",
      "epoch: 76 step: 74 loss: 0.088703506 acc: 0.9605979919433594\n",
      "epoch: 76 step: 75 loss: 0.09199584 acc: 0.9631462097167969\n",
      "epoch: 76 step: 76 loss: 0.06932973 acc: 0.9633636474609375\n",
      "epoch: 76 step: 77 loss: 0.08337207 acc: 0.9639625549316406\n",
      "epoch: 76 step: 78 loss: 0.07034578 acc: 0.9637489318847656\n",
      "epoch: 76 step: 79 loss: 0.08366337 acc: 0.9593849182128906\n",
      "epoch: 76 step: 80 loss: 0.06805849 acc: 0.9671478271484375\n",
      "epoch: 76 step: 81 loss: 0.07876862 acc: 0.9711799621582031\n",
      "epoch: 76 step: 82 loss: 0.07694268 acc: 0.9700584411621094\n",
      "epoch: 76 step: 83 loss: 0.070487164 acc: 0.9728507995605469\n",
      "epoch: 76 step: 84 loss: 0.082776524 acc: 0.96563720703125\n",
      "epoch: 76 step: 85 loss: 0.080830805 acc: 0.9628944396972656\n",
      "epoch: 76 step: 86 loss: 0.07072917 acc: 0.9710426330566406\n",
      "epoch: 76 step: 87 loss: 0.08388113 acc: 0.9631195068359375\n",
      "epoch: 76 step: 88 loss: 0.08506923 acc: 0.9586334228515625\n",
      "epoch: 76 step: 89 loss: 0.10127196 acc: 0.9591865539550781\n",
      "epoch: 76 step: 90 loss: 0.082859494 acc: 0.965545654296875\n",
      "epoch: 76 step: 91 loss: 0.09108003 acc: 0.9688720703125\n",
      "epoch: 76 step: 92 loss: 0.089271635 acc: 0.95843505859375\n",
      "epoch: 76 step: 93 loss: 0.0839225 acc: 0.9602622985839844\n",
      "epoch: 76 step: 94 loss: 0.07778971 acc: 0.9632453918457031\n",
      "epoch: 76 step: 95 loss: 0.08768855 acc: 0.9590034484863281\n",
      "epoch: 76 step: 96 loss: 0.08449809 acc: 0.9608535766601562\n",
      "epoch: 76 step: 97 loss: 0.09815162 acc: 0.9537696838378906\n",
      "epoch: 76 step: 98 loss: 0.06539218 acc: 0.9693069458007812\n",
      "epoch: 76 step: 99 loss: 0.07511239 acc: 0.9637641906738281\n",
      "epoch: 76 step: 100 loss: 0.08744709 acc: 0.9655647277832031\n",
      "epoch: 76 step: 101 loss: 0.07743462 acc: 0.968048095703125\n",
      "epoch: 76 step: 102 loss: 0.087941565 acc: 0.9643516540527344\n",
      "epoch: 76 step: 103 loss: 0.08148325 acc: 0.962554931640625\n",
      "epoch: 76 step: 104 loss: 0.086822 acc: 0.9638404846191406\n",
      "epoch: 76 step: 105 loss: 0.07159379 acc: 0.9712562561035156\n",
      "epoch: 76 step: 106 loss: 0.086964875 acc: 0.9731216430664062\n",
      "epoch: 76 step: 107 loss: 0.0772648 acc: 0.970458984375\n",
      "epoch: 76 step: 108 loss: 0.08867595 acc: 0.9638862609863281\n",
      "epoch: 76 step: 109 loss: 0.070970275 acc: 0.9700965881347656\n",
      "epoch: 76 step: 110 loss: 0.09958796 acc: 0.9520034790039062\n",
      "epoch: 76 step: 111 loss: 0.09192067 acc: 0.95721435546875\n",
      "epoch: 76 step: 112 loss: 0.094676286 acc: 0.9523544311523438\n",
      "epoch: 76 step: 113 loss: 0.07015277 acc: 0.963134765625\n",
      "epoch: 76 step: 114 loss: 0.08236836 acc: 0.9588050842285156\n",
      "epoch: 76 step: 115 loss: 0.08076436 acc: 0.9606170654296875\n",
      "epoch: 76 step: 116 loss: 0.0704051 acc: 0.9677848815917969\n",
      "epoch: 76 step: 117 loss: 0.07977537 acc: 0.9593315124511719\n",
      "epoch: 76 step: 118 loss: 0.084055945 acc: 0.9624671936035156\n",
      "epoch: 76 step: 119 loss: 0.084173635 acc: 0.9640388488769531\n",
      "epoch: 76 step: 120 loss: 0.08679602 acc: 0.9568557739257812\n",
      "epoch: 76 step: 121 loss: 0.07835275 acc: 0.9713287353515625\n",
      "epoch: 76 step: 122 loss: 0.08146847 acc: 0.9681777954101562\n",
      "epoch: 76 step: 123 loss: 0.06747275 acc: 0.9701004028320312\n",
      "epoch: 76 step: 124 loss: 0.075172 acc: 0.9698311941964286\n",
      "epoch: 76 validation_loss: 0.092 validation_dice: 0.8491739268774984\n",
      "epoch: 76 test_dataset dice: 0.7355055377502593\n",
      "time cost 0.5356251517931621 min\n",
      "dice_best: 0.8551517496360885\n",
      "******************************** epoch  76  is finished. *********************************\n",
      "epoch: 77 step: 1 loss: 0.09087019 acc: 0.9628486633300781\n",
      "epoch: 77 step: 2 loss: 0.08750321 acc: 0.9650917053222656\n",
      "epoch: 77 step: 3 loss: 0.10017848 acc: 0.9596786499023438\n",
      "epoch: 77 step: 4 loss: 0.08157268 acc: 0.9642219543457031\n",
      "epoch: 77 step: 5 loss: 0.09153654 acc: 0.9594879150390625\n",
      "epoch: 77 step: 6 loss: 0.08467168 acc: 0.9576606750488281\n",
      "epoch: 77 step: 7 loss: 0.07554853 acc: 0.9615859985351562\n",
      "epoch: 77 step: 8 loss: 0.074427254 acc: 0.9623451232910156\n",
      "epoch: 77 step: 9 loss: 0.087277316 acc: 0.9540824890136719\n",
      "epoch: 77 step: 10 loss: 0.11234656 acc: 0.9560432434082031\n",
      "epoch: 77 step: 11 loss: 0.10157828 acc: 0.9502677917480469\n",
      "epoch: 77 step: 12 loss: 0.07290161 acc: 0.968536376953125\n",
      "epoch: 77 step: 13 loss: 0.07997146 acc: 0.9670829772949219\n",
      "epoch: 77 step: 14 loss: 0.104075976 acc: 0.9631309509277344\n",
      "epoch: 77 step: 15 loss: 0.07662424 acc: 0.969635009765625\n",
      "epoch: 77 step: 16 loss: 0.07923418 acc: 0.9715156555175781\n",
      "epoch: 77 step: 17 loss: 0.07960591 acc: 0.9681053161621094\n",
      "epoch: 77 step: 18 loss: 0.08181288 acc: 0.9655838012695312\n",
      "epoch: 77 step: 19 loss: 0.08388291 acc: 0.9637908935546875\n",
      "epoch: 77 step: 20 loss: 0.08008009 acc: 0.9630889892578125\n",
      "epoch: 77 step: 21 loss: 0.0890195 acc: 0.9532814025878906\n",
      "epoch: 77 step: 22 loss: 0.08122653 acc: 0.9620819091796875\n",
      "epoch: 77 step: 23 loss: 0.08236673 acc: 0.9555931091308594\n",
      "epoch: 77 step: 24 loss: 0.08356987 acc: 0.9588241577148438\n",
      "epoch: 77 step: 25 loss: 0.082348734 acc: 0.9642105102539062\n",
      "epoch: 77 step: 26 loss: 0.08406294 acc: 0.9650192260742188\n",
      "epoch: 77 step: 27 loss: 0.09171902 acc: 0.9572410583496094\n",
      "epoch: 77 step: 28 loss: 0.081961624 acc: 0.9649658203125\n",
      "epoch: 77 step: 29 loss: 0.08692982 acc: 0.9706230163574219\n",
      "epoch: 77 step: 30 loss: 0.086904764 acc: 0.9629135131835938\n",
      "epoch: 77 step: 31 loss: 0.069663 acc: 0.9651832580566406\n",
      "epoch: 77 step: 32 loss: 0.089489594 acc: 0.9604301452636719\n",
      "epoch: 77 step: 33 loss: 0.07406542 acc: 0.9698944091796875\n",
      "epoch: 77 step: 34 loss: 0.0629754 acc: 0.9745330810546875\n",
      "epoch: 77 step: 35 loss: 0.089706525 acc: 0.9638328552246094\n",
      "epoch: 77 step: 36 loss: 0.11637987 acc: 0.9541206359863281\n",
      "epoch: 77 step: 37 loss: 0.07559855 acc: 0.9641761779785156\n",
      "epoch: 77 step: 38 loss: 0.08524635 acc: 0.9550361633300781\n",
      "epoch: 77 step: 39 loss: 0.10145888 acc: 0.9552955627441406\n",
      "epoch: 77 step: 40 loss: 0.09987499 acc: 0.949615478515625\n",
      "epoch: 77 step: 41 loss: 0.08994756 acc: 0.9637336730957031\n",
      "epoch: 77 step: 42 loss: 0.07296204 acc: 0.9631423950195312\n",
      "epoch: 77 step: 43 loss: 0.07677796 acc: 0.9681434631347656\n",
      "epoch: 77 step: 44 loss: 0.075674996 acc: 0.9697227478027344\n",
      "epoch: 77 step: 45 loss: 0.079442546 acc: 0.9630355834960938\n",
      "epoch: 77 step: 46 loss: 0.075239606 acc: 0.9661407470703125\n",
      "epoch: 77 step: 47 loss: 0.072554916 acc: 0.9647407531738281\n",
      "epoch: 77 step: 48 loss: 0.08963894 acc: 0.9617843627929688\n",
      "epoch: 77 step: 49 loss: 0.07748684 acc: 0.9728317260742188\n",
      "epoch: 77 step: 50 loss: 0.085158296 acc: 0.9657173156738281\n",
      "epoch: 77 step: 51 loss: 0.08839625 acc: 0.9584007263183594\n",
      "epoch: 77 step: 52 loss: 0.07123263 acc: 0.9654006958007812\n",
      "epoch: 77 step: 53 loss: 0.07844001 acc: 0.9636726379394531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 77 step: 54 loss: 0.096512266 acc: 0.9581413269042969\n",
      "epoch: 77 step: 55 loss: 0.07038892 acc: 0.9691162109375\n",
      "epoch: 77 step: 56 loss: 0.0913244 acc: 0.9645118713378906\n",
      "epoch: 77 step: 57 loss: 0.08683452 acc: 0.9651832580566406\n",
      "epoch: 77 step: 58 loss: 0.090729594 acc: 0.9682731628417969\n",
      "epoch: 77 step: 59 loss: 0.07403936 acc: 0.9652481079101562\n",
      "epoch: 77 step: 60 loss: 0.06964158 acc: 0.9690170288085938\n",
      "epoch: 77 step: 61 loss: 0.08172699 acc: 0.9621772766113281\n",
      "epoch: 77 step: 62 loss: 0.070176415 acc: 0.9683799743652344\n",
      "epoch: 77 step: 63 loss: 0.07383256 acc: 0.9675941467285156\n",
      "epoch: 77 step: 64 loss: 0.08425485 acc: 0.9614143371582031\n",
      "epoch: 77 step: 65 loss: 0.0764267 acc: 0.9658699035644531\n",
      "epoch: 77 step: 66 loss: 0.08910655 acc: 0.965362548828125\n",
      "epoch: 77 step: 67 loss: 0.06814692 acc: 0.9667854309082031\n",
      "epoch: 77 step: 68 loss: 0.078109875 acc: 0.9590072631835938\n",
      "epoch: 77 step: 69 loss: 0.072898135 acc: 0.9664840698242188\n",
      "epoch: 77 step: 70 loss: 0.07890228 acc: 0.9628829956054688\n",
      "epoch: 77 step: 71 loss: 0.07723131 acc: 0.9723548889160156\n",
      "epoch: 77 step: 72 loss: 0.08722669 acc: 0.9613265991210938\n",
      "epoch: 77 step: 73 loss: 0.10141483 acc: 0.9565620422363281\n",
      "epoch: 77 step: 74 loss: 0.07773411 acc: 0.9689826965332031\n",
      "epoch: 77 step: 75 loss: 0.08229013 acc: 0.9660377502441406\n",
      "epoch: 77 step: 76 loss: 0.08335218 acc: 0.9603729248046875\n",
      "epoch: 77 step: 77 loss: 0.09036142 acc: 0.9567108154296875\n",
      "epoch: 77 step: 78 loss: 0.08545361 acc: 0.96002197265625\n",
      "epoch: 77 step: 79 loss: 0.08173175 acc: 0.96759033203125\n",
      "epoch: 77 step: 80 loss: 0.08200654 acc: 0.9634246826171875\n",
      "epoch: 77 step: 81 loss: 0.0648901 acc: 0.9686241149902344\n",
      "epoch: 77 step: 82 loss: 0.08282578 acc: 0.9610710144042969\n",
      "epoch: 77 step: 83 loss: 0.09165919 acc: 0.9609107971191406\n",
      "epoch: 77 step: 84 loss: 0.078859925 acc: 0.9641227722167969\n",
      "epoch: 77 step: 85 loss: 0.07407222 acc: 0.9705581665039062\n",
      "epoch: 77 step: 86 loss: 0.075690895 acc: 0.9675064086914062\n",
      "epoch: 77 step: 87 loss: 0.075461105 acc: 0.9699630737304688\n",
      "epoch: 77 step: 88 loss: 0.06501817 acc: 0.9711990356445312\n",
      "epoch: 77 step: 89 loss: 0.098734215 acc: 0.9584388732910156\n",
      "epoch: 77 step: 90 loss: 0.06373835 acc: 0.9708213806152344\n",
      "epoch: 77 step: 91 loss: 0.10416464 acc: 0.957244873046875\n",
      "epoch: 77 step: 92 loss: 0.087263234 acc: 0.96221923828125\n",
      "epoch: 77 step: 93 loss: 0.069957614 acc: 0.9652214050292969\n",
      "epoch: 77 step: 94 loss: 0.07729692 acc: 0.9608421325683594\n",
      "epoch: 77 step: 95 loss: 0.079719834 acc: 0.9640960693359375\n",
      "epoch: 77 step: 96 loss: 0.07620016 acc: 0.96331787109375\n",
      "epoch: 77 step: 97 loss: 0.10011605 acc: 0.9642105102539062\n",
      "epoch: 77 step: 98 loss: 0.07594987 acc: 0.9695014953613281\n",
      "epoch: 77 step: 99 loss: 0.071320504 acc: 0.9656448364257812\n",
      "epoch: 77 step: 100 loss: 0.076625586 acc: 0.9654502868652344\n",
      "epoch: 77 step: 101 loss: 0.071707584 acc: 0.9643936157226562\n",
      "epoch: 77 step: 102 loss: 0.087255895 acc: 0.9595222473144531\n",
      "epoch: 77 step: 103 loss: 0.08849402 acc: 0.959197998046875\n",
      "epoch: 77 step: 104 loss: 0.08256221 acc: 0.9628944396972656\n",
      "epoch: 77 step: 105 loss: 0.08217146 acc: 0.9601249694824219\n",
      "epoch: 77 step: 106 loss: 0.063912615 acc: 0.97442626953125\n",
      "epoch: 77 step: 107 loss: 0.093446076 acc: 0.9621009826660156\n",
      "epoch: 77 step: 108 loss: 0.07239351 acc: 0.9683570861816406\n",
      "epoch: 77 step: 109 loss: 0.08493138 acc: 0.9557685852050781\n",
      "epoch: 77 step: 110 loss: 0.08330126 acc: 0.968505859375\n",
      "epoch: 77 step: 111 loss: 0.08082957 acc: 0.9656753540039062\n",
      "epoch: 77 step: 112 loss: 0.065899596 acc: 0.9708747863769531\n",
      "epoch: 77 step: 113 loss: 0.070557535 acc: 0.9689407348632812\n",
      "epoch: 77 step: 114 loss: 0.07676432 acc: 0.9681663513183594\n",
      "epoch: 77 step: 115 loss: 0.073261514 acc: 0.9771003723144531\n",
      "epoch: 77 step: 116 loss: 0.09007464 acc: 0.9554595947265625\n",
      "epoch: 77 step: 117 loss: 0.067178234 acc: 0.9686470031738281\n",
      "epoch: 77 step: 118 loss: 0.07300926 acc: 0.9635086059570312\n",
      "epoch: 77 step: 119 loss: 0.07895403 acc: 0.965240478515625\n",
      "epoch: 77 step: 120 loss: 0.08367915 acc: 0.9720611572265625\n",
      "epoch: 77 step: 121 loss: 0.08946684 acc: 0.9632530212402344\n",
      "epoch: 77 step: 122 loss: 0.07950488 acc: 0.9656906127929688\n",
      "epoch: 77 step: 123 loss: 0.08699655 acc: 0.9664382934570312\n",
      "epoch: 77 step: 124 loss: 0.08898392 acc: 0.9681658063616071\n",
      "epoch: 77 validation_loss: 0.088 validation_dice: 0.8320506644553884\n",
      "epoch: 77 test_dataset dice: 0.7447963965206438\n",
      "time cost 0.5360640446345012 min\n",
      "dice_best: 0.8551517496360885\n",
      "******************************** epoch  77  is finished. *********************************\n",
      "epoch: 78 step: 1 loss: 0.08143974 acc: 0.9608001708984375\n",
      "epoch: 78 step: 2 loss: 0.09966387 acc: 0.9611778259277344\n",
      "epoch: 78 step: 3 loss: 0.09350826 acc: 0.9648628234863281\n",
      "epoch: 78 step: 4 loss: 0.07752771 acc: 0.9655799865722656\n",
      "epoch: 78 step: 5 loss: 0.07113965 acc: 0.9704399108886719\n",
      "epoch: 78 step: 6 loss: 0.0890714 acc: 0.9545860290527344\n",
      "epoch: 78 step: 7 loss: 0.09924884 acc: 0.9598731994628906\n",
      "epoch: 78 step: 8 loss: 0.07606493 acc: 0.9702529907226562\n",
      "epoch: 78 step: 9 loss: 0.07272047 acc: 0.9638595581054688\n",
      "epoch: 78 step: 10 loss: 0.076530054 acc: 0.9640960693359375\n",
      "epoch: 78 step: 11 loss: 0.07048215 acc: 0.9628486633300781\n",
      "epoch: 78 step: 12 loss: 0.06998627 acc: 0.9648284912109375\n",
      "epoch: 78 step: 13 loss: 0.07326154 acc: 0.9660720825195312\n",
      "epoch: 78 step: 14 loss: 0.080006346 acc: 0.9620895385742188\n",
      "epoch: 78 step: 15 loss: 0.0776813 acc: 0.9679145812988281\n",
      "epoch: 78 step: 16 loss: 0.090803884 acc: 0.9676704406738281\n",
      "epoch: 78 step: 17 loss: 0.08833214 acc: 0.9647445678710938\n",
      "epoch: 78 step: 18 loss: 0.1105887 acc: 0.9597740173339844\n",
      "epoch: 78 step: 19 loss: 0.074975535 acc: 0.9693946838378906\n",
      "epoch: 78 step: 20 loss: 0.06948373 acc: 0.9642066955566406\n",
      "epoch: 78 step: 21 loss: 0.07781527 acc: 0.9619979858398438\n",
      "epoch: 78 step: 22 loss: 0.07945602 acc: 0.9579010009765625\n",
      "epoch: 78 step: 23 loss: 0.086704515 acc: 0.9628334045410156\n",
      "epoch: 78 step: 24 loss: 0.09466388 acc: 0.9597892761230469\n",
      "epoch: 78 step: 25 loss: 0.0903104 acc: 0.9558944702148438\n",
      "epoch: 78 step: 26 loss: 0.083614096 acc: 0.9569206237792969\n",
      "epoch: 78 step: 27 loss: 0.08082972 acc: 0.960479736328125\n",
      "epoch: 78 step: 28 loss: 0.07279887 acc: 0.967742919921875\n",
      "epoch: 78 step: 29 loss: 0.07885331 acc: 0.9671211242675781\n",
      "epoch: 78 step: 30 loss: 0.08605084 acc: 0.9643936157226562\n",
      "epoch: 78 step: 31 loss: 0.095295906 acc: 0.9595794677734375\n",
      "epoch: 78 step: 32 loss: 0.0856745 acc: 0.9625358581542969\n",
      "epoch: 78 step: 33 loss: 0.07613388 acc: 0.9653587341308594\n",
      "epoch: 78 step: 34 loss: 0.070884734 acc: 0.9706230163574219\n",
      "epoch: 78 step: 35 loss: 0.08832916 acc: 0.9688949584960938\n",
      "epoch: 78 step: 36 loss: 0.08464158 acc: 0.9618377685546875\n",
      "epoch: 78 step: 37 loss: 0.09049352 acc: 0.954437255859375\n",
      "epoch: 78 step: 38 loss: 0.08111076 acc: 0.9642257690429688\n",
      "epoch: 78 step: 39 loss: 0.07750163 acc: 0.9637794494628906\n",
      "epoch: 78 step: 40 loss: 0.0850076 acc: 0.9647560119628906\n",
      "epoch: 78 step: 41 loss: 0.081628345 acc: 0.9645729064941406\n",
      "epoch: 78 step: 42 loss: 0.08210842 acc: 0.9670944213867188\n",
      "epoch: 78 step: 43 loss: 0.08135723 acc: 0.9683647155761719\n",
      "epoch: 78 step: 44 loss: 0.07608395 acc: 0.9666099548339844\n",
      "epoch: 78 step: 45 loss: 0.09062561 acc: 0.9552650451660156\n",
      "epoch: 78 step: 46 loss: 0.07216719 acc: 0.9697341918945312\n",
      "epoch: 78 step: 47 loss: 0.07010222 acc: 0.9716835021972656\n",
      "epoch: 78 step: 48 loss: 0.080604285 acc: 0.9660148620605469\n",
      "epoch: 78 step: 49 loss: 0.08319937 acc: 0.9619216918945312\n",
      "epoch: 78 step: 50 loss: 0.08141601 acc: 0.9607315063476562\n",
      "epoch: 78 step: 51 loss: 0.07982399 acc: 0.966094970703125\n",
      "epoch: 78 step: 52 loss: 0.072770305 acc: 0.96929931640625\n",
      "epoch: 78 step: 53 loss: 0.07273744 acc: 0.9636192321777344\n",
      "epoch: 78 step: 54 loss: 0.07541826 acc: 0.9660606384277344\n",
      "epoch: 78 step: 55 loss: 0.08702155 acc: 0.9620361328125\n",
      "epoch: 78 step: 56 loss: 0.0813322 acc: 0.9607505798339844\n",
      "epoch: 78 step: 57 loss: 0.09320172 acc: 0.9572486877441406\n",
      "epoch: 78 step: 58 loss: 0.09691938 acc: 0.9571533203125\n",
      "epoch: 78 step: 59 loss: 0.08231437 acc: 0.9626197814941406\n",
      "epoch: 78 step: 60 loss: 0.0947852 acc: 0.9524383544921875\n",
      "epoch: 78 step: 61 loss: 0.06489418 acc: 0.9651260375976562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 78 step: 62 loss: 0.078576185 acc: 0.9611015319824219\n",
      "epoch: 78 step: 63 loss: 0.074669085 acc: 0.9608917236328125\n",
      "epoch: 78 step: 64 loss: 0.08954794 acc: 0.9538764953613281\n",
      "epoch: 78 step: 65 loss: 0.09032025 acc: 0.9550056457519531\n",
      "epoch: 78 step: 66 loss: 0.09253197 acc: 0.9541244506835938\n",
      "epoch: 78 step: 67 loss: 0.07391579 acc: 0.966217041015625\n",
      "epoch: 78 step: 68 loss: 0.08547304 acc: 0.9662094116210938\n",
      "epoch: 78 step: 69 loss: 0.084574305 acc: 0.9609870910644531\n",
      "epoch: 78 step: 70 loss: 0.072028026 acc: 0.9639434814453125\n",
      "epoch: 78 step: 71 loss: 0.07169431 acc: 0.9667625427246094\n",
      "epoch: 78 step: 72 loss: 0.07349139 acc: 0.9676170349121094\n",
      "epoch: 78 step: 73 loss: 0.07963179 acc: 0.9578399658203125\n",
      "epoch: 78 step: 74 loss: 0.07773668 acc: 0.9681167602539062\n",
      "epoch: 78 step: 75 loss: 0.081841275 acc: 0.962646484375\n",
      "epoch: 78 step: 76 loss: 0.07300831 acc: 0.9716987609863281\n",
      "epoch: 78 step: 77 loss: 0.087115444 acc: 0.9640045166015625\n",
      "epoch: 78 step: 78 loss: 0.08250463 acc: 0.9586067199707031\n",
      "epoch: 78 step: 79 loss: 0.08293164 acc: 0.9585952758789062\n",
      "epoch: 78 step: 80 loss: 0.0867612 acc: 0.9551429748535156\n",
      "epoch: 78 step: 81 loss: 0.08789828 acc: 0.9651756286621094\n",
      "epoch: 78 step: 82 loss: 0.08049007 acc: 0.9550132751464844\n",
      "epoch: 78 step: 83 loss: 0.06611923 acc: 0.971832275390625\n",
      "epoch: 78 step: 84 loss: 0.075509325 acc: 0.9683189392089844\n",
      "epoch: 78 step: 85 loss: 0.07056435 acc: 0.9667320251464844\n",
      "epoch: 78 step: 86 loss: 0.080978 acc: 0.9659576416015625\n",
      "epoch: 78 step: 87 loss: 0.11495758 acc: 0.9644927978515625\n",
      "epoch: 78 step: 88 loss: 0.09039727 acc: 0.962982177734375\n",
      "epoch: 78 step: 89 loss: 0.08266948 acc: 0.9713363647460938\n",
      "epoch: 78 step: 90 loss: 0.08570269 acc: 0.9594001770019531\n",
      "epoch: 78 step: 91 loss: 0.07342437 acc: 0.9692726135253906\n",
      "epoch: 78 step: 92 loss: 0.08910894 acc: 0.9578094482421875\n",
      "epoch: 78 step: 93 loss: 0.091105044 acc: 0.9628448486328125\n",
      "epoch: 78 step: 94 loss: 0.08652107 acc: 0.9583778381347656\n",
      "epoch: 78 step: 95 loss: 0.08006617 acc: 0.966094970703125\n",
      "epoch: 78 step: 96 loss: 0.101476245 acc: 0.9645500183105469\n",
      "epoch: 78 step: 97 loss: 0.077403784 acc: 0.9670906066894531\n",
      "epoch: 78 step: 98 loss: 0.07737375 acc: 0.9685325622558594\n",
      "epoch: 78 step: 99 loss: 0.075233065 acc: 0.9684638977050781\n",
      "epoch: 78 step: 100 loss: 0.08908108 acc: 0.9643936157226562\n",
      "epoch: 78 step: 101 loss: 0.07519479 acc: 0.9710350036621094\n",
      "epoch: 78 step: 102 loss: 0.073195726 acc: 0.9720802307128906\n",
      "epoch: 78 step: 103 loss: 0.09994286 acc: 0.9606857299804688\n",
      "epoch: 78 step: 104 loss: 0.078986004 acc: 0.96514892578125\n",
      "epoch: 78 step: 105 loss: 0.0911848 acc: 0.9611854553222656\n",
      "epoch: 78 step: 106 loss: 0.09809485 acc: 0.963836669921875\n",
      "epoch: 78 step: 107 loss: 0.07504369 acc: 0.9683265686035156\n",
      "epoch: 78 step: 108 loss: 0.07662973 acc: 0.9742698669433594\n",
      "epoch: 78 step: 109 loss: 0.08032547 acc: 0.9630546569824219\n",
      "epoch: 78 step: 110 loss: 0.08484329 acc: 0.9550743103027344\n",
      "epoch: 78 step: 111 loss: 0.08330962 acc: 0.9583587646484375\n",
      "epoch: 78 step: 112 loss: 0.089278914 acc: 0.9632720947265625\n",
      "epoch: 78 step: 113 loss: 0.078986965 acc: 0.9658889770507812\n",
      "epoch: 78 step: 114 loss: 0.08551302 acc: 0.9662628173828125\n",
      "epoch: 78 step: 115 loss: 0.08496994 acc: 0.9645843505859375\n",
      "epoch: 78 step: 116 loss: 0.09373513 acc: 0.9579620361328125\n",
      "epoch: 78 step: 117 loss: 0.082193255 acc: 0.9567985534667969\n",
      "epoch: 78 step: 118 loss: 0.099309646 acc: 0.9610671997070312\n",
      "epoch: 78 step: 119 loss: 0.08131672 acc: 0.9627685546875\n",
      "epoch: 78 step: 120 loss: 0.07695949 acc: 0.9680900573730469\n",
      "epoch: 78 step: 121 loss: 0.07864434 acc: 0.9593238830566406\n",
      "epoch: 78 step: 122 loss: 0.08316143 acc: 0.96209716796875\n",
      "epoch: 78 step: 123 loss: 0.072638854 acc: 0.9655990600585938\n",
      "epoch: 78 step: 124 loss: 0.08267426 acc: 0.9606148856026786\n",
      "epoch: 78 validation_loss: 0.097 validation_dice: 0.852068430202943\n",
      "epoch: 78 test_dataset dice: 0.7171644590929476\n",
      "time cost 0.5354261080423991 min\n",
      "dice_best: 0.8551517496360885\n",
      "******************************** epoch  78  is finished. *********************************\n",
      "epoch: 79 step: 1 loss: 0.08107482 acc: 0.9694252014160156\n",
      "epoch: 79 step: 2 loss: 0.08251424 acc: 0.9699249267578125\n",
      "epoch: 79 step: 3 loss: 0.08406322 acc: 0.9622116088867188\n",
      "epoch: 79 step: 4 loss: 0.086436585 acc: 0.9660987854003906\n",
      "epoch: 79 step: 5 loss: 0.08495626 acc: 0.9610710144042969\n",
      "epoch: 79 step: 6 loss: 0.07242073 acc: 0.9698600769042969\n",
      "epoch: 79 step: 7 loss: 0.084351644 acc: 0.97161865234375\n",
      "epoch: 79 step: 8 loss: 0.07128972 acc: 0.9694557189941406\n",
      "epoch: 79 step: 9 loss: 0.07133181 acc: 0.9692764282226562\n",
      "epoch: 79 step: 10 loss: 0.0909032 acc: 0.9579200744628906\n",
      "epoch: 79 step: 11 loss: 0.08458924 acc: 0.9642333984375\n",
      "epoch: 79 step: 12 loss: 0.08885373 acc: 0.9587249755859375\n",
      "epoch: 79 step: 13 loss: 0.09218478 acc: 0.9641151428222656\n",
      "epoch: 79 step: 14 loss: 0.07328715 acc: 0.9616813659667969\n",
      "epoch: 79 step: 15 loss: 0.07411793 acc: 0.9674186706542969\n",
      "epoch: 79 step: 16 loss: 0.07147217 acc: 0.9671211242675781\n",
      "epoch: 79 step: 17 loss: 0.089635625 acc: 0.955963134765625\n",
      "epoch: 79 step: 18 loss: 0.0647119 acc: 0.9746131896972656\n",
      "epoch: 79 step: 19 loss: 0.08253099 acc: 0.9645500183105469\n",
      "epoch: 79 step: 20 loss: 0.09848733 acc: 0.9613151550292969\n",
      "epoch: 79 step: 21 loss: 0.07272396 acc: 0.9724044799804688\n",
      "epoch: 79 step: 22 loss: 0.090251125 acc: 0.9571113586425781\n",
      "epoch: 79 step: 23 loss: 0.08899571 acc: 0.9608154296875\n",
      "epoch: 79 step: 24 loss: 0.0705588 acc: 0.9622840881347656\n",
      "epoch: 79 step: 25 loss: 0.07860289 acc: 0.9619064331054688\n",
      "epoch: 79 step: 26 loss: 0.08074209 acc: 0.9642715454101562\n",
      "epoch: 79 step: 27 loss: 0.088205695 acc: 0.9596824645996094\n",
      "epoch: 79 step: 28 loss: 0.07904352 acc: 0.9593772888183594\n",
      "epoch: 79 step: 29 loss: 0.09355743 acc: 0.9601325988769531\n",
      "epoch: 79 step: 30 loss: 0.08230706 acc: 0.9613456726074219\n",
      "epoch: 79 step: 31 loss: 0.07553715 acc: 0.9612503051757812\n",
      "epoch: 79 step: 32 loss: 0.075799726 acc: 0.9697647094726562\n",
      "epoch: 79 step: 33 loss: 0.0899934 acc: 0.9577217102050781\n",
      "epoch: 79 step: 34 loss: 0.07615595 acc: 0.964080810546875\n",
      "epoch: 79 step: 35 loss: 0.07041891 acc: 0.9700279235839844\n",
      "epoch: 79 step: 36 loss: 0.07065218 acc: 0.9672660827636719\n",
      "epoch: 79 step: 37 loss: 0.070802584 acc: 0.9605560302734375\n",
      "epoch: 79 step: 38 loss: 0.07526233 acc: 0.96929931640625\n",
      "epoch: 79 step: 39 loss: 0.08458715 acc: 0.9639091491699219\n",
      "epoch: 79 step: 40 loss: 0.08358012 acc: 0.9653358459472656\n",
      "epoch: 79 step: 41 loss: 0.07734502 acc: 0.9608993530273438\n",
      "epoch: 79 step: 42 loss: 0.06626738 acc: 0.9707603454589844\n",
      "epoch: 79 step: 43 loss: 0.07036911 acc: 0.9655532836914062\n",
      "epoch: 79 step: 44 loss: 0.076908395 acc: 0.96563720703125\n",
      "epoch: 79 step: 45 loss: 0.08620389 acc: 0.9629249572753906\n",
      "epoch: 79 step: 46 loss: 0.09114701 acc: 0.958465576171875\n",
      "epoch: 79 step: 47 loss: 0.07739399 acc: 0.9599418640136719\n",
      "epoch: 79 step: 48 loss: 0.08242343 acc: 0.960205078125\n",
      "epoch: 79 step: 49 loss: 0.08301123 acc: 0.9610671997070312\n",
      "epoch: 79 step: 50 loss: 0.07665625 acc: 0.9651260375976562\n",
      "epoch: 79 step: 51 loss: 0.074340746 acc: 0.9692764282226562\n",
      "epoch: 79 step: 52 loss: 0.08576219 acc: 0.9617500305175781\n",
      "epoch: 79 step: 53 loss: 0.0814515 acc: 0.9678688049316406\n",
      "epoch: 79 step: 54 loss: 0.07125585 acc: 0.9635086059570312\n",
      "epoch: 79 step: 55 loss: 0.07022227 acc: 0.9657249450683594\n",
      "epoch: 79 step: 56 loss: 0.089121506 acc: 0.9550056457519531\n",
      "epoch: 79 step: 57 loss: 0.07744797 acc: 0.96630859375\n",
      "epoch: 79 step: 58 loss: 0.07716493 acc: 0.9663925170898438\n",
      "epoch: 79 step: 59 loss: 0.07517281 acc: 0.9663581848144531\n",
      "epoch: 79 step: 60 loss: 0.06808043 acc: 0.9707565307617188\n",
      "epoch: 79 step: 61 loss: 0.08098989 acc: 0.9582786560058594\n",
      "epoch: 79 step: 62 loss: 0.067173004 acc: 0.97314453125\n",
      "epoch: 79 step: 63 loss: 0.08816118 acc: 0.9587974548339844\n",
      "epoch: 79 step: 64 loss: 0.0646002 acc: 0.9692916870117188\n",
      "epoch: 79 step: 65 loss: 0.07237457 acc: 0.9631385803222656\n",
      "epoch: 79 step: 66 loss: 0.09471281 acc: 0.9607276916503906\n",
      "epoch: 79 step: 67 loss: 0.082145 acc: 0.96441650390625\n",
      "epoch: 79 step: 68 loss: 0.07818127 acc: 0.96380615234375\n",
      "epoch: 79 step: 69 loss: 0.08855694 acc: 0.9686775207519531\n",
      "epoch: 79 step: 70 loss: 0.07568057 acc: 0.9634284973144531\n",
      "epoch: 79 step: 71 loss: 0.067809165 acc: 0.9717330932617188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 79 step: 72 loss: 0.075967886 acc: 0.9606285095214844\n",
      "epoch: 79 step: 73 loss: 0.08052087 acc: 0.965789794921875\n",
      "epoch: 79 step: 74 loss: 0.07292016 acc: 0.9693984985351562\n",
      "epoch: 79 step: 75 loss: 0.074896306 acc: 0.9644088745117188\n",
      "epoch: 79 step: 76 loss: 0.07378375 acc: 0.9659042358398438\n",
      "epoch: 79 step: 77 loss: 0.08989373 acc: 0.9664154052734375\n",
      "epoch: 79 step: 78 loss: 0.09828088 acc: 0.9671592712402344\n",
      "epoch: 79 step: 79 loss: 0.09136841 acc: 0.9640617370605469\n",
      "epoch: 79 step: 80 loss: 0.08215531 acc: 0.9579582214355469\n",
      "epoch: 79 step: 81 loss: 0.06610615 acc: 0.9731903076171875\n",
      "epoch: 79 step: 82 loss: 0.085129574 acc: 0.9611129760742188\n",
      "epoch: 79 step: 83 loss: 0.09339251 acc: 0.9539222717285156\n",
      "epoch: 79 step: 84 loss: 0.06899414 acc: 0.9675102233886719\n",
      "epoch: 79 step: 85 loss: 0.08538983 acc: 0.963165283203125\n",
      "epoch: 79 step: 86 loss: 0.08067617 acc: 0.9682350158691406\n",
      "epoch: 79 step: 87 loss: 0.08586056 acc: 0.9610671997070312\n",
      "epoch: 79 step: 88 loss: 0.09040122 acc: 0.9658622741699219\n",
      "epoch: 79 step: 89 loss: 0.08499671 acc: 0.961090087890625\n",
      "epoch: 79 step: 90 loss: 0.074915335 acc: 0.9709968566894531\n",
      "epoch: 79 step: 91 loss: 0.07326384 acc: 0.9673843383789062\n",
      "epoch: 79 step: 92 loss: 0.091534026 acc: 0.9585151672363281\n",
      "epoch: 79 step: 93 loss: 0.09462889 acc: 0.9615974426269531\n",
      "epoch: 79 step: 94 loss: 0.07529202 acc: 0.9681625366210938\n",
      "epoch: 79 step: 95 loss: 0.085298516 acc: 0.9640731811523438\n",
      "epoch: 79 step: 96 loss: 0.082709946 acc: 0.9643936157226562\n",
      "epoch: 79 step: 97 loss: 0.079982065 acc: 0.9619293212890625\n",
      "epoch: 79 step: 98 loss: 0.09319987 acc: 0.9694328308105469\n",
      "epoch: 79 step: 99 loss: 0.096220486 acc: 0.9501762390136719\n",
      "epoch: 79 step: 100 loss: 0.081976056 acc: 0.955535888671875\n",
      "epoch: 79 step: 101 loss: 0.08419324 acc: 0.9585113525390625\n",
      "epoch: 79 step: 102 loss: 0.08632651 acc: 0.9593887329101562\n",
      "epoch: 79 step: 103 loss: 0.09068823 acc: 0.9638633728027344\n",
      "epoch: 79 step: 104 loss: 0.09064301 acc: 0.9618339538574219\n",
      "epoch: 79 step: 105 loss: 0.08303734 acc: 0.9661636352539062\n",
      "epoch: 79 step: 106 loss: 0.08091482 acc: 0.9691619873046875\n",
      "epoch: 79 step: 107 loss: 0.08049078 acc: 0.9654541015625\n",
      "epoch: 79 step: 108 loss: 0.074745655 acc: 0.9726638793945312\n",
      "epoch: 79 step: 109 loss: 0.1127827 acc: 0.9549598693847656\n",
      "epoch: 79 step: 110 loss: 0.06806005 acc: 0.9663848876953125\n",
      "epoch: 79 step: 111 loss: 0.09781086 acc: 0.9639244079589844\n",
      "epoch: 79 step: 112 loss: 0.09897934 acc: 0.9568557739257812\n",
      "epoch: 79 step: 113 loss: 0.10474188 acc: 0.9513664245605469\n",
      "epoch: 79 step: 114 loss: 0.07011289 acc: 0.9653434753417969\n",
      "epoch: 79 step: 115 loss: 0.0900219 acc: 0.9592018127441406\n",
      "epoch: 79 step: 116 loss: 0.08016011 acc: 0.9576606750488281\n",
      "epoch: 79 step: 117 loss: 0.1075931 acc: 0.96234130859375\n",
      "epoch: 79 step: 118 loss: 0.07092336 acc: 0.9688568115234375\n",
      "epoch: 79 step: 119 loss: 0.09824997 acc: 0.9640464782714844\n",
      "epoch: 79 step: 120 loss: 0.08906617 acc: 0.9624977111816406\n",
      "epoch: 79 step: 121 loss: 0.086173914 acc: 0.9657707214355469\n",
      "epoch: 79 step: 122 loss: 0.10960374 acc: 0.955322265625\n",
      "epoch: 79 step: 123 loss: 0.08060088 acc: 0.9645156860351562\n",
      "epoch: 79 step: 124 loss: 0.13353355 acc: 0.9587576729910714\n",
      "epoch: 79 validation_loss: 0.094 validation_dice: 0.8341922597904262\n",
      "epoch: 79 test_dataset dice: 0.7360556876760914\n",
      "time cost 0.5354974508285523 min\n",
      "dice_best: 0.8551517496360885\n",
      "******************************** epoch  79  is finished. *********************************\n",
      "epoch: 80 step: 1 loss: 0.07909597 acc: 0.956207275390625\n",
      "epoch: 80 step: 2 loss: 0.08054616 acc: 0.9658203125\n",
      "epoch: 80 step: 3 loss: 0.10804725 acc: 0.954742431640625\n",
      "epoch: 80 step: 4 loss: 0.09001396 acc: 0.956878662109375\n",
      "epoch: 80 step: 5 loss: 0.12040404 acc: 0.9500656127929688\n",
      "epoch: 80 step: 6 loss: 0.08542127 acc: 0.9574203491210938\n",
      "epoch: 80 step: 7 loss: 0.09141254 acc: 0.9677772521972656\n",
      "epoch: 80 step: 8 loss: 0.08463452 acc: 0.9661788940429688\n",
      "epoch: 80 step: 9 loss: 0.07882969 acc: 0.9686203002929688\n",
      "epoch: 80 step: 10 loss: 0.08680854 acc: 0.9661331176757812\n",
      "epoch: 80 step: 11 loss: 0.079709515 acc: 0.9664344787597656\n",
      "epoch: 80 step: 12 loss: 0.08090987 acc: 0.9672355651855469\n",
      "epoch: 80 step: 13 loss: 0.08667116 acc: 0.9689445495605469\n",
      "epoch: 80 step: 14 loss: 0.07890719 acc: 0.9678955078125\n",
      "epoch: 80 step: 15 loss: 0.0842354 acc: 0.9644508361816406\n",
      "epoch: 80 step: 16 loss: 0.10561688 acc: 0.9599418640136719\n",
      "epoch: 80 step: 17 loss: 0.088955775 acc: 0.9622573852539062\n",
      "epoch: 80 step: 18 loss: 0.10290207 acc: 0.9521331787109375\n",
      "epoch: 80 step: 19 loss: 0.09739652 acc: 0.9482269287109375\n",
      "epoch: 80 step: 20 loss: 0.10518278 acc: 0.9517898559570312\n",
      "epoch: 80 step: 21 loss: 0.08490789 acc: 0.9637832641601562\n",
      "epoch: 80 step: 22 loss: 0.08828691 acc: 0.963104248046875\n",
      "epoch: 80 step: 23 loss: 0.08628322 acc: 0.9624786376953125\n",
      "epoch: 80 step: 24 loss: 0.073829375 acc: 0.972747802734375\n",
      "epoch: 80 step: 25 loss: 0.10061326 acc: 0.9616584777832031\n",
      "epoch: 80 step: 26 loss: 0.086049676 acc: 0.9698524475097656\n",
      "epoch: 80 step: 27 loss: 0.07912747 acc: 0.9699897766113281\n",
      "epoch: 80 step: 28 loss: 0.07951861 acc: 0.964874267578125\n",
      "epoch: 80 step: 29 loss: 0.10034234 acc: 0.9607391357421875\n",
      "epoch: 80 step: 30 loss: 0.08317338 acc: 0.9660873413085938\n",
      "epoch: 80 step: 31 loss: 0.08455524 acc: 0.96112060546875\n",
      "epoch: 80 step: 32 loss: 0.07459845 acc: 0.9695701599121094\n",
      "epoch: 80 step: 33 loss: 0.09270505 acc: 0.9565010070800781\n",
      "epoch: 80 step: 34 loss: 0.07444369 acc: 0.9615974426269531\n",
      "epoch: 80 step: 35 loss: 0.08853756 acc: 0.9643287658691406\n",
      "epoch: 80 step: 36 loss: 0.076887906 acc: 0.9574470520019531\n",
      "epoch: 80 step: 37 loss: 0.0815241 acc: 0.9570808410644531\n",
      "epoch: 80 step: 38 loss: 0.09703683 acc: 0.9608840942382812\n",
      "epoch: 80 step: 39 loss: 0.0787953 acc: 0.9662590026855469\n",
      "epoch: 80 step: 40 loss: 0.087636046 acc: 0.9594993591308594\n",
      "epoch: 80 step: 41 loss: 0.08164096 acc: 0.9575424194335938\n",
      "epoch: 80 step: 42 loss: 0.072737336 acc: 0.9646224975585938\n",
      "epoch: 80 step: 43 loss: 0.092287846 acc: 0.9633407592773438\n",
      "epoch: 80 step: 44 loss: 0.07068997 acc: 0.9707107543945312\n",
      "epoch: 80 step: 45 loss: 0.07410114 acc: 0.9725532531738281\n",
      "epoch: 80 step: 46 loss: 0.08101425 acc: 0.9633903503417969\n",
      "epoch: 80 step: 47 loss: 0.08235172 acc: 0.9663619995117188\n",
      "epoch: 80 step: 48 loss: 0.06324991 acc: 0.9723472595214844\n",
      "epoch: 80 step: 49 loss: 0.083596855 acc: 0.9619407653808594\n",
      "epoch: 80 step: 50 loss: 0.10490073 acc: 0.9592399597167969\n",
      "epoch: 80 step: 51 loss: 0.07480047 acc: 0.9634819030761719\n",
      "epoch: 80 step: 52 loss: 0.08041047 acc: 0.9592170715332031\n",
      "epoch: 80 step: 53 loss: 0.08044671 acc: 0.9604568481445312\n",
      "epoch: 80 step: 54 loss: 0.07020441 acc: 0.9737014770507812\n",
      "epoch: 80 step: 55 loss: 0.09016833 acc: 0.9579696655273438\n",
      "epoch: 80 step: 56 loss: 0.08987966 acc: 0.9591026306152344\n",
      "epoch: 80 step: 57 loss: 0.094714515 acc: 0.9522171020507812\n",
      "epoch: 80 step: 58 loss: 0.07495475 acc: 0.9643974304199219\n",
      "epoch: 80 step: 59 loss: 0.0862103 acc: 0.9592781066894531\n",
      "epoch: 80 step: 60 loss: 0.0883769 acc: 0.960601806640625\n",
      "epoch: 80 step: 61 loss: 0.0837348 acc: 0.9623031616210938\n",
      "epoch: 80 step: 62 loss: 0.08012191 acc: 0.95831298828125\n",
      "epoch: 80 step: 63 loss: 0.0858257 acc: 0.9670181274414062\n",
      "epoch: 80 step: 64 loss: 0.07718636 acc: 0.9673309326171875\n",
      "epoch: 80 step: 65 loss: 0.08327461 acc: 0.9572296142578125\n",
      "epoch: 80 step: 66 loss: 0.10007122 acc: 0.957489013671875\n",
      "epoch: 80 step: 67 loss: 0.077734105 acc: 0.9702186584472656\n",
      "epoch: 80 step: 68 loss: 0.07952072 acc: 0.9647941589355469\n",
      "epoch: 80 step: 69 loss: 0.08024381 acc: 0.969207763671875\n",
      "epoch: 80 step: 70 loss: 0.07592128 acc: 0.9599494934082031\n",
      "epoch: 80 step: 71 loss: 0.07069894 acc: 0.9653854370117188\n",
      "epoch: 80 step: 72 loss: 0.076801695 acc: 0.9632606506347656\n",
      "epoch: 80 step: 73 loss: 0.08251028 acc: 0.9615249633789062\n",
      "epoch: 80 step: 74 loss: 0.08164634 acc: 0.956573486328125\n",
      "epoch: 80 step: 75 loss: 0.08851181 acc: 0.9624137878417969\n",
      "epoch: 80 step: 76 loss: 0.083711 acc: 0.9576835632324219\n",
      "epoch: 80 step: 77 loss: 0.08180111 acc: 0.964874267578125\n",
      "epoch: 80 step: 78 loss: 0.07203276 acc: 0.9611587524414062\n",
      "epoch: 80 step: 79 loss: 0.07617284 acc: 0.9656562805175781\n",
      "epoch: 80 step: 80 loss: 0.0898743 acc: 0.9620742797851562\n",
      "epoch: 80 step: 81 loss: 0.071768135 acc: 0.965606689453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80 step: 82 loss: 0.083898485 acc: 0.966400146484375\n",
      "epoch: 80 step: 83 loss: 0.08819811 acc: 0.9629135131835938\n",
      "epoch: 80 step: 84 loss: 0.08906773 acc: 0.9591522216796875\n",
      "epoch: 80 step: 85 loss: 0.07439869 acc: 0.9609527587890625\n",
      "epoch: 80 step: 86 loss: 0.07172841 acc: 0.9584236145019531\n",
      "epoch: 80 step: 87 loss: 0.07086261 acc: 0.9656143188476562\n",
      "epoch: 80 step: 88 loss: 0.08281013 acc: 0.9693336486816406\n",
      "epoch: 80 step: 89 loss: 0.08436278 acc: 0.9636116027832031\n",
      "epoch: 80 step: 90 loss: 0.0926866 acc: 0.9540214538574219\n",
      "epoch: 80 step: 91 loss: 0.08911167 acc: 0.9657936096191406\n",
      "epoch: 80 step: 92 loss: 0.0825979 acc: 0.9662551879882812\n",
      "epoch: 80 step: 93 loss: 0.076334335 acc: 0.9649124145507812\n",
      "epoch: 80 step: 94 loss: 0.10056391 acc: 0.9505462646484375\n",
      "epoch: 80 step: 95 loss: 0.07167701 acc: 0.9652519226074219\n",
      "epoch: 80 step: 96 loss: 0.075485654 acc: 0.9703216552734375\n",
      "epoch: 80 step: 97 loss: 0.08632572 acc: 0.9648361206054688\n",
      "epoch: 80 step: 98 loss: 0.08752323 acc: 0.9673652648925781\n",
      "epoch: 80 step: 99 loss: 0.08379553 acc: 0.9610137939453125\n",
      "epoch: 80 step: 100 loss: 0.08393008 acc: 0.966278076171875\n",
      "epoch: 80 step: 101 loss: 0.08196458 acc: 0.9661521911621094\n",
      "epoch: 80 step: 102 loss: 0.10649167 acc: 0.9615478515625\n",
      "epoch: 80 step: 103 loss: 0.07256239 acc: 0.9638175964355469\n",
      "epoch: 80 step: 104 loss: 0.08606855 acc: 0.9571800231933594\n",
      "epoch: 80 step: 105 loss: 0.09863429 acc: 0.9609947204589844\n",
      "epoch: 80 step: 106 loss: 0.083639756 acc: 0.9618301391601562\n",
      "epoch: 80 step: 107 loss: 0.08294063 acc: 0.9587669372558594\n",
      "epoch: 80 step: 108 loss: 0.09686604 acc: 0.9633712768554688\n",
      "epoch: 80 step: 109 loss: 0.07514682 acc: 0.9656982421875\n",
      "epoch: 80 step: 110 loss: 0.117540926 acc: 0.9539527893066406\n",
      "epoch: 80 step: 111 loss: 0.103325784 acc: 0.9614028930664062\n",
      "epoch: 80 step: 112 loss: 0.07661603 acc: 0.9640579223632812\n",
      "epoch: 80 step: 113 loss: 0.07930784 acc: 0.9658126831054688\n",
      "epoch: 80 step: 114 loss: 0.10094845 acc: 0.9603233337402344\n",
      "epoch: 80 step: 115 loss: 0.08629431 acc: 0.9602813720703125\n",
      "epoch: 80 step: 116 loss: 0.06946543 acc: 0.9670677185058594\n",
      "epoch: 80 step: 117 loss: 0.082539015 acc: 0.9631080627441406\n",
      "epoch: 80 step: 118 loss: 0.07860313 acc: 0.9605293273925781\n",
      "epoch: 80 step: 119 loss: 0.08642346 acc: 0.9641494750976562\n",
      "epoch: 80 step: 120 loss: 0.07927412 acc: 0.9644088745117188\n",
      "epoch: 80 step: 121 loss: 0.09686233 acc: 0.9638748168945312\n",
      "epoch: 80 step: 122 loss: 0.078966096 acc: 0.9680862426757812\n",
      "epoch: 80 step: 123 loss: 0.091521785 acc: 0.9617118835449219\n",
      "epoch: 80 step: 124 loss: 0.09592542 acc: 0.9701974051339286\n",
      "epoch: 80 validation_loss: 0.095 validation_dice: 0.8280766059458889\n",
      "epoch: 80 test_dataset dice: 0.7299473868476984\n",
      "time cost 0.5362247983614604 min\n",
      "dice_best: 0.8551517496360885\n",
      "******************************** epoch  80  is finished. *********************************\n",
      "epoch: 81 step: 1 loss: 0.09761781 acc: 0.9572296142578125\n",
      "epoch: 81 step: 2 loss: 0.09348923 acc: 0.9651412963867188\n",
      "epoch: 81 step: 3 loss: 0.099632464 acc: 0.96221923828125\n",
      "epoch: 81 step: 4 loss: 0.08140833 acc: 0.9645118713378906\n",
      "epoch: 81 step: 5 loss: 0.091694705 acc: 0.9625778198242188\n",
      "epoch: 81 step: 6 loss: 0.08173196 acc: 0.961029052734375\n",
      "epoch: 81 step: 7 loss: 0.100235 acc: 0.9646072387695312\n",
      "epoch: 81 step: 8 loss: 0.09558578 acc: 0.9622993469238281\n",
      "epoch: 81 step: 9 loss: 0.09172529 acc: 0.9603462219238281\n",
      "epoch: 81 step: 10 loss: 0.06548874 acc: 0.9626731872558594\n",
      "epoch: 81 step: 11 loss: 0.088579625 acc: 0.9562606811523438\n",
      "epoch: 81 step: 12 loss: 0.077488445 acc: 0.9628753662109375\n",
      "epoch: 81 step: 13 loss: 0.08003844 acc: 0.9636917114257812\n",
      "epoch: 81 step: 14 loss: 0.09639771 acc: 0.9608039855957031\n",
      "epoch: 81 step: 15 loss: 0.08333298 acc: 0.9670600891113281\n",
      "epoch: 81 step: 16 loss: 0.073205575 acc: 0.9633522033691406\n",
      "epoch: 81 step: 17 loss: 0.0952624 acc: 0.9644241333007812\n",
      "epoch: 81 step: 18 loss: 0.08749091 acc: 0.9686813354492188\n",
      "epoch: 81 step: 19 loss: 0.07956892 acc: 0.9663543701171875\n",
      "epoch: 81 step: 20 loss: 0.08785387 acc: 0.966949462890625\n",
      "epoch: 81 step: 21 loss: 0.08252299 acc: 0.9687614440917969\n",
      "epoch: 81 step: 22 loss: 0.08814443 acc: 0.9612083435058594\n",
      "epoch: 81 step: 23 loss: 0.08567273 acc: 0.961395263671875\n",
      "epoch: 81 step: 24 loss: 0.08883371 acc: 0.9575920104980469\n",
      "epoch: 81 step: 25 loss: 0.087065786 acc: 0.9577293395996094\n",
      "epoch: 81 step: 26 loss: 0.08167306 acc: 0.9558639526367188\n",
      "epoch: 81 step: 27 loss: 0.08136549 acc: 0.95928955078125\n",
      "epoch: 81 step: 28 loss: 0.07816852 acc: 0.9607505798339844\n",
      "epoch: 81 step: 29 loss: 0.0823324 acc: 0.9646759033203125\n",
      "epoch: 81 step: 30 loss: 0.075122096 acc: 0.9583587646484375\n",
      "epoch: 81 step: 31 loss: 0.08517246 acc: 0.9575691223144531\n",
      "epoch: 81 step: 32 loss: 0.07377486 acc: 0.9691505432128906\n",
      "epoch: 81 step: 33 loss: 0.087527774 acc: 0.9639549255371094\n",
      "epoch: 81 step: 34 loss: 0.09987077 acc: 0.9585494995117188\n",
      "epoch: 81 step: 35 loss: 0.081036866 acc: 0.9668807983398438\n",
      "epoch: 81 step: 36 loss: 0.07286384 acc: 0.9649200439453125\n",
      "epoch: 81 step: 37 loss: 0.08201561 acc: 0.9590568542480469\n",
      "epoch: 81 step: 38 loss: 0.074126996 acc: 0.96881103515625\n",
      "epoch: 81 step: 39 loss: 0.09215149 acc: 0.9628143310546875\n",
      "epoch: 81 step: 40 loss: 0.08239125 acc: 0.9635696411132812\n",
      "epoch: 81 step: 41 loss: 0.08714562 acc: 0.9577980041503906\n",
      "epoch: 81 step: 42 loss: 0.08744217 acc: 0.9547615051269531\n",
      "epoch: 81 step: 43 loss: 0.09199163 acc: 0.9567642211914062\n",
      "epoch: 81 step: 44 loss: 0.0817853 acc: 0.9617843627929688\n",
      "epoch: 81 step: 45 loss: 0.08698629 acc: 0.9583282470703125\n",
      "epoch: 81 step: 46 loss: 0.073753916 acc: 0.9676933288574219\n",
      "epoch: 81 step: 47 loss: 0.0819938 acc: 0.9575462341308594\n",
      "epoch: 81 step: 48 loss: 0.06893986 acc: 0.9693946838378906\n",
      "epoch: 81 step: 49 loss: 0.06523369 acc: 0.9718818664550781\n",
      "epoch: 81 step: 50 loss: 0.09687471 acc: 0.97186279296875\n",
      "epoch: 81 step: 51 loss: 0.08534721 acc: 0.9702301025390625\n",
      "epoch: 81 step: 52 loss: 0.08413242 acc: 0.9650459289550781\n",
      "epoch: 81 step: 53 loss: 0.08047042 acc: 0.9650306701660156\n",
      "epoch: 81 step: 54 loss: 0.07815067 acc: 0.9647636413574219\n",
      "epoch: 81 step: 55 loss: 0.08294431 acc: 0.9627571105957031\n",
      "epoch: 81 step: 56 loss: 0.08084817 acc: 0.9638595581054688\n",
      "epoch: 81 step: 57 loss: 0.08144057 acc: 0.9581489562988281\n",
      "epoch: 81 step: 58 loss: 0.08463237 acc: 0.9561576843261719\n",
      "epoch: 81 step: 59 loss: 0.079407275 acc: 0.9652290344238281\n",
      "epoch: 81 step: 60 loss: 0.08576749 acc: 0.960113525390625\n",
      "epoch: 81 step: 61 loss: 0.07670537 acc: 0.9670181274414062\n",
      "epoch: 81 step: 62 loss: 0.08630573 acc: 0.96533203125\n",
      "epoch: 81 step: 63 loss: 0.07736156 acc: 0.9651985168457031\n",
      "epoch: 81 step: 64 loss: 0.090668894 acc: 0.9624290466308594\n",
      "epoch: 81 step: 65 loss: 0.08074993 acc: 0.9697151184082031\n",
      "epoch: 81 step: 66 loss: 0.08046804 acc: 0.9644012451171875\n",
      "epoch: 81 step: 67 loss: 0.08597558 acc: 0.9652061462402344\n",
      "epoch: 81 step: 68 loss: 0.07424853 acc: 0.9628067016601562\n",
      "epoch: 81 step: 69 loss: 0.089061685 acc: 0.9643211364746094\n",
      "epoch: 81 step: 70 loss: 0.08797363 acc: 0.9647941589355469\n",
      "epoch: 81 step: 71 loss: 0.07942949 acc: 0.9639892578125\n",
      "epoch: 81 step: 72 loss: 0.08671259 acc: 0.9592704772949219\n",
      "epoch: 81 step: 73 loss: 0.081931524 acc: 0.9618568420410156\n",
      "epoch: 81 step: 74 loss: 0.07459615 acc: 0.9633407592773438\n",
      "epoch: 81 step: 75 loss: 0.0760636 acc: 0.9657630920410156\n",
      "epoch: 81 step: 76 loss: 0.073173486 acc: 0.9701118469238281\n",
      "epoch: 81 step: 77 loss: 0.075453214 acc: 0.9599571228027344\n",
      "epoch: 81 step: 78 loss: 0.080562286 acc: 0.9626197814941406\n",
      "epoch: 81 step: 79 loss: 0.07154818 acc: 0.9620857238769531\n",
      "epoch: 81 step: 80 loss: 0.105317965 acc: 0.96429443359375\n",
      "epoch: 81 step: 81 loss: 0.06871762 acc: 0.9704208374023438\n",
      "epoch: 81 step: 82 loss: 0.0711782 acc: 0.9683494567871094\n",
      "epoch: 81 step: 83 loss: 0.08944238 acc: 0.9636306762695312\n",
      "epoch: 81 step: 84 loss: 0.06722482 acc: 0.9663848876953125\n",
      "epoch: 81 step: 85 loss: 0.081417836 acc: 0.9607391357421875\n",
      "epoch: 81 step: 86 loss: 0.07747423 acc: 0.965179443359375\n",
      "epoch: 81 step: 87 loss: 0.08259475 acc: 0.9682235717773438\n",
      "epoch: 81 step: 88 loss: 0.08409892 acc: 0.96380615234375\n",
      "epoch: 81 step: 89 loss: 0.07688708 acc: 0.9624824523925781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 81 step: 90 loss: 0.075086504 acc: 0.9671821594238281\n",
      "epoch: 81 step: 91 loss: 0.08977991 acc: 0.9563140869140625\n",
      "epoch: 81 step: 92 loss: 0.07506797 acc: 0.9657630920410156\n",
      "epoch: 81 step: 93 loss: 0.07835681 acc: 0.9677696228027344\n",
      "epoch: 81 step: 94 loss: 0.06903935 acc: 0.9744186401367188\n",
      "epoch: 81 step: 95 loss: 0.078510664 acc: 0.9702110290527344\n",
      "epoch: 81 step: 96 loss: 0.08872554 acc: 0.963104248046875\n",
      "epoch: 81 step: 97 loss: 0.10888835 acc: 0.9541549682617188\n",
      "epoch: 81 step: 98 loss: 0.070806526 acc: 0.959381103515625\n",
      "epoch: 81 step: 99 loss: 0.07200077 acc: 0.9656295776367188\n",
      "epoch: 81 step: 100 loss: 0.07673237 acc: 0.9529876708984375\n",
      "epoch: 81 step: 101 loss: 0.08775988 acc: 0.9522018432617188\n",
      "epoch: 81 step: 102 loss: 0.07279955 acc: 0.9596023559570312\n",
      "epoch: 81 step: 103 loss: 0.08761174 acc: 0.9638595581054688\n",
      "epoch: 81 step: 104 loss: 0.09066131 acc: 0.9608078002929688\n",
      "epoch: 81 step: 105 loss: 0.07516869 acc: 0.966583251953125\n",
      "epoch: 81 step: 106 loss: 0.07383145 acc: 0.9697265625\n",
      "epoch: 81 step: 107 loss: 0.07257553 acc: 0.9668769836425781\n",
      "epoch: 81 step: 108 loss: 0.08406697 acc: 0.9644966125488281\n",
      "epoch: 81 step: 109 loss: 0.078265324 acc: 0.9634933471679688\n",
      "epoch: 81 step: 110 loss: 0.06753348 acc: 0.9709281921386719\n",
      "epoch: 81 step: 111 loss: 0.07488531 acc: 0.9652824401855469\n",
      "epoch: 81 step: 112 loss: 0.07857541 acc: 0.9675521850585938\n",
      "epoch: 81 step: 113 loss: 0.08281208 acc: 0.9677200317382812\n",
      "epoch: 81 step: 114 loss: 0.0831403 acc: 0.9616508483886719\n",
      "epoch: 81 step: 115 loss: 0.09604774 acc: 0.96002197265625\n",
      "epoch: 81 step: 116 loss: 0.07327883 acc: 0.9642372131347656\n",
      "epoch: 81 step: 117 loss: 0.08467628 acc: 0.9624061584472656\n",
      "epoch: 81 step: 118 loss: 0.06622214 acc: 0.9679832458496094\n",
      "epoch: 81 step: 119 loss: 0.07685584 acc: 0.9605560302734375\n",
      "epoch: 81 step: 120 loss: 0.07767563 acc: 0.96087646484375\n",
      "epoch: 81 step: 121 loss: 0.07972346 acc: 0.9617195129394531\n",
      "epoch: 81 step: 122 loss: 0.07687238 acc: 0.96044921875\n",
      "epoch: 81 step: 123 loss: 0.10894095 acc: 0.9581756591796875\n",
      "epoch: 81 step: 124 loss: 0.114999935 acc: 0.9427315848214286\n",
      "epoch: 81 validation_loss: 0.089 validation_dice: 0.8456118536761794\n",
      "epoch: 81 test_dataset dice: 0.7365591019652826\n",
      "time cost 0.5379103779792785 min\n",
      "dice_best: 0.8551517496360885\n",
      "******************************** epoch  81  is finished. *********************************\n",
      "epoch: 82 step: 1 loss: 0.08127509 acc: 0.9603042602539062\n",
      "epoch: 82 step: 2 loss: 0.072914355 acc: 0.9635505676269531\n",
      "epoch: 82 step: 3 loss: 0.073985934 acc: 0.9624977111816406\n",
      "epoch: 82 step: 4 loss: 0.105707586 acc: 0.9601097106933594\n",
      "epoch: 82 step: 5 loss: 0.09366358 acc: 0.9536476135253906\n",
      "epoch: 82 step: 6 loss: 0.07329261 acc: 0.9695930480957031\n",
      "epoch: 82 step: 7 loss: 0.07862925 acc: 0.9606971740722656\n",
      "epoch: 82 step: 8 loss: 0.075161725 acc: 0.9636611938476562\n",
      "epoch: 82 step: 9 loss: 0.08524374 acc: 0.9704742431640625\n",
      "epoch: 82 step: 10 loss: 0.08401604 acc: 0.9612503051757812\n",
      "epoch: 82 step: 11 loss: 0.07819792 acc: 0.9634017944335938\n",
      "epoch: 82 step: 12 loss: 0.080887966 acc: 0.9599037170410156\n",
      "epoch: 82 step: 13 loss: 0.09469019 acc: 0.9599418640136719\n",
      "epoch: 82 step: 14 loss: 0.07959407 acc: 0.9590950012207031\n",
      "epoch: 82 step: 15 loss: 0.099519774 acc: 0.9585952758789062\n",
      "epoch: 82 step: 16 loss: 0.07845811 acc: 0.9663047790527344\n",
      "epoch: 82 step: 17 loss: 0.09282009 acc: 0.9538726806640625\n",
      "epoch: 82 step: 18 loss: 0.08543101 acc: 0.9567031860351562\n",
      "epoch: 82 step: 19 loss: 0.07736475 acc: 0.9602508544921875\n",
      "epoch: 82 step: 20 loss: 0.07252682 acc: 0.9638023376464844\n",
      "epoch: 82 step: 21 loss: 0.084434316 acc: 0.9564971923828125\n",
      "epoch: 82 step: 22 loss: 0.09161198 acc: 0.9558143615722656\n",
      "epoch: 82 step: 23 loss: 0.077161945 acc: 0.9678688049316406\n",
      "epoch: 82 step: 24 loss: 0.083149955 acc: 0.9607315063476562\n",
      "epoch: 82 step: 25 loss: 0.08937034 acc: 0.9595184326171875\n",
      "epoch: 82 step: 26 loss: 0.092132434 acc: 0.9672203063964844\n",
      "epoch: 82 step: 27 loss: 0.07622528 acc: 0.9743118286132812\n",
      "epoch: 82 step: 28 loss: 0.08730574 acc: 0.9653701782226562\n",
      "epoch: 82 step: 29 loss: 0.08501417 acc: 0.9608001708984375\n",
      "epoch: 82 step: 30 loss: 0.074859716 acc: 0.9701995849609375\n",
      "epoch: 82 step: 31 loss: 0.065379016 acc: 0.9666709899902344\n",
      "epoch: 82 step: 32 loss: 0.066801295 acc: 0.9638862609863281\n",
      "epoch: 82 step: 33 loss: 0.09528305 acc: 0.9568977355957031\n",
      "epoch: 82 step: 34 loss: 0.08298742 acc: 0.9614677429199219\n",
      "epoch: 82 step: 35 loss: 0.08932743 acc: 0.959869384765625\n",
      "epoch: 82 step: 36 loss: 0.06736791 acc: 0.9639625549316406\n",
      "epoch: 82 step: 37 loss: 0.077552356 acc: 0.9629707336425781\n",
      "epoch: 82 step: 38 loss: 0.07751054 acc: 0.965728759765625\n",
      "epoch: 82 step: 39 loss: 0.08076885 acc: 0.9695625305175781\n",
      "epoch: 82 step: 40 loss: 0.06718829 acc: 0.96783447265625\n",
      "epoch: 82 step: 41 loss: 0.07413833 acc: 0.9738311767578125\n",
      "epoch: 82 step: 42 loss: 0.07401448 acc: 0.9682464599609375\n",
      "epoch: 82 step: 43 loss: 0.080594234 acc: 0.9758033752441406\n",
      "epoch: 82 step: 44 loss: 0.07803597 acc: 0.9631919860839844\n",
      "epoch: 82 step: 45 loss: 0.12330873 acc: 0.9671173095703125\n",
      "epoch: 82 step: 46 loss: 0.07795684 acc: 0.9651527404785156\n",
      "epoch: 82 step: 47 loss: 0.082978405 acc: 0.9646224975585938\n",
      "epoch: 82 step: 48 loss: 0.09140078 acc: 0.9553451538085938\n",
      "epoch: 82 step: 49 loss: 0.09116604 acc: 0.9602890014648438\n",
      "epoch: 82 step: 50 loss: 0.08972334 acc: 0.9574508666992188\n",
      "epoch: 82 step: 51 loss: 0.095224254 acc: 0.9599685668945312\n",
      "epoch: 82 step: 52 loss: 0.092586 acc: 0.9632186889648438\n",
      "epoch: 82 step: 53 loss: 0.07969947 acc: 0.9585494995117188\n",
      "epoch: 82 step: 54 loss: 0.10653088 acc: 0.9575042724609375\n",
      "epoch: 82 step: 55 loss: 0.0874056 acc: 0.9600105285644531\n",
      "epoch: 82 step: 56 loss: 0.104000986 acc: 0.9577674865722656\n",
      "epoch: 82 step: 57 loss: 0.07628778 acc: 0.9667282104492188\n",
      "epoch: 82 step: 58 loss: 0.110814445 acc: 0.9640579223632812\n",
      "epoch: 82 step: 59 loss: 0.099128395 acc: 0.9529953002929688\n",
      "epoch: 82 step: 60 loss: 0.07657867 acc: 0.96563720703125\n",
      "epoch: 82 step: 61 loss: 0.081093885 acc: 0.9615898132324219\n",
      "epoch: 82 step: 62 loss: 0.09780959 acc: 0.960174560546875\n",
      "epoch: 82 step: 63 loss: 0.09754133 acc: 0.9618110656738281\n",
      "epoch: 82 step: 64 loss: 0.08599578 acc: 0.9641685485839844\n",
      "epoch: 82 step: 65 loss: 0.07993219 acc: 0.9614524841308594\n",
      "epoch: 82 step: 66 loss: 0.06372892 acc: 0.9688339233398438\n",
      "epoch: 82 step: 67 loss: 0.08544363 acc: 0.965728759765625\n",
      "epoch: 82 step: 68 loss: 0.0814007 acc: 0.9599723815917969\n",
      "epoch: 82 step: 69 loss: 0.10512721 acc: 0.9560623168945312\n",
      "epoch: 82 step: 70 loss: 0.070932865 acc: 0.9716873168945312\n",
      "epoch: 82 step: 71 loss: 0.07243665 acc: 0.9693031311035156\n",
      "epoch: 82 step: 72 loss: 0.074564256 acc: 0.9623794555664062\n",
      "epoch: 82 step: 73 loss: 0.075569555 acc: 0.964599609375\n",
      "epoch: 82 step: 74 loss: 0.084512345 acc: 0.9662246704101562\n",
      "epoch: 82 step: 75 loss: 0.075743966 acc: 0.9633598327636719\n",
      "epoch: 82 step: 76 loss: 0.08505874 acc: 0.9677543640136719\n",
      "epoch: 82 step: 77 loss: 0.09762132 acc: 0.9631538391113281\n",
      "epoch: 82 step: 78 loss: 0.09622207 acc: 0.9599800109863281\n",
      "epoch: 82 step: 79 loss: 0.089893736 acc: 0.9609222412109375\n",
      "epoch: 82 step: 80 loss: 0.094425276 acc: 0.9491424560546875\n",
      "epoch: 82 step: 81 loss: 0.0829946 acc: 0.9705963134765625\n",
      "epoch: 82 step: 82 loss: 0.08138128 acc: 0.9561882019042969\n",
      "epoch: 82 step: 83 loss: 0.06383919 acc: 0.9703865051269531\n",
      "epoch: 82 step: 84 loss: 0.076749094 acc: 0.9579658508300781\n",
      "epoch: 82 step: 85 loss: 0.07554763 acc: 0.9638137817382812\n",
      "epoch: 82 step: 86 loss: 0.09207547 acc: 0.9621200561523438\n",
      "epoch: 82 step: 87 loss: 0.079845816 acc: 0.9607391357421875\n",
      "epoch: 82 step: 88 loss: 0.07208727 acc: 0.9658584594726562\n",
      "epoch: 82 step: 89 loss: 0.08416337 acc: 0.9637069702148438\n",
      "epoch: 82 step: 90 loss: 0.07292507 acc: 0.9691429138183594\n",
      "epoch: 82 step: 91 loss: 0.07461977 acc: 0.969879150390625\n",
      "epoch: 82 step: 92 loss: 0.09492972 acc: 0.9620552062988281\n",
      "epoch: 82 step: 93 loss: 0.08366558 acc: 0.963623046875\n",
      "epoch: 82 step: 94 loss: 0.09094563 acc: 0.9546928405761719\n",
      "epoch: 82 step: 95 loss: 0.06637759 acc: 0.970123291015625\n",
      "epoch: 82 step: 96 loss: 0.077752426 acc: 0.9653396606445312\n",
      "epoch: 82 step: 97 loss: 0.08501646 acc: 0.9570426940917969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 82 step: 98 loss: 0.08158942 acc: 0.9620933532714844\n",
      "epoch: 82 step: 99 loss: 0.071295716 acc: 0.9654350280761719\n",
      "epoch: 82 step: 100 loss: 0.09024829 acc: 0.9622077941894531\n",
      "epoch: 82 step: 101 loss: 0.10234047 acc: 0.9599266052246094\n",
      "epoch: 82 step: 102 loss: 0.07794386 acc: 0.9620399475097656\n",
      "epoch: 82 step: 103 loss: 0.06439986 acc: 0.9692840576171875\n",
      "epoch: 82 step: 104 loss: 0.06265868 acc: 0.9736900329589844\n",
      "epoch: 82 step: 105 loss: 0.06650484 acc: 0.9723625183105469\n",
      "epoch: 82 step: 106 loss: 0.07677462 acc: 0.9649124145507812\n",
      "epoch: 82 step: 107 loss: 0.0881102 acc: 0.9608573913574219\n",
      "epoch: 82 step: 108 loss: 0.07257709 acc: 0.9620437622070312\n",
      "epoch: 82 step: 109 loss: 0.07239416 acc: 0.9678688049316406\n",
      "epoch: 82 step: 110 loss: 0.07490439 acc: 0.9697723388671875\n",
      "epoch: 82 step: 111 loss: 0.06799945 acc: 0.9719123840332031\n",
      "epoch: 82 step: 112 loss: 0.0907637 acc: 0.9601936340332031\n",
      "epoch: 82 step: 113 loss: 0.09904222 acc: 0.9625625610351562\n",
      "epoch: 82 step: 114 loss: 0.07902043 acc: 0.9691390991210938\n",
      "epoch: 82 step: 115 loss: 0.07981605 acc: 0.9676971435546875\n",
      "epoch: 82 step: 116 loss: 0.07938788 acc: 0.9683761596679688\n",
      "epoch: 82 step: 117 loss: 0.07608624 acc: 0.9570846557617188\n",
      "epoch: 82 step: 118 loss: 0.080435105 acc: 0.9583015441894531\n",
      "epoch: 82 step: 119 loss: 0.082553625 acc: 0.9632644653320312\n",
      "epoch: 82 step: 120 loss: 0.06950806 acc: 0.9726028442382812\n",
      "epoch: 82 step: 121 loss: 0.084372275 acc: 0.9670143127441406\n",
      "epoch: 82 step: 122 loss: 0.080897525 acc: 0.9613609313964844\n",
      "epoch: 82 step: 123 loss: 0.07632462 acc: 0.969268798828125\n",
      "epoch: 82 step: 124 loss: 0.08391587 acc: 0.9583042689732143\n",
      "epoch: 82 validation_loss: 0.087 validation_dice: 0.8622422678227601\n",
      "epoch: 82 test_dataset dice: 0.7450616221828902\n",
      "time cost 0.5362996459007263 min\n",
      "dice_best: 0.8622422678227601\n",
      "******************************** epoch  82  is finished. *********************************\n",
      "epoch: 83 step: 1 loss: 0.09233724 acc: 0.9617576599121094\n",
      "epoch: 83 step: 2 loss: 0.08320249 acc: 0.9622840881347656\n",
      "epoch: 83 step: 3 loss: 0.09234635 acc: 0.9548530578613281\n",
      "epoch: 83 step: 4 loss: 0.095074505 acc: 0.9633674621582031\n",
      "epoch: 83 step: 5 loss: 0.088353395 acc: 0.9586334228515625\n",
      "epoch: 83 step: 6 loss: 0.07284917 acc: 0.9617042541503906\n",
      "epoch: 83 step: 7 loss: 0.068030916 acc: 0.9720382690429688\n",
      "epoch: 83 step: 8 loss: 0.08102144 acc: 0.9677810668945312\n",
      "epoch: 83 step: 9 loss: 0.068923905 acc: 0.9692955017089844\n",
      "epoch: 83 step: 10 loss: 0.10124882 acc: 0.9617118835449219\n",
      "epoch: 83 step: 11 loss: 0.07463043 acc: 0.967559814453125\n",
      "epoch: 83 step: 12 loss: 0.081372574 acc: 0.9680442810058594\n",
      "epoch: 83 step: 13 loss: 0.08322881 acc: 0.9632492065429688\n",
      "epoch: 83 step: 14 loss: 0.105442956 acc: 0.9489364624023438\n",
      "epoch: 83 step: 15 loss: 0.08712817 acc: 0.9592475891113281\n",
      "epoch: 83 step: 16 loss: 0.08145549 acc: 0.9626693725585938\n",
      "epoch: 83 step: 17 loss: 0.08805658 acc: 0.9591255187988281\n",
      "epoch: 83 step: 18 loss: 0.077390455 acc: 0.9599418640136719\n",
      "epoch: 83 step: 19 loss: 0.10123672 acc: 0.9543609619140625\n",
      "epoch: 83 step: 20 loss: 0.09008793 acc: 0.9675064086914062\n",
      "epoch: 83 step: 21 loss: 0.083813034 acc: 0.95892333984375\n",
      "epoch: 83 step: 22 loss: 0.08412039 acc: 0.9627685546875\n",
      "epoch: 83 step: 23 loss: 0.07805014 acc: 0.9648857116699219\n",
      "epoch: 83 step: 24 loss: 0.084288426 acc: 0.9718475341796875\n",
      "epoch: 83 step: 25 loss: 0.06959426 acc: 0.9735145568847656\n",
      "epoch: 83 step: 26 loss: 0.07653701 acc: 0.9685134887695312\n",
      "epoch: 83 step: 27 loss: 0.07545041 acc: 0.9660911560058594\n",
      "epoch: 83 step: 28 loss: 0.08709017 acc: 0.9643173217773438\n",
      "epoch: 83 step: 29 loss: 0.07158577 acc: 0.9726600646972656\n",
      "epoch: 83 step: 30 loss: 0.08057435 acc: 0.9660072326660156\n",
      "epoch: 83 step: 31 loss: 0.092778884 acc: 0.9621086120605469\n",
      "epoch: 83 step: 32 loss: 0.06767809 acc: 0.9712142944335938\n",
      "epoch: 83 step: 33 loss: 0.07146329 acc: 0.9710845947265625\n",
      "epoch: 83 step: 34 loss: 0.07951287 acc: 0.9632606506347656\n",
      "epoch: 83 step: 35 loss: 0.088198826 acc: 0.9622001647949219\n",
      "epoch: 83 step: 36 loss: 0.09331911 acc: 0.9589500427246094\n",
      "epoch: 83 step: 37 loss: 0.07513703 acc: 0.9648246765136719\n",
      "epoch: 83 step: 38 loss: 0.08078555 acc: 0.9643516540527344\n",
      "epoch: 83 step: 39 loss: 0.078293815 acc: 0.96209716796875\n",
      "epoch: 83 step: 40 loss: 0.07200845 acc: 0.9650421142578125\n",
      "epoch: 83 step: 41 loss: 0.08894083 acc: 0.9614334106445312\n",
      "epoch: 83 step: 42 loss: 0.07261699 acc: 0.9704399108886719\n",
      "epoch: 83 step: 43 loss: 0.085526675 acc: 0.9589004516601562\n",
      "epoch: 83 step: 44 loss: 0.082359955 acc: 0.9615631103515625\n",
      "epoch: 83 step: 45 loss: 0.083553664 acc: 0.961212158203125\n",
      "epoch: 83 step: 46 loss: 0.06470325 acc: 0.9741401672363281\n",
      "epoch: 83 step: 47 loss: 0.0852721 acc: 0.9613037109375\n",
      "epoch: 83 step: 48 loss: 0.07437934 acc: 0.9597702026367188\n",
      "epoch: 83 step: 49 loss: 0.08234184 acc: 0.9585456848144531\n",
      "epoch: 83 step: 50 loss: 0.0833844 acc: 0.9621772766113281\n",
      "epoch: 83 step: 51 loss: 0.08717059 acc: 0.9585037231445312\n",
      "epoch: 83 step: 52 loss: 0.07159655 acc: 0.9704132080078125\n",
      "epoch: 83 step: 53 loss: 0.07336012 acc: 0.965423583984375\n",
      "epoch: 83 step: 54 loss: 0.08066501 acc: 0.9647865295410156\n",
      "epoch: 83 step: 55 loss: 0.08321583 acc: 0.9714851379394531\n",
      "epoch: 83 step: 56 loss: 0.07405814 acc: 0.9585456848144531\n",
      "epoch: 83 step: 57 loss: 0.081894144 acc: 0.9639739990234375\n",
      "epoch: 83 step: 58 loss: 0.10309526 acc: 0.9524154663085938\n",
      "epoch: 83 step: 59 loss: 0.080395535 acc: 0.9620246887207031\n",
      "epoch: 83 step: 60 loss: 0.08903062 acc: 0.9542007446289062\n",
      "epoch: 83 step: 61 loss: 0.08521336 acc: 0.9609184265136719\n",
      "epoch: 83 step: 62 loss: 0.07920076 acc: 0.9597015380859375\n",
      "epoch: 83 step: 63 loss: 0.081583194 acc: 0.9607925415039062\n",
      "epoch: 83 step: 64 loss: 0.08407271 acc: 0.9546928405761719\n",
      "epoch: 83 step: 65 loss: 0.08417927 acc: 0.9560279846191406\n",
      "epoch: 83 step: 66 loss: 0.08775752 acc: 0.9672584533691406\n",
      "epoch: 83 step: 67 loss: 0.07641653 acc: 0.9665870666503906\n",
      "epoch: 83 step: 68 loss: 0.09337509 acc: 0.9588661193847656\n",
      "epoch: 83 step: 69 loss: 0.062047105 acc: 0.9726905822753906\n",
      "epoch: 83 step: 70 loss: 0.08253473 acc: 0.9619522094726562\n",
      "epoch: 83 step: 71 loss: 0.09081486 acc: 0.9575691223144531\n",
      "epoch: 83 step: 72 loss: 0.08148066 acc: 0.96484375\n",
      "epoch: 83 step: 73 loss: 0.07509922 acc: 0.9641036987304688\n",
      "epoch: 83 step: 74 loss: 0.074584745 acc: 0.9594535827636719\n",
      "epoch: 83 step: 75 loss: 0.07363608 acc: 0.9716529846191406\n",
      "epoch: 83 step: 76 loss: 0.0894461 acc: 0.9621849060058594\n",
      "epoch: 83 step: 77 loss: 0.07357445 acc: 0.9672927856445312\n",
      "epoch: 83 step: 78 loss: 0.077160925 acc: 0.9646530151367188\n",
      "epoch: 83 step: 79 loss: 0.077494256 acc: 0.9648513793945312\n",
      "epoch: 83 step: 80 loss: 0.072702214 acc: 0.9726448059082031\n",
      "epoch: 83 step: 81 loss: 0.067609675 acc: 0.9667205810546875\n",
      "epoch: 83 step: 82 loss: 0.07069191 acc: 0.9662628173828125\n",
      "epoch: 83 step: 83 loss: 0.08379394 acc: 0.9663772583007812\n",
      "epoch: 83 step: 84 loss: 0.08056332 acc: 0.9674072265625\n",
      "epoch: 83 step: 85 loss: 0.07585818 acc: 0.9660568237304688\n",
      "epoch: 83 step: 86 loss: 0.09109691 acc: 0.9685630798339844\n",
      "epoch: 83 step: 87 loss: 0.079902165 acc: 0.9674949645996094\n",
      "epoch: 83 step: 88 loss: 0.07482299 acc: 0.9674720764160156\n",
      "epoch: 83 step: 89 loss: 0.068431124 acc: 0.9704818725585938\n",
      "epoch: 83 step: 90 loss: 0.07283459 acc: 0.9724388122558594\n",
      "epoch: 83 step: 91 loss: 0.095006265 acc: 0.9511909484863281\n",
      "epoch: 83 step: 92 loss: 0.0727705 acc: 0.9618377685546875\n",
      "epoch: 83 step: 93 loss: 0.09384251 acc: 0.9540901184082031\n",
      "epoch: 83 step: 94 loss: 0.07715152 acc: 0.952850341796875\n",
      "epoch: 83 step: 95 loss: 0.09590159 acc: 0.9460983276367188\n",
      "epoch: 83 step: 96 loss: 0.07546961 acc: 0.9649620056152344\n",
      "epoch: 83 step: 97 loss: 0.08255494 acc: 0.9570159912109375\n",
      "epoch: 83 step: 98 loss: 0.08235956 acc: 0.9630279541015625\n",
      "epoch: 83 step: 99 loss: 0.09838834 acc: 0.9672355651855469\n",
      "epoch: 83 step: 100 loss: 0.10052702 acc: 0.9605293273925781\n",
      "epoch: 83 step: 101 loss: 0.08123676 acc: 0.9646682739257812\n",
      "epoch: 83 step: 102 loss: 0.06671136 acc: 0.9692840576171875\n",
      "epoch: 83 step: 103 loss: 0.07889993 acc: 0.9666481018066406\n",
      "epoch: 83 step: 104 loss: 0.08136443 acc: 0.9647102355957031\n",
      "epoch: 83 step: 105 loss: 0.09540033 acc: 0.9640846252441406\n",
      "epoch: 83 step: 106 loss: 0.068744145 acc: 0.9623756408691406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 83 step: 107 loss: 0.070519045 acc: 0.9651374816894531\n",
      "epoch: 83 step: 108 loss: 0.07930201 acc: 0.9584884643554688\n",
      "epoch: 83 step: 109 loss: 0.06813791 acc: 0.9664535522460938\n",
      "epoch: 83 step: 110 loss: 0.07495987 acc: 0.9671173095703125\n",
      "epoch: 83 step: 111 loss: 0.06528217 acc: 0.96978759765625\n",
      "epoch: 83 step: 112 loss: 0.07225025 acc: 0.9623146057128906\n",
      "epoch: 83 step: 113 loss: 0.07590287 acc: 0.9671745300292969\n",
      "epoch: 83 step: 114 loss: 0.06860871 acc: 0.969024658203125\n",
      "epoch: 83 step: 115 loss: 0.079119906 acc: 0.968231201171875\n",
      "epoch: 83 step: 116 loss: 0.0681579 acc: 0.9693717956542969\n",
      "epoch: 83 step: 117 loss: 0.068164065 acc: 0.9722824096679688\n",
      "epoch: 83 step: 118 loss: 0.07273463 acc: 0.9727783203125\n",
      "epoch: 83 step: 119 loss: 0.07143557 acc: 0.97088623046875\n",
      "epoch: 83 step: 120 loss: 0.08496711 acc: 0.9698600769042969\n",
      "epoch: 83 step: 121 loss: 0.08688444 acc: 0.958282470703125\n",
      "epoch: 83 step: 122 loss: 0.09553908 acc: 0.95745849609375\n",
      "epoch: 83 step: 123 loss: 0.0798362 acc: 0.9642486572265625\n",
      "epoch: 83 step: 124 loss: 0.05779021 acc: 0.9681832449776786\n",
      "epoch: 83 validation_loss: 0.09 validation_dice: 0.8293578835268204\n",
      "epoch: 83 test_dataset dice: 0.7475638983711909\n",
      "time cost 0.5361288110415141 min\n",
      "dice_best: 0.8622422678227601\n",
      "******************************** epoch  83  is finished. *********************************\n",
      "epoch: 84 step: 1 loss: 0.075102545 acc: 0.966094970703125\n",
      "epoch: 84 step: 2 loss: 0.07882826 acc: 0.9555549621582031\n",
      "epoch: 84 step: 3 loss: 0.07482957 acc: 0.9722900390625\n",
      "epoch: 84 step: 4 loss: 0.07769015 acc: 0.9570198059082031\n",
      "epoch: 84 step: 5 loss: 0.07461637 acc: 0.9660377502441406\n",
      "epoch: 84 step: 6 loss: 0.07292312 acc: 0.9700546264648438\n",
      "epoch: 84 step: 7 loss: 0.07264671 acc: 0.9649620056152344\n",
      "epoch: 84 step: 8 loss: 0.08813182 acc: 0.9657440185546875\n",
      "epoch: 84 step: 9 loss: 0.08080599 acc: 0.9667854309082031\n",
      "epoch: 84 step: 10 loss: 0.080519006 acc: 0.9652595520019531\n",
      "epoch: 84 step: 11 loss: 0.06263375 acc: 0.9677543640136719\n",
      "epoch: 84 step: 12 loss: 0.073520385 acc: 0.9691581726074219\n",
      "epoch: 84 step: 13 loss: 0.08933203 acc: 0.9525680541992188\n",
      "epoch: 84 step: 14 loss: 0.08282385 acc: 0.9584083557128906\n",
      "epoch: 84 step: 15 loss: 0.07419081 acc: 0.9693145751953125\n",
      "epoch: 84 step: 16 loss: 0.074980125 acc: 0.9652175903320312\n",
      "epoch: 84 step: 17 loss: 0.06715743 acc: 0.9648094177246094\n",
      "epoch: 84 step: 18 loss: 0.07258897 acc: 0.9709739685058594\n",
      "epoch: 84 step: 19 loss: 0.07939766 acc: 0.9657249450683594\n",
      "epoch: 84 step: 20 loss: 0.0763658 acc: 0.9665451049804688\n",
      "epoch: 84 step: 21 loss: 0.08271154 acc: 0.9662551879882812\n",
      "epoch: 84 step: 22 loss: 0.086442955 acc: 0.958984375\n",
      "epoch: 84 step: 23 loss: 0.078166015 acc: 0.9605255126953125\n",
      "epoch: 84 step: 24 loss: 0.08514919 acc: 0.9609642028808594\n",
      "epoch: 84 step: 25 loss: 0.08097629 acc: 0.9575767517089844\n",
      "epoch: 84 step: 26 loss: 0.068087064 acc: 0.9652442932128906\n",
      "epoch: 84 step: 27 loss: 0.07455927 acc: 0.9664154052734375\n",
      "epoch: 84 step: 28 loss: 0.07585904 acc: 0.96221923828125\n",
      "epoch: 84 step: 29 loss: 0.083150476 acc: 0.9655876159667969\n",
      "epoch: 84 step: 30 loss: 0.07595235 acc: 0.9656181335449219\n",
      "epoch: 84 step: 31 loss: 0.072321795 acc: 0.9638633728027344\n",
      "epoch: 84 step: 32 loss: 0.09723115 acc: 0.9606094360351562\n",
      "epoch: 84 step: 33 loss: 0.09103008 acc: 0.9611701965332031\n",
      "epoch: 84 step: 34 loss: 0.06480406 acc: 0.9685783386230469\n",
      "epoch: 84 step: 35 loss: 0.06808593 acc: 0.9630966186523438\n",
      "epoch: 84 step: 36 loss: 0.0834459 acc: 0.9600028991699219\n",
      "epoch: 84 step: 37 loss: 0.0678383 acc: 0.9619255065917969\n",
      "epoch: 84 step: 38 loss: 0.07349253 acc: 0.9635009765625\n",
      "epoch: 84 step: 39 loss: 0.06444052 acc: 0.9668502807617188\n",
      "epoch: 84 step: 40 loss: 0.073096894 acc: 0.9639549255371094\n",
      "epoch: 84 step: 41 loss: 0.0759711 acc: 0.9596405029296875\n",
      "epoch: 84 step: 42 loss: 0.090306215 acc: 0.9569816589355469\n",
      "epoch: 84 step: 43 loss: 0.08321891 acc: 0.9651832580566406\n",
      "epoch: 84 step: 44 loss: 0.079958536 acc: 0.9615287780761719\n",
      "epoch: 84 step: 45 loss: 0.080808036 acc: 0.9688491821289062\n",
      "epoch: 84 step: 46 loss: 0.082055174 acc: 0.9638137817382812\n",
      "epoch: 84 step: 47 loss: 0.0927679 acc: 0.9559593200683594\n",
      "epoch: 84 step: 48 loss: 0.07387157 acc: 0.9637184143066406\n",
      "epoch: 84 step: 49 loss: 0.077876404 acc: 0.9625511169433594\n",
      "epoch: 84 step: 50 loss: 0.08298238 acc: 0.9609756469726562\n",
      "epoch: 84 step: 51 loss: 0.064313285 acc: 0.9648628234863281\n",
      "epoch: 84 step: 52 loss: 0.080166765 acc: 0.9580307006835938\n",
      "epoch: 84 step: 53 loss: 0.09027348 acc: 0.9565162658691406\n",
      "epoch: 84 step: 54 loss: 0.07529389 acc: 0.9630546569824219\n",
      "epoch: 84 step: 55 loss: 0.06696076 acc: 0.9682121276855469\n",
      "epoch: 84 step: 56 loss: 0.075942025 acc: 0.9673347473144531\n",
      "epoch: 84 step: 57 loss: 0.063000284 acc: 0.9724845886230469\n",
      "epoch: 84 step: 58 loss: 0.07785642 acc: 0.9719352722167969\n",
      "epoch: 84 step: 59 loss: 0.08708198 acc: 0.9648284912109375\n",
      "epoch: 84 step: 60 loss: 0.06737749 acc: 0.9677810668945312\n",
      "epoch: 84 step: 61 loss: 0.072909646 acc: 0.9640426635742188\n",
      "epoch: 84 step: 62 loss: 0.075342394 acc: 0.9664268493652344\n",
      "epoch: 84 step: 63 loss: 0.07464956 acc: 0.9672088623046875\n",
      "epoch: 84 step: 64 loss: 0.06750715 acc: 0.965667724609375\n",
      "epoch: 84 step: 65 loss: 0.09234943 acc: 0.9548263549804688\n",
      "epoch: 84 step: 66 loss: 0.07837066 acc: 0.9628868103027344\n",
      "epoch: 84 step: 67 loss: 0.06990818 acc: 0.9624290466308594\n",
      "epoch: 84 step: 68 loss: 0.08923481 acc: 0.9533233642578125\n",
      "epoch: 84 step: 69 loss: 0.0749749 acc: 0.9682655334472656\n",
      "epoch: 84 step: 70 loss: 0.07951752 acc: 0.9668388366699219\n",
      "epoch: 84 step: 71 loss: 0.08145609 acc: 0.9679641723632812\n",
      "epoch: 84 step: 72 loss: 0.0720476 acc: 0.9666328430175781\n",
      "epoch: 84 step: 73 loss: 0.07456744 acc: 0.9693870544433594\n",
      "epoch: 84 step: 74 loss: 0.07492384 acc: 0.9703559875488281\n",
      "epoch: 84 step: 75 loss: 0.067802295 acc: 0.9734420776367188\n",
      "epoch: 84 step: 76 loss: 0.077692375 acc: 0.9691963195800781\n",
      "epoch: 84 step: 77 loss: 0.073208526 acc: 0.963287353515625\n",
      "epoch: 84 step: 78 loss: 0.06785018 acc: 0.9679832458496094\n",
      "epoch: 84 step: 79 loss: 0.07259451 acc: 0.9619636535644531\n",
      "epoch: 84 step: 80 loss: 0.0686017 acc: 0.9653091430664062\n",
      "epoch: 84 step: 81 loss: 0.07587647 acc: 0.9571380615234375\n",
      "epoch: 84 step: 82 loss: 0.06927447 acc: 0.9701690673828125\n",
      "epoch: 84 step: 83 loss: 0.06774564 acc: 0.967987060546875\n",
      "epoch: 84 step: 84 loss: 0.078698024 acc: 0.9696121215820312\n",
      "epoch: 84 step: 85 loss: 0.07394139 acc: 0.9673957824707031\n",
      "epoch: 84 step: 86 loss: 0.081686005 acc: 0.9671173095703125\n",
      "epoch: 84 step: 87 loss: 0.079711705 acc: 0.9587554931640625\n",
      "epoch: 84 step: 88 loss: 0.079523675 acc: 0.9648399353027344\n",
      "epoch: 84 step: 89 loss: 0.07952638 acc: 0.9654312133789062\n",
      "epoch: 84 step: 90 loss: 0.08747649 acc: 0.960205078125\n",
      "epoch: 84 step: 91 loss: 0.07339568 acc: 0.957763671875\n",
      "epoch: 84 step: 92 loss: 0.071887426 acc: 0.9667396545410156\n",
      "epoch: 84 step: 93 loss: 0.07993206 acc: 0.9641380310058594\n",
      "epoch: 84 step: 94 loss: 0.077087454 acc: 0.9634819030761719\n",
      "epoch: 84 step: 95 loss: 0.07133942 acc: 0.9687271118164062\n",
      "epoch: 84 step: 96 loss: 0.0848879 acc: 0.9679069519042969\n",
      "epoch: 84 step: 97 loss: 0.06913413 acc: 0.9685478210449219\n",
      "epoch: 84 step: 98 loss: 0.06704583 acc: 0.9684371948242188\n",
      "epoch: 84 step: 99 loss: 0.085688524 acc: 0.9622383117675781\n",
      "epoch: 84 step: 100 loss: 0.08171301 acc: 0.9673843383789062\n",
      "epoch: 84 step: 101 loss: 0.085708216 acc: 0.9597244262695312\n",
      "epoch: 84 step: 102 loss: 0.093591586 acc: 0.9594345092773438\n",
      "epoch: 84 step: 103 loss: 0.077652045 acc: 0.9677047729492188\n",
      "epoch: 84 step: 104 loss: 0.0744823 acc: 0.9642372131347656\n",
      "epoch: 84 step: 105 loss: 0.07263412 acc: 0.9662551879882812\n",
      "epoch: 84 step: 106 loss: 0.07235828 acc: 0.9678382873535156\n",
      "epoch: 84 step: 107 loss: 0.07915324 acc: 0.9659347534179688\n",
      "epoch: 84 step: 108 loss: 0.06675518 acc: 0.9634246826171875\n",
      "epoch: 84 step: 109 loss: 0.08548723 acc: 0.9597854614257812\n",
      "epoch: 84 step: 110 loss: 0.07862807 acc: 0.9703254699707031\n",
      "epoch: 84 step: 111 loss: 0.07447871 acc: 0.96087646484375\n",
      "epoch: 84 step: 112 loss: 0.071145006 acc: 0.9699974060058594\n",
      "epoch: 84 step: 113 loss: 0.07718753 acc: 0.9613494873046875\n",
      "epoch: 84 step: 114 loss: 0.06948049 acc: 0.9695320129394531\n",
      "epoch: 84 step: 115 loss: 0.066963606 acc: 0.9675369262695312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 84 step: 116 loss: 0.07996584 acc: 0.9711112976074219\n",
      "epoch: 84 step: 117 loss: 0.0890789 acc: 0.9683723449707031\n",
      "epoch: 84 step: 118 loss: 0.06915211 acc: 0.96685791015625\n",
      "epoch: 84 step: 119 loss: 0.075355016 acc: 0.9613990783691406\n",
      "epoch: 84 step: 120 loss: 0.075979844 acc: 0.9626426696777344\n",
      "epoch: 84 step: 121 loss: 0.08322219 acc: 0.9599723815917969\n",
      "epoch: 84 step: 122 loss: 0.08961596 acc: 0.9613800048828125\n",
      "epoch: 84 step: 123 loss: 0.073751144 acc: 0.9670028686523438\n",
      "epoch: 84 step: 124 loss: 0.073592 acc: 0.9679827008928571\n",
      "epoch: 84 validation_loss: 0.087 validation_dice: 0.8470309751468118\n",
      "epoch: 84 test_dataset dice: 0.7384915042929688\n",
      "time cost 0.5360439022382101 min\n",
      "dice_best: 0.8622422678227601\n",
      "******************************** epoch  84  is finished. *********************************\n",
      "epoch: 85 step: 1 loss: 0.071510024 acc: 0.9661178588867188\n",
      "epoch: 85 step: 2 loss: 0.067921914 acc: 0.9644966125488281\n",
      "epoch: 85 step: 3 loss: 0.061474603 acc: 0.9680213928222656\n",
      "epoch: 85 step: 4 loss: 0.06324148 acc: 0.9709129333496094\n",
      "epoch: 85 step: 5 loss: 0.06571167 acc: 0.9697418212890625\n",
      "epoch: 85 step: 6 loss: 0.07014776 acc: 0.967437744140625\n",
      "epoch: 85 step: 7 loss: 0.07341192 acc: 0.968902587890625\n",
      "epoch: 85 step: 8 loss: 0.07426992 acc: 0.9645919799804688\n",
      "epoch: 85 step: 9 loss: 0.09029891 acc: 0.9583930969238281\n",
      "epoch: 85 step: 10 loss: 0.07188746 acc: 0.9674720764160156\n",
      "epoch: 85 step: 11 loss: 0.07484006 acc: 0.9665794372558594\n",
      "epoch: 85 step: 12 loss: 0.07348405 acc: 0.9663619995117188\n",
      "epoch: 85 step: 13 loss: 0.0749775 acc: 0.9689407348632812\n",
      "epoch: 85 step: 14 loss: 0.0714799 acc: 0.9643592834472656\n",
      "epoch: 85 step: 15 loss: 0.06537447 acc: 0.9686203002929688\n",
      "epoch: 85 step: 16 loss: 0.061656903 acc: 0.9710350036621094\n",
      "epoch: 85 step: 17 loss: 0.07194792 acc: 0.9691276550292969\n",
      "epoch: 85 step: 18 loss: 0.07204142 acc: 0.9651718139648438\n",
      "epoch: 85 step: 19 loss: 0.066432156 acc: 0.9719047546386719\n",
      "epoch: 85 step: 20 loss: 0.07667761 acc: 0.9617424011230469\n",
      "epoch: 85 step: 21 loss: 0.07685379 acc: 0.9659233093261719\n",
      "epoch: 85 step: 22 loss: 0.073867485 acc: 0.9676399230957031\n",
      "epoch: 85 step: 23 loss: 0.08708795 acc: 0.9640121459960938\n",
      "epoch: 85 step: 24 loss: 0.080848046 acc: 0.9606704711914062\n",
      "epoch: 85 step: 25 loss: 0.06959511 acc: 0.96563720703125\n",
      "epoch: 85 step: 26 loss: 0.07123027 acc: 0.9655876159667969\n",
      "epoch: 85 step: 27 loss: 0.07816129 acc: 0.9579277038574219\n",
      "epoch: 85 step: 28 loss: 0.0772075 acc: 0.9614067077636719\n",
      "epoch: 85 step: 29 loss: 0.063371345 acc: 0.9668083190917969\n",
      "epoch: 85 step: 30 loss: 0.07845728 acc: 0.9609260559082031\n",
      "epoch: 85 step: 31 loss: 0.07142897 acc: 0.9647216796875\n",
      "epoch: 85 step: 32 loss: 0.09008317 acc: 0.9728546142578125\n",
      "epoch: 85 step: 33 loss: 0.07180838 acc: 0.9695320129394531\n",
      "epoch: 85 step: 34 loss: 0.083752766 acc: 0.9663543701171875\n",
      "epoch: 85 step: 35 loss: 0.08011056 acc: 0.9661521911621094\n",
      "epoch: 85 step: 36 loss: 0.08097098 acc: 0.9619216918945312\n",
      "epoch: 85 step: 37 loss: 0.079513475 acc: 0.96014404296875\n",
      "epoch: 85 step: 38 loss: 0.079913214 acc: 0.9596328735351562\n",
      "epoch: 85 step: 39 loss: 0.07762494 acc: 0.9593009948730469\n",
      "epoch: 85 step: 40 loss: 0.076875314 acc: 0.9656143188476562\n",
      "epoch: 85 step: 41 loss: 0.07319384 acc: 0.9625892639160156\n",
      "epoch: 85 step: 42 loss: 0.07686261 acc: 0.9706459045410156\n",
      "epoch: 85 step: 43 loss: 0.07973083 acc: 0.9705924987792969\n",
      "epoch: 85 step: 44 loss: 0.06335079 acc: 0.9685516357421875\n",
      "epoch: 85 step: 45 loss: 0.07315358 acc: 0.9631423950195312\n",
      "epoch: 85 step: 46 loss: 0.08380379 acc: 0.9644012451171875\n",
      "epoch: 85 step: 47 loss: 0.080212705 acc: 0.962158203125\n",
      "epoch: 85 step: 48 loss: 0.06806648 acc: 0.9672012329101562\n",
      "epoch: 85 step: 49 loss: 0.07817223 acc: 0.968902587890625\n",
      "epoch: 85 step: 50 loss: 0.08126425 acc: 0.9647026062011719\n",
      "epoch: 85 step: 51 loss: 0.06093432 acc: 0.9739265441894531\n",
      "epoch: 85 step: 52 loss: 0.07493956 acc: 0.9628829956054688\n",
      "epoch: 85 step: 53 loss: 0.074415535 acc: 0.9622268676757812\n",
      "epoch: 85 step: 54 loss: 0.08058502 acc: 0.9590644836425781\n",
      "epoch: 85 step: 55 loss: 0.07877013 acc: 0.9583244323730469\n",
      "epoch: 85 step: 56 loss: 0.076685295 acc: 0.9723014831542969\n",
      "epoch: 85 step: 57 loss: 0.076315746 acc: 0.9655494689941406\n",
      "epoch: 85 step: 58 loss: 0.06476012 acc: 0.9716758728027344\n",
      "epoch: 85 step: 59 loss: 0.07364248 acc: 0.9679107666015625\n",
      "epoch: 85 step: 60 loss: 0.07811148 acc: 0.9657783508300781\n",
      "epoch: 85 step: 61 loss: 0.07638876 acc: 0.9691810607910156\n",
      "epoch: 85 step: 62 loss: 0.08066688 acc: 0.9681892395019531\n",
      "epoch: 85 step: 63 loss: 0.061898693 acc: 0.97515869140625\n",
      "epoch: 85 step: 64 loss: 0.11999265 acc: 0.9613418579101562\n",
      "epoch: 85 step: 65 loss: 0.06899539 acc: 0.9667510986328125\n",
      "epoch: 85 step: 66 loss: 0.075534545 acc: 0.9666824340820312\n",
      "epoch: 85 step: 67 loss: 0.07886176 acc: 0.9622268676757812\n",
      "epoch: 85 step: 68 loss: 0.07297757 acc: 0.9675979614257812\n",
      "epoch: 85 step: 69 loss: 0.088191375 acc: 0.956573486328125\n",
      "epoch: 85 step: 70 loss: 0.10493724 acc: 0.9527091979980469\n",
      "epoch: 85 step: 71 loss: 0.06653799 acc: 0.9683494567871094\n",
      "epoch: 85 step: 72 loss: 0.0819486 acc: 0.9612960815429688\n",
      "epoch: 85 step: 73 loss: 0.09770168 acc: 0.9600257873535156\n",
      "epoch: 85 step: 74 loss: 0.072583616 acc: 0.9681777954101562\n",
      "epoch: 85 step: 75 loss: 0.08587878 acc: 0.9680366516113281\n",
      "epoch: 85 step: 76 loss: 0.075688 acc: 0.9698257446289062\n",
      "epoch: 85 step: 77 loss: 0.07269888 acc: 0.9671058654785156\n",
      "epoch: 85 step: 78 loss: 0.08317412 acc: 0.9691314697265625\n",
      "epoch: 85 step: 79 loss: 0.08169018 acc: 0.9693679809570312\n",
      "epoch: 85 step: 80 loss: 0.07690698 acc: 0.9589767456054688\n",
      "epoch: 85 step: 81 loss: 0.07209723 acc: 0.9652671813964844\n",
      "epoch: 85 step: 82 loss: 0.080502324 acc: 0.9632720947265625\n",
      "epoch: 85 step: 83 loss: 0.083869465 acc: 0.9583015441894531\n",
      "epoch: 85 step: 84 loss: 0.06391837 acc: 0.9650306701660156\n",
      "epoch: 85 step: 85 loss: 0.08388605 acc: 0.963775634765625\n",
      "epoch: 85 step: 86 loss: 0.079192944 acc: 0.9611854553222656\n",
      "epoch: 85 step: 87 loss: 0.083400376 acc: 0.9679374694824219\n",
      "epoch: 85 step: 88 loss: 0.083480075 acc: 0.9658470153808594\n",
      "epoch: 85 step: 89 loss: 0.069948226 acc: 0.9669075012207031\n",
      "epoch: 85 step: 90 loss: 0.08459044 acc: 0.96185302734375\n",
      "epoch: 85 step: 91 loss: 0.091739126 acc: 0.9625930786132812\n",
      "epoch: 85 step: 92 loss: 0.097947896 acc: 0.9615592956542969\n",
      "epoch: 85 step: 93 loss: 0.085306324 acc: 0.9591522216796875\n",
      "epoch: 85 step: 94 loss: 0.09305153 acc: 0.951873779296875\n",
      "epoch: 85 step: 95 loss: 0.08170057 acc: 0.9644050598144531\n",
      "epoch: 85 step: 96 loss: 0.089868605 acc: 0.9582252502441406\n",
      "epoch: 85 step: 97 loss: 0.0716619 acc: 0.9656524658203125\n",
      "epoch: 85 step: 98 loss: 0.11225794 acc: 0.9592437744140625\n",
      "epoch: 85 step: 99 loss: 0.08924351 acc: 0.9593505859375\n",
      "epoch: 85 step: 100 loss: 0.08691002 acc: 0.9573860168457031\n",
      "epoch: 85 step: 101 loss: 0.07032124 acc: 0.9676055908203125\n",
      "epoch: 85 step: 102 loss: 0.08261894 acc: 0.9619674682617188\n",
      "epoch: 85 step: 103 loss: 0.08285658 acc: 0.9637069702148438\n",
      "epoch: 85 step: 104 loss: 0.0801852 acc: 0.9712867736816406\n",
      "epoch: 85 step: 105 loss: 0.093030386 acc: 0.9655189514160156\n",
      "epoch: 85 step: 106 loss: 0.09746152 acc: 0.9642562866210938\n",
      "epoch: 85 step: 107 loss: 0.0960222 acc: 0.9645347595214844\n",
      "epoch: 85 step: 108 loss: 0.08089704 acc: 0.9660758972167969\n",
      "epoch: 85 step: 109 loss: 0.07326963 acc: 0.9668464660644531\n",
      "epoch: 85 step: 110 loss: 0.08150017 acc: 0.9599456787109375\n",
      "epoch: 85 step: 111 loss: 0.09069295 acc: 0.9600410461425781\n",
      "epoch: 85 step: 112 loss: 0.096107826 acc: 0.9605751037597656\n",
      "epoch: 85 step: 113 loss: 0.11835344 acc: 0.9523735046386719\n",
      "epoch: 85 step: 114 loss: 0.08486609 acc: 0.9656639099121094\n",
      "epoch: 85 step: 115 loss: 0.09154644 acc: 0.9580574035644531\n",
      "epoch: 85 step: 116 loss: 0.098926395 acc: 0.9616432189941406\n",
      "epoch: 85 step: 117 loss: 0.07551722 acc: 0.9717826843261719\n",
      "epoch: 85 step: 118 loss: 0.096163735 acc: 0.9619903564453125\n",
      "epoch: 85 step: 119 loss: 0.0884169 acc: 0.9628982543945312\n",
      "epoch: 85 step: 120 loss: 0.09486041 acc: 0.9600372314453125\n",
      "epoch: 85 step: 121 loss: 0.09588009 acc: 0.9602470397949219\n",
      "epoch: 85 step: 122 loss: 0.068147995 acc: 0.9687843322753906\n",
      "epoch: 85 step: 123 loss: 0.08743276 acc: 0.962493896484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 85 step: 124 loss: 0.108434066 acc: 0.9633614676339286\n",
      "epoch: 85 validation_loss: 0.097 validation_dice: 0.840053267260172\n",
      "epoch: 85 test_dataset dice: 0.7183772357514675\n",
      "time cost 0.5371460119883219 min\n",
      "dice_best: 0.8622422678227601\n",
      "******************************** epoch  85  is finished. *********************************\n",
      "epoch: 86 step: 1 loss: 0.08705003 acc: 0.9655647277832031\n",
      "epoch: 86 step: 2 loss: 0.10145614 acc: 0.96795654296875\n",
      "epoch: 86 step: 3 loss: 0.13065283 acc: 0.9569206237792969\n",
      "epoch: 86 step: 4 loss: 0.094054535 acc: 0.9642143249511719\n",
      "epoch: 86 step: 5 loss: 0.09109608 acc: 0.9591102600097656\n",
      "epoch: 86 step: 6 loss: 0.093779735 acc: 0.9619522094726562\n",
      "epoch: 86 step: 7 loss: 0.0901129 acc: 0.9643783569335938\n",
      "epoch: 86 step: 8 loss: 0.096232474 acc: 0.9571533203125\n",
      "epoch: 86 step: 9 loss: 0.08944499 acc: 0.9662208557128906\n",
      "epoch: 86 step: 10 loss: 0.07868252 acc: 0.971160888671875\n",
      "epoch: 86 step: 11 loss: 0.07653459 acc: 0.9639015197753906\n",
      "epoch: 86 step: 12 loss: 0.08608971 acc: 0.9677658081054688\n",
      "epoch: 86 step: 13 loss: 0.07919604 acc: 0.9652061462402344\n",
      "epoch: 86 step: 14 loss: 0.1106479 acc: 0.9617156982421875\n",
      "epoch: 86 step: 15 loss: 0.09070419 acc: 0.9663352966308594\n",
      "epoch: 86 step: 16 loss: 0.08584063 acc: 0.9657974243164062\n",
      "epoch: 86 step: 17 loss: 0.08577863 acc: 0.9635772705078125\n",
      "epoch: 86 step: 18 loss: 0.08618291 acc: 0.9634933471679688\n",
      "epoch: 86 step: 19 loss: 0.102844656 acc: 0.9631156921386719\n",
      "epoch: 86 step: 20 loss: 0.09854863 acc: 0.9577293395996094\n",
      "epoch: 86 step: 21 loss: 0.07610038 acc: 0.9706573486328125\n",
      "epoch: 86 step: 22 loss: 0.09415518 acc: 0.9624977111816406\n",
      "epoch: 86 step: 23 loss: 0.079445995 acc: 0.9655303955078125\n",
      "epoch: 86 step: 24 loss: 0.10228962 acc: 0.9597282409667969\n",
      "epoch: 86 step: 25 loss: 0.071412385 acc: 0.9679985046386719\n",
      "epoch: 86 step: 26 loss: 0.07601808 acc: 0.9703216552734375\n",
      "epoch: 86 step: 27 loss: 0.11965455 acc: 0.9511070251464844\n",
      "epoch: 86 step: 28 loss: 0.10137291 acc: 0.9620590209960938\n",
      "epoch: 86 step: 29 loss: 0.10660888 acc: 0.9615058898925781\n",
      "epoch: 86 step: 30 loss: 0.094036214 acc: 0.9598426818847656\n",
      "epoch: 86 step: 31 loss: 0.08952376 acc: 0.9660072326660156\n",
      "epoch: 86 step: 32 loss: 0.09587048 acc: 0.9592857360839844\n",
      "epoch: 86 step: 33 loss: 0.08508334 acc: 0.9590835571289062\n",
      "epoch: 86 step: 34 loss: 0.102432854 acc: 0.9525146484375\n",
      "epoch: 86 step: 35 loss: 0.08225496 acc: 0.9647712707519531\n",
      "epoch: 86 step: 36 loss: 0.08548923 acc: 0.9641265869140625\n",
      "epoch: 86 step: 37 loss: 0.09836481 acc: 0.956939697265625\n",
      "epoch: 86 step: 38 loss: 0.085916 acc: 0.9641494750976562\n",
      "epoch: 86 step: 39 loss: 0.08382278 acc: 0.9596900939941406\n",
      "epoch: 86 step: 40 loss: 0.070582524 acc: 0.9652786254882812\n",
      "epoch: 86 step: 41 loss: 0.08843876 acc: 0.9626808166503906\n",
      "epoch: 86 step: 42 loss: 0.08710126 acc: 0.9592399597167969\n",
      "epoch: 86 step: 43 loss: 0.10076823 acc: 0.9615325927734375\n",
      "epoch: 86 step: 44 loss: 0.08631863 acc: 0.9631881713867188\n",
      "epoch: 86 step: 45 loss: 0.07997815 acc: 0.9646186828613281\n",
      "epoch: 86 step: 46 loss: 0.081078894 acc: 0.9674758911132812\n",
      "epoch: 86 step: 47 loss: 0.09470242 acc: 0.9639396667480469\n",
      "epoch: 86 step: 48 loss: 0.10416343 acc: 0.953369140625\n",
      "epoch: 86 step: 49 loss: 0.098760195 acc: 0.9577217102050781\n",
      "epoch: 86 step: 50 loss: 0.09558111 acc: 0.9512062072753906\n",
      "epoch: 86 step: 51 loss: 0.091005385 acc: 0.9634208679199219\n",
      "epoch: 86 step: 52 loss: 0.08847229 acc: 0.9590187072753906\n",
      "epoch: 86 step: 53 loss: 0.07026135 acc: 0.9610023498535156\n",
      "epoch: 86 step: 54 loss: 0.10052138 acc: 0.9529266357421875\n",
      "epoch: 86 step: 55 loss: 0.08511141 acc: 0.9584274291992188\n",
      "epoch: 86 step: 56 loss: 0.075544 acc: 0.9696617126464844\n",
      "epoch: 86 step: 57 loss: 0.08875846 acc: 0.9618759155273438\n",
      "epoch: 86 step: 58 loss: 0.08683111 acc: 0.9640426635742188\n",
      "epoch: 86 step: 59 loss: 0.072197974 acc: 0.9718513488769531\n",
      "epoch: 86 step: 60 loss: 0.09025287 acc: 0.9624252319335938\n",
      "epoch: 86 step: 61 loss: 0.090113774 acc: 0.9709587097167969\n",
      "epoch: 86 step: 62 loss: 0.0857566 acc: 0.9638824462890625\n",
      "epoch: 86 step: 63 loss: 0.09355325 acc: 0.9667091369628906\n",
      "epoch: 86 step: 64 loss: 0.07091611 acc: 0.9691390991210938\n",
      "epoch: 86 step: 65 loss: 0.06456467 acc: 0.9746170043945312\n",
      "epoch: 86 step: 66 loss: 0.06802314 acc: 0.967926025390625\n",
      "epoch: 86 step: 67 loss: 0.077042624 acc: 0.9707870483398438\n",
      "epoch: 86 step: 68 loss: 0.072481796 acc: 0.9638824462890625\n",
      "epoch: 86 step: 69 loss: 0.09466024 acc: 0.9533424377441406\n",
      "epoch: 86 step: 70 loss: 0.068607785 acc: 0.9661636352539062\n",
      "epoch: 86 step: 71 loss: 0.07930633 acc: 0.9677963256835938\n",
      "epoch: 86 step: 72 loss: 0.08750493 acc: 0.96063232421875\n",
      "epoch: 86 step: 73 loss: 0.07890696 acc: 0.9683837890625\n",
      "epoch: 86 step: 74 loss: 0.06238323 acc: 0.9728431701660156\n",
      "epoch: 86 step: 75 loss: 0.08002487 acc: 0.9623832702636719\n",
      "epoch: 86 step: 76 loss: 0.07464111 acc: 0.9751815795898438\n",
      "epoch: 86 step: 77 loss: 0.082591854 acc: 0.9642486572265625\n",
      "epoch: 86 step: 78 loss: 0.08635211 acc: 0.9626274108886719\n",
      "epoch: 86 step: 79 loss: 0.084504806 acc: 0.9580154418945312\n",
      "epoch: 86 step: 80 loss: 0.072650224 acc: 0.9671516418457031\n",
      "epoch: 86 step: 81 loss: 0.07330703 acc: 0.967529296875\n",
      "epoch: 86 step: 82 loss: 0.08719823 acc: 0.9523086547851562\n",
      "epoch: 86 step: 83 loss: 0.093601674 acc: 0.9510879516601562\n",
      "epoch: 86 step: 84 loss: 0.10598675 acc: 0.9631729125976562\n",
      "epoch: 86 step: 85 loss: 0.067528956 acc: 0.9696846008300781\n",
      "epoch: 86 step: 86 loss: 0.05970958 acc: 0.9708213806152344\n",
      "epoch: 86 step: 87 loss: 0.09400646 acc: 0.9651336669921875\n",
      "epoch: 86 step: 88 loss: 0.07417514 acc: 0.96966552734375\n",
      "epoch: 86 step: 89 loss: 0.091332756 acc: 0.9648933410644531\n",
      "epoch: 86 step: 90 loss: 0.07566844 acc: 0.96868896484375\n",
      "epoch: 86 step: 91 loss: 0.0797945 acc: 0.9666519165039062\n",
      "epoch: 86 step: 92 loss: 0.0901545 acc: 0.9646835327148438\n",
      "epoch: 86 step: 93 loss: 0.08118867 acc: 0.9687919616699219\n",
      "epoch: 86 step: 94 loss: 0.10167105 acc: 0.9551162719726562\n",
      "epoch: 86 step: 95 loss: 0.07753347 acc: 0.9677886962890625\n",
      "epoch: 86 step: 96 loss: 0.08807387 acc: 0.9553070068359375\n",
      "epoch: 86 step: 97 loss: 0.07450574 acc: 0.9674224853515625\n",
      "epoch: 86 step: 98 loss: 0.07974981 acc: 0.9595603942871094\n",
      "epoch: 86 step: 99 loss: 0.08288474 acc: 0.9564704895019531\n",
      "epoch: 86 step: 100 loss: 0.06689465 acc: 0.9687385559082031\n",
      "epoch: 86 step: 101 loss: 0.09939733 acc: 0.9484786987304688\n",
      "epoch: 86 step: 102 loss: 0.077942595 acc: 0.9587059020996094\n",
      "epoch: 86 step: 103 loss: 0.07278439 acc: 0.9654083251953125\n",
      "epoch: 86 step: 104 loss: 0.07923173 acc: 0.9630699157714844\n",
      "epoch: 86 step: 105 loss: 0.06474323 acc: 0.972137451171875\n",
      "epoch: 86 step: 106 loss: 0.06277127 acc: 0.9707489013671875\n",
      "epoch: 86 step: 107 loss: 0.06540323 acc: 0.9683723449707031\n",
      "epoch: 86 step: 108 loss: 0.06633114 acc: 0.9713211059570312\n",
      "epoch: 86 step: 109 loss: 0.09631315 acc: 0.9628181457519531\n",
      "epoch: 86 step: 110 loss: 0.0787471 acc: 0.9672470092773438\n",
      "epoch: 86 step: 111 loss: 0.081629194 acc: 0.960205078125\n",
      "epoch: 86 step: 112 loss: 0.0774715 acc: 0.9644508361816406\n",
      "epoch: 86 step: 113 loss: 0.072022445 acc: 0.9707489013671875\n",
      "epoch: 86 step: 114 loss: 0.06975934 acc: 0.96844482421875\n",
      "epoch: 86 step: 115 loss: 0.07668091 acc: 0.9613990783691406\n",
      "epoch: 86 step: 116 loss: 0.07701103 acc: 0.9640579223632812\n",
      "epoch: 86 step: 117 loss: 0.085713126 acc: 0.9605712890625\n",
      "epoch: 86 step: 118 loss: 0.07035958 acc: 0.9647598266601562\n",
      "epoch: 86 step: 119 loss: 0.07833968 acc: 0.9601097106933594\n",
      "epoch: 86 step: 120 loss: 0.08738723 acc: 0.9587020874023438\n",
      "epoch: 86 step: 121 loss: 0.08477085 acc: 0.9581146240234375\n",
      "epoch: 86 step: 122 loss: 0.06611558 acc: 0.967559814453125\n",
      "epoch: 86 step: 123 loss: 0.08083394 acc: 0.9541206359863281\n",
      "epoch: 86 step: 124 loss: 0.08394669 acc: 0.95489501953125\n",
      "epoch: 86 validation_loss: 0.085 validation_dice: 0.857899649012929\n",
      "epoch: 86 test_dataset dice: 0.7339960200540755\n",
      "time cost 0.5357621510823568 min\n",
      "dice_best: 0.8622422678227601\n",
      "******************************** epoch  86  is finished. *********************************\n",
      "epoch: 87 step: 1 loss: 0.07372538 acc: 0.9607696533203125\n",
      "epoch: 87 step: 2 loss: 0.06902068 acc: 0.9645919799804688\n",
      "epoch: 87 step: 3 loss: 0.07183603 acc: 0.966064453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 87 step: 4 loss: 0.09235777 acc: 0.9573593139648438\n",
      "epoch: 87 step: 5 loss: 0.08322961 acc: 0.9598350524902344\n",
      "epoch: 87 step: 6 loss: 0.07435108 acc: 0.9694557189941406\n",
      "epoch: 87 step: 7 loss: 0.07560825 acc: 0.9695053100585938\n",
      "epoch: 87 step: 8 loss: 0.070652284 acc: 0.9691429138183594\n",
      "epoch: 87 step: 9 loss: 0.06709617 acc: 0.9714927673339844\n",
      "epoch: 87 step: 10 loss: 0.07084678 acc: 0.9639739990234375\n",
      "epoch: 87 step: 11 loss: 0.08835302 acc: 0.9589424133300781\n",
      "epoch: 87 step: 12 loss: 0.07322869 acc: 0.9687652587890625\n",
      "epoch: 87 step: 13 loss: 0.07280887 acc: 0.9617385864257812\n",
      "epoch: 87 step: 14 loss: 0.058100253 acc: 0.9740753173828125\n",
      "epoch: 87 step: 15 loss: 0.070732646 acc: 0.9683685302734375\n",
      "epoch: 87 step: 16 loss: 0.08701828 acc: 0.9575080871582031\n",
      "epoch: 87 step: 17 loss: 0.07197986 acc: 0.9663467407226562\n",
      "epoch: 87 step: 18 loss: 0.08357077 acc: 0.964752197265625\n",
      "epoch: 87 step: 19 loss: 0.06870167 acc: 0.9691963195800781\n",
      "epoch: 87 step: 20 loss: 0.067797326 acc: 0.9671630859375\n",
      "epoch: 87 step: 21 loss: 0.07774563 acc: 0.9598388671875\n",
      "epoch: 87 step: 22 loss: 0.078750275 acc: 0.9661483764648438\n",
      "epoch: 87 step: 23 loss: 0.07922692 acc: 0.9550666809082031\n",
      "epoch: 87 step: 24 loss: 0.07600797 acc: 0.963592529296875\n",
      "epoch: 87 step: 25 loss: 0.06425439 acc: 0.969451904296875\n",
      "epoch: 87 step: 26 loss: 0.08596574 acc: 0.9567337036132812\n",
      "epoch: 87 step: 27 loss: 0.074840054 acc: 0.9573020935058594\n",
      "epoch: 87 step: 28 loss: 0.07217516 acc: 0.9572868347167969\n",
      "epoch: 87 step: 29 loss: 0.07934412 acc: 0.9677810668945312\n",
      "epoch: 87 step: 30 loss: 0.07750659 acc: 0.9666709899902344\n",
      "epoch: 87 step: 31 loss: 0.0947098 acc: 0.9597663879394531\n",
      "epoch: 87 step: 32 loss: 0.063346855 acc: 0.9717178344726562\n",
      "epoch: 87 step: 33 loss: 0.0656649 acc: 0.967529296875\n",
      "epoch: 87 step: 34 loss: 0.06380956 acc: 0.9695358276367188\n",
      "epoch: 87 step: 35 loss: 0.07728737 acc: 0.95928955078125\n",
      "epoch: 87 step: 36 loss: 0.08908985 acc: 0.9688606262207031\n",
      "epoch: 87 step: 37 loss: 0.07648747 acc: 0.9646949768066406\n",
      "epoch: 87 step: 38 loss: 0.082576506 acc: 0.9598770141601562\n",
      "epoch: 87 step: 39 loss: 0.06585034 acc: 0.9665641784667969\n",
      "epoch: 87 step: 40 loss: 0.071016565 acc: 0.9623641967773438\n",
      "epoch: 87 step: 41 loss: 0.06559534 acc: 0.96337890625\n",
      "epoch: 87 step: 42 loss: 0.0751918 acc: 0.961578369140625\n",
      "epoch: 87 step: 43 loss: 0.07684651 acc: 0.95831298828125\n",
      "epoch: 87 step: 44 loss: 0.072905354 acc: 0.9698867797851562\n",
      "epoch: 87 step: 45 loss: 0.08467139 acc: 0.963836669921875\n",
      "epoch: 87 step: 46 loss: 0.06347476 acc: 0.9736137390136719\n",
      "epoch: 87 step: 47 loss: 0.091202736 acc: 0.9678306579589844\n",
      "epoch: 87 step: 48 loss: 0.059860904 acc: 0.9724655151367188\n",
      "epoch: 87 step: 49 loss: 0.075733654 acc: 0.9712142944335938\n",
      "epoch: 87 step: 50 loss: 0.06921963 acc: 0.9678535461425781\n",
      "epoch: 87 step: 51 loss: 0.06007836 acc: 0.9747428894042969\n",
      "epoch: 87 step: 52 loss: 0.06919473 acc: 0.9665679931640625\n",
      "epoch: 87 step: 53 loss: 0.07742688 acc: 0.9619789123535156\n",
      "epoch: 87 step: 54 loss: 0.09363854 acc: 0.9592933654785156\n",
      "epoch: 87 step: 55 loss: 0.08054952 acc: 0.9618263244628906\n",
      "epoch: 87 step: 56 loss: 0.08602409 acc: 0.9687080383300781\n",
      "epoch: 87 step: 57 loss: 0.06972573 acc: 0.965911865234375\n",
      "epoch: 87 step: 58 loss: 0.0726 acc: 0.9640312194824219\n",
      "epoch: 87 step: 59 loss: 0.06342127 acc: 0.96771240234375\n",
      "epoch: 87 step: 60 loss: 0.072931804 acc: 0.9597396850585938\n",
      "epoch: 87 step: 61 loss: 0.07761787 acc: 0.9611587524414062\n",
      "epoch: 87 step: 62 loss: 0.07598368 acc: 0.9711227416992188\n",
      "epoch: 87 step: 63 loss: 0.08306521 acc: 0.9650192260742188\n",
      "epoch: 87 step: 64 loss: 0.071026936 acc: 0.9748001098632812\n",
      "epoch: 87 step: 65 loss: 0.09006565 acc: 0.9645652770996094\n",
      "epoch: 87 step: 66 loss: 0.08396617 acc: 0.9696044921875\n",
      "epoch: 87 step: 67 loss: 0.066791 acc: 0.9701919555664062\n",
      "epoch: 87 step: 68 loss: 0.06462239 acc: 0.9735031127929688\n",
      "epoch: 87 step: 69 loss: 0.066757016 acc: 0.9702072143554688\n",
      "epoch: 87 step: 70 loss: 0.062870935 acc: 0.9700050354003906\n",
      "epoch: 87 step: 71 loss: 0.08368012 acc: 0.9626922607421875\n",
      "epoch: 87 step: 72 loss: 0.10544876 acc: 0.9643363952636719\n",
      "epoch: 87 step: 73 loss: 0.06928752 acc: 0.9686813354492188\n",
      "epoch: 87 step: 74 loss: 0.07453799 acc: 0.9600334167480469\n",
      "epoch: 87 step: 75 loss: 0.06749094 acc: 0.9672393798828125\n",
      "epoch: 87 step: 76 loss: 0.06789629 acc: 0.9656448364257812\n",
      "epoch: 87 step: 77 loss: 0.073486045 acc: 0.9716796875\n",
      "epoch: 87 step: 78 loss: 0.109059006 acc: 0.9558677673339844\n",
      "epoch: 87 step: 79 loss: 0.08719456 acc: 0.9611167907714844\n",
      "epoch: 87 step: 80 loss: 0.0739222 acc: 0.9659194946289062\n",
      "epoch: 87 step: 81 loss: 0.066371955 acc: 0.9675674438476562\n",
      "epoch: 87 step: 82 loss: 0.08445821 acc: 0.9638557434082031\n",
      "epoch: 87 step: 83 loss: 0.07441347 acc: 0.9661331176757812\n",
      "epoch: 87 step: 84 loss: 0.08011722 acc: 0.9635887145996094\n",
      "epoch: 87 step: 85 loss: 0.08189636 acc: 0.9623908996582031\n",
      "epoch: 87 step: 86 loss: 0.076265305 acc: 0.9605979919433594\n",
      "epoch: 87 step: 87 loss: 0.090528235 acc: 0.9597892761230469\n",
      "epoch: 87 step: 88 loss: 0.07574495 acc: 0.9605216979980469\n",
      "epoch: 87 step: 89 loss: 0.08658291 acc: 0.9555816650390625\n",
      "epoch: 87 step: 90 loss: 0.06918225 acc: 0.9658699035644531\n",
      "epoch: 87 step: 91 loss: 0.08305113 acc: 0.9521598815917969\n",
      "epoch: 87 step: 92 loss: 0.07481793 acc: 0.9641609191894531\n",
      "epoch: 87 step: 93 loss: 0.0748008 acc: 0.97113037109375\n",
      "epoch: 87 step: 94 loss: 0.07579597 acc: 0.9668464660644531\n",
      "epoch: 87 step: 95 loss: 0.08852096 acc: 0.9688796997070312\n",
      "epoch: 87 step: 96 loss: 0.07734992 acc: 0.9681663513183594\n",
      "epoch: 87 step: 97 loss: 0.080348484 acc: 0.9639244079589844\n",
      "epoch: 87 step: 98 loss: 0.07630461 acc: 0.9607925415039062\n",
      "epoch: 87 step: 99 loss: 0.06420713 acc: 0.9671554565429688\n",
      "epoch: 87 step: 100 loss: 0.06438721 acc: 0.9687767028808594\n",
      "epoch: 87 step: 101 loss: 0.06690056 acc: 0.9638023376464844\n",
      "epoch: 87 step: 102 loss: 0.06653493 acc: 0.9709358215332031\n",
      "epoch: 87 step: 103 loss: 0.07450741 acc: 0.9653778076171875\n",
      "epoch: 87 step: 104 loss: 0.069623366 acc: 0.9683418273925781\n",
      "epoch: 87 step: 105 loss: 0.06073996 acc: 0.9689254760742188\n",
      "epoch: 87 step: 106 loss: 0.07198578 acc: 0.96807861328125\n",
      "epoch: 87 step: 107 loss: 0.06632821 acc: 0.9685249328613281\n",
      "epoch: 87 step: 108 loss: 0.079628505 acc: 0.9680252075195312\n",
      "epoch: 87 step: 109 loss: 0.08161769 acc: 0.9659500122070312\n",
      "epoch: 87 step: 110 loss: 0.074689075 acc: 0.9639625549316406\n",
      "epoch: 87 step: 111 loss: 0.07276997 acc: 0.9637947082519531\n",
      "epoch: 87 step: 112 loss: 0.0747419 acc: 0.9661674499511719\n",
      "epoch: 87 step: 113 loss: 0.081499666 acc: 0.9628257751464844\n",
      "epoch: 87 step: 114 loss: 0.08245132 acc: 0.9571151733398438\n",
      "epoch: 87 step: 115 loss: 0.08452663 acc: 0.9570808410644531\n",
      "epoch: 87 step: 116 loss: 0.0858514 acc: 0.9554939270019531\n",
      "epoch: 87 step: 117 loss: 0.08778745 acc: 0.9565696716308594\n",
      "epoch: 87 step: 118 loss: 0.075441234 acc: 0.9647407531738281\n",
      "epoch: 87 step: 119 loss: 0.07161219 acc: 0.9675445556640625\n",
      "epoch: 87 step: 120 loss: 0.0865529 acc: 0.9590339660644531\n",
      "epoch: 87 step: 121 loss: 0.06315529 acc: 0.9705848693847656\n",
      "epoch: 87 step: 122 loss: 0.06933574 acc: 0.9677200317382812\n",
      "epoch: 87 step: 123 loss: 0.068205275 acc: 0.9710807800292969\n",
      "epoch: 87 step: 124 loss: 0.07579676 acc: 0.9629167829241071\n",
      "epoch: 87 validation_loss: 0.08 validation_dice: 0.8501306644765549\n",
      "epoch: 87 test_dataset dice: 0.7455138789895905\n",
      "time cost 0.536165452003479 min\n",
      "dice_best: 0.8622422678227601\n",
      "******************************** epoch  87  is finished. *********************************\n",
      "epoch: 88 step: 1 loss: 0.077022195 acc: 0.9688148498535156\n",
      "epoch: 88 step: 2 loss: 0.085818514 acc: 0.9666557312011719\n",
      "epoch: 88 step: 3 loss: 0.08699043 acc: 0.9636306762695312\n",
      "epoch: 88 step: 4 loss: 0.062649325 acc: 0.9727706909179688\n",
      "epoch: 88 step: 5 loss: 0.08141102 acc: 0.9635353088378906\n",
      "epoch: 88 step: 6 loss: 0.06225164 acc: 0.9690208435058594\n",
      "epoch: 88 step: 7 loss: 0.07319885 acc: 0.9684829711914062\n",
      "epoch: 88 step: 8 loss: 0.07569441 acc: 0.9610557556152344\n",
      "epoch: 88 step: 9 loss: 0.07187324 acc: 0.9661750793457031\n",
      "epoch: 88 step: 10 loss: 0.09171405 acc: 0.9562606811523438\n",
      "epoch: 88 step: 11 loss: 0.06132247 acc: 0.9691543579101562\n",
      "epoch: 88 step: 12 loss: 0.070422776 acc: 0.9670448303222656\n",
      "epoch: 88 step: 13 loss: 0.07550861 acc: 0.9657211303710938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 88 step: 14 loss: 0.07358948 acc: 0.967254638671875\n",
      "epoch: 88 step: 15 loss: 0.0665416 acc: 0.972320556640625\n",
      "epoch: 88 step: 16 loss: 0.079418495 acc: 0.9586830139160156\n",
      "epoch: 88 step: 17 loss: 0.07087367 acc: 0.9645614624023438\n",
      "epoch: 88 step: 18 loss: 0.08944482 acc: 0.9606742858886719\n",
      "epoch: 88 step: 19 loss: 0.06671299 acc: 0.9657554626464844\n",
      "epoch: 88 step: 20 loss: 0.06392037 acc: 0.9729957580566406\n",
      "epoch: 88 step: 21 loss: 0.07291578 acc: 0.9645309448242188\n",
      "epoch: 88 step: 22 loss: 0.06950479 acc: 0.9702644348144531\n",
      "epoch: 88 step: 23 loss: 0.07282903 acc: 0.967041015625\n",
      "epoch: 88 step: 24 loss: 0.06904282 acc: 0.9699211120605469\n",
      "epoch: 88 step: 25 loss: 0.06643284 acc: 0.9704360961914062\n",
      "epoch: 88 step: 26 loss: 0.07694721 acc: 0.9680023193359375\n",
      "epoch: 88 step: 27 loss: 0.060167957 acc: 0.9720916748046875\n",
      "epoch: 88 step: 28 loss: 0.06700588 acc: 0.9720039367675781\n",
      "epoch: 88 step: 29 loss: 0.06584586 acc: 0.9718666076660156\n",
      "epoch: 88 step: 30 loss: 0.06860009 acc: 0.9708366394042969\n",
      "epoch: 88 step: 31 loss: 0.07083552 acc: 0.9649200439453125\n",
      "epoch: 88 step: 32 loss: 0.062946215 acc: 0.96844482421875\n",
      "epoch: 88 step: 33 loss: 0.0761816 acc: 0.9646530151367188\n",
      "epoch: 88 step: 34 loss: 0.08438749 acc: 0.9550666809082031\n",
      "epoch: 88 step: 35 loss: 0.08991478 acc: 0.9640731811523438\n",
      "epoch: 88 step: 36 loss: 0.07814609 acc: 0.9621734619140625\n",
      "epoch: 88 step: 37 loss: 0.10104973 acc: 0.9574012756347656\n",
      "epoch: 88 step: 38 loss: 0.07372241 acc: 0.9627723693847656\n",
      "epoch: 88 step: 39 loss: 0.071415566 acc: 0.9609260559082031\n",
      "epoch: 88 step: 40 loss: 0.075406596 acc: 0.9628524780273438\n",
      "epoch: 88 step: 41 loss: 0.083541386 acc: 0.9566192626953125\n",
      "epoch: 88 step: 42 loss: 0.06522574 acc: 0.9704627990722656\n",
      "epoch: 88 step: 43 loss: 0.06783859 acc: 0.9661827087402344\n",
      "epoch: 88 step: 44 loss: 0.092518054 acc: 0.9586753845214844\n",
      "epoch: 88 step: 45 loss: 0.082655154 acc: 0.9656143188476562\n",
      "epoch: 88 step: 46 loss: 0.06712802 acc: 0.9684333801269531\n",
      "epoch: 88 step: 47 loss: 0.0795574 acc: 0.96490478515625\n",
      "epoch: 88 step: 48 loss: 0.073125385 acc: 0.9710540771484375\n",
      "epoch: 88 step: 49 loss: 0.07775207 acc: 0.9614448547363281\n",
      "epoch: 88 step: 50 loss: 0.08201174 acc: 0.9608688354492188\n",
      "epoch: 88 step: 51 loss: 0.07410123 acc: 0.9691734313964844\n",
      "epoch: 88 step: 52 loss: 0.062253583 acc: 0.9662857055664062\n",
      "epoch: 88 step: 53 loss: 0.08109983 acc: 0.9597854614257812\n",
      "epoch: 88 step: 54 loss: 0.07246938 acc: 0.96820068359375\n",
      "epoch: 88 step: 55 loss: 0.064576924 acc: 0.9712028503417969\n",
      "epoch: 88 step: 56 loss: 0.08796714 acc: 0.9612045288085938\n",
      "epoch: 88 step: 57 loss: 0.07307425 acc: 0.9597892761230469\n",
      "epoch: 88 step: 58 loss: 0.077031426 acc: 0.9649620056152344\n",
      "epoch: 88 step: 59 loss: 0.070533164 acc: 0.9619712829589844\n",
      "epoch: 88 step: 60 loss: 0.08002131 acc: 0.9616966247558594\n",
      "epoch: 88 step: 61 loss: 0.07475989 acc: 0.9650726318359375\n",
      "epoch: 88 step: 62 loss: 0.081841804 acc: 0.96868896484375\n",
      "epoch: 88 step: 63 loss: 0.070257366 acc: 0.96844482421875\n",
      "epoch: 88 step: 64 loss: 0.067880236 acc: 0.9691925048828125\n",
      "epoch: 88 step: 65 loss: 0.08425952 acc: 0.9597053527832031\n",
      "epoch: 88 step: 66 loss: 0.0703873 acc: 0.9731979370117188\n",
      "epoch: 88 step: 67 loss: 0.084102534 acc: 0.9628753662109375\n",
      "epoch: 88 step: 68 loss: 0.07436679 acc: 0.968505859375\n",
      "epoch: 88 step: 69 loss: 0.06908639 acc: 0.9705810546875\n",
      "epoch: 88 step: 70 loss: 0.082723856 acc: 0.9603462219238281\n",
      "epoch: 88 step: 71 loss: 0.09276299 acc: 0.961700439453125\n",
      "epoch: 88 step: 72 loss: 0.080077596 acc: 0.9620933532714844\n",
      "epoch: 88 step: 73 loss: 0.07941253 acc: 0.9614448547363281\n",
      "epoch: 88 step: 74 loss: 0.07971627 acc: 0.963134765625\n",
      "epoch: 88 step: 75 loss: 0.082703285 acc: 0.9630661010742188\n",
      "epoch: 88 step: 76 loss: 0.09232929 acc: 0.9567146301269531\n",
      "epoch: 88 step: 77 loss: 0.073734164 acc: 0.9659957885742188\n",
      "epoch: 88 step: 78 loss: 0.067647085 acc: 0.972900390625\n",
      "epoch: 88 step: 79 loss: 0.08621258 acc: 0.9677505493164062\n",
      "epoch: 88 step: 80 loss: 0.090736374 acc: 0.9645462036132812\n",
      "epoch: 88 step: 81 loss: 0.06968982 acc: 0.9717369079589844\n",
      "epoch: 88 step: 82 loss: 0.08718685 acc: 0.9628562927246094\n",
      "epoch: 88 step: 83 loss: 0.0820245 acc: 0.9697685241699219\n",
      "epoch: 88 step: 84 loss: 0.07279498 acc: 0.9673690795898438\n",
      "epoch: 88 step: 85 loss: 0.083421506 acc: 0.9685554504394531\n",
      "epoch: 88 step: 86 loss: 0.07653217 acc: 0.9551277160644531\n",
      "epoch: 88 step: 87 loss: 0.089354284 acc: 0.9537887573242188\n",
      "epoch: 88 step: 88 loss: 0.061605334 acc: 0.9637794494628906\n",
      "epoch: 88 step: 89 loss: 0.07141999 acc: 0.958953857421875\n",
      "epoch: 88 step: 90 loss: 0.07575289 acc: 0.9674110412597656\n",
      "epoch: 88 step: 91 loss: 0.08233938 acc: 0.96112060546875\n",
      "epoch: 88 step: 92 loss: 0.0749372 acc: 0.9678153991699219\n",
      "epoch: 88 step: 93 loss: 0.083312735 acc: 0.9739494323730469\n",
      "epoch: 88 step: 94 loss: 0.06910417 acc: 0.97515869140625\n",
      "epoch: 88 step: 95 loss: 0.07433244 acc: 0.9713630676269531\n",
      "epoch: 88 step: 96 loss: 0.09004278 acc: 0.9586334228515625\n",
      "epoch: 88 step: 97 loss: 0.08397995 acc: 0.9656143188476562\n",
      "epoch: 88 step: 98 loss: 0.074650586 acc: 0.9689712524414062\n",
      "epoch: 88 step: 99 loss: 0.091414124 acc: 0.9733505249023438\n",
      "epoch: 88 step: 100 loss: 0.093921095 acc: 0.964447021484375\n",
      "epoch: 88 step: 101 loss: 0.1422406 acc: 0.9688148498535156\n",
      "epoch: 88 step: 102 loss: 0.074494936 acc: 0.9640769958496094\n",
      "epoch: 88 step: 103 loss: 0.06708251 acc: 0.9678993225097656\n",
      "epoch: 88 step: 104 loss: 0.09711252 acc: 0.9527702331542969\n",
      "epoch: 88 step: 105 loss: 0.10191505 acc: 0.9580230712890625\n",
      "epoch: 88 step: 106 loss: 0.07538054 acc: 0.9685134887695312\n",
      "epoch: 88 step: 107 loss: 0.081428066 acc: 0.9596290588378906\n",
      "epoch: 88 step: 108 loss: 0.0744393 acc: 0.9666824340820312\n",
      "epoch: 88 step: 109 loss: 0.09864545 acc: 0.9575996398925781\n",
      "epoch: 88 step: 110 loss: 0.08962399 acc: 0.9589042663574219\n",
      "epoch: 88 step: 111 loss: 0.08419171 acc: 0.9644317626953125\n",
      "epoch: 88 step: 112 loss: 0.068526104 acc: 0.97076416015625\n",
      "epoch: 88 step: 113 loss: 0.119205564 acc: 0.9488105773925781\n",
      "epoch: 88 step: 114 loss: 0.08917361 acc: 0.9658164978027344\n",
      "epoch: 88 step: 115 loss: 0.07706657 acc: 0.9677276611328125\n",
      "epoch: 88 step: 116 loss: 0.08011646 acc: 0.9608383178710938\n",
      "epoch: 88 step: 117 loss: 0.07401687 acc: 0.9659080505371094\n",
      "epoch: 88 step: 118 loss: 0.079223536 acc: 0.9655303955078125\n",
      "epoch: 88 step: 119 loss: 0.08507283 acc: 0.9582023620605469\n",
      "epoch: 88 step: 120 loss: 0.09960327 acc: 0.964324951171875\n",
      "epoch: 88 step: 121 loss: 0.12638055 acc: 0.954559326171875\n",
      "epoch: 88 step: 122 loss: 0.06810125 acc: 0.9667282104492188\n",
      "epoch: 88 step: 123 loss: 0.08322993 acc: 0.95977783203125\n",
      "epoch: 88 step: 124 loss: 0.07113685 acc: 0.9683925083705357\n",
      "epoch: 88 validation_loss: 0.097 validation_dice: 0.8518842635264644\n",
      "epoch: 88 test_dataset dice: 0.7354191290758423\n",
      "time cost 0.535428261756897 min\n",
      "dice_best: 0.8622422678227601\n",
      "******************************** epoch  88  is finished. *********************************\n",
      "epoch: 89 step: 1 loss: 0.090139195 acc: 0.9606094360351562\n",
      "epoch: 89 step: 2 loss: 0.08540439 acc: 0.9634513854980469\n",
      "epoch: 89 step: 3 loss: 0.08757609 acc: 0.9678153991699219\n",
      "epoch: 89 step: 4 loss: 0.0878314 acc: 0.9737625122070312\n",
      "epoch: 89 step: 5 loss: 0.079437256 acc: 0.9659500122070312\n",
      "epoch: 89 step: 6 loss: 0.08723746 acc: 0.9679107666015625\n",
      "epoch: 89 step: 7 loss: 0.10695416 acc: 0.9613265991210938\n",
      "epoch: 89 step: 8 loss: 0.07695189 acc: 0.963592529296875\n",
      "epoch: 89 step: 9 loss: 0.08268714 acc: 0.9637832641601562\n",
      "epoch: 89 step: 10 loss: 0.07644253 acc: 0.9685287475585938\n",
      "epoch: 89 step: 11 loss: 0.080466084 acc: 0.9597206115722656\n",
      "epoch: 89 step: 12 loss: 0.071100324 acc: 0.9632682800292969\n",
      "epoch: 89 step: 13 loss: 0.082272135 acc: 0.9691352844238281\n",
      "epoch: 89 step: 14 loss: 0.09848404 acc: 0.9578018188476562\n",
      "epoch: 89 step: 15 loss: 0.084915906 acc: 0.9627799987792969\n",
      "epoch: 89 step: 16 loss: 0.07079579 acc: 0.9673843383789062\n",
      "epoch: 89 step: 17 loss: 0.07985984 acc: 0.9605064392089844\n",
      "epoch: 89 step: 18 loss: 0.07563062 acc: 0.9692039489746094\n",
      "epoch: 89 step: 19 loss: 0.079390146 acc: 0.9628410339355469\n",
      "epoch: 89 step: 20 loss: 0.08373958 acc: 0.9610328674316406\n",
      "epoch: 89 step: 21 loss: 0.07754245 acc: 0.9615325927734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 89 step: 22 loss: 0.095112085 acc: 0.9568824768066406\n",
      "epoch: 89 step: 23 loss: 0.07915246 acc: 0.963714599609375\n",
      "epoch: 89 step: 24 loss: 0.065980844 acc: 0.970184326171875\n",
      "epoch: 89 step: 25 loss: 0.088308536 acc: 0.9612388610839844\n",
      "epoch: 89 step: 26 loss: 0.084248975 acc: 0.9636306762695312\n",
      "epoch: 89 step: 27 loss: 0.07616092 acc: 0.96435546875\n",
      "epoch: 89 step: 28 loss: 0.07935748 acc: 0.96783447265625\n",
      "epoch: 89 step: 29 loss: 0.07603205 acc: 0.962982177734375\n",
      "epoch: 89 step: 30 loss: 0.06832139 acc: 0.9704475402832031\n",
      "epoch: 89 step: 31 loss: 0.07506509 acc: 0.9622688293457031\n",
      "epoch: 89 step: 32 loss: 0.08040242 acc: 0.9680099487304688\n",
      "epoch: 89 step: 33 loss: 0.06476311 acc: 0.9707145690917969\n",
      "epoch: 89 step: 34 loss: 0.06945779 acc: 0.9717292785644531\n",
      "epoch: 89 step: 35 loss: 0.07466572 acc: 0.9651069641113281\n",
      "epoch: 89 step: 36 loss: 0.08081 acc: 0.9681854248046875\n",
      "epoch: 89 step: 37 loss: 0.070802435 acc: 0.9685249328613281\n",
      "epoch: 89 step: 38 loss: 0.07283651 acc: 0.9625740051269531\n",
      "epoch: 89 step: 39 loss: 0.072032765 acc: 0.9670486450195312\n",
      "epoch: 89 step: 40 loss: 0.070739746 acc: 0.9734268188476562\n",
      "epoch: 89 step: 41 loss: 0.06107855 acc: 0.9738349914550781\n",
      "epoch: 89 step: 42 loss: 0.093535565 acc: 0.9608345031738281\n",
      "epoch: 89 step: 43 loss: 0.06820818 acc: 0.9670448303222656\n",
      "epoch: 89 step: 44 loss: 0.078808375 acc: 0.9555625915527344\n",
      "epoch: 89 step: 45 loss: 0.07144738 acc: 0.9648361206054688\n",
      "epoch: 89 step: 46 loss: 0.08214525 acc: 0.9667510986328125\n",
      "epoch: 89 step: 47 loss: 0.07550091 acc: 0.9609756469726562\n",
      "epoch: 89 step: 48 loss: 0.07141072 acc: 0.9607124328613281\n",
      "epoch: 89 step: 49 loss: 0.066503674 acc: 0.9668769836425781\n",
      "epoch: 89 step: 50 loss: 0.07262531 acc: 0.967193603515625\n",
      "epoch: 89 step: 51 loss: 0.0748034 acc: 0.9683837890625\n",
      "epoch: 89 step: 52 loss: 0.07169372 acc: 0.9669342041015625\n",
      "epoch: 89 step: 53 loss: 0.071616665 acc: 0.9738082885742188\n",
      "epoch: 89 step: 54 loss: 0.07246021 acc: 0.9603309631347656\n",
      "epoch: 89 step: 55 loss: 0.06518002 acc: 0.9690818786621094\n",
      "epoch: 89 step: 56 loss: 0.064431995 acc: 0.9661598205566406\n",
      "epoch: 89 step: 57 loss: 0.07432655 acc: 0.9602203369140625\n",
      "epoch: 89 step: 58 loss: 0.08912543 acc: 0.9588775634765625\n",
      "epoch: 89 step: 59 loss: 0.07194523 acc: 0.9683303833007812\n",
      "epoch: 89 step: 60 loss: 0.07068104 acc: 0.9640350341796875\n",
      "epoch: 89 step: 61 loss: 0.07124122 acc: 0.9704666137695312\n",
      "epoch: 89 step: 62 loss: 0.058926653 acc: 0.9754867553710938\n",
      "epoch: 89 step: 63 loss: 0.0874006 acc: 0.9644699096679688\n",
      "epoch: 89 step: 64 loss: 0.06462727 acc: 0.9735984802246094\n",
      "epoch: 89 step: 65 loss: 0.07687549 acc: 0.9718170166015625\n",
      "epoch: 89 step: 66 loss: 0.08095437 acc: 0.963226318359375\n",
      "epoch: 89 step: 67 loss: 0.07248311 acc: 0.9682273864746094\n",
      "epoch: 89 step: 68 loss: 0.06106794 acc: 0.9680976867675781\n",
      "epoch: 89 step: 69 loss: 0.073913015 acc: 0.9600753784179688\n",
      "epoch: 89 step: 70 loss: 0.07641365 acc: 0.9576835632324219\n",
      "epoch: 89 step: 71 loss: 0.077136196 acc: 0.9621200561523438\n",
      "epoch: 89 step: 72 loss: 0.09413193 acc: 0.9600753784179688\n",
      "epoch: 89 step: 73 loss: 0.080934495 acc: 0.9655036926269531\n",
      "epoch: 89 step: 74 loss: 0.06908774 acc: 0.9647140502929688\n",
      "epoch: 89 step: 75 loss: 0.066712886 acc: 0.9729270935058594\n",
      "epoch: 89 step: 76 loss: 0.07984164 acc: 0.9592704772949219\n",
      "epoch: 89 step: 77 loss: 0.06794733 acc: 0.9677505493164062\n",
      "epoch: 89 step: 78 loss: 0.07944501 acc: 0.9577217102050781\n",
      "epoch: 89 step: 79 loss: 0.07392243 acc: 0.9699134826660156\n",
      "epoch: 89 step: 80 loss: 0.08540785 acc: 0.9614906311035156\n",
      "epoch: 89 step: 81 loss: 0.07679258 acc: 0.9651908874511719\n",
      "epoch: 89 step: 82 loss: 0.08284774 acc: 0.9607620239257812\n",
      "epoch: 89 step: 83 loss: 0.073228896 acc: 0.9655952453613281\n",
      "epoch: 89 step: 84 loss: 0.06875267 acc: 0.9641304016113281\n",
      "epoch: 89 step: 85 loss: 0.08324411 acc: 0.9565467834472656\n",
      "epoch: 89 step: 86 loss: 0.07413136 acc: 0.9682884216308594\n",
      "epoch: 89 step: 87 loss: 0.087649345 acc: 0.9617958068847656\n",
      "epoch: 89 step: 88 loss: 0.069438115 acc: 0.9693336486816406\n",
      "epoch: 89 step: 89 loss: 0.07992558 acc: 0.9637413024902344\n",
      "epoch: 89 step: 90 loss: 0.06874887 acc: 0.9722633361816406\n",
      "epoch: 89 step: 91 loss: 0.08407905 acc: 0.9618301391601562\n",
      "epoch: 89 step: 92 loss: 0.064421356 acc: 0.970428466796875\n",
      "epoch: 89 step: 93 loss: 0.072563216 acc: 0.9671783447265625\n",
      "epoch: 89 step: 94 loss: 0.08070787 acc: 0.9645957946777344\n",
      "epoch: 89 step: 95 loss: 0.07026579 acc: 0.9655303955078125\n",
      "epoch: 89 step: 96 loss: 0.08187711 acc: 0.9600868225097656\n",
      "epoch: 89 step: 97 loss: 0.074724026 acc: 0.9666862487792969\n",
      "epoch: 89 step: 98 loss: 0.080470204 acc: 0.9746475219726562\n",
      "epoch: 89 step: 99 loss: 0.07137076 acc: 0.9687080383300781\n",
      "epoch: 89 step: 100 loss: 0.07071441 acc: 0.9625282287597656\n",
      "epoch: 89 step: 101 loss: 0.061696354 acc: 0.9741325378417969\n",
      "epoch: 89 step: 102 loss: 0.07818048 acc: 0.9680938720703125\n",
      "epoch: 89 step: 103 loss: 0.09604202 acc: 0.9563026428222656\n",
      "epoch: 89 step: 104 loss: 0.113170676 acc: 0.9533843994140625\n",
      "epoch: 89 step: 105 loss: 0.071252 acc: 0.9711151123046875\n",
      "epoch: 89 step: 106 loss: 0.07463197 acc: 0.9639663696289062\n",
      "epoch: 89 step: 107 loss: 0.06470873 acc: 0.9669647216796875\n",
      "epoch: 89 step: 108 loss: 0.07860379 acc: 0.9644775390625\n",
      "epoch: 89 step: 109 loss: 0.08243726 acc: 0.962677001953125\n",
      "epoch: 89 step: 110 loss: 0.07628457 acc: 0.9681434631347656\n",
      "epoch: 89 step: 111 loss: 0.07802786 acc: 0.9640846252441406\n",
      "epoch: 89 step: 112 loss: 0.070131734 acc: 0.9658393859863281\n",
      "epoch: 89 step: 113 loss: 0.08097529 acc: 0.9596710205078125\n",
      "epoch: 89 step: 114 loss: 0.07275782 acc: 0.9662590026855469\n",
      "epoch: 89 step: 115 loss: 0.09671209 acc: 0.9587669372558594\n",
      "epoch: 89 step: 116 loss: 0.07060107 acc: 0.9632987976074219\n",
      "epoch: 89 step: 117 loss: 0.06670336 acc: 0.9645500183105469\n",
      "epoch: 89 step: 118 loss: 0.07921073 acc: 0.964202880859375\n",
      "epoch: 89 step: 119 loss: 0.06785211 acc: 0.9660453796386719\n",
      "epoch: 89 step: 120 loss: 0.07125782 acc: 0.9625587463378906\n",
      "epoch: 89 step: 121 loss: 0.068998836 acc: 0.9678230285644531\n",
      "epoch: 89 step: 122 loss: 0.07659734 acc: 0.9633979797363281\n",
      "epoch: 89 step: 123 loss: 0.07172975 acc: 0.9605026245117188\n",
      "epoch: 89 step: 124 loss: 0.07516805 acc: 0.9703543526785714\n",
      "epoch: 89 validation_loss: 0.09 validation_dice: 0.8549562588834041\n",
      "epoch: 89 test_dataset dice: 0.7279361017386501\n",
      "time cost 0.5352841893831889 min\n",
      "dice_best: 0.8622422678227601\n",
      "******************************** epoch  89  is finished. *********************************\n",
      "epoch: 90 step: 1 loss: 0.09348014 acc: 0.9701309204101562\n",
      "epoch: 90 step: 2 loss: 0.06539036 acc: 0.96527099609375\n",
      "epoch: 90 step: 3 loss: 0.07879471 acc: 0.9655609130859375\n",
      "epoch: 90 step: 4 loss: 0.068943635 acc: 0.9668617248535156\n",
      "epoch: 90 step: 5 loss: 0.07210782 acc: 0.9609489440917969\n",
      "epoch: 90 step: 6 loss: 0.08839039 acc: 0.9585685729980469\n",
      "epoch: 90 step: 7 loss: 0.078780554 acc: 0.9566268920898438\n",
      "epoch: 90 step: 8 loss: 0.07357841 acc: 0.963226318359375\n",
      "epoch: 90 step: 9 loss: 0.07146622 acc: 0.9652366638183594\n",
      "epoch: 90 step: 10 loss: 0.069511846 acc: 0.9627151489257812\n",
      "epoch: 90 step: 11 loss: 0.07297717 acc: 0.9599266052246094\n",
      "epoch: 90 step: 12 loss: 0.087185055 acc: 0.9618034362792969\n",
      "epoch: 90 step: 13 loss: 0.069332846 acc: 0.9685401916503906\n",
      "epoch: 90 step: 14 loss: 0.06694753 acc: 0.9727058410644531\n",
      "epoch: 90 step: 15 loss: 0.065247536 acc: 0.9749565124511719\n",
      "epoch: 90 step: 16 loss: 0.080489725 acc: 0.9683418273925781\n",
      "epoch: 90 step: 17 loss: 0.07688312 acc: 0.9647026062011719\n",
      "epoch: 90 step: 18 loss: 0.08001453 acc: 0.9681243896484375\n",
      "epoch: 90 step: 19 loss: 0.08506504 acc: 0.9617576599121094\n",
      "epoch: 90 step: 20 loss: 0.08227294 acc: 0.95806884765625\n",
      "epoch: 90 step: 21 loss: 0.06151853 acc: 0.9717483520507812\n",
      "epoch: 90 step: 22 loss: 0.07583781 acc: 0.963134765625\n",
      "epoch: 90 step: 23 loss: 0.0713887 acc: 0.9636344909667969\n",
      "epoch: 90 step: 24 loss: 0.07204023 acc: 0.9658241271972656\n",
      "epoch: 90 step: 25 loss: 0.072584115 acc: 0.967437744140625\n",
      "epoch: 90 step: 26 loss: 0.0586653 acc: 0.9727363586425781\n",
      "epoch: 90 step: 27 loss: 0.06720257 acc: 0.9706230163574219\n",
      "epoch: 90 step: 28 loss: 0.0657096 acc: 0.9715461730957031\n",
      "epoch: 90 step: 29 loss: 0.069836676 acc: 0.96942138671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 90 step: 30 loss: 0.07610252 acc: 0.9675407409667969\n",
      "epoch: 90 step: 31 loss: 0.06373569 acc: 0.97064208984375\n",
      "epoch: 90 step: 32 loss: 0.06789139 acc: 0.9657402038574219\n",
      "epoch: 90 step: 33 loss: 0.07368942 acc: 0.96630859375\n",
      "epoch: 90 step: 34 loss: 0.08289107 acc: 0.9621620178222656\n",
      "epoch: 90 step: 35 loss: 0.06839795 acc: 0.9638519287109375\n",
      "epoch: 90 step: 36 loss: 0.0639896 acc: 0.965667724609375\n",
      "epoch: 90 step: 37 loss: 0.07775931 acc: 0.9613418579101562\n",
      "epoch: 90 step: 38 loss: 0.072833486 acc: 0.9652252197265625\n",
      "epoch: 90 step: 39 loss: 0.08080632 acc: 0.9643821716308594\n",
      "epoch: 90 step: 40 loss: 0.06803057 acc: 0.9728279113769531\n",
      "epoch: 90 step: 41 loss: 0.05842388 acc: 0.970703125\n",
      "epoch: 90 step: 42 loss: 0.06930668 acc: 0.9634017944335938\n",
      "epoch: 90 step: 43 loss: 0.0659091 acc: 0.9716720581054688\n",
      "epoch: 90 step: 44 loss: 0.093179636 acc: 0.9685478210449219\n",
      "epoch: 90 step: 45 loss: 0.07255131 acc: 0.9684257507324219\n",
      "epoch: 90 step: 46 loss: 0.07311878 acc: 0.9633026123046875\n",
      "epoch: 90 step: 47 loss: 0.08356482 acc: 0.9608879089355469\n",
      "epoch: 90 step: 48 loss: 0.07659512 acc: 0.9657554626464844\n",
      "epoch: 90 step: 49 loss: 0.07179576 acc: 0.9630088806152344\n",
      "epoch: 90 step: 50 loss: 0.08085019 acc: 0.9591941833496094\n",
      "epoch: 90 step: 51 loss: 0.07054824 acc: 0.9658775329589844\n",
      "epoch: 90 step: 52 loss: 0.08792194 acc: 0.9590492248535156\n",
      "epoch: 90 step: 53 loss: 0.09639826 acc: 0.9663848876953125\n",
      "epoch: 90 step: 54 loss: 0.067173414 acc: 0.9645423889160156\n",
      "epoch: 90 step: 55 loss: 0.07733152 acc: 0.9632377624511719\n",
      "epoch: 90 step: 56 loss: 0.068509124 acc: 0.9700088500976562\n",
      "epoch: 90 step: 57 loss: 0.08003202 acc: 0.9695053100585938\n",
      "epoch: 90 step: 58 loss: 0.077883095 acc: 0.9657936096191406\n",
      "epoch: 90 step: 59 loss: 0.0791448 acc: 0.9682121276855469\n",
      "epoch: 90 step: 60 loss: 0.0812115 acc: 0.965911865234375\n",
      "epoch: 90 step: 61 loss: 0.0768694 acc: 0.96319580078125\n",
      "epoch: 90 step: 62 loss: 0.07870741 acc: 0.9583511352539062\n",
      "epoch: 90 step: 63 loss: 0.069024846 acc: 0.9609756469726562\n",
      "epoch: 90 step: 64 loss: 0.08616842 acc: 0.9626731872558594\n",
      "epoch: 90 step: 65 loss: 0.095387675 acc: 0.9602241516113281\n",
      "epoch: 90 step: 66 loss: 0.10038777 acc: 0.9532356262207031\n",
      "epoch: 90 step: 67 loss: 0.06960416 acc: 0.9655647277832031\n",
      "epoch: 90 step: 68 loss: 0.07609881 acc: 0.9621467590332031\n",
      "epoch: 90 step: 69 loss: 0.070476115 acc: 0.9675254821777344\n",
      "epoch: 90 step: 70 loss: 0.07057119 acc: 0.9680404663085938\n",
      "epoch: 90 step: 71 loss: 0.06519115 acc: 0.9666824340820312\n",
      "epoch: 90 step: 72 loss: 0.06613724 acc: 0.9704818725585938\n",
      "epoch: 90 step: 73 loss: 0.07362479 acc: 0.9694252014160156\n",
      "epoch: 90 step: 74 loss: 0.09240124 acc: 0.9610519409179688\n",
      "epoch: 90 step: 75 loss: 0.07478602 acc: 0.9597282409667969\n",
      "epoch: 90 step: 76 loss: 0.0756618 acc: 0.96966552734375\n",
      "epoch: 90 step: 77 loss: 0.08790465 acc: 0.9619483947753906\n",
      "epoch: 90 step: 78 loss: 0.078815185 acc: 0.9644355773925781\n",
      "epoch: 90 step: 79 loss: 0.06829445 acc: 0.9681587219238281\n",
      "epoch: 90 step: 80 loss: 0.075738996 acc: 0.9693183898925781\n",
      "epoch: 90 step: 81 loss: 0.083322555 acc: 0.95428466796875\n",
      "epoch: 90 step: 82 loss: 0.08450286 acc: 0.9626846313476562\n",
      "epoch: 90 step: 83 loss: 0.08104225 acc: 0.9577217102050781\n",
      "epoch: 90 step: 84 loss: 0.07376787 acc: 0.9626312255859375\n",
      "epoch: 90 step: 85 loss: 0.08018963 acc: 0.9658927917480469\n",
      "epoch: 90 step: 86 loss: 0.07942781 acc: 0.9620018005371094\n",
      "epoch: 90 step: 87 loss: 0.073018745 acc: 0.9690093994140625\n",
      "epoch: 90 step: 88 loss: 0.07338363 acc: 0.9695014953613281\n",
      "epoch: 90 step: 89 loss: 0.078394234 acc: 0.9706001281738281\n",
      "epoch: 90 step: 90 loss: 0.076825336 acc: 0.962371826171875\n",
      "epoch: 90 step: 91 loss: 0.068192706 acc: 0.9669380187988281\n",
      "epoch: 90 step: 92 loss: 0.06914093 acc: 0.9662628173828125\n",
      "epoch: 90 step: 93 loss: 0.08203156 acc: 0.9586982727050781\n",
      "epoch: 90 step: 94 loss: 0.0719109 acc: 0.9619407653808594\n",
      "epoch: 90 step: 95 loss: 0.07655201 acc: 0.9674301147460938\n",
      "epoch: 90 step: 96 loss: 0.069953494 acc: 0.9678459167480469\n",
      "epoch: 90 step: 97 loss: 0.07964261 acc: 0.960662841796875\n",
      "epoch: 90 step: 98 loss: 0.07704832 acc: 0.9628028869628906\n",
      "epoch: 90 step: 99 loss: 0.07848391 acc: 0.9647865295410156\n",
      "epoch: 90 step: 100 loss: 0.07524639 acc: 0.9683151245117188\n",
      "epoch: 90 step: 101 loss: 0.07227341 acc: 0.9658393859863281\n",
      "epoch: 90 step: 102 loss: 0.074575245 acc: 0.9638710021972656\n",
      "epoch: 90 step: 103 loss: 0.07429975 acc: 0.9758872985839844\n",
      "epoch: 90 step: 104 loss: 0.07162498 acc: 0.9711952209472656\n",
      "epoch: 90 step: 105 loss: 0.070331044 acc: 0.9720077514648438\n",
      "epoch: 90 step: 106 loss: 0.06489276 acc: 0.9674415588378906\n",
      "epoch: 90 step: 107 loss: 0.075334996 acc: 0.9677658081054688\n",
      "epoch: 90 step: 108 loss: 0.09717445 acc: 0.9508934020996094\n",
      "epoch: 90 step: 109 loss: 0.086791374 acc: 0.9598045349121094\n",
      "epoch: 90 step: 110 loss: 0.08412614 acc: 0.9595565795898438\n",
      "epoch: 90 step: 111 loss: 0.07354942 acc: 0.9665679931640625\n",
      "epoch: 90 step: 112 loss: 0.07136571 acc: 0.9657402038574219\n",
      "epoch: 90 step: 113 loss: 0.07040028 acc: 0.9633865356445312\n",
      "epoch: 90 step: 114 loss: 0.0904767 acc: 0.9604263305664062\n",
      "epoch: 90 step: 115 loss: 0.07955784 acc: 0.9665679931640625\n",
      "epoch: 90 step: 116 loss: 0.07422827 acc: 0.9664421081542969\n",
      "epoch: 90 step: 117 loss: 0.0672773 acc: 0.9665679931640625\n",
      "epoch: 90 step: 118 loss: 0.07542717 acc: 0.9680557250976562\n",
      "epoch: 90 step: 119 loss: 0.081940465 acc: 0.9670600891113281\n",
      "epoch: 90 step: 120 loss: 0.07017944 acc: 0.9644012451171875\n",
      "epoch: 90 step: 121 loss: 0.0716606 acc: 0.9653816223144531\n",
      "epoch: 90 step: 122 loss: 0.07833846 acc: 0.9555625915527344\n",
      "epoch: 90 step: 123 loss: 0.0669397 acc: 0.9673309326171875\n",
      "epoch: 90 step: 124 loss: 0.0847369 acc: 0.9654105050223214\n",
      "epoch: 90 validation_loss: 0.086 validation_dice: 0.854329103292418\n",
      "epoch: 90 test_dataset dice: 0.7339452570877295\n",
      "time cost 0.5359733462333679 min\n",
      "dice_best: 0.8622422678227601\n",
      "******************************** epoch  90  is finished. *********************************\n",
      "epoch: 91 step: 1 loss: 0.089468166 acc: 0.9610404968261719\n",
      "epoch: 91 step: 2 loss: 0.06994293 acc: 0.9593772888183594\n",
      "epoch: 91 step: 3 loss: 0.07422893 acc: 0.9622726440429688\n",
      "epoch: 91 step: 4 loss: 0.067675255 acc: 0.9701271057128906\n",
      "epoch: 91 step: 5 loss: 0.07879186 acc: 0.9608039855957031\n",
      "epoch: 91 step: 6 loss: 0.07756201 acc: 0.9597396850585938\n",
      "epoch: 91 step: 7 loss: 0.0659815 acc: 0.9661827087402344\n",
      "epoch: 91 step: 8 loss: 0.071701996 acc: 0.9629669189453125\n",
      "epoch: 91 step: 9 loss: 0.06880412 acc: 0.9641761779785156\n",
      "epoch: 91 step: 10 loss: 0.06751133 acc: 0.9667549133300781\n",
      "epoch: 91 step: 11 loss: 0.063397765 acc: 0.9712028503417969\n",
      "epoch: 91 step: 12 loss: 0.063315466 acc: 0.9723014831542969\n",
      "epoch: 91 step: 13 loss: 0.07272622 acc: 0.9656410217285156\n",
      "epoch: 91 step: 14 loss: 0.07371218 acc: 0.9647216796875\n",
      "epoch: 91 step: 15 loss: 0.07275295 acc: 0.9677352905273438\n",
      "epoch: 91 step: 16 loss: 0.06791291 acc: 0.970916748046875\n",
      "epoch: 91 step: 17 loss: 0.065989494 acc: 0.9685516357421875\n",
      "epoch: 91 step: 18 loss: 0.069592915 acc: 0.967041015625\n",
      "epoch: 91 step: 19 loss: 0.071632184 acc: 0.9628715515136719\n",
      "epoch: 91 step: 20 loss: 0.06733453 acc: 0.96124267578125\n",
      "epoch: 91 step: 21 loss: 0.071883015 acc: 0.96240234375\n",
      "epoch: 91 step: 22 loss: 0.0774067 acc: 0.9583778381347656\n",
      "epoch: 91 step: 23 loss: 0.062797554 acc: 0.9658470153808594\n",
      "epoch: 91 step: 24 loss: 0.06535139 acc: 0.9663124084472656\n",
      "epoch: 91 step: 25 loss: 0.058059685 acc: 0.9719429016113281\n",
      "epoch: 91 step: 26 loss: 0.06297343 acc: 0.9717597961425781\n",
      "epoch: 91 step: 27 loss: 0.071251646 acc: 0.9667549133300781\n",
      "epoch: 91 step: 28 loss: 0.09232331 acc: 0.9582328796386719\n",
      "epoch: 91 step: 29 loss: 0.066559575 acc: 0.9728622436523438\n",
      "epoch: 91 step: 30 loss: 0.07147395 acc: 0.9656867980957031\n",
      "epoch: 91 step: 31 loss: 0.07308479 acc: 0.962188720703125\n",
      "epoch: 91 step: 32 loss: 0.08331738 acc: 0.9604835510253906\n",
      "epoch: 91 step: 33 loss: 0.05997686 acc: 0.9665565490722656\n",
      "epoch: 91 step: 34 loss: 0.06876616 acc: 0.9712562561035156\n",
      "epoch: 91 step: 35 loss: 0.07189335 acc: 0.9635047912597656\n",
      "epoch: 91 step: 36 loss: 0.06487739 acc: 0.9681282043457031\n",
      "epoch: 91 step: 37 loss: 0.07048555 acc: 0.9700965881347656\n",
      "epoch: 91 step: 38 loss: 0.059538566 acc: 0.971160888671875\n",
      "epoch: 91 step: 39 loss: 0.07643715 acc: 0.9677505493164062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 91 step: 40 loss: 0.07713086 acc: 0.9661712646484375\n",
      "epoch: 91 step: 41 loss: 0.061311692 acc: 0.9752998352050781\n",
      "epoch: 91 step: 42 loss: 0.06530497 acc: 0.9723930358886719\n",
      "epoch: 91 step: 43 loss: 0.05669225 acc: 0.9724388122558594\n",
      "epoch: 91 step: 44 loss: 0.07156133 acc: 0.9668235778808594\n",
      "epoch: 91 step: 45 loss: 0.073180474 acc: 0.964874267578125\n",
      "epoch: 91 step: 46 loss: 0.07559136 acc: 0.9705810546875\n",
      "epoch: 91 step: 47 loss: 0.071141325 acc: 0.9672927856445312\n",
      "epoch: 91 step: 48 loss: 0.06779451 acc: 0.9653129577636719\n",
      "epoch: 91 step: 49 loss: 0.06860453 acc: 0.9708137512207031\n",
      "epoch: 91 step: 50 loss: 0.06593319 acc: 0.9630851745605469\n",
      "epoch: 91 step: 51 loss: 0.071557544 acc: 0.9654464721679688\n",
      "epoch: 91 step: 52 loss: 0.07797018 acc: 0.9578170776367188\n",
      "epoch: 91 step: 53 loss: 0.066194795 acc: 0.9688491821289062\n",
      "epoch: 91 step: 54 loss: 0.068372205 acc: 0.9694747924804688\n",
      "epoch: 91 step: 55 loss: 0.06652112 acc: 0.9682502746582031\n",
      "epoch: 91 step: 56 loss: 0.07019602 acc: 0.9670906066894531\n",
      "epoch: 91 step: 57 loss: 0.09262779 acc: 0.9576263427734375\n",
      "epoch: 91 step: 58 loss: 0.07597438 acc: 0.9638748168945312\n",
      "epoch: 91 step: 59 loss: 0.06655976 acc: 0.9714393615722656\n",
      "epoch: 91 step: 60 loss: 0.06165079 acc: 0.9743843078613281\n",
      "epoch: 91 step: 61 loss: 0.06743149 acc: 0.9665260314941406\n",
      "epoch: 91 step: 62 loss: 0.08034262 acc: 0.9571533203125\n",
      "epoch: 91 step: 63 loss: 0.06598137 acc: 0.9658584594726562\n",
      "epoch: 91 step: 64 loss: 0.07622701 acc: 0.9623756408691406\n",
      "epoch: 91 step: 65 loss: 0.075097844 acc: 0.9693145751953125\n",
      "epoch: 91 step: 66 loss: 0.07311439 acc: 0.9719161987304688\n",
      "epoch: 91 step: 67 loss: 0.072423115 acc: 0.9652786254882812\n",
      "epoch: 91 step: 68 loss: 0.06645544 acc: 0.9706764221191406\n",
      "epoch: 91 step: 69 loss: 0.080499806 acc: 0.9627532958984375\n",
      "epoch: 91 step: 70 loss: 0.06650254 acc: 0.9695854187011719\n",
      "epoch: 91 step: 71 loss: 0.06399391 acc: 0.9685211181640625\n",
      "epoch: 91 step: 72 loss: 0.07091881 acc: 0.9727821350097656\n",
      "epoch: 91 step: 73 loss: 0.06606795 acc: 0.9730873107910156\n",
      "epoch: 91 step: 74 loss: 0.07495021 acc: 0.9702606201171875\n",
      "epoch: 91 step: 75 loss: 0.064911135 acc: 0.9665260314941406\n",
      "epoch: 91 step: 76 loss: 0.07455232 acc: 0.9652786254882812\n",
      "epoch: 91 step: 77 loss: 0.085004315 acc: 0.9575920104980469\n",
      "epoch: 91 step: 78 loss: 0.08351029 acc: 0.9575157165527344\n",
      "epoch: 91 step: 79 loss: 0.08259705 acc: 0.9630546569824219\n",
      "epoch: 91 step: 80 loss: 0.07842091 acc: 0.9622116088867188\n",
      "epoch: 91 step: 81 loss: 0.06508196 acc: 0.9678306579589844\n",
      "epoch: 91 step: 82 loss: 0.08552075 acc: 0.9595069885253906\n",
      "epoch: 91 step: 83 loss: 0.063007094 acc: 0.9685287475585938\n",
      "epoch: 91 step: 84 loss: 0.070667624 acc: 0.9660415649414062\n",
      "epoch: 91 step: 85 loss: 0.07095509 acc: 0.9631996154785156\n",
      "epoch: 91 step: 86 loss: 0.07264612 acc: 0.9623298645019531\n",
      "epoch: 91 step: 87 loss: 0.069505185 acc: 0.967010498046875\n",
      "epoch: 91 step: 88 loss: 0.07190068 acc: 0.9600410461425781\n",
      "epoch: 91 step: 89 loss: 0.0656479 acc: 0.970489501953125\n",
      "epoch: 91 step: 90 loss: 0.072619855 acc: 0.9653053283691406\n",
      "epoch: 91 step: 91 loss: 0.08527197 acc: 0.9643821716308594\n",
      "epoch: 91 step: 92 loss: 0.090413034 acc: 0.9665489196777344\n",
      "epoch: 91 step: 93 loss: 0.06351939 acc: 0.9694137573242188\n",
      "epoch: 91 step: 94 loss: 0.08873377 acc: 0.9611053466796875\n",
      "epoch: 91 step: 95 loss: 0.07768153 acc: 0.9590606689453125\n",
      "epoch: 91 step: 96 loss: 0.07586829 acc: 0.9577980041503906\n",
      "epoch: 91 step: 97 loss: 0.07087803 acc: 0.9588203430175781\n",
      "epoch: 91 step: 98 loss: 0.06526146 acc: 0.9607810974121094\n",
      "epoch: 91 step: 99 loss: 0.07337709 acc: 0.9614067077636719\n",
      "epoch: 91 step: 100 loss: 0.0756324 acc: 0.9621200561523438\n",
      "epoch: 91 step: 101 loss: 0.0710204 acc: 0.9687004089355469\n",
      "epoch: 91 step: 102 loss: 0.089422725 acc: 0.9520034790039062\n",
      "epoch: 91 step: 103 loss: 0.09262897 acc: 0.9619712829589844\n",
      "epoch: 91 step: 104 loss: 0.06978287 acc: 0.9637069702148438\n",
      "epoch: 91 step: 105 loss: 0.07511561 acc: 0.9641342163085938\n",
      "epoch: 91 step: 106 loss: 0.07781727 acc: 0.9616661071777344\n",
      "epoch: 91 step: 107 loss: 0.06747552 acc: 0.9691276550292969\n",
      "epoch: 91 step: 108 loss: 0.077881664 acc: 0.9651679992675781\n",
      "epoch: 91 step: 109 loss: 0.07742282 acc: 0.9649734497070312\n",
      "epoch: 91 step: 110 loss: 0.05420978 acc: 0.9747390747070312\n",
      "epoch: 91 step: 111 loss: 0.082869634 acc: 0.9587173461914062\n",
      "epoch: 91 step: 112 loss: 0.0734346 acc: 0.9668693542480469\n",
      "epoch: 91 step: 113 loss: 0.070996374 acc: 0.9682846069335938\n",
      "epoch: 91 step: 114 loss: 0.07695951 acc: 0.9654502868652344\n",
      "epoch: 91 step: 115 loss: 0.08227294 acc: 0.9601860046386719\n",
      "epoch: 91 step: 116 loss: 0.084035374 acc: 0.9586677551269531\n",
      "epoch: 91 step: 117 loss: 0.05760943 acc: 0.9698410034179688\n",
      "epoch: 91 step: 118 loss: 0.0795227 acc: 0.9626274108886719\n",
      "epoch: 91 step: 119 loss: 0.07192302 acc: 0.9645614624023438\n",
      "epoch: 91 step: 120 loss: 0.073512085 acc: 0.9674148559570312\n",
      "epoch: 91 step: 121 loss: 0.06899166 acc: 0.9645423889160156\n",
      "epoch: 91 step: 122 loss: 0.07879123 acc: 0.9664268493652344\n",
      "epoch: 91 step: 123 loss: 0.080080606 acc: 0.9662208557128906\n",
      "epoch: 91 step: 124 loss: 0.073441856 acc: 0.9663783482142857\n",
      "epoch: 91 validation_loss: 0.083 validation_dice: 0.8485471137244401\n",
      "epoch: 91 test_dataset dice: 0.7360038598607939\n",
      "time cost 0.5363330245018005 min\n",
      "dice_best: 0.8622422678227601\n",
      "******************************** epoch  91  is finished. *********************************\n",
      "epoch: 92 step: 1 loss: 0.07489065 acc: 0.9661407470703125\n",
      "epoch: 92 step: 2 loss: 0.07887939 acc: 0.9634857177734375\n",
      "epoch: 92 step: 3 loss: 0.08078915 acc: 0.9644088745117188\n",
      "epoch: 92 step: 4 loss: 0.066070415 acc: 0.971099853515625\n",
      "epoch: 92 step: 5 loss: 0.08214668 acc: 0.963714599609375\n",
      "epoch: 92 step: 6 loss: 0.09197449 acc: 0.9614295959472656\n",
      "epoch: 92 step: 7 loss: 0.07977688 acc: 0.9580764770507812\n",
      "epoch: 92 step: 8 loss: 0.06741248 acc: 0.9666709899902344\n",
      "epoch: 92 step: 9 loss: 0.0779891 acc: 0.9640388488769531\n",
      "epoch: 92 step: 10 loss: 0.08219149 acc: 0.9659080505371094\n",
      "epoch: 92 step: 11 loss: 0.071693674 acc: 0.9694786071777344\n",
      "epoch: 92 step: 12 loss: 0.078966126 acc: 0.9636802673339844\n",
      "epoch: 92 step: 13 loss: 0.06902798 acc: 0.9692726135253906\n",
      "epoch: 92 step: 14 loss: 0.09592071 acc: 0.9647903442382812\n",
      "epoch: 92 step: 15 loss: 0.07776199 acc: 0.9623680114746094\n",
      "epoch: 92 step: 16 loss: 0.08870791 acc: 0.9634475708007812\n",
      "epoch: 92 step: 17 loss: 0.08191654 acc: 0.9632377624511719\n",
      "epoch: 92 step: 18 loss: 0.0685791 acc: 0.9684944152832031\n",
      "epoch: 92 step: 19 loss: 0.07820327 acc: 0.9638252258300781\n",
      "epoch: 92 step: 20 loss: 0.07078325 acc: 0.9700164794921875\n",
      "epoch: 92 step: 21 loss: 0.092392914 acc: 0.9564056396484375\n",
      "epoch: 92 step: 22 loss: 0.094038 acc: 0.9558982849121094\n",
      "epoch: 92 step: 23 loss: 0.07583408 acc: 0.969146728515625\n",
      "epoch: 92 step: 24 loss: 0.08549001 acc: 0.9614372253417969\n",
      "epoch: 92 step: 25 loss: 0.09264648 acc: 0.9619140625\n",
      "epoch: 92 step: 26 loss: 0.07774999 acc: 0.9721527099609375\n",
      "epoch: 92 step: 27 loss: 0.07597555 acc: 0.9629898071289062\n",
      "epoch: 92 step: 28 loss: 0.07884652 acc: 0.9653205871582031\n",
      "epoch: 92 step: 29 loss: 0.060561936 acc: 0.9763221740722656\n",
      "epoch: 92 step: 30 loss: 0.06523922 acc: 0.9669189453125\n",
      "epoch: 92 step: 31 loss: 0.07886007 acc: 0.9626007080078125\n",
      "epoch: 92 step: 32 loss: 0.07244908 acc: 0.9631385803222656\n",
      "epoch: 92 step: 33 loss: 0.076450646 acc: 0.9575843811035156\n",
      "epoch: 92 step: 34 loss: 0.08680249 acc: 0.9568252563476562\n",
      "epoch: 92 step: 35 loss: 0.07618336 acc: 0.9608421325683594\n",
      "epoch: 92 step: 36 loss: 0.07632222 acc: 0.9591560363769531\n",
      "epoch: 92 step: 37 loss: 0.07960928 acc: 0.9635391235351562\n",
      "epoch: 92 step: 38 loss: 0.074718885 acc: 0.9706497192382812\n",
      "epoch: 92 step: 39 loss: 0.07743377 acc: 0.9724807739257812\n",
      "epoch: 92 step: 40 loss: 0.08061972 acc: 0.9672622680664062\n",
      "epoch: 92 step: 41 loss: 0.07838288 acc: 0.9694175720214844\n",
      "epoch: 92 step: 42 loss: 0.07950155 acc: 0.9678421020507812\n",
      "epoch: 92 step: 43 loss: 0.07520971 acc: 0.9667587280273438\n",
      "epoch: 92 step: 44 loss: 0.08199325 acc: 0.9600563049316406\n",
      "epoch: 92 step: 45 loss: 0.0641921 acc: 0.9703750610351562\n",
      "epoch: 92 step: 46 loss: 0.076513976 acc: 0.9629096984863281\n",
      "epoch: 92 step: 47 loss: 0.07975355 acc: 0.9628028869628906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 92 step: 48 loss: 0.07523095 acc: 0.9657669067382812\n",
      "epoch: 92 step: 49 loss: 0.07503165 acc: 0.959625244140625\n",
      "epoch: 92 step: 50 loss: 0.07288479 acc: 0.9621849060058594\n",
      "epoch: 92 step: 51 loss: 0.08321537 acc: 0.9588775634765625\n",
      "epoch: 92 step: 52 loss: 0.08282705 acc: 0.9617233276367188\n",
      "epoch: 92 step: 53 loss: 0.054140277 acc: 0.9768638610839844\n",
      "epoch: 92 step: 54 loss: 0.07421167 acc: 0.9604949951171875\n",
      "epoch: 92 step: 55 loss: 0.08057604 acc: 0.9703865051269531\n",
      "epoch: 92 step: 56 loss: 0.07043974 acc: 0.9689178466796875\n",
      "epoch: 92 step: 57 loss: 0.092087634 acc: 0.961639404296875\n",
      "epoch: 92 step: 58 loss: 0.06440152 acc: 0.9693069458007812\n",
      "epoch: 92 step: 59 loss: 0.074532054 acc: 0.9635086059570312\n",
      "epoch: 92 step: 60 loss: 0.08120165 acc: 0.9655647277832031\n",
      "epoch: 92 step: 61 loss: 0.07676003 acc: 0.9631004333496094\n",
      "epoch: 92 step: 62 loss: 0.060399678 acc: 0.9766807556152344\n",
      "epoch: 92 step: 63 loss: 0.075217366 acc: 0.9603996276855469\n",
      "epoch: 92 step: 64 loss: 0.06915737 acc: 0.9642448425292969\n",
      "epoch: 92 step: 65 loss: 0.07415789 acc: 0.9636688232421875\n",
      "epoch: 92 step: 66 loss: 0.068474464 acc: 0.9688606262207031\n",
      "epoch: 92 step: 67 loss: 0.10083018 acc: 0.9562873840332031\n",
      "epoch: 92 step: 68 loss: 0.066170216 acc: 0.9645957946777344\n",
      "epoch: 92 step: 69 loss: 0.07444894 acc: 0.9659004211425781\n",
      "epoch: 92 step: 70 loss: 0.06997684 acc: 0.969879150390625\n",
      "epoch: 92 step: 71 loss: 0.07616057 acc: 0.9681129455566406\n",
      "epoch: 92 step: 72 loss: 0.06677342 acc: 0.971954345703125\n",
      "epoch: 92 step: 73 loss: 0.064610474 acc: 0.9706344604492188\n",
      "epoch: 92 step: 74 loss: 0.059628792 acc: 0.9704017639160156\n",
      "epoch: 92 step: 75 loss: 0.08367283 acc: 0.9604377746582031\n",
      "epoch: 92 step: 76 loss: 0.06848044 acc: 0.96514892578125\n",
      "epoch: 92 step: 77 loss: 0.08302811 acc: 0.9593238830566406\n",
      "epoch: 92 step: 78 loss: 0.07416392 acc: 0.9580116271972656\n",
      "epoch: 92 step: 79 loss: 0.06933014 acc: 0.9664382934570312\n",
      "epoch: 92 step: 80 loss: 0.073435165 acc: 0.9628105163574219\n",
      "epoch: 92 step: 81 loss: 0.09647292 acc: 0.9585723876953125\n",
      "epoch: 92 step: 82 loss: 0.07006831 acc: 0.9775314331054688\n",
      "epoch: 92 step: 83 loss: 0.071977384 acc: 0.9652023315429688\n",
      "epoch: 92 step: 84 loss: 0.07673072 acc: 0.9642791748046875\n",
      "epoch: 92 step: 85 loss: 0.07649175 acc: 0.9638023376464844\n",
      "epoch: 92 step: 86 loss: 0.070208244 acc: 0.9717559814453125\n",
      "epoch: 92 step: 87 loss: 0.07075971 acc: 0.9679145812988281\n",
      "epoch: 92 step: 88 loss: 0.08185035 acc: 0.9580230712890625\n",
      "epoch: 92 step: 89 loss: 0.06996397 acc: 0.9682998657226562\n",
      "epoch: 92 step: 90 loss: 0.07549662 acc: 0.9626502990722656\n",
      "epoch: 92 step: 91 loss: 0.06713122 acc: 0.9641189575195312\n",
      "epoch: 92 step: 92 loss: 0.06928189 acc: 0.9688911437988281\n",
      "epoch: 92 step: 93 loss: 0.070622675 acc: 0.9695205688476562\n",
      "epoch: 92 step: 94 loss: 0.069494806 acc: 0.96905517578125\n",
      "epoch: 92 step: 95 loss: 0.06751586 acc: 0.9741668701171875\n",
      "epoch: 92 step: 96 loss: 0.06228344 acc: 0.9703407287597656\n",
      "epoch: 92 step: 97 loss: 0.072958805 acc: 0.9611663818359375\n",
      "epoch: 92 step: 98 loss: 0.06503592 acc: 0.9700698852539062\n",
      "epoch: 92 step: 99 loss: 0.066326685 acc: 0.9657974243164062\n",
      "epoch: 92 step: 100 loss: 0.07710842 acc: 0.9633750915527344\n",
      "epoch: 92 step: 101 loss: 0.07444224 acc: 0.9662895202636719\n",
      "epoch: 92 step: 102 loss: 0.070174485 acc: 0.9715194702148438\n",
      "epoch: 92 step: 103 loss: 0.06985201 acc: 0.9713325500488281\n",
      "epoch: 92 step: 104 loss: 0.06494183 acc: 0.9678001403808594\n",
      "epoch: 92 step: 105 loss: 0.07418475 acc: 0.9657402038574219\n",
      "epoch: 92 step: 106 loss: 0.06749314 acc: 0.9689064025878906\n",
      "epoch: 92 step: 107 loss: 0.0699969 acc: 0.9708099365234375\n",
      "epoch: 92 step: 108 loss: 0.0807178 acc: 0.9633064270019531\n",
      "epoch: 92 step: 109 loss: 0.07417086 acc: 0.9606475830078125\n",
      "epoch: 92 step: 110 loss: 0.072713114 acc: 0.9603996276855469\n",
      "epoch: 92 step: 111 loss: 0.076158494 acc: 0.9616851806640625\n",
      "epoch: 92 step: 112 loss: 0.081156015 acc: 0.9560356140136719\n",
      "epoch: 92 step: 113 loss: 0.070938155 acc: 0.9637374877929688\n",
      "epoch: 92 step: 114 loss: 0.06666623 acc: 0.9708137512207031\n",
      "epoch: 92 step: 115 loss: 0.07895056 acc: 0.9590225219726562\n",
      "epoch: 92 step: 116 loss: 0.061365902 acc: 0.9701385498046875\n",
      "epoch: 92 step: 117 loss: 0.06546969 acc: 0.9731636047363281\n",
      "epoch: 92 step: 118 loss: 0.067437515 acc: 0.965087890625\n",
      "epoch: 92 step: 119 loss: 0.07628641 acc: 0.9644088745117188\n",
      "epoch: 92 step: 120 loss: 0.06612729 acc: 0.9652824401855469\n",
      "epoch: 92 step: 121 loss: 0.06647319 acc: 0.9692497253417969\n",
      "epoch: 92 step: 122 loss: 0.07643224 acc: 0.9619331359863281\n",
      "epoch: 92 step: 123 loss: 0.07080919 acc: 0.966522216796875\n",
      "epoch: 92 step: 124 loss: 0.102072634 acc: 0.9576503208705357\n",
      "epoch: 92 validation_loss: 0.079 validation_dice: 0.856911518686407\n",
      "epoch: 92 test_dataset dice: 0.7406511693828629\n",
      "time cost 0.5367125352223714 min\n",
      "dice_best: 0.8622422678227601\n",
      "******************************** epoch  92  is finished. *********************************\n",
      "epoch: 93 step: 1 loss: 0.07442986 acc: 0.9621849060058594\n",
      "epoch: 93 step: 2 loss: 0.06820937 acc: 0.966552734375\n",
      "epoch: 93 step: 3 loss: 0.07807487 acc: 0.9601707458496094\n",
      "epoch: 93 step: 4 loss: 0.058159158 acc: 0.9717864990234375\n",
      "epoch: 93 step: 5 loss: 0.076416805 acc: 0.959930419921875\n",
      "epoch: 93 step: 6 loss: 0.06679725 acc: 0.9695701599121094\n",
      "epoch: 93 step: 7 loss: 0.06817316 acc: 0.9655990600585938\n",
      "epoch: 93 step: 8 loss: 0.06661574 acc: 0.964996337890625\n",
      "epoch: 93 step: 9 loss: 0.074713565 acc: 0.9735794067382812\n",
      "epoch: 93 step: 10 loss: 0.08448005 acc: 0.9598541259765625\n",
      "epoch: 93 step: 11 loss: 0.07230216 acc: 0.9675102233886719\n",
      "epoch: 93 step: 12 loss: 0.057120066 acc: 0.974945068359375\n",
      "epoch: 93 step: 13 loss: 0.06788894 acc: 0.9636497497558594\n",
      "epoch: 93 step: 14 loss: 0.07358881 acc: 0.959320068359375\n",
      "epoch: 93 step: 15 loss: 0.06389038 acc: 0.975433349609375\n",
      "epoch: 93 step: 16 loss: 0.064371675 acc: 0.9690513610839844\n",
      "epoch: 93 step: 17 loss: 0.064760335 acc: 0.9693145751953125\n",
      "epoch: 93 step: 18 loss: 0.06833219 acc: 0.9643363952636719\n",
      "epoch: 93 step: 19 loss: 0.0696164 acc: 0.96826171875\n",
      "epoch: 93 step: 20 loss: 0.063068256 acc: 0.9702835083007812\n",
      "epoch: 93 step: 21 loss: 0.058793362 acc: 0.9738235473632812\n",
      "epoch: 93 step: 22 loss: 0.07338458 acc: 0.9710426330566406\n",
      "epoch: 93 step: 23 loss: 0.076726705 acc: 0.9627189636230469\n",
      "epoch: 93 step: 24 loss: 0.060363464 acc: 0.976654052734375\n",
      "epoch: 93 step: 25 loss: 0.06878199 acc: 0.9619979858398438\n",
      "epoch: 93 step: 26 loss: 0.08268681 acc: 0.9600715637207031\n",
      "epoch: 93 step: 27 loss: 0.06346687 acc: 0.9698867797851562\n",
      "epoch: 93 step: 28 loss: 0.078964986 acc: 0.9637489318847656\n",
      "epoch: 93 step: 29 loss: 0.08040686 acc: 0.9592666625976562\n",
      "epoch: 93 step: 30 loss: 0.07614178 acc: 0.9631080627441406\n",
      "epoch: 93 step: 31 loss: 0.07886391 acc: 0.9679985046386719\n",
      "epoch: 93 step: 32 loss: 0.06792824 acc: 0.9673957824707031\n",
      "epoch: 93 step: 33 loss: 0.07486012 acc: 0.9626884460449219\n",
      "epoch: 93 step: 34 loss: 0.0688461 acc: 0.9731979370117188\n",
      "epoch: 93 step: 35 loss: 0.0721929 acc: 0.9620552062988281\n",
      "epoch: 93 step: 36 loss: 0.08503151 acc: 0.9607887268066406\n",
      "epoch: 93 step: 37 loss: 0.07548282 acc: 0.9628410339355469\n",
      "epoch: 93 step: 38 loss: 0.07527041 acc: 0.9619102478027344\n",
      "epoch: 93 step: 39 loss: 0.0614094 acc: 0.9710960388183594\n",
      "epoch: 93 step: 40 loss: 0.07964099 acc: 0.9669456481933594\n",
      "epoch: 93 step: 41 loss: 0.08301079 acc: 0.9698753356933594\n",
      "epoch: 93 step: 42 loss: 0.06496862 acc: 0.967437744140625\n",
      "epoch: 93 step: 43 loss: 0.08126926 acc: 0.9619064331054688\n",
      "epoch: 93 step: 44 loss: 0.085195504 acc: 0.9548912048339844\n",
      "epoch: 93 step: 45 loss: 0.0730912 acc: 0.9597206115722656\n",
      "epoch: 93 step: 46 loss: 0.06528004 acc: 0.9668502807617188\n",
      "epoch: 93 step: 47 loss: 0.070516095 acc: 0.9628791809082031\n",
      "epoch: 93 step: 48 loss: 0.07775924 acc: 0.96612548828125\n",
      "epoch: 93 step: 49 loss: 0.08520981 acc: 0.9663467407226562\n",
      "epoch: 93 step: 50 loss: 0.06510654 acc: 0.9690017700195312\n",
      "epoch: 93 step: 51 loss: 0.06810476 acc: 0.9704360961914062\n",
      "epoch: 93 step: 52 loss: 0.07865438 acc: 0.9625129699707031\n",
      "epoch: 93 step: 53 loss: 0.085415155 acc: 0.9622039794921875\n",
      "epoch: 93 step: 54 loss: 0.077632755 acc: 0.9659423828125\n",
      "epoch: 93 step: 55 loss: 0.07637675 acc: 0.9577407836914062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 93 step: 56 loss: 0.06765419 acc: 0.966522216796875\n",
      "epoch: 93 step: 57 loss: 0.052110292 acc: 0.9721336364746094\n",
      "epoch: 93 step: 58 loss: 0.07702377 acc: 0.967803955078125\n",
      "epoch: 93 step: 59 loss: 0.073148936 acc: 0.964447021484375\n",
      "epoch: 93 step: 60 loss: 0.06734888 acc: 0.9697456359863281\n",
      "epoch: 93 step: 61 loss: 0.07606849 acc: 0.9634132385253906\n",
      "epoch: 93 step: 62 loss: 0.07490906 acc: 0.9670982360839844\n",
      "epoch: 93 step: 63 loss: 0.07751263 acc: 0.9674453735351562\n",
      "epoch: 93 step: 64 loss: 0.067840114 acc: 0.9668083190917969\n",
      "epoch: 93 step: 65 loss: 0.08062626 acc: 0.9616546630859375\n",
      "epoch: 93 step: 66 loss: 0.07353741 acc: 0.9680824279785156\n",
      "epoch: 93 step: 67 loss: 0.074274026 acc: 0.9672584533691406\n",
      "epoch: 93 step: 68 loss: 0.062403746 acc: 0.965484619140625\n",
      "epoch: 93 step: 69 loss: 0.06853138 acc: 0.9643173217773438\n",
      "epoch: 93 step: 70 loss: 0.07990724 acc: 0.9624481201171875\n",
      "epoch: 93 step: 71 loss: 0.097093925 acc: 0.9468727111816406\n",
      "epoch: 93 step: 72 loss: 0.062042613 acc: 0.964141845703125\n",
      "epoch: 93 step: 73 loss: 0.07393951 acc: 0.9612655639648438\n",
      "epoch: 93 step: 74 loss: 0.06023899 acc: 0.9655418395996094\n",
      "epoch: 93 step: 75 loss: 0.06396716 acc: 0.9698524475097656\n",
      "epoch: 93 step: 76 loss: 0.088326104 acc: 0.96044921875\n",
      "epoch: 93 step: 77 loss: 0.06581258 acc: 0.9691581726074219\n",
      "epoch: 93 step: 78 loss: 0.05726094 acc: 0.9735488891601562\n",
      "epoch: 93 step: 79 loss: 0.07361015 acc: 0.9666099548339844\n",
      "epoch: 93 step: 80 loss: 0.06634572 acc: 0.9733772277832031\n",
      "epoch: 93 step: 81 loss: 0.06839206 acc: 0.965728759765625\n",
      "epoch: 93 step: 82 loss: 0.08405091 acc: 0.9725532531738281\n",
      "epoch: 93 step: 83 loss: 0.07546268 acc: 0.9663543701171875\n",
      "epoch: 93 step: 84 loss: 0.07183077 acc: 0.9615478515625\n",
      "epoch: 93 step: 85 loss: 0.07305394 acc: 0.9599800109863281\n",
      "epoch: 93 step: 86 loss: 0.06005161 acc: 0.9723129272460938\n",
      "epoch: 93 step: 87 loss: 0.07041572 acc: 0.9670829772949219\n",
      "epoch: 93 step: 88 loss: 0.059667863 acc: 0.9689598083496094\n",
      "epoch: 93 step: 89 loss: 0.07069888 acc: 0.9662666320800781\n",
      "epoch: 93 step: 90 loss: 0.09347343 acc: 0.9567108154296875\n",
      "epoch: 93 step: 91 loss: 0.07421444 acc: 0.9585456848144531\n",
      "epoch: 93 step: 92 loss: 0.082321815 acc: 0.9672660827636719\n",
      "epoch: 93 step: 93 loss: 0.07850404 acc: 0.9666404724121094\n",
      "epoch: 93 step: 94 loss: 0.072783954 acc: 0.965728759765625\n",
      "epoch: 93 step: 95 loss: 0.06427482 acc: 0.9714202880859375\n",
      "epoch: 93 step: 96 loss: 0.07783421 acc: 0.9705734252929688\n",
      "epoch: 93 step: 97 loss: 0.08548666 acc: 0.9708099365234375\n",
      "epoch: 93 step: 98 loss: 0.06793583 acc: 0.9626083374023438\n",
      "epoch: 93 step: 99 loss: 0.07654659 acc: 0.9673004150390625\n",
      "epoch: 93 step: 100 loss: 0.072807334 acc: 0.9673347473144531\n",
      "epoch: 93 step: 101 loss: 0.07018689 acc: 0.970458984375\n",
      "epoch: 93 step: 102 loss: 0.07247869 acc: 0.9646034240722656\n",
      "epoch: 93 step: 103 loss: 0.06706124 acc: 0.9676742553710938\n",
      "epoch: 93 step: 104 loss: 0.06714601 acc: 0.9664421081542969\n",
      "epoch: 93 step: 105 loss: 0.060659125 acc: 0.9735488891601562\n",
      "epoch: 93 step: 106 loss: 0.0711202 acc: 0.9661788940429688\n",
      "epoch: 93 step: 107 loss: 0.071456686 acc: 0.9628448486328125\n",
      "epoch: 93 step: 108 loss: 0.07340259 acc: 0.9603385925292969\n",
      "epoch: 93 step: 109 loss: 0.07049484 acc: 0.9639320373535156\n",
      "epoch: 93 step: 110 loss: 0.10344466 acc: 0.9599952697753906\n",
      "epoch: 93 step: 111 loss: 0.07705927 acc: 0.9640350341796875\n",
      "epoch: 93 step: 112 loss: 0.08764579 acc: 0.9623947143554688\n",
      "epoch: 93 step: 113 loss: 0.084434144 acc: 0.9612350463867188\n",
      "epoch: 93 step: 114 loss: 0.065072626 acc: 0.9728279113769531\n",
      "epoch: 93 step: 115 loss: 0.078511156 acc: 0.9643669128417969\n",
      "epoch: 93 step: 116 loss: 0.06389597 acc: 0.9693984985351562\n",
      "epoch: 93 step: 117 loss: 0.085057005 acc: 0.965240478515625\n",
      "epoch: 93 step: 118 loss: 0.08147985 acc: 0.9584579467773438\n",
      "epoch: 93 step: 119 loss: 0.06717663 acc: 0.9657363891601562\n",
      "epoch: 93 step: 120 loss: 0.08835953 acc: 0.9612045288085938\n",
      "epoch: 93 step: 121 loss: 0.09480203 acc: 0.9566078186035156\n",
      "epoch: 93 step: 122 loss: 0.07081541 acc: 0.9654273986816406\n",
      "epoch: 93 step: 123 loss: 0.06984982 acc: 0.9702224731445312\n",
      "epoch: 93 step: 124 loss: 0.08600026 acc: 0.9603445870535714\n",
      "epoch: 93 validation_loss: 0.082 validation_dice: 0.8518934416629943\n",
      "epoch: 93 test_dataset dice: 0.7516050931062043\n",
      "time cost 0.5363741715749105 min\n",
      "dice_best: 0.8622422678227601\n",
      "******************************** epoch  93  is finished. *********************************\n",
      "epoch: 94 step: 1 loss: 0.06415871 acc: 0.9706268310546875\n",
      "epoch: 94 step: 2 loss: 0.0690901 acc: 0.9651908874511719\n",
      "epoch: 94 step: 3 loss: 0.072803885 acc: 0.968231201171875\n",
      "epoch: 94 step: 4 loss: 0.06565699 acc: 0.9677581787109375\n",
      "epoch: 94 step: 5 loss: 0.060585223 acc: 0.9714088439941406\n",
      "epoch: 94 step: 6 loss: 0.07420892 acc: 0.968475341796875\n",
      "epoch: 94 step: 7 loss: 0.06504322 acc: 0.9727210998535156\n",
      "epoch: 94 step: 8 loss: 0.0780636 acc: 0.9674758911132812\n",
      "epoch: 94 step: 9 loss: 0.07275134 acc: 0.9660072326660156\n",
      "epoch: 94 step: 10 loss: 0.06649493 acc: 0.9702262878417969\n",
      "epoch: 94 step: 11 loss: 0.06464441 acc: 0.9689598083496094\n",
      "epoch: 94 step: 12 loss: 0.07288317 acc: 0.9643440246582031\n",
      "epoch: 94 step: 13 loss: 0.07135617 acc: 0.9668083190917969\n",
      "epoch: 94 step: 14 loss: 0.06854577 acc: 0.9665412902832031\n",
      "epoch: 94 step: 15 loss: 0.07527332 acc: 0.9639663696289062\n",
      "epoch: 94 step: 16 loss: 0.085210904 acc: 0.9666519165039062\n",
      "epoch: 94 step: 17 loss: 0.06922439 acc: 0.9659652709960938\n",
      "epoch: 94 step: 18 loss: 0.06781229 acc: 0.9643173217773438\n",
      "epoch: 94 step: 19 loss: 0.07795521 acc: 0.9587898254394531\n",
      "epoch: 94 step: 20 loss: 0.093925655 acc: 0.9528236389160156\n",
      "epoch: 94 step: 21 loss: 0.06327737 acc: 0.9715042114257812\n",
      "epoch: 94 step: 22 loss: 0.06916848 acc: 0.9672012329101562\n",
      "epoch: 94 step: 23 loss: 0.07538838 acc: 0.96405029296875\n",
      "epoch: 94 step: 24 loss: 0.06442474 acc: 0.9707450866699219\n",
      "epoch: 94 step: 25 loss: 0.06156392 acc: 0.9786376953125\n",
      "epoch: 94 step: 26 loss: 0.0752818 acc: 0.97393798828125\n",
      "epoch: 94 step: 27 loss: 0.07326236 acc: 0.9693717956542969\n",
      "epoch: 94 step: 28 loss: 0.08249402 acc: 0.9693107604980469\n",
      "epoch: 94 step: 29 loss: 0.09096773 acc: 0.956878662109375\n",
      "epoch: 94 step: 30 loss: 0.07163454 acc: 0.9676513671875\n",
      "epoch: 94 step: 31 loss: 0.07315292 acc: 0.9659500122070312\n",
      "epoch: 94 step: 32 loss: 0.06969897 acc: 0.9604148864746094\n",
      "epoch: 94 step: 33 loss: 0.070479654 acc: 0.9630203247070312\n",
      "epoch: 94 step: 34 loss: 0.075026035 acc: 0.9638519287109375\n",
      "epoch: 94 step: 35 loss: 0.083810955 acc: 0.9641075134277344\n",
      "epoch: 94 step: 36 loss: 0.0667025 acc: 0.9662322998046875\n",
      "epoch: 94 step: 37 loss: 0.064884745 acc: 0.9636001586914062\n",
      "epoch: 94 step: 38 loss: 0.084410004 acc: 0.9562110900878906\n",
      "epoch: 94 step: 39 loss: 0.070864 acc: 0.9637298583984375\n",
      "epoch: 94 step: 40 loss: 0.07247854 acc: 0.965484619140625\n",
      "epoch: 94 step: 41 loss: 0.05533772 acc: 0.9773635864257812\n",
      "epoch: 94 step: 42 loss: 0.07207053 acc: 0.9691886901855469\n",
      "epoch: 94 step: 43 loss: 0.07867775 acc: 0.9679298400878906\n",
      "epoch: 94 step: 44 loss: 0.061960705 acc: 0.9731636047363281\n",
      "epoch: 94 step: 45 loss: 0.059093483 acc: 0.9782905578613281\n",
      "epoch: 94 step: 46 loss: 0.077853814 acc: 0.9681129455566406\n",
      "epoch: 94 step: 47 loss: 0.06250451 acc: 0.9717941284179688\n",
      "epoch: 94 step: 48 loss: 0.062229928 acc: 0.9745368957519531\n",
      "epoch: 94 step: 49 loss: 0.07205304 acc: 0.9677047729492188\n",
      "epoch: 94 step: 50 loss: 0.07295626 acc: 0.96722412109375\n",
      "epoch: 94 step: 51 loss: 0.07128324 acc: 0.9685401916503906\n",
      "epoch: 94 step: 52 loss: 0.059971884 acc: 0.970001220703125\n",
      "epoch: 94 step: 53 loss: 0.06711636 acc: 0.9674835205078125\n",
      "epoch: 94 step: 54 loss: 0.07536795 acc: 0.9606399536132812\n",
      "epoch: 94 step: 55 loss: 0.0779967 acc: 0.9657173156738281\n",
      "epoch: 94 step: 56 loss: 0.06986336 acc: 0.963409423828125\n",
      "epoch: 94 step: 57 loss: 0.07141255 acc: 0.963836669921875\n",
      "epoch: 94 step: 58 loss: 0.07327596 acc: 0.9666709899902344\n",
      "epoch: 94 step: 59 loss: 0.07690161 acc: 0.9601707458496094\n",
      "epoch: 94 step: 60 loss: 0.08262383 acc: 0.9622535705566406\n",
      "epoch: 94 step: 61 loss: 0.05820642 acc: 0.9734878540039062\n",
      "epoch: 94 step: 62 loss: 0.08950755 acc: 0.9533576965332031\n",
      "epoch: 94 step: 63 loss: 0.07715422 acc: 0.9623298645019531\n",
      "epoch: 94 step: 64 loss: 0.07287046 acc: 0.965301513671875\n",
      "epoch: 94 step: 65 loss: 0.066089205 acc: 0.9707832336425781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 94 step: 66 loss: 0.067341775 acc: 0.9694023132324219\n",
      "epoch: 94 step: 67 loss: 0.07072769 acc: 0.97357177734375\n",
      "epoch: 94 step: 68 loss: 0.06428468 acc: 0.9723777770996094\n",
      "epoch: 94 step: 69 loss: 0.07095785 acc: 0.9689292907714844\n",
      "epoch: 94 step: 70 loss: 0.067509145 acc: 0.9661216735839844\n",
      "epoch: 94 step: 71 loss: 0.07734741 acc: 0.9634132385253906\n",
      "epoch: 94 step: 72 loss: 0.08646407 acc: 0.9590797424316406\n",
      "epoch: 94 step: 73 loss: 0.07550663 acc: 0.9591636657714844\n",
      "epoch: 94 step: 74 loss: 0.068596005 acc: 0.9670639038085938\n",
      "epoch: 94 step: 75 loss: 0.0826437 acc: 0.9566459655761719\n",
      "epoch: 94 step: 76 loss: 0.07032046 acc: 0.9673309326171875\n",
      "epoch: 94 step: 77 loss: 0.08475632 acc: 0.9671859741210938\n",
      "epoch: 94 step: 78 loss: 0.067501165 acc: 0.9635429382324219\n",
      "epoch: 94 step: 79 loss: 0.06598426 acc: 0.9709091186523438\n",
      "epoch: 94 step: 80 loss: 0.06441764 acc: 0.9711875915527344\n",
      "epoch: 94 step: 81 loss: 0.07304799 acc: 0.969207763671875\n",
      "epoch: 94 step: 82 loss: 0.080275424 acc: 0.9657630920410156\n",
      "epoch: 94 step: 83 loss: 0.072992675 acc: 0.9605064392089844\n",
      "epoch: 94 step: 84 loss: 0.07164534 acc: 0.9637336730957031\n",
      "epoch: 94 step: 85 loss: 0.07849868 acc: 0.9622230529785156\n",
      "epoch: 94 step: 86 loss: 0.058266085 acc: 0.9733428955078125\n",
      "epoch: 94 step: 87 loss: 0.097585075 acc: 0.9553337097167969\n",
      "epoch: 94 step: 88 loss: 0.072724536 acc: 0.9645538330078125\n",
      "epoch: 94 step: 89 loss: 0.082907654 acc: 0.9507293701171875\n",
      "epoch: 94 step: 90 loss: 0.07349798 acc: 0.9622230529785156\n",
      "epoch: 94 step: 91 loss: 0.07665847 acc: 0.9654579162597656\n",
      "epoch: 94 step: 92 loss: 0.06555832 acc: 0.968505859375\n",
      "epoch: 94 step: 93 loss: 0.061631322 acc: 0.9712867736816406\n",
      "epoch: 94 step: 94 loss: 0.075338505 acc: 0.9664649963378906\n",
      "epoch: 94 step: 95 loss: 0.06203458 acc: 0.9742279052734375\n",
      "epoch: 94 step: 96 loss: 0.073799916 acc: 0.9651451110839844\n",
      "epoch: 94 step: 97 loss: 0.08171313 acc: 0.9598236083984375\n",
      "epoch: 94 step: 98 loss: 0.075897574 acc: 0.9644660949707031\n",
      "epoch: 94 step: 99 loss: 0.07809626 acc: 0.9715805053710938\n",
      "epoch: 94 step: 100 loss: 0.06243573 acc: 0.9752845764160156\n",
      "epoch: 94 step: 101 loss: 0.081146896 acc: 0.9675483703613281\n",
      "epoch: 94 step: 102 loss: 0.060330447 acc: 0.9769783020019531\n",
      "epoch: 94 step: 103 loss: 0.07656245 acc: 0.9640045166015625\n",
      "epoch: 94 step: 104 loss: 0.09137396 acc: 0.9626884460449219\n",
      "epoch: 94 step: 105 loss: 0.083465256 acc: 0.9636878967285156\n",
      "epoch: 94 step: 106 loss: 0.07105509 acc: 0.9697380065917969\n",
      "epoch: 94 step: 107 loss: 0.06701759 acc: 0.9725494384765625\n",
      "epoch: 94 step: 108 loss: 0.06369853 acc: 0.9708900451660156\n",
      "epoch: 94 step: 109 loss: 0.068810396 acc: 0.965087890625\n",
      "epoch: 94 step: 110 loss: 0.07475997 acc: 0.9645462036132812\n",
      "epoch: 94 step: 111 loss: 0.08762125 acc: 0.9557571411132812\n",
      "epoch: 94 step: 112 loss: 0.07787589 acc: 0.9664878845214844\n",
      "epoch: 94 step: 113 loss: 0.08148492 acc: 0.9631156921386719\n",
      "epoch: 94 step: 114 loss: 0.06285136 acc: 0.9715042114257812\n",
      "epoch: 94 step: 115 loss: 0.08209199 acc: 0.9660530090332031\n",
      "epoch: 94 step: 116 loss: 0.08014417 acc: 0.9655036926269531\n",
      "epoch: 94 step: 117 loss: 0.061180975 acc: 0.9691047668457031\n",
      "epoch: 94 step: 118 loss: 0.09639911 acc: 0.9554862976074219\n",
      "epoch: 94 step: 119 loss: 0.09082968 acc: 0.9581947326660156\n",
      "epoch: 94 step: 120 loss: 0.06761597 acc: 0.9717826843261719\n",
      "epoch: 94 step: 121 loss: 0.068635456 acc: 0.9707717895507812\n",
      "epoch: 94 step: 122 loss: 0.065964386 acc: 0.9684677124023438\n",
      "epoch: 94 step: 123 loss: 0.06790808 acc: 0.967742919921875\n",
      "epoch: 94 step: 124 loss: 0.06955235 acc: 0.97247314453125\n",
      "epoch: 94 validation_loss: 0.082 validation_dice: 0.8477027073199132\n",
      "epoch: 94 test_dataset dice: 0.7289777414770111\n",
      "time cost 0.5356716473897298 min\n",
      "dice_best: 0.8622422678227601\n",
      "******************************** epoch  94  is finished. *********************************\n",
      "epoch: 95 step: 1 loss: 0.07319688 acc: 0.9673690795898438\n",
      "epoch: 95 step: 2 loss: 0.075916655 acc: 0.9586410522460938\n",
      "epoch: 95 step: 3 loss: 0.0664997 acc: 0.9724845886230469\n",
      "epoch: 95 step: 4 loss: 0.06951113 acc: 0.9623374938964844\n",
      "epoch: 95 step: 5 loss: 0.08039315 acc: 0.96185302734375\n",
      "epoch: 95 step: 6 loss: 0.07399587 acc: 0.9652862548828125\n",
      "epoch: 95 step: 7 loss: 0.067037225 acc: 0.9657249450683594\n",
      "epoch: 95 step: 8 loss: 0.067198865 acc: 0.9719581604003906\n",
      "epoch: 95 step: 9 loss: 0.06779218 acc: 0.9644508361816406\n",
      "epoch: 95 step: 10 loss: 0.08350034 acc: 0.9611434936523438\n",
      "epoch: 95 step: 11 loss: 0.07611817 acc: 0.9651107788085938\n",
      "epoch: 95 step: 12 loss: 0.06830893 acc: 0.9724845886230469\n",
      "epoch: 95 step: 13 loss: 0.063839 acc: 0.9709663391113281\n",
      "epoch: 95 step: 14 loss: 0.07398397 acc: 0.96728515625\n",
      "epoch: 95 step: 15 loss: 0.0676501 acc: 0.9696388244628906\n",
      "epoch: 95 step: 16 loss: 0.072135344 acc: 0.9644126892089844\n",
      "epoch: 95 step: 17 loss: 0.058650356 acc: 0.9690895080566406\n",
      "epoch: 95 step: 18 loss: 0.06358175 acc: 0.9680633544921875\n",
      "epoch: 95 step: 19 loss: 0.06781888 acc: 0.9678840637207031\n",
      "epoch: 95 step: 20 loss: 0.06583611 acc: 0.9703102111816406\n",
      "epoch: 95 step: 21 loss: 0.056085326 acc: 0.9740371704101562\n",
      "epoch: 95 step: 22 loss: 0.06783628 acc: 0.9661331176757812\n",
      "epoch: 95 step: 23 loss: 0.0926542 acc: 0.95867919921875\n",
      "epoch: 95 step: 24 loss: 0.06566748 acc: 0.9693603515625\n",
      "epoch: 95 step: 25 loss: 0.07761196 acc: 0.9635543823242188\n",
      "epoch: 95 step: 26 loss: 0.0677423 acc: 0.9655647277832031\n",
      "epoch: 95 step: 27 loss: 0.07412184 acc: 0.96087646484375\n",
      "epoch: 95 step: 28 loss: 0.07186963 acc: 0.9691123962402344\n",
      "epoch: 95 step: 29 loss: 0.07260152 acc: 0.963043212890625\n",
      "epoch: 95 step: 30 loss: 0.07017946 acc: 0.9666595458984375\n",
      "epoch: 95 step: 31 loss: 0.068211906 acc: 0.9639320373535156\n",
      "epoch: 95 step: 32 loss: 0.079816535 acc: 0.9652633666992188\n",
      "epoch: 95 step: 33 loss: 0.08635145 acc: 0.9629592895507812\n",
      "epoch: 95 step: 34 loss: 0.07036983 acc: 0.9665756225585938\n",
      "epoch: 95 step: 35 loss: 0.066743106 acc: 0.9658966064453125\n",
      "epoch: 95 step: 36 loss: 0.071126394 acc: 0.965911865234375\n",
      "epoch: 95 step: 37 loss: 0.07746761 acc: 0.965850830078125\n",
      "epoch: 95 step: 38 loss: 0.07144135 acc: 0.9700508117675781\n",
      "epoch: 95 step: 39 loss: 0.07480137 acc: 0.9682464599609375\n",
      "epoch: 95 step: 40 loss: 0.060313217 acc: 0.976837158203125\n",
      "epoch: 95 step: 41 loss: 0.071084134 acc: 0.9677276611328125\n",
      "epoch: 95 step: 42 loss: 0.07330997 acc: 0.9671440124511719\n",
      "epoch: 95 step: 43 loss: 0.07418269 acc: 0.9628028869628906\n",
      "epoch: 95 step: 44 loss: 0.0629509 acc: 0.9699020385742188\n",
      "epoch: 95 step: 45 loss: 0.0780766 acc: 0.9665107727050781\n",
      "epoch: 95 step: 46 loss: 0.07965392 acc: 0.9690742492675781\n",
      "epoch: 95 step: 47 loss: 0.06221711 acc: 0.9718894958496094\n",
      "epoch: 95 step: 48 loss: 0.07805862 acc: 0.9632186889648438\n",
      "epoch: 95 step: 49 loss: 0.07812734 acc: 0.9673538208007812\n",
      "epoch: 95 step: 50 loss: 0.067969754 acc: 0.9643096923828125\n",
      "epoch: 95 step: 51 loss: 0.06292276 acc: 0.9687919616699219\n",
      "epoch: 95 step: 52 loss: 0.067121044 acc: 0.9690513610839844\n",
      "epoch: 95 step: 53 loss: 0.06776288 acc: 0.96630859375\n",
      "epoch: 95 step: 54 loss: 0.0730405 acc: 0.9672317504882812\n",
      "epoch: 95 step: 55 loss: 0.071423605 acc: 0.9752349853515625\n",
      "epoch: 95 step: 56 loss: 0.07717706 acc: 0.9631690979003906\n",
      "epoch: 95 step: 57 loss: 0.075051755 acc: 0.9685554504394531\n",
      "epoch: 95 step: 58 loss: 0.07010007 acc: 0.9642791748046875\n",
      "epoch: 95 step: 59 loss: 0.072889976 acc: 0.9625473022460938\n",
      "epoch: 95 step: 60 loss: 0.07870326 acc: 0.9598617553710938\n",
      "epoch: 95 step: 61 loss: 0.07372017 acc: 0.9641571044921875\n",
      "epoch: 95 step: 62 loss: 0.069273524 acc: 0.97039794921875\n",
      "epoch: 95 step: 63 loss: 0.06293144 acc: 0.9659233093261719\n",
      "epoch: 95 step: 64 loss: 0.06809932 acc: 0.9685020446777344\n",
      "epoch: 95 step: 65 loss: 0.07313589 acc: 0.9637794494628906\n",
      "epoch: 95 step: 66 loss: 0.066316806 acc: 0.9584121704101562\n",
      "epoch: 95 step: 67 loss: 0.05952413 acc: 0.971649169921875\n",
      "epoch: 95 step: 68 loss: 0.07558915 acc: 0.9679450988769531\n",
      "epoch: 95 step: 69 loss: 0.07064129 acc: 0.9678459167480469\n",
      "epoch: 95 step: 70 loss: 0.074088685 acc: 0.9689445495605469\n",
      "epoch: 95 step: 71 loss: 0.07725182 acc: 0.9594001770019531\n",
      "epoch: 95 step: 72 loss: 0.07488425 acc: 0.969573974609375\n",
      "epoch: 95 step: 73 loss: 0.06286967 acc: 0.9715576171875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 95 step: 74 loss: 0.07043865 acc: 0.9714508056640625\n",
      "epoch: 95 step: 75 loss: 0.069733255 acc: 0.966339111328125\n",
      "epoch: 95 step: 76 loss: 0.07291953 acc: 0.9658889770507812\n",
      "epoch: 95 step: 77 loss: 0.07762407 acc: 0.9632682800292969\n",
      "epoch: 95 step: 78 loss: 0.060968608 acc: 0.9690055847167969\n",
      "epoch: 95 step: 79 loss: 0.060509764 acc: 0.9753646850585938\n",
      "epoch: 95 step: 80 loss: 0.07521911 acc: 0.9751968383789062\n",
      "epoch: 95 step: 81 loss: 0.067372955 acc: 0.9687118530273438\n",
      "epoch: 95 step: 82 loss: 0.08167744 acc: 0.9620475769042969\n",
      "epoch: 95 step: 83 loss: 0.06653947 acc: 0.9721298217773438\n",
      "epoch: 95 step: 84 loss: 0.06862266 acc: 0.9657859802246094\n",
      "epoch: 95 step: 85 loss: 0.07474375 acc: 0.9632034301757812\n",
      "epoch: 95 step: 86 loss: 0.07671817 acc: 0.969635009765625\n",
      "epoch: 95 step: 87 loss: 0.059745435 acc: 0.97003173828125\n",
      "epoch: 95 step: 88 loss: 0.06957199 acc: 0.9658660888671875\n",
      "epoch: 95 step: 89 loss: 0.0705411 acc: 0.969818115234375\n",
      "epoch: 95 step: 90 loss: 0.06653525 acc: 0.9726104736328125\n",
      "epoch: 95 step: 91 loss: 0.06680761 acc: 0.9720802307128906\n",
      "epoch: 95 step: 92 loss: 0.058688167 acc: 0.9748992919921875\n",
      "epoch: 95 step: 93 loss: 0.08288141 acc: 0.9660797119140625\n",
      "epoch: 95 step: 94 loss: 0.078584 acc: 0.9622573852539062\n",
      "epoch: 95 step: 95 loss: 0.06561175 acc: 0.967681884765625\n",
      "epoch: 95 step: 96 loss: 0.08380032 acc: 0.9680213928222656\n",
      "epoch: 95 step: 97 loss: 0.07922733 acc: 0.9588623046875\n",
      "epoch: 95 step: 98 loss: 0.06280557 acc: 0.9706497192382812\n",
      "epoch: 95 step: 99 loss: 0.07117769 acc: 0.9645423889160156\n",
      "epoch: 95 step: 100 loss: 0.07177304 acc: 0.9646186828613281\n",
      "epoch: 95 step: 101 loss: 0.064326905 acc: 0.9751205444335938\n",
      "epoch: 95 step: 102 loss: 0.068227716 acc: 0.9683570861816406\n",
      "epoch: 95 step: 103 loss: 0.06387668 acc: 0.9706153869628906\n",
      "epoch: 95 step: 104 loss: 0.07753323 acc: 0.9667167663574219\n",
      "epoch: 95 step: 105 loss: 0.07227343 acc: 0.9712600708007812\n",
      "epoch: 95 step: 106 loss: 0.06833293 acc: 0.9695777893066406\n",
      "epoch: 95 step: 107 loss: 0.07402755 acc: 0.9642791748046875\n",
      "epoch: 95 step: 108 loss: 0.07634831 acc: 0.9597549438476562\n",
      "epoch: 95 step: 109 loss: 0.06599845 acc: 0.9638481140136719\n",
      "epoch: 95 step: 110 loss: 0.07763376 acc: 0.9618377685546875\n",
      "epoch: 95 step: 111 loss: 0.071828455 acc: 0.9568634033203125\n",
      "epoch: 95 step: 112 loss: 0.06635558 acc: 0.9688682556152344\n",
      "epoch: 95 step: 113 loss: 0.07752447 acc: 0.9612922668457031\n",
      "epoch: 95 step: 114 loss: 0.060129326 acc: 0.9756393432617188\n",
      "epoch: 95 step: 115 loss: 0.070356466 acc: 0.9725227355957031\n",
      "epoch: 95 step: 116 loss: 0.079838 acc: 0.9637985229492188\n",
      "epoch: 95 step: 117 loss: 0.06405213 acc: 0.9685783386230469\n",
      "epoch: 95 step: 118 loss: 0.07315672 acc: 0.9627037048339844\n",
      "epoch: 95 step: 119 loss: 0.06501256 acc: 0.9706840515136719\n",
      "epoch: 95 step: 120 loss: 0.077965006 acc: 0.9592819213867188\n",
      "epoch: 95 step: 121 loss: 0.07867176 acc: 0.9633598327636719\n",
      "epoch: 95 step: 122 loss: 0.07891473 acc: 0.9607925415039062\n",
      "epoch: 95 step: 123 loss: 0.0710775 acc: 0.9651527404785156\n",
      "epoch: 95 step: 124 loss: 0.055775445 acc: 0.9747052873883929\n",
      "epoch: 95 validation_loss: 0.078 validation_dice: 0.8646678032603314\n",
      "epoch: 95 test_dataset dice: 0.7416293667412696\n",
      "time cost 0.535976235071818 min\n",
      "dice_best: 0.8646678032603314\n",
      "******************************** epoch  95  is finished. *********************************\n",
      "epoch: 96 step: 1 loss: 0.0655199 acc: 0.9656867980957031\n",
      "epoch: 96 step: 2 loss: 0.06411019 acc: 0.9668426513671875\n",
      "epoch: 96 step: 3 loss: 0.059360348 acc: 0.9740867614746094\n",
      "epoch: 96 step: 4 loss: 0.068236485 acc: 0.9679908752441406\n",
      "epoch: 96 step: 5 loss: 0.065955155 acc: 0.9689064025878906\n",
      "epoch: 96 step: 6 loss: 0.07222702 acc: 0.9723014831542969\n",
      "epoch: 96 step: 7 loss: 0.062458944 acc: 0.9718246459960938\n",
      "epoch: 96 step: 8 loss: 0.07395664 acc: 0.966217041015625\n",
      "epoch: 96 step: 9 loss: 0.06287494 acc: 0.969940185546875\n",
      "epoch: 96 step: 10 loss: 0.076739885 acc: 0.9620552062988281\n",
      "epoch: 96 step: 11 loss: 0.068774104 acc: 0.9647331237792969\n",
      "epoch: 96 step: 12 loss: 0.059467383 acc: 0.9709129333496094\n",
      "epoch: 96 step: 13 loss: 0.059788465 acc: 0.9673004150390625\n",
      "epoch: 96 step: 14 loss: 0.06065191 acc: 0.977874755859375\n",
      "epoch: 96 step: 15 loss: 0.06668226 acc: 0.970550537109375\n",
      "epoch: 96 step: 16 loss: 0.07008435 acc: 0.9644432067871094\n",
      "epoch: 96 step: 17 loss: 0.053357977 acc: 0.9761238098144531\n",
      "epoch: 96 step: 18 loss: 0.068036385 acc: 0.9715957641601562\n",
      "epoch: 96 step: 19 loss: 0.068912156 acc: 0.9639472961425781\n",
      "epoch: 96 step: 20 loss: 0.07125883 acc: 0.9636116027832031\n",
      "epoch: 96 step: 21 loss: 0.06427445 acc: 0.9741363525390625\n",
      "epoch: 96 step: 22 loss: 0.07274428 acc: 0.9709243774414062\n",
      "epoch: 96 step: 23 loss: 0.06827422 acc: 0.9629478454589844\n",
      "epoch: 96 step: 24 loss: 0.08435078 acc: 0.9586181640625\n",
      "epoch: 96 step: 25 loss: 0.07101945 acc: 0.967315673828125\n",
      "epoch: 96 step: 26 loss: 0.06311498 acc: 0.9697685241699219\n",
      "epoch: 96 step: 27 loss: 0.0715462 acc: 0.957672119140625\n",
      "epoch: 96 step: 28 loss: 0.065058514 acc: 0.9633064270019531\n",
      "epoch: 96 step: 29 loss: 0.07165248 acc: 0.9686508178710938\n",
      "epoch: 96 step: 30 loss: 0.06597624 acc: 0.9652481079101562\n",
      "epoch: 96 step: 31 loss: 0.07233027 acc: 0.9662246704101562\n",
      "epoch: 96 step: 32 loss: 0.071020335 acc: 0.9615287780761719\n",
      "epoch: 96 step: 33 loss: 0.069937006 acc: 0.9689598083496094\n",
      "epoch: 96 step: 34 loss: 0.061705768 acc: 0.9747390747070312\n",
      "epoch: 96 step: 35 loss: 0.07278803 acc: 0.9640731811523438\n",
      "epoch: 96 step: 36 loss: 0.0706144 acc: 0.9673004150390625\n",
      "epoch: 96 step: 37 loss: 0.07636423 acc: 0.9618453979492188\n",
      "epoch: 96 step: 38 loss: 0.0628651 acc: 0.9675254821777344\n",
      "epoch: 96 step: 39 loss: 0.07069273 acc: 0.9630012512207031\n",
      "epoch: 96 step: 40 loss: 0.0644594 acc: 0.9674835205078125\n",
      "epoch: 96 step: 41 loss: 0.06872106 acc: 0.9693756103515625\n",
      "epoch: 96 step: 42 loss: 0.060222235 acc: 0.9725799560546875\n",
      "epoch: 96 step: 43 loss: 0.07153009 acc: 0.9697227478027344\n",
      "epoch: 96 step: 44 loss: 0.07447624 acc: 0.9610443115234375\n",
      "epoch: 96 step: 45 loss: 0.08347449 acc: 0.9627876281738281\n",
      "epoch: 96 step: 46 loss: 0.06947643 acc: 0.9669036865234375\n",
      "epoch: 96 step: 47 loss: 0.07194148 acc: 0.9628486633300781\n",
      "epoch: 96 step: 48 loss: 0.074942335 acc: 0.9624481201171875\n",
      "epoch: 96 step: 49 loss: 0.079367295 acc: 0.9576835632324219\n",
      "epoch: 96 step: 50 loss: 0.06838505 acc: 0.9687957763671875\n",
      "epoch: 96 step: 51 loss: 0.050610937 acc: 0.9753074645996094\n",
      "epoch: 96 step: 52 loss: 0.061775375 acc: 0.9686927795410156\n",
      "epoch: 96 step: 53 loss: 0.06963295 acc: 0.9646873474121094\n",
      "epoch: 96 step: 54 loss: 0.07934027 acc: 0.9619178771972656\n",
      "epoch: 96 step: 55 loss: 0.06850232 acc: 0.9707298278808594\n",
      "epoch: 96 step: 56 loss: 0.06366896 acc: 0.9686851501464844\n",
      "epoch: 96 step: 57 loss: 0.07897092 acc: 0.9573593139648438\n",
      "epoch: 96 step: 58 loss: 0.0669694 acc: 0.9712791442871094\n",
      "epoch: 96 step: 59 loss: 0.072267056 acc: 0.9672966003417969\n",
      "epoch: 96 step: 60 loss: 0.068696566 acc: 0.9688682556152344\n",
      "epoch: 96 step: 61 loss: 0.068740085 acc: 0.9718208312988281\n",
      "epoch: 96 step: 62 loss: 0.07634784 acc: 0.9571571350097656\n",
      "epoch: 96 step: 63 loss: 0.086954646 acc: 0.9568595886230469\n",
      "epoch: 96 step: 64 loss: 0.061529364 acc: 0.9702186584472656\n",
      "epoch: 96 step: 65 loss: 0.07910762 acc: 0.9610023498535156\n",
      "epoch: 96 step: 66 loss: 0.06405357 acc: 0.9658164978027344\n",
      "epoch: 96 step: 67 loss: 0.073369786 acc: 0.9622039794921875\n",
      "epoch: 96 step: 68 loss: 0.07485014 acc: 0.9637527465820312\n",
      "epoch: 96 step: 69 loss: 0.05726358 acc: 0.9649314880371094\n",
      "epoch: 96 step: 70 loss: 0.070929065 acc: 0.9649505615234375\n",
      "epoch: 96 step: 71 loss: 0.077788986 acc: 0.9599342346191406\n",
      "epoch: 96 step: 72 loss: 0.09122118 acc: 0.9587745666503906\n",
      "epoch: 96 step: 73 loss: 0.064268164 acc: 0.9703521728515625\n",
      "epoch: 96 step: 74 loss: 0.070006646 acc: 0.9715309143066406\n",
      "epoch: 96 step: 75 loss: 0.07140511 acc: 0.9700241088867188\n",
      "epoch: 96 step: 76 loss: 0.07388854 acc: 0.9648170471191406\n",
      "epoch: 96 step: 77 loss: 0.05662179 acc: 0.9775772094726562\n",
      "epoch: 96 step: 78 loss: 0.060424507 acc: 0.9703750610351562\n",
      "epoch: 96 step: 79 loss: 0.0854485 acc: 0.9616165161132812\n",
      "epoch: 96 step: 80 loss: 0.07588245 acc: 0.9592552185058594\n",
      "epoch: 96 step: 81 loss: 0.08131979 acc: 0.9668960571289062\n",
      "epoch: 96 step: 82 loss: 0.05760416 acc: 0.9741325378417969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 96 step: 83 loss: 0.06634623 acc: 0.9704475402832031\n",
      "epoch: 96 step: 84 loss: 0.07513833 acc: 0.9684486389160156\n",
      "epoch: 96 step: 85 loss: 0.08262734 acc: 0.9639244079589844\n",
      "epoch: 96 step: 86 loss: 0.0725152 acc: 0.966033935546875\n",
      "epoch: 96 step: 87 loss: 0.06423325 acc: 0.9723091125488281\n",
      "epoch: 96 step: 88 loss: 0.0560934 acc: 0.9686660766601562\n",
      "epoch: 96 step: 89 loss: 0.07603737 acc: 0.9618339538574219\n",
      "epoch: 96 step: 90 loss: 0.09690173 acc: 0.9571800231933594\n",
      "epoch: 96 step: 91 loss: 0.06687484 acc: 0.9672393798828125\n",
      "epoch: 96 step: 92 loss: 0.07210189 acc: 0.96630859375\n",
      "epoch: 96 step: 93 loss: 0.092597716 acc: 0.9532585144042969\n",
      "epoch: 96 step: 94 loss: 0.06992972 acc: 0.9689254760742188\n",
      "epoch: 96 step: 95 loss: 0.08171313 acc: 0.9608268737792969\n",
      "epoch: 96 step: 96 loss: 0.07829184 acc: 0.9620475769042969\n",
      "epoch: 96 step: 97 loss: 0.0671093 acc: 0.9718437194824219\n",
      "epoch: 96 step: 98 loss: 0.076663814 acc: 0.9600143432617188\n",
      "epoch: 96 step: 99 loss: 0.080146246 acc: 0.9599571228027344\n",
      "epoch: 96 step: 100 loss: 0.06994534 acc: 0.9721908569335938\n",
      "epoch: 96 step: 101 loss: 0.07154816 acc: 0.9696884155273438\n",
      "epoch: 96 step: 102 loss: 0.071421534 acc: 0.968963623046875\n",
      "epoch: 96 step: 103 loss: 0.06873633 acc: 0.9672431945800781\n",
      "epoch: 96 step: 104 loss: 0.08339553 acc: 0.9672813415527344\n",
      "epoch: 96 step: 105 loss: 0.081100695 acc: 0.9624366760253906\n",
      "epoch: 96 step: 106 loss: 0.077478416 acc: 0.9598045349121094\n",
      "epoch: 96 step: 107 loss: 0.065381885 acc: 0.973175048828125\n",
      "epoch: 96 step: 108 loss: 0.08174508 acc: 0.964569091796875\n",
      "epoch: 96 step: 109 loss: 0.056303892 acc: 0.9741554260253906\n",
      "epoch: 96 step: 110 loss: 0.064709134 acc: 0.9687423706054688\n",
      "epoch: 96 step: 111 loss: 0.079669304 acc: 0.9595413208007812\n",
      "epoch: 96 step: 112 loss: 0.07826963 acc: 0.9592781066894531\n",
      "epoch: 96 step: 113 loss: 0.060132463 acc: 0.9724464416503906\n",
      "epoch: 96 step: 114 loss: 0.07662336 acc: 0.9655189514160156\n",
      "epoch: 96 step: 115 loss: 0.0851931 acc: 0.9596176147460938\n",
      "epoch: 96 step: 116 loss: 0.07020511 acc: 0.9660682678222656\n",
      "epoch: 96 step: 117 loss: 0.07338799 acc: 0.9719161987304688\n",
      "epoch: 96 step: 118 loss: 0.06311375 acc: 0.9697303771972656\n",
      "epoch: 96 step: 119 loss: 0.07811573 acc: 0.9617881774902344\n",
      "epoch: 96 step: 120 loss: 0.073577486 acc: 0.9609794616699219\n",
      "epoch: 96 step: 121 loss: 0.06766298 acc: 0.9629707336425781\n",
      "epoch: 96 step: 122 loss: 0.06559817 acc: 0.9704971313476562\n",
      "epoch: 96 step: 123 loss: 0.06422398 acc: 0.971221923828125\n",
      "epoch: 96 step: 124 loss: 0.09611463 acc: 0.9580514090401786\n",
      "epoch: 96 validation_loss: 0.078 validation_dice: 0.854244018996434\n",
      "epoch: 96 test_dataset dice: 0.7353608416447193\n",
      "time cost 0.537408177057902 min\n",
      "dice_best: 0.8646678032603314\n",
      "******************************** epoch  96  is finished. *********************************\n",
      "epoch: 97 step: 1 loss: 0.060833693 acc: 0.974273681640625\n",
      "epoch: 97 step: 2 loss: 0.059102144 acc: 0.9695777893066406\n",
      "epoch: 97 step: 3 loss: 0.07991741 acc: 0.9602508544921875\n",
      "epoch: 97 step: 4 loss: 0.0588525 acc: 0.9717445373535156\n",
      "epoch: 97 step: 5 loss: 0.06579098 acc: 0.9679603576660156\n",
      "epoch: 97 step: 6 loss: 0.07161571 acc: 0.962066650390625\n",
      "epoch: 97 step: 7 loss: 0.07189812 acc: 0.9672088623046875\n",
      "epoch: 97 step: 8 loss: 0.07173175 acc: 0.9657363891601562\n",
      "epoch: 97 step: 9 loss: 0.0831537 acc: 0.9587135314941406\n",
      "epoch: 97 step: 10 loss: 0.054977354 acc: 0.9755401611328125\n",
      "epoch: 97 step: 11 loss: 0.0689502 acc: 0.9603652954101562\n",
      "epoch: 97 step: 12 loss: 0.06507454 acc: 0.9680557250976562\n",
      "epoch: 97 step: 13 loss: 0.067358226 acc: 0.9608726501464844\n",
      "epoch: 97 step: 14 loss: 0.07044393 acc: 0.9613380432128906\n",
      "epoch: 97 step: 15 loss: 0.06101122 acc: 0.9688987731933594\n",
      "epoch: 97 step: 16 loss: 0.07096926 acc: 0.9671974182128906\n",
      "epoch: 97 step: 17 loss: 0.06284087 acc: 0.9722785949707031\n",
      "epoch: 97 step: 18 loss: 0.062583886 acc: 0.9696273803710938\n",
      "epoch: 97 step: 19 loss: 0.07574521 acc: 0.9758377075195312\n",
      "epoch: 97 step: 20 loss: 0.07685934 acc: 0.9661788940429688\n",
      "epoch: 97 step: 21 loss: 0.058973014 acc: 0.9713478088378906\n",
      "epoch: 97 step: 22 loss: 0.07302135 acc: 0.965423583984375\n",
      "epoch: 97 step: 23 loss: 0.1019683 acc: 0.9690093994140625\n",
      "epoch: 97 step: 24 loss: 0.06974776 acc: 0.9672508239746094\n",
      "epoch: 97 step: 25 loss: 0.06188568 acc: 0.9649467468261719\n",
      "epoch: 97 step: 26 loss: 0.07185203 acc: 0.9699172973632812\n",
      "epoch: 97 step: 27 loss: 0.07846737 acc: 0.9566650390625\n",
      "epoch: 97 step: 28 loss: 0.062612325 acc: 0.9695014953613281\n",
      "epoch: 97 step: 29 loss: 0.08465167 acc: 0.9586944580078125\n",
      "epoch: 97 step: 30 loss: 0.07868209 acc: 0.9640884399414062\n",
      "epoch: 97 step: 31 loss: 0.06831011 acc: 0.9709930419921875\n",
      "epoch: 97 step: 32 loss: 0.06986602 acc: 0.9688644409179688\n",
      "epoch: 97 step: 33 loss: 0.07317073 acc: 0.9717292785644531\n",
      "epoch: 97 step: 34 loss: 0.07554416 acc: 0.9677543640136719\n",
      "epoch: 97 step: 35 loss: 0.07751459 acc: 0.9646682739257812\n",
      "epoch: 97 step: 36 loss: 0.06714022 acc: 0.9630851745605469\n",
      "epoch: 97 step: 37 loss: 0.08102441 acc: 0.96026611328125\n",
      "epoch: 97 step: 38 loss: 0.07295792 acc: 0.962554931640625\n",
      "epoch: 97 step: 39 loss: 0.076045305 acc: 0.9581146240234375\n",
      "epoch: 97 step: 40 loss: 0.08350001 acc: 0.9577102661132812\n",
      "epoch: 97 step: 41 loss: 0.070383035 acc: 0.9604454040527344\n",
      "epoch: 97 step: 42 loss: 0.06598362 acc: 0.9691276550292969\n",
      "epoch: 97 step: 43 loss: 0.06709597 acc: 0.9674911499023438\n",
      "epoch: 97 step: 44 loss: 0.07150497 acc: 0.9708213806152344\n",
      "epoch: 97 step: 45 loss: 0.06766143 acc: 0.9758148193359375\n",
      "epoch: 97 step: 46 loss: 0.068558276 acc: 0.9710006713867188\n",
      "epoch: 97 step: 47 loss: 0.07555555 acc: 0.97222900390625\n",
      "epoch: 97 step: 48 loss: 0.06710135 acc: 0.9708900451660156\n",
      "epoch: 97 step: 49 loss: 0.07851737 acc: 0.9672126770019531\n",
      "epoch: 97 step: 50 loss: 0.069964156 acc: 0.9676399230957031\n",
      "epoch: 97 step: 51 loss: 0.07326466 acc: 0.964599609375\n",
      "epoch: 97 step: 52 loss: 0.07157314 acc: 0.9608154296875\n",
      "epoch: 97 step: 53 loss: 0.07823506 acc: 0.9608726501464844\n",
      "epoch: 97 step: 54 loss: 0.066149406 acc: 0.962860107421875\n",
      "epoch: 97 step: 55 loss: 0.083774105 acc: 0.957244873046875\n",
      "epoch: 97 step: 56 loss: 0.089362524 acc: 0.95037841796875\n",
      "epoch: 97 step: 57 loss: 0.08352131 acc: 0.9567222595214844\n",
      "epoch: 97 step: 58 loss: 0.0810466 acc: 0.9609489440917969\n",
      "epoch: 97 step: 59 loss: 0.07718478 acc: 0.9577903747558594\n",
      "epoch: 97 step: 60 loss: 0.07802826 acc: 0.9650001525878906\n",
      "epoch: 97 step: 61 loss: 0.06489328 acc: 0.9708290100097656\n",
      "epoch: 97 step: 62 loss: 0.06587116 acc: 0.970916748046875\n",
      "epoch: 97 step: 63 loss: 0.06383871 acc: 0.9763412475585938\n",
      "epoch: 97 step: 64 loss: 0.07679217 acc: 0.9696540832519531\n",
      "epoch: 97 step: 65 loss: 0.07001082 acc: 0.965301513671875\n",
      "epoch: 97 step: 66 loss: 0.07019545 acc: 0.9680061340332031\n",
      "epoch: 97 step: 67 loss: 0.080482066 acc: 0.9669876098632812\n",
      "epoch: 97 step: 68 loss: 0.067343496 acc: 0.9693832397460938\n",
      "epoch: 97 step: 69 loss: 0.05730935 acc: 0.9724769592285156\n",
      "epoch: 97 step: 70 loss: 0.07045851 acc: 0.9620323181152344\n",
      "epoch: 97 step: 71 loss: 0.06253177 acc: 0.9661445617675781\n",
      "epoch: 97 step: 72 loss: 0.06967851 acc: 0.9701042175292969\n",
      "epoch: 97 step: 73 loss: 0.077475734 acc: 0.9619483947753906\n",
      "epoch: 97 step: 74 loss: 0.07217422 acc: 0.9677505493164062\n",
      "epoch: 97 step: 75 loss: 0.071226075 acc: 0.9653854370117188\n",
      "epoch: 97 step: 76 loss: 0.07609049 acc: 0.9610366821289062\n",
      "epoch: 97 step: 77 loss: 0.08011526 acc: 0.9589080810546875\n",
      "epoch: 97 step: 78 loss: 0.061758976 acc: 0.9755821228027344\n",
      "epoch: 97 step: 79 loss: 0.069455326 acc: 0.9700508117675781\n",
      "epoch: 97 step: 80 loss: 0.08068675 acc: 0.9683647155761719\n",
      "epoch: 97 step: 81 loss: 0.08138404 acc: 0.9621162414550781\n",
      "epoch: 97 step: 82 loss: 0.07481453 acc: 0.9655838012695312\n",
      "epoch: 97 step: 83 loss: 0.07011991 acc: 0.9688529968261719\n",
      "epoch: 97 step: 84 loss: 0.06847296 acc: 0.9649276733398438\n",
      "epoch: 97 step: 85 loss: 0.06986777 acc: 0.9702987670898438\n",
      "epoch: 97 step: 86 loss: 0.08135161 acc: 0.9634895324707031\n",
      "epoch: 97 step: 87 loss: 0.07794108 acc: 0.9591598510742188\n",
      "epoch: 97 step: 88 loss: 0.08418603 acc: 0.9581756591796875\n",
      "epoch: 97 step: 89 loss: 0.065642424 acc: 0.9651832580566406\n",
      "epoch: 97 step: 90 loss: 0.072995484 acc: 0.9629707336425781\n",
      "epoch: 97 step: 91 loss: 0.07400068 acc: 0.9638214111328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 97 step: 92 loss: 0.07148887 acc: 0.9594459533691406\n",
      "epoch: 97 step: 93 loss: 0.06593296 acc: 0.9685287475585938\n",
      "epoch: 97 step: 94 loss: 0.07618164 acc: 0.970458984375\n",
      "epoch: 97 step: 95 loss: 0.06256023 acc: 0.9693527221679688\n",
      "epoch: 97 step: 96 loss: 0.06747867 acc: 0.9687728881835938\n",
      "epoch: 97 step: 97 loss: 0.084869914 acc: 0.9674644470214844\n",
      "epoch: 97 step: 98 loss: 0.09152647 acc: 0.960479736328125\n",
      "epoch: 97 step: 99 loss: 0.0683681 acc: 0.9689102172851562\n",
      "epoch: 97 step: 100 loss: 0.0709512 acc: 0.9690513610839844\n",
      "epoch: 97 step: 101 loss: 0.07452843 acc: 0.9622726440429688\n",
      "epoch: 97 step: 102 loss: 0.08260853 acc: 0.9586639404296875\n",
      "epoch: 97 step: 103 loss: 0.07194055 acc: 0.9669685363769531\n",
      "epoch: 97 step: 104 loss: 0.074246764 acc: 0.9625968933105469\n",
      "epoch: 97 step: 105 loss: 0.07827631 acc: 0.962860107421875\n",
      "epoch: 97 step: 106 loss: 0.06346133 acc: 0.9680252075195312\n",
      "epoch: 97 step: 107 loss: 0.0701798 acc: 0.9679794311523438\n",
      "epoch: 97 step: 108 loss: 0.06921919 acc: 0.9637908935546875\n",
      "epoch: 97 step: 109 loss: 0.05853641 acc: 0.9749908447265625\n",
      "epoch: 97 step: 110 loss: 0.06568435 acc: 0.9746208190917969\n",
      "epoch: 97 step: 111 loss: 0.0688786 acc: 0.9693870544433594\n",
      "epoch: 97 step: 112 loss: 0.07747006 acc: 0.9605522155761719\n",
      "epoch: 97 step: 113 loss: 0.05533022 acc: 0.9747810363769531\n",
      "epoch: 97 step: 114 loss: 0.07362301 acc: 0.9625892639160156\n",
      "epoch: 97 step: 115 loss: 0.06684533 acc: 0.9692955017089844\n",
      "epoch: 97 step: 116 loss: 0.060330253 acc: 0.9721832275390625\n",
      "epoch: 97 step: 117 loss: 0.075178504 acc: 0.9623680114746094\n",
      "epoch: 97 step: 118 loss: 0.07177629 acc: 0.9645156860351562\n",
      "epoch: 97 step: 119 loss: 0.07963917 acc: 0.9624252319335938\n",
      "epoch: 97 step: 120 loss: 0.061420083 acc: 0.9718170166015625\n",
      "epoch: 97 step: 121 loss: 0.0764181 acc: 0.9620895385742188\n",
      "epoch: 97 step: 122 loss: 0.07785681 acc: 0.9602165222167969\n",
      "epoch: 97 step: 123 loss: 0.098099254 acc: 0.9576454162597656\n",
      "epoch: 97 step: 124 loss: 0.078683764 acc: 0.9677559988839286\n",
      "epoch: 97 validation_loss: 0.082 validation_dice: 0.8502190395832361\n",
      "epoch: 97 test_dataset dice: 0.7298936381345301\n",
      "time cost 0.536280898253123 min\n",
      "dice_best: 0.8646678032603314\n",
      "******************************** epoch  97  is finished. *********************************\n",
      "epoch: 98 step: 1 loss: 0.05355976 acc: 0.9748458862304688\n",
      "epoch: 98 step: 2 loss: 0.0623244 acc: 0.9744720458984375\n",
      "epoch: 98 step: 3 loss: 0.07130788 acc: 0.9704399108886719\n",
      "epoch: 98 step: 4 loss: 0.058305997 acc: 0.9732589721679688\n",
      "epoch: 98 step: 5 loss: 0.08301833 acc: 0.9629745483398438\n",
      "epoch: 98 step: 6 loss: 0.08116181 acc: 0.9683189392089844\n",
      "epoch: 98 step: 7 loss: 0.060486846 acc: 0.9672317504882812\n",
      "epoch: 98 step: 8 loss: 0.07328336 acc: 0.9696617126464844\n",
      "epoch: 98 step: 9 loss: 0.0615736 acc: 0.9744377136230469\n",
      "epoch: 98 step: 10 loss: 0.06961453 acc: 0.967742919921875\n",
      "epoch: 98 step: 11 loss: 0.07233311 acc: 0.959320068359375\n",
      "epoch: 98 step: 12 loss: 0.07731884 acc: 0.9545211791992188\n",
      "epoch: 98 step: 13 loss: 0.0654763 acc: 0.9627342224121094\n",
      "epoch: 98 step: 14 loss: 0.07964821 acc: 0.9612045288085938\n",
      "epoch: 98 step: 15 loss: 0.07278811 acc: 0.9587059020996094\n",
      "epoch: 98 step: 16 loss: 0.08456346 acc: 0.9538345336914062\n",
      "epoch: 98 step: 17 loss: 0.06387709 acc: 0.9678153991699219\n",
      "epoch: 98 step: 18 loss: 0.076335125 acc: 0.9648399353027344\n",
      "epoch: 98 step: 19 loss: 0.06134171 acc: 0.9659461975097656\n",
      "epoch: 98 step: 20 loss: 0.059088565 acc: 0.9726524353027344\n",
      "epoch: 98 step: 21 loss: 0.062968835 acc: 0.9758262634277344\n",
      "epoch: 98 step: 22 loss: 0.070163704 acc: 0.9738616943359375\n",
      "epoch: 98 step: 23 loss: 0.071429595 acc: 0.9703941345214844\n",
      "epoch: 98 step: 24 loss: 0.061431296 acc: 0.9703140258789062\n",
      "epoch: 98 step: 25 loss: 0.07091271 acc: 0.9673843383789062\n",
      "epoch: 98 step: 26 loss: 0.0863451 acc: 0.963836669921875\n",
      "epoch: 98 step: 27 loss: 0.06865513 acc: 0.9662513732910156\n",
      "epoch: 98 step: 28 loss: 0.065868214 acc: 0.9637184143066406\n",
      "epoch: 98 step: 29 loss: 0.06305434 acc: 0.9649238586425781\n",
      "epoch: 98 step: 30 loss: 0.07712333 acc: 0.9566192626953125\n",
      "epoch: 98 step: 31 loss: 0.07592883 acc: 0.9578857421875\n",
      "epoch: 98 step: 32 loss: 0.052625023 acc: 0.9762420654296875\n",
      "epoch: 98 step: 33 loss: 0.094803885 acc: 0.9695472717285156\n",
      "epoch: 98 step: 34 loss: 0.07041609 acc: 0.9684066772460938\n",
      "epoch: 98 step: 35 loss: 0.06044863 acc: 0.9708900451660156\n",
      "epoch: 98 step: 36 loss: 0.060244694 acc: 0.9694442749023438\n",
      "epoch: 98 step: 37 loss: 0.07339485 acc: 0.9689826965332031\n",
      "epoch: 98 step: 38 loss: 0.07113128 acc: 0.9698829650878906\n",
      "epoch: 98 step: 39 loss: 0.08080235 acc: 0.9617729187011719\n",
      "epoch: 98 step: 40 loss: 0.0860944 acc: 0.9726028442382812\n",
      "epoch: 98 step: 41 loss: 0.07526431 acc: 0.9653244018554688\n",
      "epoch: 98 step: 42 loss: 0.07541352 acc: 0.9609832763671875\n",
      "epoch: 98 step: 43 loss: 0.07506059 acc: 0.9628028869628906\n",
      "epoch: 98 step: 44 loss: 0.073904835 acc: 0.9595909118652344\n",
      "epoch: 98 step: 45 loss: 0.07284105 acc: 0.9611930847167969\n",
      "epoch: 98 step: 46 loss: 0.066916056 acc: 0.9656486511230469\n",
      "epoch: 98 step: 47 loss: 0.0650158 acc: 0.9682273864746094\n",
      "epoch: 98 step: 48 loss: 0.061389584 acc: 0.9706153869628906\n",
      "epoch: 98 step: 49 loss: 0.08424259 acc: 0.9637794494628906\n",
      "epoch: 98 step: 50 loss: 0.06291568 acc: 0.9700088500976562\n",
      "epoch: 98 step: 51 loss: 0.060773283 acc: 0.9718475341796875\n",
      "epoch: 98 step: 52 loss: 0.091972 acc: 0.9635162353515625\n",
      "epoch: 98 step: 53 loss: 0.082568124 acc: 0.963623046875\n",
      "epoch: 98 step: 54 loss: 0.06294016 acc: 0.9688758850097656\n",
      "epoch: 98 step: 55 loss: 0.07451403 acc: 0.9630393981933594\n",
      "epoch: 98 step: 56 loss: 0.068328 acc: 0.9704055786132812\n",
      "epoch: 98 step: 57 loss: 0.07413103 acc: 0.9641532897949219\n",
      "epoch: 98 step: 58 loss: 0.0591641 acc: 0.9714126586914062\n",
      "epoch: 98 step: 59 loss: 0.074569926 acc: 0.959686279296875\n",
      "epoch: 98 step: 60 loss: 0.07056421 acc: 0.9648780822753906\n",
      "epoch: 98 step: 61 loss: 0.09616083 acc: 0.9525527954101562\n",
      "epoch: 98 step: 62 loss: 0.066959396 acc: 0.9666213989257812\n",
      "epoch: 98 step: 63 loss: 0.07308249 acc: 0.9671363830566406\n",
      "epoch: 98 step: 64 loss: 0.08639903 acc: 0.9586029052734375\n",
      "epoch: 98 step: 65 loss: 0.07241634 acc: 0.9570198059082031\n",
      "epoch: 98 step: 66 loss: 0.058529496 acc: 0.9717559814453125\n",
      "epoch: 98 step: 67 loss: 0.06654577 acc: 0.9689064025878906\n",
      "epoch: 98 step: 68 loss: 0.07681004 acc: 0.9575462341308594\n",
      "epoch: 98 step: 69 loss: 0.07313966 acc: 0.9674568176269531\n",
      "epoch: 98 step: 70 loss: 0.071400814 acc: 0.9649276733398438\n",
      "epoch: 98 step: 71 loss: 0.06555429 acc: 0.9699745178222656\n",
      "epoch: 98 step: 72 loss: 0.075264975 acc: 0.962371826171875\n",
      "epoch: 98 step: 73 loss: 0.06577595 acc: 0.9662895202636719\n",
      "epoch: 98 step: 74 loss: 0.06474816 acc: 0.9640617370605469\n",
      "epoch: 98 step: 75 loss: 0.1054179 acc: 0.9526519775390625\n",
      "epoch: 98 step: 76 loss: 0.07013922 acc: 0.9596595764160156\n",
      "epoch: 98 step: 77 loss: 0.07099827 acc: 0.9671859741210938\n",
      "epoch: 98 step: 78 loss: 0.07630834 acc: 0.9597206115722656\n",
      "epoch: 98 step: 79 loss: 0.11746899 acc: 0.9576301574707031\n",
      "epoch: 98 step: 80 loss: 0.073561005 acc: 0.9734992980957031\n",
      "epoch: 98 step: 81 loss: 0.066613026 acc: 0.9619522094726562\n",
      "epoch: 98 step: 82 loss: 0.08504704 acc: 0.9654502868652344\n",
      "epoch: 98 step: 83 loss: 0.07878703 acc: 0.9621429443359375\n",
      "epoch: 98 step: 84 loss: 0.083228804 acc: 0.9593582153320312\n",
      "epoch: 98 step: 85 loss: 0.0821522 acc: 0.9656906127929688\n",
      "epoch: 98 step: 86 loss: 0.103184655 acc: 0.9632377624511719\n",
      "epoch: 98 step: 87 loss: 0.08139397 acc: 0.9647750854492188\n",
      "epoch: 98 step: 88 loss: 0.07785532 acc: 0.9645957946777344\n",
      "epoch: 98 step: 89 loss: 0.08242071 acc: 0.9573173522949219\n",
      "epoch: 98 step: 90 loss: 0.096497595 acc: 0.9556846618652344\n",
      "epoch: 98 step: 91 loss: 0.07532969 acc: 0.9719009399414062\n",
      "epoch: 98 step: 92 loss: 0.07122671 acc: 0.9615974426269531\n",
      "epoch: 98 step: 93 loss: 0.06675298 acc: 0.9705238342285156\n",
      "epoch: 98 step: 94 loss: 0.08866475 acc: 0.9634742736816406\n",
      "epoch: 98 step: 95 loss: 0.06640775 acc: 0.9664573669433594\n",
      "epoch: 98 step: 96 loss: 0.06428193 acc: 0.9714431762695312\n",
      "epoch: 98 step: 97 loss: 0.08534191 acc: 0.96783447265625\n",
      "epoch: 98 step: 98 loss: 0.09958947 acc: 0.9633560180664062\n",
      "epoch: 98 step: 99 loss: 0.08821559 acc: 0.9635543823242188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 98 step: 100 loss: 0.08764763 acc: 0.9638519287109375\n",
      "epoch: 98 step: 101 loss: 0.067745864 acc: 0.9682807922363281\n",
      "epoch: 98 step: 102 loss: 0.07510952 acc: 0.9603614807128906\n",
      "epoch: 98 step: 103 loss: 0.078085445 acc: 0.9599571228027344\n",
      "epoch: 98 step: 104 loss: 0.083805695 acc: 0.9561233520507812\n",
      "epoch: 98 step: 105 loss: 0.0710439 acc: 0.9601249694824219\n",
      "epoch: 98 step: 106 loss: 0.06782811 acc: 0.9737586975097656\n",
      "epoch: 98 step: 107 loss: 0.073067814 acc: 0.964935302734375\n",
      "epoch: 98 step: 108 loss: 0.09076106 acc: 0.9608535766601562\n",
      "epoch: 98 step: 109 loss: 0.07227662 acc: 0.9686851501464844\n",
      "epoch: 98 step: 110 loss: 0.07294082 acc: 0.9700698852539062\n",
      "epoch: 98 step: 111 loss: 0.07908264 acc: 0.968231201171875\n",
      "epoch: 98 step: 112 loss: 0.07572747 acc: 0.9644355773925781\n",
      "epoch: 98 step: 113 loss: 0.07439227 acc: 0.9597091674804688\n",
      "epoch: 98 step: 114 loss: 0.09189427 acc: 0.9521598815917969\n",
      "epoch: 98 step: 115 loss: 0.07067914 acc: 0.9569511413574219\n",
      "epoch: 98 step: 116 loss: 0.06952894 acc: 0.9630851745605469\n",
      "epoch: 98 step: 117 loss: 0.07821046 acc: 0.9565811157226562\n",
      "epoch: 98 step: 118 loss: 0.085382536 acc: 0.9675407409667969\n",
      "epoch: 98 step: 119 loss: 0.0644745 acc: 0.9684181213378906\n",
      "epoch: 98 step: 120 loss: 0.06982802 acc: 0.9683799743652344\n",
      "epoch: 98 step: 121 loss: 0.0704233 acc: 0.97210693359375\n",
      "epoch: 98 step: 122 loss: 0.061451823 acc: 0.9680938720703125\n",
      "epoch: 98 step: 123 loss: 0.0698913 acc: 0.9685325622558594\n",
      "epoch: 98 step: 124 loss: 0.07431144 acc: 0.9637451171875\n",
      "epoch: 98 validation_loss: 0.083 validation_dice: 0.8501571531627252\n",
      "epoch: 98 test_dataset dice: 0.7325912181440764\n",
      "time cost 0.5349230249722798 min\n",
      "dice_best: 0.8646678032603314\n",
      "******************************** epoch  98  is finished. *********************************\n",
      "epoch: 99 step: 1 loss: 0.060410526 acc: 0.9706306457519531\n",
      "epoch: 99 step: 2 loss: 0.06505455 acc: 0.9705238342285156\n",
      "epoch: 99 step: 3 loss: 0.06769542 acc: 0.9707794189453125\n",
      "epoch: 99 step: 4 loss: 0.06721081 acc: 0.9651603698730469\n",
      "epoch: 99 step: 5 loss: 0.074935175 acc: 0.9629478454589844\n",
      "epoch: 99 step: 6 loss: 0.059806652 acc: 0.9709510803222656\n",
      "epoch: 99 step: 7 loss: 0.07245299 acc: 0.9644317626953125\n",
      "epoch: 99 step: 8 loss: 0.07214662 acc: 0.9616279602050781\n",
      "epoch: 99 step: 9 loss: 0.075377576 acc: 0.9586982727050781\n",
      "epoch: 99 step: 10 loss: 0.073447324 acc: 0.9665794372558594\n",
      "epoch: 99 step: 11 loss: 0.072016545 acc: 0.96929931640625\n",
      "epoch: 99 step: 12 loss: 0.0804327 acc: 0.9684181213378906\n",
      "epoch: 99 step: 13 loss: 0.07751228 acc: 0.9646415710449219\n",
      "epoch: 99 step: 14 loss: 0.06695674 acc: 0.9662971496582031\n",
      "epoch: 99 step: 15 loss: 0.0680668 acc: 0.9682426452636719\n",
      "epoch: 99 step: 16 loss: 0.07402626 acc: 0.96484375\n",
      "epoch: 99 step: 17 loss: 0.06830061 acc: 0.9678382873535156\n",
      "epoch: 99 step: 18 loss: 0.06639655 acc: 0.9682083129882812\n",
      "epoch: 99 step: 19 loss: 0.08199439 acc: 0.9622955322265625\n",
      "epoch: 99 step: 20 loss: 0.06621624 acc: 0.9720077514648438\n",
      "epoch: 99 step: 21 loss: 0.07244568 acc: 0.9682388305664062\n",
      "epoch: 99 step: 22 loss: 0.07815222 acc: 0.9770774841308594\n",
      "epoch: 99 step: 23 loss: 0.08317698 acc: 0.9607429504394531\n",
      "epoch: 99 step: 24 loss: 0.07913139 acc: 0.9617843627929688\n",
      "epoch: 99 step: 25 loss: 0.07056428 acc: 0.9641189575195312\n",
      "epoch: 99 step: 26 loss: 0.06327043 acc: 0.9715194702148438\n",
      "epoch: 99 step: 27 loss: 0.063931696 acc: 0.9693679809570312\n",
      "epoch: 99 step: 28 loss: 0.06328602 acc: 0.9671707153320312\n",
      "epoch: 99 step: 29 loss: 0.069513276 acc: 0.9654617309570312\n",
      "epoch: 99 step: 30 loss: 0.072267175 acc: 0.9650497436523438\n",
      "epoch: 99 step: 31 loss: 0.072133176 acc: 0.9661712646484375\n",
      "epoch: 99 step: 32 loss: 0.06712374 acc: 0.9658088684082031\n",
      "epoch: 99 step: 33 loss: 0.066884965 acc: 0.9661941528320312\n",
      "epoch: 99 step: 34 loss: 0.06784631 acc: 0.9666671752929688\n",
      "epoch: 99 step: 35 loss: 0.07950834 acc: 0.9663887023925781\n",
      "epoch: 99 step: 36 loss: 0.07126881 acc: 0.9655914306640625\n",
      "epoch: 99 step: 37 loss: 0.075067684 acc: 0.966278076171875\n",
      "epoch: 99 step: 38 loss: 0.087682955 acc: 0.9649085998535156\n",
      "epoch: 99 step: 39 loss: 0.071868494 acc: 0.9663658142089844\n",
      "epoch: 99 step: 40 loss: 0.07147687 acc: 0.9646148681640625\n",
      "epoch: 99 step: 41 loss: 0.060908433 acc: 0.9703903198242188\n",
      "epoch: 99 step: 42 loss: 0.0701877 acc: 0.966796875\n",
      "epoch: 99 step: 43 loss: 0.079685465 acc: 0.9588279724121094\n",
      "epoch: 99 step: 44 loss: 0.078147165 acc: 0.9658279418945312\n",
      "epoch: 99 step: 45 loss: 0.06598673 acc: 0.9641685485839844\n",
      "epoch: 99 step: 46 loss: 0.07263887 acc: 0.9653892517089844\n",
      "epoch: 99 step: 47 loss: 0.0765163 acc: 0.9579238891601562\n",
      "epoch: 99 step: 48 loss: 0.07149668 acc: 0.9623565673828125\n",
      "epoch: 99 step: 49 loss: 0.061443795 acc: 0.9721832275390625\n",
      "epoch: 99 step: 50 loss: 0.07942254 acc: 0.9630203247070312\n",
      "epoch: 99 step: 51 loss: 0.06425401 acc: 0.9718589782714844\n",
      "epoch: 99 step: 52 loss: 0.06661896 acc: 0.9664039611816406\n",
      "epoch: 99 step: 53 loss: 0.063450314 acc: 0.9634437561035156\n",
      "epoch: 99 step: 54 loss: 0.07175934 acc: 0.9695549011230469\n",
      "epoch: 99 step: 55 loss: 0.06820077 acc: 0.9652366638183594\n",
      "epoch: 99 step: 56 loss: 0.06972639 acc: 0.9656753540039062\n",
      "epoch: 99 step: 57 loss: 0.06260362 acc: 0.9707832336425781\n",
      "epoch: 99 step: 58 loss: 0.07165256 acc: 0.9643707275390625\n",
      "epoch: 99 step: 59 loss: 0.0785207 acc: 0.9605636596679688\n",
      "epoch: 99 step: 60 loss: 0.06331063 acc: 0.9679298400878906\n",
      "epoch: 99 step: 61 loss: 0.067605495 acc: 0.9668807983398438\n",
      "epoch: 99 step: 62 loss: 0.066471286 acc: 0.9638252258300781\n",
      "epoch: 99 step: 63 loss: 0.068940096 acc: 0.9702873229980469\n",
      "epoch: 99 step: 64 loss: 0.05914998 acc: 0.9775428771972656\n",
      "epoch: 99 step: 65 loss: 0.065661326 acc: 0.9672317504882812\n",
      "epoch: 99 step: 66 loss: 0.06917138 acc: 0.9627151489257812\n",
      "epoch: 99 step: 67 loss: 0.07432161 acc: 0.9623451232910156\n",
      "epoch: 99 step: 68 loss: 0.067095004 acc: 0.969757080078125\n",
      "epoch: 99 step: 69 loss: 0.07348488 acc: 0.9670867919921875\n",
      "epoch: 99 step: 70 loss: 0.0708028 acc: 0.9610366821289062\n",
      "epoch: 99 step: 71 loss: 0.07214302 acc: 0.9674644470214844\n",
      "epoch: 99 step: 72 loss: 0.08170572 acc: 0.9611320495605469\n",
      "epoch: 99 step: 73 loss: 0.060506802 acc: 0.9767341613769531\n",
      "epoch: 99 step: 74 loss: 0.07458265 acc: 0.9629287719726562\n",
      "epoch: 99 step: 75 loss: 0.07078042 acc: 0.9708633422851562\n",
      "epoch: 99 step: 76 loss: 0.06271988 acc: 0.9675407409667969\n",
      "epoch: 99 step: 77 loss: 0.06465332 acc: 0.9726371765136719\n",
      "epoch: 99 step: 78 loss: 0.070465975 acc: 0.9654541015625\n",
      "epoch: 99 step: 79 loss: 0.065847576 acc: 0.9680328369140625\n",
      "epoch: 99 step: 80 loss: 0.061185535 acc: 0.971893310546875\n",
      "epoch: 99 step: 81 loss: 0.088133246 acc: 0.9556045532226562\n",
      "epoch: 99 step: 82 loss: 0.07672856 acc: 0.9686393737792969\n",
      "epoch: 99 step: 83 loss: 0.08366519 acc: 0.9607162475585938\n",
      "epoch: 99 step: 84 loss: 0.0754848 acc: 0.9733772277832031\n",
      "epoch: 99 step: 85 loss: 0.07738573 acc: 0.9651908874511719\n",
      "epoch: 99 step: 86 loss: 0.05995327 acc: 0.9743499755859375\n",
      "epoch: 99 step: 87 loss: 0.06981946 acc: 0.9659576416015625\n",
      "epoch: 99 step: 88 loss: 0.066752784 acc: 0.9684295654296875\n",
      "epoch: 99 step: 89 loss: 0.07603063 acc: 0.9601936340332031\n",
      "epoch: 99 step: 90 loss: 0.081717946 acc: 0.9613456726074219\n",
      "epoch: 99 step: 91 loss: 0.06421826 acc: 0.9643821716308594\n",
      "epoch: 99 step: 92 loss: 0.08450337 acc: 0.9541664123535156\n",
      "epoch: 99 step: 93 loss: 0.07124469 acc: 0.9620361328125\n",
      "epoch: 99 step: 94 loss: 0.056365184 acc: 0.9722671508789062\n",
      "epoch: 99 step: 95 loss: 0.07125637 acc: 0.9676246643066406\n",
      "epoch: 99 step: 96 loss: 0.081317306 acc: 0.970428466796875\n",
      "epoch: 99 step: 97 loss: 0.088345714 acc: 0.955108642578125\n",
      "epoch: 99 step: 98 loss: 0.07209984 acc: 0.9627914428710938\n",
      "epoch: 99 step: 99 loss: 0.07319875 acc: 0.964385986328125\n",
      "epoch: 99 step: 100 loss: 0.072838046 acc: 0.9656333923339844\n",
      "epoch: 99 step: 101 loss: 0.06980502 acc: 0.9640235900878906\n",
      "epoch: 99 step: 102 loss: 0.07492738 acc: 0.9584426879882812\n",
      "epoch: 99 step: 103 loss: 0.06814317 acc: 0.9663734436035156\n",
      "epoch: 99 step: 104 loss: 0.08179242 acc: 0.9682502746582031\n",
      "epoch: 99 step: 105 loss: 0.06422868 acc: 0.9742393493652344\n",
      "epoch: 99 step: 106 loss: 0.075269766 acc: 0.9640274047851562\n",
      "epoch: 99 step: 107 loss: 0.05231595 acc: 0.974456787109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 99 step: 108 loss: 0.072683156 acc: 0.9759178161621094\n",
      "epoch: 99 step: 109 loss: 0.07052439 acc: 0.9727821350097656\n",
      "epoch: 99 step: 110 loss: 0.05764927 acc: 0.9731559753417969\n",
      "epoch: 99 step: 111 loss: 0.07063282 acc: 0.9638710021972656\n",
      "epoch: 99 step: 112 loss: 0.06280866 acc: 0.9696311950683594\n",
      "epoch: 99 step: 113 loss: 0.076230004 acc: 0.9714164733886719\n",
      "epoch: 99 step: 114 loss: 0.0696047 acc: 0.9697151184082031\n",
      "epoch: 99 step: 115 loss: 0.059344538 acc: 0.9700431823730469\n",
      "epoch: 99 step: 116 loss: 0.06376087 acc: 0.9698600769042969\n",
      "epoch: 99 step: 117 loss: 0.0844073 acc: 0.9606971740722656\n",
      "epoch: 99 step: 118 loss: 0.061190948 acc: 0.9712448120117188\n",
      "epoch: 99 step: 119 loss: 0.07439959 acc: 0.9707374572753906\n",
      "epoch: 99 step: 120 loss: 0.06381552 acc: 0.9721260070800781\n",
      "epoch: 99 step: 121 loss: 0.07580103 acc: 0.9686050415039062\n",
      "epoch: 99 step: 122 loss: 0.06852572 acc: 0.9738845825195312\n",
      "epoch: 99 step: 123 loss: 0.09642185 acc: 0.9601364135742188\n",
      "epoch: 99 step: 124 loss: 0.06635347 acc: 0.9719935825892857\n",
      "epoch: 99 validation_loss: 0.084 validation_dice: 0.8566483719430162\n",
      "epoch: 99 test_dataset dice: 0.744021259240691\n",
      "time cost 0.5350666681925456 min\n",
      "dice_best: 0.8646678032603314\n",
      "******************************** epoch  99  is finished. *********************************\n",
      "epoch: 100 step: 1 loss: 0.07124109 acc: 0.9760475158691406\n",
      "epoch: 100 step: 2 loss: 0.07014565 acc: 0.9708175659179688\n",
      "epoch: 100 step: 3 loss: 0.06693333 acc: 0.9671592712402344\n",
      "epoch: 100 step: 4 loss: 0.06768403 acc: 0.9642181396484375\n",
      "epoch: 100 step: 5 loss: 0.07399588 acc: 0.9629974365234375\n",
      "epoch: 100 step: 6 loss: 0.082883716 acc: 0.9547080993652344\n",
      "epoch: 100 step: 7 loss: 0.06832614 acc: 0.9698562622070312\n",
      "epoch: 100 step: 8 loss: 0.07713407 acc: 0.9692802429199219\n",
      "epoch: 100 step: 9 loss: 0.06065831 acc: 0.9693412780761719\n",
      "epoch: 100 step: 10 loss: 0.076031804 acc: 0.9672126770019531\n",
      "epoch: 100 step: 11 loss: 0.05777964 acc: 0.9730949401855469\n",
      "epoch: 100 step: 12 loss: 0.076439515 acc: 0.9721565246582031\n",
      "epoch: 100 step: 13 loss: 0.067693286 acc: 0.9728431701660156\n",
      "epoch: 100 step: 14 loss: 0.07825973 acc: 0.9626502990722656\n",
      "epoch: 100 step: 15 loss: 0.071588956 acc: 0.9669380187988281\n",
      "epoch: 100 step: 16 loss: 0.050585747 acc: 0.9748306274414062\n",
      "epoch: 100 step: 17 loss: 0.07849827 acc: 0.9663238525390625\n",
      "epoch: 100 step: 18 loss: 0.061714277 acc: 0.9707183837890625\n",
      "epoch: 100 step: 19 loss: 0.085678905 acc: 0.9581260681152344\n",
      "epoch: 100 step: 20 loss: 0.078325614 acc: 0.9553565979003906\n",
      "epoch: 100 step: 21 loss: 0.07731624 acc: 0.9593696594238281\n",
      "epoch: 100 step: 22 loss: 0.07113955 acc: 0.9646644592285156\n",
      "epoch: 100 step: 23 loss: 0.06592857 acc: 0.9721527099609375\n",
      "epoch: 100 step: 24 loss: 0.07030355 acc: 0.976043701171875\n",
      "epoch: 100 step: 25 loss: 0.07123748 acc: 0.9639511108398438\n",
      "epoch: 100 step: 26 loss: 0.085104905 acc: 0.9611854553222656\n",
      "epoch: 100 step: 27 loss: 0.0630212 acc: 0.9747390747070312\n",
      "epoch: 100 step: 28 loss: 0.08479389 acc: 0.9593963623046875\n",
      "epoch: 100 step: 29 loss: 0.09022445 acc: 0.9626083374023438\n",
      "epoch: 100 step: 30 loss: 0.060670424 acc: 0.9732322692871094\n",
      "epoch: 100 step: 31 loss: 0.06737112 acc: 0.9703559875488281\n",
      "epoch: 100 step: 32 loss: 0.06540322 acc: 0.96978759765625\n",
      "epoch: 100 step: 33 loss: 0.0969312 acc: 0.9638710021972656\n",
      "epoch: 100 step: 34 loss: 0.0669848 acc: 0.9702911376953125\n",
      "epoch: 100 step: 35 loss: 0.07348288 acc: 0.96881103515625\n",
      "epoch: 100 step: 36 loss: 0.06766941 acc: 0.96685791015625\n",
      "epoch: 100 step: 37 loss: 0.0783193 acc: 0.9660415649414062\n",
      "epoch: 100 step: 38 loss: 0.076264374 acc: 0.9626312255859375\n",
      "epoch: 100 step: 39 loss: 0.06687531 acc: 0.963897705078125\n",
      "epoch: 100 step: 40 loss: 0.071001925 acc: 0.9673423767089844\n",
      "epoch: 100 step: 41 loss: 0.09526785 acc: 0.9578895568847656\n",
      "epoch: 100 step: 42 loss: 0.078980185 acc: 0.9635162353515625\n",
      "epoch: 100 step: 43 loss: 0.07851227 acc: 0.9611129760742188\n",
      "epoch: 100 step: 44 loss: 0.07090966 acc: 0.9612388610839844\n",
      "epoch: 100 step: 45 loss: 0.06662374 acc: 0.9703941345214844\n",
      "epoch: 100 step: 46 loss: 0.065839544 acc: 0.97418212890625\n",
      "epoch: 100 step: 47 loss: 0.06827669 acc: 0.9676437377929688\n",
      "epoch: 100 step: 48 loss: 0.057568733 acc: 0.9717979431152344\n",
      "epoch: 100 step: 49 loss: 0.066538975 acc: 0.9671745300292969\n",
      "epoch: 100 step: 50 loss: 0.07577642 acc: 0.9641609191894531\n",
      "epoch: 100 step: 51 loss: 0.06219321 acc: 0.9716262817382812\n",
      "epoch: 100 step: 52 loss: 0.06636022 acc: 0.9678802490234375\n",
      "epoch: 100 step: 53 loss: 0.07318804 acc: 0.9676055908203125\n",
      "epoch: 100 step: 54 loss: 0.06533175 acc: 0.9660301208496094\n",
      "epoch: 100 step: 55 loss: 0.06980941 acc: 0.9677696228027344\n",
      "epoch: 100 step: 56 loss: 0.06552957 acc: 0.9719772338867188\n",
      "epoch: 100 step: 57 loss: 0.07302396 acc: 0.9643402099609375\n",
      "epoch: 100 step: 58 loss: 0.08168829 acc: 0.969268798828125\n",
      "epoch: 100 step: 59 loss: 0.06840325 acc: 0.9709434509277344\n",
      "epoch: 100 step: 60 loss: 0.076484464 acc: 0.9613533020019531\n",
      "epoch: 100 step: 61 loss: 0.056082334 acc: 0.9700355529785156\n",
      "epoch: 100 step: 62 loss: 0.06772217 acc: 0.9696464538574219\n",
      "epoch: 100 step: 63 loss: 0.06588446 acc: 0.9676170349121094\n",
      "epoch: 100 step: 64 loss: 0.064112276 acc: 0.9661598205566406\n",
      "epoch: 100 step: 65 loss: 0.074313164 acc: 0.9627494812011719\n",
      "epoch: 100 step: 66 loss: 0.07659801 acc: 0.966278076171875\n",
      "epoch: 100 step: 67 loss: 0.073803805 acc: 0.9620132446289062\n",
      "epoch: 100 step: 68 loss: 0.05655574 acc: 0.9732322692871094\n",
      "epoch: 100 step: 69 loss: 0.062702864 acc: 0.9720687866210938\n",
      "epoch: 100 step: 70 loss: 0.06717172 acc: 0.9716987609863281\n",
      "epoch: 100 step: 71 loss: 0.06956271 acc: 0.9650535583496094\n",
      "epoch: 100 step: 72 loss: 0.064388104 acc: 0.9656639099121094\n",
      "epoch: 100 step: 73 loss: 0.066047154 acc: 0.9727058410644531\n",
      "epoch: 100 step: 74 loss: 0.06804983 acc: 0.9627189636230469\n",
      "epoch: 100 step: 75 loss: 0.071643695 acc: 0.9644050598144531\n",
      "epoch: 100 step: 76 loss: 0.07941749 acc: 0.9600677490234375\n",
      "epoch: 100 step: 77 loss: 0.06787756 acc: 0.9665870666503906\n",
      "epoch: 100 step: 78 loss: 0.06187593 acc: 0.9657402038574219\n",
      "epoch: 100 step: 79 loss: 0.07708427 acc: 0.9608955383300781\n",
      "epoch: 100 step: 80 loss: 0.07629428 acc: 0.9628639221191406\n",
      "epoch: 100 step: 81 loss: 0.07008638 acc: 0.9651031494140625\n",
      "epoch: 100 step: 82 loss: 0.06800167 acc: 0.9649009704589844\n",
      "epoch: 100 step: 83 loss: 0.057425328 acc: 0.9710273742675781\n",
      "epoch: 100 step: 84 loss: 0.07287385 acc: 0.9698905944824219\n",
      "epoch: 100 step: 85 loss: 0.07064808 acc: 0.9680557250976562\n",
      "epoch: 100 step: 86 loss: 0.12278381 acc: 0.9544219970703125\n",
      "epoch: 100 step: 87 loss: 0.06976474 acc: 0.9638328552246094\n",
      "epoch: 100 step: 88 loss: 0.0650514 acc: 0.9643402099609375\n",
      "epoch: 100 step: 89 loss: 0.060365178 acc: 0.9705772399902344\n",
      "epoch: 100 step: 90 loss: 0.07461524 acc: 0.9652481079101562\n",
      "epoch: 100 step: 91 loss: 0.08643827 acc: 0.96270751953125\n",
      "epoch: 100 step: 92 loss: 0.090503484 acc: 0.9674873352050781\n",
      "epoch: 100 step: 93 loss: 0.06522022 acc: 0.9701652526855469\n",
      "epoch: 100 step: 94 loss: 0.090854675 acc: 0.9692649841308594\n",
      "epoch: 100 step: 95 loss: 0.080344625 acc: 0.9666938781738281\n",
      "epoch: 100 step: 96 loss: 0.09670838 acc: 0.9640426635742188\n",
      "epoch: 100 step: 97 loss: 0.08650391 acc: 0.9585990905761719\n",
      "epoch: 100 step: 98 loss: 0.08018434 acc: 0.9603195190429688\n",
      "epoch: 100 step: 99 loss: 0.084452435 acc: 0.9628486633300781\n",
      "epoch: 100 step: 100 loss: 0.09468186 acc: 0.9591903686523438\n",
      "epoch: 100 step: 101 loss: 0.120640844 acc: 0.9573898315429688\n",
      "epoch: 100 step: 102 loss: 0.073804416 acc: 0.9681053161621094\n",
      "epoch: 100 step: 103 loss: 0.101772465 acc: 0.9619789123535156\n",
      "epoch: 100 step: 104 loss: 0.095281966 acc: 0.9635734558105469\n",
      "epoch: 100 step: 105 loss: 0.0993406 acc: 0.9584541320800781\n",
      "epoch: 100 step: 106 loss: 0.08653505 acc: 0.9587173461914062\n",
      "epoch: 100 step: 107 loss: 0.12868483 acc: 0.9397811889648438\n",
      "epoch: 100 step: 108 loss: 0.08352187 acc: 0.9638290405273438\n",
      "epoch: 100 step: 109 loss: 0.076989196 acc: 0.9634933471679688\n",
      "epoch: 100 step: 110 loss: 0.098277576 acc: 0.9570884704589844\n",
      "epoch: 100 step: 111 loss: 0.09136349 acc: 0.964996337890625\n",
      "epoch: 100 step: 112 loss: 0.07041153 acc: 0.9672737121582031\n",
      "epoch: 100 step: 113 loss: 0.07895494 acc: 0.9665794372558594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 step: 114 loss: 0.111115165 acc: 0.9566535949707031\n",
      "epoch: 100 step: 115 loss: 0.08239883 acc: 0.9617195129394531\n",
      "epoch: 100 step: 116 loss: 0.0698018 acc: 0.9669342041015625\n",
      "epoch: 100 step: 117 loss: 0.08818233 acc: 0.9601669311523438\n",
      "epoch: 100 step: 118 loss: 0.0825345 acc: 0.9613380432128906\n",
      "epoch: 100 step: 119 loss: 0.09602694 acc: 0.9562873840332031\n",
      "epoch: 100 step: 120 loss: 0.113785736 acc: 0.9626579284667969\n",
      "epoch: 100 step: 121 loss: 0.08640171 acc: 0.95855712890625\n",
      "epoch: 100 step: 122 loss: 0.06911305 acc: 0.9607009887695312\n",
      "epoch: 100 step: 123 loss: 0.08764902 acc: 0.9606437683105469\n",
      "epoch: 100 step: 124 loss: 0.08615677 acc: 0.9648699079241071\n",
      "epoch: 100 validation_loss: 0.091 validation_dice: 0.8379365181058289\n",
      "epoch: 100 test_dataset dice: 0.7486222687079288\n",
      "time cost 0.5360908031463623 min\n",
      "dice_best: 0.8646678032603314\n",
      "******************************** epoch  100  is finished. *********************************\n"
     ]
    }
   ],
   "source": [
    "dice_best = 0 \n",
    "for epoch in range(1,101):\n",
    "    start_time = time.time()#record the time begin\n",
    "    train_loss_each_step=np.load('train_loss_each_step.npy')#record each iteration's loss\n",
    "    train_acc_each_step = np.load('train_acc_each_step.npy')\n",
    "    vali_dice_each_epoch = np.load('vali_dice_each_epoch.npy')\n",
    "    vali_loss_each_epoch = np.load('vali_loss_each_epoch.npy')\n",
    "    vali_acc_each_epoch = np.load('vali_acc_each_epoch.npy')\n",
    "    test_dice_each_epoch = np.load('test_dice_each_epoch.npy')\n",
    "    \n",
    "    # run the model\n",
    "    train_loss_each_step,train_acc_each_step,vali_dice_each_epoch,vali_loss_each_epoch,vali_acc_each_epoch,test_dice_each_epoch = fit(epoch,\n",
    "                                                                                                                 model,\n",
    "                                                                                                                train_dataloader,\n",
    "                                                                                                                test_dataloader,\n",
    "                                                                                                                train_loss_each_step,\n",
    "                                                                                                                train_acc_each_step,\n",
    "                                                                                                                vali_dice_each_epoch,\n",
    "                                                                                                                vali_loss_each_epoch,\n",
    "                                                                                                                vali_acc_each_epoch,\n",
    "                                                                                                                 test_dice_each_epoch)\n",
    "                                                              \n",
    "    end_time = time.time();time_cost = end_time - start_time;\n",
    "    print('time cost',time_cost/60,'min');\n",
    "    \n",
    "    # save record\n",
    "    np.save('train_loss_each_step.npy',train_loss_each_step)\n",
    "    np.save('train_acc_each_step.npy',train_acc_each_step)\n",
    "    np.save('vali_dice_each_epoch.npy',vali_dice_each_epoch)\n",
    "    np.save('vali_loss_each_epoch.npy',vali_loss_each_epoch)\n",
    "    np.save('vali_acc_each_epoch.npy',vali_acc_each_epoch)\n",
    "    np.save('test_dice_each_epoch.npy',test_dice_each_epoch)\n",
    "\n",
    "\n",
    " \n",
    "    if (epoch>7):\n",
    "      i = epoch\n",
    "      vali_dice_each_epoch = np.load('vali_dice_each_epoch.npy')\n",
    "      if (i==8):\n",
    "          torch.save(model.state_dict(),'./Save_weights/2dUnet_model_weight_best.pth')\n",
    "          dice_best = vali_dice_each_epoch[7]\n",
    "      else:\n",
    "          if (vali_dice_each_epoch[i-1]>dice_best):\n",
    "              dice_best = vali_dice_each_epoch[i-1]\n",
    "              torch.save(model.state_dict(),'./Save_weights/2dUnet_model_weight_best.pth')\n",
    "    \n",
    "    print('dice_best:', dice_best)\n",
    "    print('******************************** epoch ',epoch,' is finished. *********************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae78ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
